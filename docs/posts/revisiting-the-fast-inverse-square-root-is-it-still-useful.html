<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://hllmn.net/blog/2023-04-20_rsqrt/">Original</a>
    <h1>Revisiting the Fast Inverse Square Root – Is It Still Useful?</h1>
    
    <div id="readability-page-1" class="page"><article id="md">
<p><em>April 20, 2023</em></p>
<p>In 2005, id Software released the source code for their 1999 game <em>Quake III
Arena</em> under the GPL-2 license. In the file <a href="https://github.com/id-Software/Quake-III-Arena/blob/master/code/game/q_math.c#L552">code/game/q_math.c</a>,
there is a function for calculating the reciprocal square root of a number
which at first glance seems to use a very peculiar algorithm:</p>
<pre><code><span>float </span><span>Q_rsqrt( </span><span>float </span><span>number )
</span><span>{
</span><span>    </span><span>long</span><span> i;
</span><span>    </span><span>float</span><span> x2, y;
</span><span>    </span><span>const </span><span>float</span><span> threehalfs = </span><span>1.5F</span><span>;
</span><span>
</span><span>    x2 = number * </span><span>0.5F</span><span>;
</span><span>    y  = number;
</span><span>    i  = * ( </span><span>long </span><span>* ) &amp;y;                       </span><span>// evil floating point bit level hacking
</span><span>    i  = </span><span>0x5f3759df </span><span>- ( i &gt;&gt; </span><span>1 </span><span>);               </span><span>// what the fuck?
</span><span>    y  = * ( </span><span>float </span><span>* ) &amp;i;
</span><span>    y  = y * ( threehalfs - ( x2 * y * y ) );   </span><span>// 1st iteration
</span><span>//  y  = y * ( threehalfs - ( x2 * y * y ) );   // 2nd iteration, this can be removed
</span><span>
</span><span>    </span><span>return</span><span> y;
</span><span>}
</span></code></pre>
<p>Many articles have been written about this particular algorithm and it has its
own well written Wikipedia <a href="https://en.wikipedia.org/wiki/Fast_inverse_square_root">page</a> where it is referred to as the
<em>fast inverse square root</em>. The algorithm actually appeared on various forums
before the Q3 source code was released. Ryszard of Beyond3D did some
<a href="https://www.beyond3d.com/content/articles/8/">investigating</a> in 2004-2005 and <a href="https://www.beyond3d.com/content/articles/15">eventually</a> tracked down
the original author of the algorithm to be Greg Walsh at Ardent Computer who
created it more than a decade earlier.</p>

<h2 id="how-does-it-work">How does it work?</h2>
<p>So how does the method work, anyway? It is performed in two steps:</p>
<ol>
<li>obtain a rough approximation <code>y</code> for the reciprocal square root of our
<code>number</code>:<pre><code><span>y  = number;
</span><span>i  = * ( </span><span>long </span><span>* ) &amp;y;
</span><span>i  = </span><span>0x5f3759df </span><span>- ( i &gt;&gt; </span><span>1 </span><span>);
</span><span>y  = * ( </span><span>float </span><span>* ) &amp;i;
</span></code></pre>
</li>
<li>improve the approximation using a single step of the Newton-Raphson (NR) method:<pre><code><span>const </span><span>float</span><span> threehalfs = </span><span>1.5F</span><span>;
</span><span>x2 = number * </span><span>0.5F</span><span>;
</span><span>y  = y * ( threehalfs - ( x2 * y * y ) );
</span></code></pre>
</li>
</ol>
<h3 id="first-approximation">First approximation</h3>
<p>The most interesting part is the first one. It uses a seemingly magic number
<code>0x5f3759df</code> and some bit shifting and somehow ends up with the reciprocal
square root. The first line stores the 32-bit floating-point number <code>y</code> as a
32-bit integer <code>i</code> by taking a pointer to <code>y</code>, converting it to a <code>long</code>
pointer and dereferencing it. So <code>y</code> and <code>i</code> hold two identical 32-bit vectors,
but one is interpreted as a floating-point number and the other is interpreted
as an integer number. Then, the integer number is shifted one step to the
right, negated, and the constant <code>0x5f3759df</code> is added. Finally, the resulting
value is interpreted as a floating number again by dereferencing a <code>float</code>
pointer that points to the integer <code>i</code> value.</p>
<p>Here, shifting, negation and addition is performed in the integer domain, how
do these operations affect the number in the floating-point domain? In order to
understand how this can yield an approximation of the reciprocal square root we
must be familiar with how floating point numbers are represented in memory. A
floating-point number consists of a sign <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>s</mi><mo>∈</mo><mo>{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>}</mo></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$s \in \{0,1\}$</annotation></semantics></math>, exponent <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>e</mi><mo>∈</mo><mi mathvariant="double-struck">Z</mi></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$e\in \mathbb{Z}$</annotation></semantics></math> and a fractional part <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mn>0</mn><mo>≤</mo><mi>f</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$0\leq{f}&lt;1$</annotation></semantics></math>. The value of the
floating-point number is then</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>y</mi><mo>=</mo><mo>(</mo><mo>-</mo><mn>1</mn><msup><mo>)</mo><mi>s</mi></msup><mo>·</mo><mo>(</mo><mn>1</mn><mo>+</mo><mi>f</mi><mo>)</mo><mo>·</mo><msup><mn>2</mn><mi>e</mi></msup><mo>.</mo></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$y = (-1)^s \cdot (1 + f) \cdot 2^e.
$</annotation></semantics></math></p>
<p>In our case, we can assume that our <code>float</code> is in the IEEE 754 <a href="https://en.wikipedia.org/wiki/Single-precision_floating-point_format">binary32</a>
format, the bits are then ordered as shown below.</p>
<p><img src="https://hllmn.net/blog/2023-04-20_rsqrt/fp32.svg" alt=""/></p>
<p>The most significant bit is the sign bit <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>S</mi></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$S$</annotation></semantics></math>, followed by 8 bits (<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>E</mi></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$E$</annotation></semantics></math>)
representing the exponent <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>e</mi></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$e$</annotation></semantics></math> and the remaining 23 bits (<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>F</mi></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$F$</annotation></semantics></math>) representing
the fractional part <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>f</mi></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$f$</annotation></semantics></math>. The number is negative when <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>S</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$S=1$</annotation></semantics></math>. The 8-bit
number <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>E</mi></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$E$</annotation></semantics></math> is not directly used as the exponent, it has an offset or bias of
<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><msup><mn>2</mn><mn>8</mn></msup><mo>-</mo><mn>1</mn><mo>=</mo><mn>127</mn></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$2^8-1 = 127$</annotation></semantics></math>. So <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>E</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$E=0$</annotation></semantics></math> means that the exponent is <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>e</mi><mo>=</mo><mo>-</mo><mn>127</mn></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$e=-127$</annotation></semantics></math>. <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>F</mi></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$F$</annotation></semantics></math> is
simply a fractional binary number with the decimal point before the first digit
such that <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>f</mi><mo>=</mo><mi>F</mi><mo>·</mo><msup><mn>2</mn><mrow><mo>-</mo><mn>23</mn></mrow></msup></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$f=F\cdot2^{-23}$</annotation></semantics></math>.</p>
<p>We can write a simple C program <code>interp.c</code> to print both the integer and
floating-point interpretations of a given number and also extract the different
parts:</p>
<pre><code><span>#include </span><span>&lt;stdlib.h&gt;
</span><span>#include </span><span>&lt;stdio.h&gt;
</span><span>#include </span><span>&lt;string.h&gt;
</span><span>#include </span><span>&lt;stdint.h&gt;
</span><span>
</span><span>int </span><span>main(</span><span>int </span><span>argc, </span><span>char </span><span>*args[]) {
</span><span>    </span><span>/* parse number from args */
</span><span>    </span><span>uint32_t</span><span> i;
</span><span>    </span><span>int</span><span> ret;
</span><span>    </span><span>if </span><span>(argc == </span><span>2</span><span>) {
</span><span>        ret = sscanf(args[</span><span>1</span><span>], </span><span>&#34;</span><span>%u</span><span>&#34;</span><span>, &amp;i);
</span><span>    } </span><span>else if </span><span>(argc == </span><span>3 </span><span>&amp;&amp; strcmp(args[</span><span>1</span><span>], </span><span>&#34;-h&#34;</span><span>) == </span><span>0</span><span>) {
</span><span>        ret = sscanf(args[</span><span>2</span><span>], </span><span>&#34;</span><span>%x</span><span>&#34;</span><span>, &amp;i);
</span><span>    } </span><span>else if </span><span>(argc == </span><span>3 </span><span>&amp;&amp; strcmp(args[</span><span>1</span><span>], </span><span>&#34;-f&#34;</span><span>) == </span><span>0</span><span>) {
</span><span>        </span><span>float</span><span> y;
</span><span>        ret = sscanf(args[</span><span>2</span><span>], </span><span>&#34;</span><span>%f</span><span>&#34;</span><span>, &amp;y);
</span><span>        i = *(</span><span>uint32_t</span><span>*)&amp;y;
</span><span>    } </span><span>else </span><span>{
</span><span>        </span><span>return</span><span> EXIT_FAILURE;
</span><span>    }
</span><span>    </span><span>if </span><span>(ret != </span><span>1</span><span>) </span><span>return</span><span> EXIT_FAILURE;
</span><span>
</span><span>    </span><span>/* print representations */
</span><span>    printf(</span><span>&#34;hexadecimal: </span><span>%x\n</span><span>&#34;</span><span>, i);
</span><span>    printf(</span><span>&#34;unsigned int: </span><span>%u\n</span><span>&#34;</span><span>, i);
</span><span>    printf(</span><span>&#34;signed int: </span><span>%d\n</span><span>&#34;</span><span>, i);
</span><span>    printf(</span><span>&#34;floating-point: </span><span>%f\n</span><span>&#34;</span><span>, *(</span><span>float</span><span>*)&amp;i);
</span><span>
</span><span>    </span><span>/* print components */
</span><span>    </span><span>int</span><span> S = i &gt;&gt; </span><span>31</span><span>;
</span><span>    </span><span>int</span><span> E = (i &gt;&gt; </span><span>23</span><span>) &amp; ((</span><span>1 </span><span>&lt;&lt; </span><span>8</span><span>)-</span><span>1</span><span>);
</span><span>    </span><span>int</span><span> e = E - </span><span>127</span><span>;
</span><span>    </span><span>int</span><span> F = i &amp; ((</span><span>1 </span><span>&lt;&lt; </span><span>23</span><span>)-</span><span>1</span><span>);
</span><span>    </span><span>float</span><span> f = (</span><span>float</span><span>)F / (</span><span>1 </span><span>&lt;&lt; </span><span>23</span><span>);
</span><span>    printf(</span><span>&#34;S: </span><span>%d\n</span><span>&#34;</span><span>, S);
</span><span>    printf(</span><span>&#34;E: </span><span>%d</span><span> (0x</span><span>%x</span><span>) &lt;=&gt; e: </span><span>%d\n</span><span>&#34;</span><span>, E, E, e);
</span><span>    printf(</span><span>&#34;F: </span><span>%d</span><span> (0x</span><span>%x</span><span>) &lt;=&gt; f: </span><span>%f\n</span><span>&#34;</span><span>, F, F, f);
</span><span>
</span><span>    </span><span>return</span><span> EXIT_SUCCESS;
</span><span>}
</span></code></pre>
<p>We can for example look at the number <code>0x40b00000</code>:</p>
<pre><code><span>$ ./interp -h 40b00000
</span><span>hexadecimal: 40b00000
</span><span>unsigned int: 1085276160
</span><span>signed int: 1085276160
</span><span>floating-point: 5.500000
</span><span>S: 0
</span><span>E: 129 (0x81) &lt;=&gt; e: 2
</span><span>F: 3145728 (0x300000) &lt;=&gt; f: 0.375000
</span></code></pre>
<p>We can also extract the parts of a floating-point number:</p>
<pre><code><span>$ ./interp -f -32.1
</span><span>hexadecimal: c2006666
</span><span>unsigned int: 3254806118
</span><span>signed int: -1040161178
</span><span>floating-point: -32.099998
</span><span>S: 1
</span><span>E: 132 (0x84) &lt;=&gt; e: 5
</span><span>F: 26214 (0x6666) &lt;=&gt; f: 0.003125
</span></code></pre>
<p>Even now when we know how the floating-point numbers are represented in memory,
it is not entirely obvious how performing operations in the integer domain
would affect the floating-point domain. At first we can try to simply iterate
over a range of floating-point number and see what integer values we get:</p>
<pre><code><span>#include </span><span>&lt;stdio.h&gt;
</span><span>
</span><span>int </span><span>main() {
</span><span>    </span><span>float</span><span> x;
</span><span>    </span><span>for </span><span>(x = </span><span>0.1</span><span>; x &lt;= </span><span>8.0</span><span>; x += </span><span>0.1</span><span>) {
</span><span>        printf(</span><span>&#34;</span><span>%f\t%d\n</span><span>&#34;</span><span>, x, *(</span><span>int</span><span>*)&amp;x);
</span><span>    }
</span><span>}
</span></code></pre>
<p>We can then plot the floating-point values on the x-axis and the integer values
on the y-axis with e.g. <a href="http://www.gnuplot.info/">gnuplot</a> to get a plot like this:</p>
<p><img src="https://hllmn.net/blog/2023-04-20_rsqrt/plot/float_integer.svg" alt=""/></p>
<p>Well, this curve looks quite familiar. We can look further at some of the data
points using our previous program:</p>
<pre><code><span>$ ./interp -f 1.0
</span><span>hexadecimal: 3f800000
</span><span>unsigned int: 1065353216
</span><span>signed int: 1065353216
</span><span>floating-point: 1.000000
</span><span>S: 0
</span><span>E: 127 (0x7f) &lt;=&gt; e: 0
</span><span>F: 0 (0x0) &lt;=&gt; f: 0.000000
</span><span>
</span><span>$ ./interp -f 2.0
</span><span>hexadecimal: 40000000
</span><span>unsigned int: 1073741824
</span><span>signed int: 1073741824
</span><span>floating-point: 2.000000
</span><span>S: 0
</span><span>E: 128 (0x80) &lt;=&gt; e: 1
</span><span>F: 0 (0x0) &lt;=&gt; f: 0.000000
</span><span>
</span><span>$ ./interp -f 3.0
</span><span>hexadecimal: 40400000
</span><span>unsigned int: 1077936128
</span><span>signed int: 1077936128
</span><span>floating-point: 3.000000
</span><span>S: 0
</span><span>E: 128 (0x80) &lt;=&gt; e: 1
</span><span>F: 4194304 (0x400000) &lt;=&gt; f: 0.500000
</span></code></pre>
<p>For 1.0 and 2.0 we get <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>S</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$S=0$</annotation></semantics></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>F</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$F=0$</annotation></semantics></math> and a non-zero biased exponent <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>E</mi></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$E$</annotation></semantics></math>.
If we remove the bias from this number (subtract by <code>127 &lt;&lt; 23</code>) and then shift
it to the far right we end up with the exponent <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>e</mi></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$e$</annotation></semantics></math>, in other words the base
2 logarithm of the floating-point number. However, this only works when <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>S</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$S=0$</annotation></semantics></math>
and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>F</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$F=0$</annotation></semantics></math>, i.e. positive integers. If <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>S</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$S=1$</annotation></semantics></math> we have a negative number for
which the logarithm is undefined. But if <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>F</mi><mo>≠</mo><mrow></mrow><mn>0</mn></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$F\ne{}0$</annotation></semantics></math> and we shift the exponent
to the far right we will simply lose all of that data. We can instead convert
it to a floating-point value and divide by <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><msup><mn>2</mn><mn>23</mn></msup></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$2^{23}$</annotation></semantics></math>, such that the fractional
part scales our resulting value linearly:</p>
<pre><code><span>(</span><span>float</span><span>) (*(</span><span>int</span><span>*)&amp;x - (</span><span>127 </span><span>&lt;&lt; </span><span>23</span><span>)) / (</span><span>1 </span><span>&lt;&lt; </span><span>23</span><span>)
</span></code></pre>
<p>Then we don’t exactly get the logarithm but we do get a linear approximation
for all non power of two values. We can plot the approximation together with
the actual logarithmic function:</p>
<p><img src="https://hllmn.net/blog/2023-04-20_rsqrt/plot/log.svg" alt=""/></p>
<p>This means that when we take a floating-point number and interpret it as an
integer number, we obtain an approximation of the logarithm of that number,
with some offset and scaling. And when we interpret an integer number as a
floating-point number, we get opposite, i.e. the exponential or antilogarithm
of our integer value. This basically means that when we perform operations in
the integer domain, it is as if we perform operations in the logarithmic
domain.  For example, if we remember our <a href="https://en.wikipedia.org/wiki/List_of_logarithmic_identities">logarithmic identities</a>, we know that
if we take the logarithm of two numbers and add them together, we get the
logarithm of their product:</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>log</mi><mi>a</mi><mo>+</mo><mi>log</mi><mi>b</mi><mo>=</mo><mi>log</mi><mrow><mo>(</mo><mi>a</mi><mo>·</mo><mi>b</mi><mo>)</mo></mrow><mo>.</mo></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$\log{a} + \log{b} = \log{(a \cdot b)}.
$</annotation></semantics></math></p>
<p>In other words, if we perform addition in the integer domain we get
multiplication in the floating-point domain — approximately anyway. We can
try this with another simple C program. One thing we need to consider is how
our operation affects the exponent bias. When we add two numbers with biased
exponents we get double bias:</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtable columnalign="left"><mtr><mtd><msub><mi>E</mi><mn>1</mn></msub><mo>+</mo><msub><mi>E</mi><mn>2</mn></msub></mtd><mtd><mo>=</mo></mtd><mtd><mo>(</mo><msub><mi>e</mi><mn>1</mn></msub><mo>+</mo><mi>B</mi><mo>)</mo><mo>+</mo><mo>(</mo><msub><mi>e</mi><mn>2</mn></msub><mo>+</mo><mi>B</mi><mo>)</mo></mtd></mtr><mtr><mtd></mtd><mtd><mo>=</mo></mtd><mtd><msub><mi>e</mi><mn>1</mn></msub><mo>+</mo><msub><mi>e</mi><mn>2</mn></msub><mo>+</mo><mn>2</mn><mi>B</mi><mo>.</mo></mtd></mtr></mtable></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$\begin{align}
E_1 + E_2 &amp;=&amp; (e_1 + B) + (e_2 + B) \\
          &amp;=&amp; e_1 + e_2 + 2B.
\end{align}
$</annotation></semantics></math></p>
<p>We want our bias to remain as <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>B</mi></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$B$</annotation></semantics></math> rather than <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mn>2</mn><mi>B</mi></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$2B$</annotation></semantics></math> so in order to counter
this we simply subtract the result by <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>B</mi></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$B$</annotation></semantics></math>. Our C program that performs
floating-point multiplication using integer addition may then look like this:</p>
<pre><code><span>#include </span><span>&lt;stdio.h&gt;
</span><span>#include </span><span>&lt;stdlib.h&gt;
</span><span>#include </span><span>&lt;stdint.h&gt;
</span><span>
</span><span>const </span><span>uint32_t</span><span> B = (</span><span>127 </span><span>&lt;&lt; </span><span>23</span><span>);
</span><span>
</span><span>int </span><span>main(</span><span>int </span><span>argc, </span><span>char </span><span>*args[]) {
</span><span>    </span><span>/* parse factors from args */
</span><span>    </span><span>float</span><span> a, b;
</span><span>    </span><span>if </span><span>(argc == </span><span>3</span><span>) {
</span><span>        </span><span>int</span><span> ret = sscanf(args[</span><span>1</span><span>], </span><span>&#34;</span><span>%f</span><span>&#34;</span><span>, &amp;a);
</span><span>        ret += sscanf(args[</span><span>2</span><span>], </span><span>&#34;</span><span>%f</span><span>&#34;</span><span>, &amp;b);
</span><span>        </span><span>if </span><span>(ret != </span><span>2</span><span>) </span><span>return</span><span> EXIT_FAILURE;
</span><span>    } </span><span>else </span><span>{
</span><span>        </span><span>return</span><span> EXIT_FAILURE;
</span><span>    }
</span><span>
</span><span>    </span><span>/* perform multiplication (integer addition) */
</span><span>    </span><span>uint32_t</span><span> sum = *(</span><span>uint32_t</span><span>*)&amp;a + *(</span><span>uint32_t</span><span>*)&amp;b - B;
</span><span>    </span><span>float</span><span> y = *(</span><span>float</span><span>*)&amp;sum;
</span><span>
</span><span>    </span><span>/* compare with actual */
</span><span>    </span><span>float</span><span> y_actual = a*b;
</span><span>    </span><span>float</span><span> rel_err = (y - y_actual) / y_actual;
</span><span>
</span><span>    printf(</span><span>&#34;</span><span>%f</span><span> =? </span><span>%f</span><span> (</span><span>%.2f%%</span><span>)</span><span>\n</span><span>&#34;</span><span>, y, y_actual, </span><span>100</span><span>*rel_err);
</span><span>}
</span></code></pre>
<p>Let’s try it out:</p>
<pre><code><span>$ ./mul 3.14159 8.0
</span><span>25.132721 =? 25.132721 (0.00%)
</span><span>
</span><span>$ ./mul 3.14159 0.2389047
</span><span>0.741016 =? 0.750541 (-1.27%)
</span><span>
</span><span>$ ./mul -15.0 3.0
</span><span>-44.000000 =? -45.000000 (-2.22%)
</span><span>
</span><span>$ ./mul 6.0 3.0
</span><span>16.000000 =? 18.000000 (-11.11%)
</span><span>
</span><span>$ ./mul 0.0 10.0
</span><span>0.000000 =? 0.000000 (inf%)
</span></code></pre>
<p>Most of the time it is not perfectly accurate, it is correct only if one of the
factors is a power of two, and least accurate when both factors are right
between two powers of 2.</p>
<p>How about the reciprocal square root? The reciprocal square root
<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mfrac><mn>1</mn><msqrt><mi>x</mi></msqrt></mfrac></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$\frac{1}{\sqrt{x}}$</annotation></semantics></math> is equivalent to <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><msup><mi>x</mi><mrow><mo>-</mo><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$x^{-1/2}$</annotation></semantics></math> so we will need another
logarithmic identity:</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p</mi><mi>log</mi><mi>x</mi><mo>=</mo><mi>log</mi><msup><mi>x</mi><mi>p</mi></msup></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$p\log{x} = \log{x^p}
$</annotation></semantics></math></p>
<p>This means that if we perform multiplication in the integer domain, we get
exponentiation in the floating-point domain. Depending on our exponent <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>p</mi></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$p$</annotation></semantics></math> we
can obtain several different functions, e.g:</p>
<table><thead><tr><th><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>p</mi></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$p$</annotation></semantics></math></th><th><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$f(x)$</annotation></semantics></math></th></tr></thead><tbody>
<tr><td>2</td><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><msup><mi>x</mi><mn>2</mn></msup></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$x^2$</annotation></semantics></math></td></tr>
<tr><td>1/2</td><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><msqrt><mi>x</mi></msqrt></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$\sqrt{x}$</annotation></semantics></math></td></tr>
<tr><td>-1</td><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mfrac><mn>1</mn><mi>x</mi></mfrac></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$\frac{1}{x}$</annotation></semantics></math></td></tr>
<tr><td>-1/2</td><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mfrac><mn>1</mn><msqrt><mi>x</mi></msqrt></mfrac></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$\frac{1}{\sqrt{x}}$</annotation></semantics></math></td></tr>
</tbody></table>
<p>In order to get a first approximation of the reciprocal square root, we simply
need to multiply by -1/2 in the integer domain and adjust for the bias.  The
bias will then be <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mo>-</mo><mi>B</mi><mo>/</mo><mn>2</mn></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$-B/2$</annotation></semantics></math> and we want the bias to be <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>B</mi></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$B$</annotation></semantics></math> so we simply need
to add <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mn>3</mn><mi>B</mi><mo>/</mo><mn>2</mn><mo>=</mo><mrow><mn>0</mn><mi mathvariant="monospace">x</mi><mn>5</mn><mi mathvariant="monospace">f</mi><mn>400000</mn></mrow></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$3B/2 = \texttt{0x5f400000}$</annotation></semantics></math>. So, we will multiply by -1/2 by shifting
right one step and negating, and then add the bias:</p>
<pre><code><span>- (i &lt;&lt; </span><span>1</span><span>) + </span><span>0x5f400000</span><span>;
</span></code></pre>
<p>This is now identical to the Q3 source code except that the constant value
differs slightly. They used <code>0x5f3759df</code> while we currently have <code>0x5f400000</code>.
We can see if it is possible to make improvements by looking at our error. We
simply subtract our approximate value for the reciprocal square root by the
exact value and plot the result for a certain range of numbers:</p>
<p><img src="https://hllmn.net/blog/2023-04-20_rsqrt/plot/error.svg" alt=""/></p>
<p>The graph repeats horizontally in both directions (only in different scale) so
we only need to look at this part to understand the error for all (normal)
floating-point numbers. We can see that the approximate value is always
overestimating, by simply subtracting a constant that is around half the
maximum error we can make it symmetric and thus decrease the average absolute
error. Looking at the graph, subtracting something like 0x7a120 might work. Our
constant would then be 0x5f385ee0 which is closer to the constant used in Q3.
In the integer domain, our error will simply center the error around the x-axis
in the above diagram.  In the floating-point domain, the error is affected
similarly except when our subtraction borrows from the exponent:</p>
<p><img src="https://hllmn.net/blog/2023-04-20_rsqrt/plot/error_new.svg" alt=""/></p>
<p>We could potentially try to find an actual optimum for some reasonable
<a href="https://en.wikipedia.org/wiki/Objective_function">objective function</a> but we will stop here. In the case of the original Q3
constant, it is not really clear how it was chosen, perhaps using trial and
error.</p>
<h3 id="improving-the-approximation">Improving the approximation</h3>
<p>The second part is less unconventional. When a first approximation has been
obtained, one can improve it by using a method known as Newton-Raphson (NR). If
you are unfamiliar with it, Wikipedia has a good <a href="https://en.wikipedia.org/wiki/Newton%27s_method">article</a> on it. The
NR method is used to improve an approximation for the root of an equation.
Since we want the reciprocal square root we need an equation <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>f</mi><mo>(</mo><mi>y</mi><mo>)</mo></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$f(y)$</annotation></semantics></math> that is
zero when <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>y</mi></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$y$</annotation></semantics></math> is exactly the reciprocal square root of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>x</mi></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$x$</annotation></semantics></math>:</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtable columnalign="left"><mtr><mtd><mi>y</mi><mo>=</mo><mfrac><mn>1</mn><msqrt><mi>x</mi></msqrt></mfrac><mspace width="0.22222222em"></mspace><mo>⇔</mo><mspace width="0.22222222em"></mspace><mfrac><mn>1</mn><msup><mi>y</mi><mn>2</mn></msup></mfrac><mo>=</mo><mi>x</mi></mtd></mtr><mtr><mtd><mo>⇒</mo><mi>f</mi><mo>(</mo><mi>y</mi><mo>)</mo><mo>=</mo><mfrac><mn>1</mn><msup><mi>y</mi><mn>2</mn></msup></mfrac><mo>-</mo><mi>x</mi><mo>=</mo><mn>0</mn></mtd></mtr></mtable></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$\begin{align}
    y = \frac{1}{\sqrt{x}} \: \Leftrightarrow \: \frac{1}{y^2} = x \\
    \Rightarrow f(y) = \frac{1}{y^2} - x = 0
\end{align}
$</annotation></semantics></math></p>
<p>If we have an approximate value <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><msub><mi>y</mi><mi>n</mi></msub></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$y_n$</annotation></semantics></math> we can get a better approximation
<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><msub><mi>y</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$y_{n+1}$</annotation></semantics></math> by calculating where the tangent of the function’s graph at
<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>y</mi><mo>=</mo><msub><mi>y</mi><mi>n</mi></msub></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$y=y_n$</annotation></semantics></math> (i.e.  the derivative) intersects <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>f</mi><mo>(</mo><mi>y</mi><mo>)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$f(y)=0$</annotation></semantics></math>. That value can be
expressed as</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtable columnalign="left"><mtr><mtd><msub><mi>y</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub></mtd><mtd><mo>=</mo></mtd><mtd><msub><mi>y</mi><mi>n</mi></msub><mo>-</mo><mfrac><mrow><mi>f</mi><mo>(</mo><msub><mi>y</mi><mi>n</mi></msub><mo>)</mo></mrow><mrow><msup><mi>f</mi><mo>′</mo></msup><mo>(</mo><msub><mi>y</mi><mi>n</mi></msub><mo>)</mo></mrow></mfrac></mtd></mtr><mtr><mtd></mtd><mtd><mo>=</mo></mtd><mtd><msub><mi>y</mi><mi>n</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mfrac><mn>3</mn><mn>2</mn></mfrac><mo>-</mo><mfrac><mi>x</mi><mn>2</mn></mfrac><mo>·</mo><msup><msub><mi>y</mi><mi>n</mi></msub><mn>2</mn></msup></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr></mtable></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$\begin{align}
    y_{n+1} &amp;=&amp; y_n - \frac{f(y_n)}{f&#39;(y_n)} \\
            &amp;=&amp; y_n \left( \frac{3}{2} - \frac{x}{2} \cdot {y_n}^2 \right)
\end{align}
$</annotation></semantics></math></p>
<p>which is the exact same expression that is used in the second part of the Q3
function.</p>
<h2 id="how-fast-is-it">How fast is it?</h2>
<p>Back in 2003 Chris Lomont wrote an <a href="https://www.lomont.org/papers/2003/InvSqrt.pdf">article</a> about his
investigations of the algorithm. His testing yielded that the algorithm was
four times faster than using the more straightforward way of simply using
<code>sqrt(x)</code> from the standard library and taking its reciprocal.</p>
<p>In 2009, Elan Ruskin made a post, <a href="https://web.archive.org/web/20210208132927/http://assemblyrequired.crashworks.org/timing-square-root/">Timing Square Root</a>, where he
primarily looked at the square root function but also compared the fast inverse
square root algorithm to other methods. On his Intel Core 2, the fast inverse
square root was 4 times slower than using <code>rsqrtss</code>, or 30% slower than
<code>rsqrtss</code> with a single NR step.</p>
<p>Since then, there has come several new extensions to the x86 instruction set. I
have tried to sum up all square root instructions currently available:</p>
<table><thead><tr><th>Set</th><th><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><msqrt><mi>x</mi></msqrt></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$\sqrt{x}$</annotation></semantics></math></th><th><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mfrac><mn>1</mn><msqrt><mi>x</mi></msqrt></mfrac></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$\frac{1}{\sqrt{x}}$</annotation></semantics></math></th><th>Width</th></tr></thead><tbody>
<tr><td><a href="https://en.wikipedia.org/wiki/8087">x87</a> (1980)</td><td><code>fsqrt</code></td><td></td><td>32</td></tr>
<tr><td><a href="https://en.wikipedia.org/wiki/3DNow!">3DNow!</a> (1998)</td><td></td><td><code>pfrsqrt</code></td><td>128</td></tr>
<tr><td><a href="https://en.wikipedia.org/wiki/Streaming_SIMD_Extensions">SSE</a> (1999)</td><td><code>sqrtps</code>, <code>sqrtss</code></td><td><code>rsqrtps</code>, <code>rsqrtss</code></td><td>128</td></tr>
<tr><td><a href="https://en.wikipedia.org/wiki/SSE2">SSE2</a> (2000)</td><td><code>sqrtpd</code>, <code>sqrtsd</code></td><td></td><td>128</td></tr>
<tr><td><a href="https://en.wikipedia.org/wiki/Advanced_Vector_Extensions">AVX</a> (2011)</td><td><code>vsqrtps</code>, <code>vsqrtpd</code>, <code>vsqrtps_nr</code>,</td><td><code>vrsqrtps</code>, <code>vrsqrtps_nr</code></td><td>256</td></tr>
<tr><td><a href="https://en.wikipedia.org/wiki/AVX-512">AVX-512</a> (2014)</td><td></td><td><code>vrsqrt14pd</code>, <code>vrsqrt14ps</code>, <code>vrsqrt14sd</code>, <code>vrsqrt14ss</code></td><td>512</td></tr>
</tbody></table>
<p>The <code>fsqrt</code> is quite obsolete by now. The 3DNow! extension has also been
deprecated and is no longer supported. All x86-64 processors support at least
SSE and SSE2. Most processors support AVX and some support AVX-512, but e.g.
GCC currently chooses to not emit any AVX instructions by default.</p>
<p>The <code>p</code> and <code>s</code> is short for “packed” and “scalar”. The packed instructions are
vector <a href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_data">SIMD</a> instructions while the scalar ones only operate on a single value
at a time. With a register width of e.g. 256 bits, the packed instruction can
perform <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mn>256</mn><mo>/</mo><mn>32</mn><mo>=</mo><mn>8</mn></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$256/32=8$</annotation></semantics></math> calculations in parallel. The <code>s</code> or <code>d</code> is short for
“single” or “double” precision floating-point. Since we are considering
approximations we will be using single precision floating-point numbers. We may
then use either the <code>ps</code> or <code>ss</code> variants.</p>
<p>The fast inverse square root method had a pretty hard time against the
<code>rsqrtss</code> instruction back in 2009 already. And since then, multiple extensions
with specialized SIMD instructions has been implemented in modern x86
processors. Surely, the fast inverse square root has no chance today and its
time has passed?</p>
<p>Why don’t we give it a try ourselves right now, we can start by running some
tests on my current machine which has a relatively modern processor: an AMD Zen
3 5950X from late 2020.</p>
<h3 id="initial-testing">Initial testing</h3>
<p>We will write a C program that tries to calculate the reciprocal square root
using three different methods:</p>
<ul>
<li><code>exact</code>: simply <code>1.0 / sqrtf(x)</code>, using the <code>sqrtf</code> function from the C
standard library,</li>
<li><code>appr</code>: first approximation from the Q3 source as explained above,</li>
<li><code>appr_nr</code>: the full Q3 method with one iteration of Newton-Raphson.</li>
</ul>
<p>For each method we perform the calculation for each value in a randomized input
array and time how long it takes in total. We can use the <code>clock_gettime</code>
function from libc (for POSIX systems) to get the time before and after we
perform the calculations and calculate the difference. We will then repeat this
many times to decrease the random variations. The C program looks like this:</p>
<pre><code><span>#include </span><span>&lt;stdlib.h&gt;
</span><span>#include </span><span>&lt;stdio.h&gt;
</span><span>#include </span><span>&lt;stdint.h&gt;
</span><span>#include </span><span>&lt;time.h&gt;
</span><span>#include </span><span>&lt;math.h&gt;
</span><span>
</span><span>#define N </span><span>4096
</span><span>#define T </span><span>1000
</span><span>#define E9 </span><span>1000000000
</span><span>
</span><span>#ifndef CLOCK_REALTIME
</span><span>#define CLOCK_REALTIME </span><span>0
</span><span>#endif
</span><span>
</span><span>enum </span><span>methods { EXACT, APPR, APPR_NR, M };
</span><span>const </span><span>char </span><span>*METHODS[] = { </span><span>&#34;exact&#34;</span><span>, </span><span>&#34;appr&#34;</span><span>, </span><span>&#34;appr_nr&#34; </span><span>};
</span><span>
</span><span>static inline </span><span>float </span><span>rsqrt_exact(</span><span>float </span><span>x) { </span><span>return </span><span>1.0f </span><span>/ sqrtf(x); }
</span><span>static inline </span><span>float </span><span>rsqrt_appr(</span><span>float </span><span>x) {
</span><span>    </span><span>uint32_t</span><span> i = *(</span><span>uint32_t</span><span>*)&amp;x;
</span><span>    i = -(i &gt;&gt; </span><span>1</span><span>) + </span><span>0x5f3759df</span><span>;
</span><span>    </span><span>return </span><span>*(</span><span>float</span><span>*)&amp;i;
</span><span>}
</span><span>static inline </span><span>float </span><span>rsqrt_nr(</span><span>float </span><span>x, </span><span>float </span><span>y) { </span><span>return</span><span> y * (</span><span>1.5f </span><span>- x*</span><span>0.5f</span><span>*y*y); }
</span><span>
</span><span>static inline </span><span>float </span><span>rsqrt_appr_nr(</span><span>float </span><span>x) {
</span><span>    </span><span>float</span><span> y = rsqrt_appr(x);
</span><span>    </span><span>return </span><span>rsqrt_nr(x, y);
</span><span>}
</span><span>
</span><span>int </span><span>main() {
</span><span>    srand(time(</span><span>NULL</span><span>));
</span><span>
</span><span>    </span><span>float</span><span> y_sum[M] = {</span><span>0</span><span>};
</span><span>    </span><span>double</span><span> t[M] = {</span><span>0</span><span>};
</span><span>
</span><span>    </span><span>for </span><span>(</span><span>int</span><span> trial = </span><span>0</span><span>; trial &lt; T; trial++) {
</span><span>        </span><span>struct</span><span> timespec start, stop;
</span><span>        </span><span>float</span><span> x[N], y[N];
</span><span>        </span><span>for </span><span>(</span><span>int</span><span> i = </span><span>0</span><span>; i &lt; N; i++) { x[i] = rand(); }
</span><span>
</span><span>        clock_gettime(CLOCK_REALTIME, &amp;start);
</span><span>        </span><span>for </span><span>(</span><span>int</span><span> i = </span><span>0</span><span>; i &lt; N; i++) { y[i] = rsqrt_exact(x[i]); }
</span><span>        clock_gettime(CLOCK_REALTIME, &amp;stop);
</span><span>        </span><span>for </span><span>(</span><span>int</span><span> i = </span><span>0</span><span>; i &lt; N; i++) { y_sum[EXACT] += y[i]; }
</span><span>        t[EXACT] += ((stop</span><span>.</span><span>tv_sec-start</span><span>.</span><span>tv_sec)*E9 + stop</span><span>.</span><span>tv_nsec-start</span><span>.</span><span>tv_nsec);
</span><span>
</span><span>        clock_gettime(CLOCK_REALTIME, &amp;start);
</span><span>        </span><span>for </span><span>(</span><span>int</span><span> i = </span><span>0</span><span>; i &lt; N; i++) { y[i] = rsqrt_appr(x[i]); }
</span><span>        clock_gettime(CLOCK_REALTIME, &amp;stop);
</span><span>        </span><span>for </span><span>(</span><span>int</span><span> i = </span><span>0</span><span>; i &lt; N; i++) { y_sum[APPR] += y[i]; }
</span><span>        t[APPR] += ((stop</span><span>.</span><span>tv_sec-start</span><span>.</span><span>tv_sec)*E9 + stop</span><span>.</span><span>tv_nsec-start</span><span>.</span><span>tv_nsec);
</span><span>
</span><span>        clock_gettime(CLOCK_REALTIME, &amp;start);
</span><span>        </span><span>for </span><span>(</span><span>int</span><span> i = </span><span>0</span><span>; i &lt; N; i++) { y[i] = rsqrt_appr_nr(x[i]); }
</span><span>        clock_gettime(CLOCK_REALTIME, &amp;stop);
</span><span>        </span><span>for </span><span>(</span><span>int</span><span> i = </span><span>0</span><span>; i &lt; N; i++) { y_sum[APPR_NR] += y[i]; }
</span><span>        t[APPR_NR] += ((stop</span><span>.</span><span>tv_sec-start</span><span>.</span><span>tv_sec)*E9 + stop</span><span>.</span><span>tv_nsec-start</span><span>.</span><span>tv_nsec);
</span><span>    }
</span><span>
</span><span>    printf(</span><span>&#34;rsqrt</span><span>\t</span><span>fs/op</span><span>\t</span><span>ratio</span><span>\t</span><span>err</span><span>\n</span><span>&#34;</span><span>);
</span><span>    </span><span>for </span><span>(</span><span>int</span><span> m = </span><span>0</span><span>; m &lt; M; m++) {
</span><span>        printf(</span><span>&#34;</span><span>%s\t%.0f\t%.2f\t%.4f\n</span><span>&#34;</span><span>,
</span><span>               METHODS[m],
</span><span>               t[m] * </span><span>1000.0f </span><span>/ N / T,
</span><span>               (</span><span>double</span><span>) t[EXACT] / t[m],
</span><span>               (y_sum[m] - y_sum[EXACT]) / y_sum[EXACT]);
</span><span>    }
</span><span>
</span><span>    </span><span>return </span><span>0</span><span>;
</span><span>}
</span></code></pre>
<p>At the end of the program we print three things for each method:</p>
<ul>
<li>the average time to calculate a single operation in femtoseconds – the lower
the better,</li>
<li>the ratio of the calculation time compared to the exact method – the higher
the faster,</li>
<li>the average error between the method and the exact method – just to make
sure the calculations are performed correctly.</li>
</ul>
<p>So, what do we expect? There are dedicated functions for calculating the
reciprocal square root in the x86 instruction set that the compiler should be
able to emit. The throughput may then be higther than in the approximate method
where we perform multiple operations.</p>
<p>Let’s go ahead and try it, we’ll compile it using GCC without any optimizations
at first, explicitly with <code>-O0</code>.  Since we are using <code>math.h</code> for the exact
method we will also need to link the math library using <code>-lm</code>:</p>
<pre><code><span>$ gcc -lm -O0 rsqrt.c
</span><span>$ ./a.out
</span><span>rsqrt	fs/op	ratio	err
</span><span>exact	3330	1.00	0.0000
</span><span>appr	2020	1.65	0.0193
</span><span>appr_nr	6115	0.54	-0.0010
</span></code></pre>
<p>This seems reasonable. The error is noticeable for the first approximation but
reduced after one iteration of NR. The first approximation is actually faster
than the exact method but when done together with a step of NR it is twice as
slow. The NR method requires more operations so this seems reasonable.</p>
<p>Alright, but this is only a debug build, let’s try adding optimizations using
the <code>-O3</code> flag. This will enable all optimizations that do not disregard any
standards compliance.</p>
<pre><code><span>$ gcc -lm -O3 rsqrt.c
</span><span>$ ./a.out
</span><span>rsqrt	fs/op	ratio	err
</span><span>exact	1879	1.00	0.0000
</span><span>appr	72	26.01	0.0193
</span><span>appr_nr	178	10.54	-0.0010
</span></code></pre>
<p>Hmm, now the approximations are actually a lot faster than before but the time
of the exact method has only halved, making the approximation with NR more than
ten times faster than the exact method. Perhaps the compiler failed to emit the
reciprocal square root functions? Maybe it will improve if we use the <code>-Ofast</code>
flag instead which is described by the <a href="https://www.man7.org/linux/man-pages/man1/gcc.1.html">gcc(1)</a> man page:</p>
<blockquote>
<p>Disregard strict standards compliance. -Ofast enables all -O3 optimizations.
It also enables optimizations that are not valid for all standard- compliant
programs. It turns on -ffast-math, -fallow-store-data-races and the
Fortran-specific -fstack-arrays, unless -fmax-stack-var-size is specified,
and -fno-protect-parens.  It turns off -fsemantic-interposition.</p>
</blockquote>
<p>Our exact method may no longer be as accurate as before, but it may be faster.</p>
<pre><code><span>$ gcc -lm -Ofast rsqrt.c
</span><span>$ ./a.out
</span><span>rsqrt	fs/op	ratio	err
</span><span>exact	153	1.00	0.0000
</span><span>appr	118	1.30	0.0137
</span><span>appr_nr	179	0.85	-0.0009
</span></code></pre>
<p>And it is indeed faster. The first approximation is still faster, but with a
step of NR it is slower than the exact method. The error has decreased slightly
for the approximations because we are still comparing against the “exact”
method which now yields different results. Oddly enough, the first
approximation has become half as fast. This seems to be a quirk of GCC, as
Clang does not have this issue, otherwise it produces similar results:</p>
<pre><code><span>$ clang -lm -O0 rsqrt.c
</span><span>$ ./a.out
</span><span>rsqrt	fs/op	ratio	err
</span><span>exact	3715	1.00	0.0000
</span><span>appr	1933	1.92	0.0193
</span><span>appr_nr	6001	0.62	-0.0010
</span><span>
</span><span>$ clang -lm -O3 rsqrt.c
</span><span>$ ./a.out
</span><span>rsqrt	fs/op	ratio	err
</span><span>exact	1900	1.00	0.0000
</span><span>appr	61	31.26	0.0193
</span><span>appr_nr	143	13.24	-0.0010
</span><span>
</span><span>$ clang -lm -Ofast rsqrt.c
</span><span>$ ./a.out
</span><span>rsqrt	fs/op	ratio	err
</span><span>exact	148	1.00	0.0000
</span><span>appr	62	2.40	0.0144
</span><span>appr_nr	145	1.02	-0.0009
</span></code></pre>
<h3 id="disassembly">Disassembly</h3>
<p>For both compilers, there is quite a large difference between <code>-O3</code> and
<code>-Ofast</code>. We can look at the disassembly to see what is going on. We will need
to provide the <code>-g</code> flag to the compiler in order to get debug symbols in the
binary that tell us which object code corresponds to which source code.
Thereafter we can run <code>objdump -d</code> with the <code>-S</code> flag to see the disassembled
instructions next to the source code:</p>
<pre><code><span>$ gcc -lm -O3 -g rsqrt.c
</span><span>$ objdump -d -S a.out
</span><span>...
</span><span>static inline float rsqrt_exact(float x) { return 1.0f / sqrtf(x); }
</span><span>    118e:       66 0f ef db             pxor   %xmm3,%xmm3
</span><span>    1192:       0f 2e d8                ucomiss %xmm0,%xmm3
</span><span>    1195:       0f 87 e1 02 00 00       ja     147c &lt;main+0x3bc&gt;
</span><span>    119b:       f3 0f 51 c0             sqrtss %xmm0,%xmm0
</span><span>    119f:       f3 0f 10 0d 99 0e 00    movss  0xe99(%rip),%xmm1    # 2040
</span><span>    11a6:       00
</span><span>...
</span><span>    11ab:       f3 0f 5e c8             divss  %xmm0,%xmm1
</span><span>...
</span><span>    2040:       00 00 80 3f             1.0f
</span></code></pre>
<p>In case you are unfamiliar, this is the AT&amp;T syntax for x86-64 assembly. Note
that the source operand is always before the destination operand. The
parentheses indicate an address, for example <code>movss 0xecd(%rip),%xmm1</code> copies
the value located 0xecd bytes ahead of the address in the <code>rip</code> register
(instruction pointer, a.k.a. PC) to the <code>xmm1</code> register. The <code>xmmN</code> registers
are 128 bits wide, or 4 words. However, the <code>ss</code> instructions are for scalar
single-precision values, so it will only apply the operation on a single
floating-point value in the least significant 32 bits.</p>
<p>In the <code>-O3</code> case we use the scalar <code>sqrtss</code> followed by <code>divss</code>. There is also
a compare <code>ucomiss</code> and a jump <code>ja</code> that will set <a href="https://en.wikipedia.org/wiki/Errno.h"><code>errno</code></a> to <code>EDOM</code> in case
the input is less than -0. We are not using <code>errno</code> at all so we can remove the
setting of <code>errno</code> by providing the <code>-fno-math-errno</code> flag:</p>
<pre><code><span>$ gcc -lm -O3 -g -fno-math-errno rsqrt.c
</span><span>$ ./a.out
</span><span>rsqrt	fs/op	ratio	err
</span><span>exact	479	1.00	0.0000
</span><span>appr	116	4.13	0.0193
</span><span>appr_nr	175	2.74	-0.0010
</span></code></pre>
<pre><code><span>$ objdump -d -S a.out
</span><span>...
</span><span>static inline float rsqrt_exact(float x) { return 1.0f / sqrtf(x); }
</span><span>    1170:       0f 51 0c 28             sqrtps (%rax,%rbp,1),%xmm1
</span><span>    1174:       f3 0f 10 1d c4 0e 00    movss  0xec4(%rip),%xmm3    # 2040
</span><span>    117b:       00
</span><span>    117c:       48 83 c0 10             add    $0x10,%rax
</span><span>    1180:       0f c6 db 00             shufps $0x0,%xmm3,%xmm3
</span><span>    1184:       0f 28 c3                movaps %xmm3,%xmm0
</span><span>    1187:       0f 5e c1                divps  %xmm1,%xmm0
</span><span>...
</span><span>    2040:       00 00 80 3f             1.0f
</span></code></pre>
<p>This prevents us from having to check every input value individually and thus
allows us to use the packed variants of the instructions, performing 4
operations at a time. This improved the performance a lot. However, we still
use <a href="https://www.felixcloutier.com/x86/sqrtps"><code>sqrtps</code></a> followed by <a href="https://www.felixcloutier.com/x86/divps"><code>divps</code></a>. We will have to also
enable <code>-funsafe-math-optimizations</code> and <code>-ffinite-math-only</code> in
order to make GCC emit <a href="https://www.felixcloutier.com/x86/rsqrtps"><code>rsqrtps</code></a> instead. We then get identical code
to when we used <code>-Ofast</code>:</p>
<pre><code><span>$ gcc -lm -O3 -g -fno-math-errno -funsafe-math-optimizations -ffinite-math-only rsqrt.c
</span><span>$ ./a.out
</span><span>rsqrt	fs/op	ratio	err
</span><span>exact	155	1.00	0.0000
</span><span>appr	120	1.29	0.0137
</span><span>appr_nr	182	0.85	-0.0009
</span></code></pre>
<pre><code><span>$ objdump -d -S a.out
</span><span>...
</span><span>static inline float rsqrt_exact(float x) { return 1.0f / sqrtf(x); }
</span><span>    1170:       0f 52 0c 28             rsqrtps (%rax,%rbp,1),%xmm1
</span><span>    1174:       0f 28 04 28             movaps (%rax,%rbp,1),%xmm0
</span><span>    1178:       48 83 c0 10             add    $0x10,%rax
</span><span>    117c:       0f 59 c1                mulps  %xmm1,%xmm0
</span><span>    117f:       0f 59 c1                mulps  %xmm1,%xmm0
</span><span>    1182:       0f 59 0d c7 0e 00 00    mulps  0xec7(%rip),%xmm1    # 2050
</span><span>    1189:       0f 58 05 b0 0e 00 00    addps  0xeb0(%rip),%xmm0    # 2040
</span><span>    1190:       0f 59 c1                mulps  %xmm1,%xmm0
</span><span>...
</span><span>    2040:       00 00 40 c0             -3.0f
</span><span>...
</span><span>    2050:       00 00 00 bf             -0.5f
</span></code></pre>
<p>Now it uses <code>rsqrtps</code>, but it also has several multiplication instructions as
well as an addition. Why are these needed, isn’t the reciprocal square root all
we need? We can get a hint from looking at the disassembly of the <code>appr_nr</code>
function:</p>
<pre><code><span>static inline float rsqrt_nr(float x, float y) { return y * (1.5f - x*0.5f*y*y); }
</span><span>    12f8:       f3 0f 10 1d 80 0d 00    movss  0xd80(%rip),%xmm3    # 2080
</span><span>    12ff:       00
</span><span>...
</span><span>    1304:       0f 59 05 65 0d 00 00    mulps  0xd65(%rip),%xmm0    # 2070
</span><span>...
</span><span>    1310:       0f c6 db 00             shufps $0x0,%xmm3,%xmm3
</span><span>...
</span><span>    1318:       0f 28 d1                movaps %xmm1,%xmm2
</span><span>    131b:       0f 59 d1                mulps  %xmm1,%xmm2
</span><span>    131e:       0f 59 d0                mulps  %xmm0,%xmm2
</span><span>    1321:       0f 28 c3                movaps %xmm3,%xmm0
</span><span>    1324:       0f 5c c2                subps  %xmm2,%xmm0
</span><span>    1327:       0f 59 c1                mulps  %xmm1,%xmm0
</span><span>...
</span><span>    2070:       00 00 00 3f             0.5f
</span><span>...
</span><span>    2080:       00 00 c0 3f             1.5f
</span></code></pre>
<p>The last part looks quite similar, because it is actually doing the same thing:
an iteration of Newton-Raphson. This is hinted in the man page of
<a href="https://www.man7.org/linux/man-pages/man1/gcc.1.html">gcc(1)</a>:</p>
<blockquote>
<p>This option enables use of the reciprocal estimate and reciprocal square root
estimate instructions with additional Newton-Raphson steps to increase
precision instead of doing a divide or square root and divide for
floating-point arguments.</p>
</blockquote>
<p>The <code>rsqrtps</code> instruction only guarantees a relative error smaller than
<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mn>1.5</mn><mo>·</mo><msup><mn>2</mn><mrow><mo>-</mo><mn>12</mn></mrow></msup></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$1.5\cdot2^{-12}$</annotation></semantics></math>, the NR iteration reduces it further just like in the Q3
code.</p>
<p>If we do not need this extra precision, can we get a speedup by skipping the NR
step? We can use built-in compiler <a href="https://en.wikipedia.org/wiki/Intrinsic_function">intrinsics</a> in order to make the compiler
only emit the <code>rsqrtps</code> instruction. The GCC manual has a
<a href="https://gcc.gnu.org/onlinedocs/gcc-12.2.0/gcc/x86-Built-in-Functions.html">list</a> of built-in functions for the x86 instruction set.
There is a <code>__builtin_ia32_rsqrtps</code> function that will emit the <code>rsqrtps</code>
instruction:</p>
<pre><code><span>v4sf __builtin_ia32_rsqrtps (v4sf);
</span></code></pre>
<p>The manual also has a <a href="https://gcc.gnu.org/onlinedocs/gcc/Vector-Extensions.html">chapter</a> about how to use these vector
instructions with built-in functions. We need to add a <code>typedef</code> for the <code>v4sf</code>
type which contains four floating point numbers. We will then use an array of
<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>N</mi><mo>/</mo><mn>4</mn></mrow><annotation encoding="delim"> </annotation><annotation encoding="application/x-tex">$N/4$</annotation></semantics></math> of these vectors and simply provide one vector at a time to the
built-in function. N is a multiple of four so there are no half full vectors.
We can simply cast our previous <code>float</code> input array to a <code>vfs4</code> pointer. We
will add these parts to our previous program:</p>
<pre><code><span>typedef float</span><span> v4sf </span><span>__attribute__ </span><span>((vector_size(</span><span>16</span><span>)));
</span><span>v4sf rsqrt_intr(v4sf x) { </span><span>return </span><span>__builtin_ia32_rsqrtps(x); };
</span><span>
</span><span>
</span><span>        v4sf *xv = (v4sf*)x, *yv = (v4sf*)y;
</span><span>        clock_gettime(CLOCK_REALTIME, &amp;start);
</span><span>        </span><span>for </span><span>(</span><span>int</span><span> i = </span><span>0</span><span>; i &lt; N/</span><span>4</span><span>; i++) { yv[i] = rsqrt_intr(xv[i]); }
</span><span>        clock_gettime(CLOCK_REALTIME, &amp;stop);
</span><span>        </span><span>for </span><span>(</span><span>int</span><span> i = </span><span>0</span><span>; i &lt; N; i++) { y_sum[INTR] += y[i]; }
</span><span>        t[INTR] += ((stop</span><span>.</span><span>tv_sec-start</span><span>.</span><span>tv_sec)*E9 + stop</span><span>.</span><span>tv_nsec-start</span><span>.</span><span>tv_nsec);
</span></code></pre>
<p>We can compile it in order to run and disassemble it:</p>
<pre><code><span>$ gcc -lm -O3 -g rsqrt_vec.c
</span><span>$ ./a.out
</span><span>rsqrt	fs/op	ratio	err
</span><span>exact	1895	1.00	0.0000
</span><span>appr	72	26.39	0.0193
</span><span>appr_nr	175	10.81	-0.0010
</span><span>rsqrtps	61	31.00	0.0000
</span></code></pre>
<pre><code><span>$ objdump -d -S a.out
</span><span>...
</span><span>v4sf rsqrt_intr(v4sf x) { return __builtin_ia32_rsqrtps(x); };
</span><span>    1238:       41 0f 52 04 04          rsqrtps (%r12,%rax,1),%xmm0
</span><span>...
</span></code></pre>
<p>Now we are down to a single instruction and it is slightly faster than before.</p>
<p>There are also extensions that not all processors support that we can try to
use. We can tell the compiler to use any extensions that are available on our
processor using <code>-march=native</code>. This may make the binary incompatible with
other processors, though.</p>
<pre><code><span>$ gcc -lm -Ofast -g -march=native rsqrt_vec.c
</span><span>$ ./a.out
</span><span>rsqrt	fs/op	ratio	err
</span><span>exact	78	1.00	0.0000
</span><span>appr	40	1.96	0.0137
</span><span>appr_nr	85	0.91	-0.0009
</span><span>rsqrtps	62	1.25	0.0000
</span></code></pre>
<p>Now we are down to almost as good as the first approximation. The intrinsic one
is pretty much just as fast. The “exact” method got replaced by a 256-bit
<a href="https://www.felixcloutier.com/x86/rsqrtps"><code>vrsqrtps</code></a> and a step of NR:</p>
<pre><code><span>static inline float rsqrt_exact(float x) { return 1.0f / sqrtf(x); }
</span><span>    11d0:       c5 fc 52 0c 18          vrsqrtps (%rax,%rbx,1),%ymm1
</span><span>    11d5:       c5 f4 59 04 18          vmulps (%rax,%rbx,1),%ymm1,%ymm0
</span><span>    11da:       48 83 c0 20             add    $0x20,%rax
</span><span>    11de:       c4 e2 75 a8 05 79 0e    vfmadd213ps 0xe79(%rip),%ymm1,%ymm0
</span><span>    11e5:       00 00
</span><span>    11e7:       c5 f4 59 0d 91 0e 00    vmulps 0xe91(%rip),%ymm1,%ymm1
</span><span>    11ee:       00
</span><span>    11ef:       c5 fc 59 c1             vmulps %ymm1,%ymm0,%ymm0
</span></code></pre>
<p>The <code>__builtin_ia32_rsqrtps</code> is now using a single <code>vrsqrtps</code> and no NR step,
however, it still uses only 128-bit registers.</p>
<h3 id="broad-sweep">Broad sweep</h3>
<p>So, we did some testing on my machine and got some insight into what kind of
instructions we can use to calculate the reciprocal square root and how they
might perform. We will now try to run these benchmarks on several machines to
give us an idea how well our findings apply in general. Those machines include
all the ones that I happen to have convenient SSH access to. All resulting data
can be downloaded from <a href="https://git.sr.ht/~nhellman/hllmn/tree/master/item/content/blog/2023-04-20_rsqrt/bench/res">here</a>, it also includes results for the
inverse and square root functions, separately.</p>
<p>Below is a list of the x86 machines that were tested along with their CPUs and
their release date. All the previous tests were run on on the computer labeled
as “igelkott”.</p>
<table><thead><tr><th>Hostname</th><th>CPU Family</th><th>CPU Model</th><th>Year</th><th>Form factor</th></tr></thead><tbody>
<tr><td>jackalope</td><td><a href="https://en.wikipedia.org/wiki/Core_(microarchitecture)">Core</a></td><td>Intel Celeron 550</td><td>2007</td><td>i686 laptop</td></tr>
<tr><td>narwhal</td><td><a href="https://en.wikipedia.org/wiki/Piledriver_(microarchitecture)">Piledriver</a></td><td>AMD FX-6300</td><td>2012</td><td>x86_64 desktop</td></tr>
<tr><td>silverback</td><td><a href="https://en.wikipedia.org/wiki/Ivy_Bridge_(microarchitecture)">Ivy Bridge</a></td><td>Intel Xeon E5-1410</td><td>2014</td><td>x86_64 server</td></tr>
<tr><td>bovinae</td><td><a href="https://en.wikipedia.org/wiki/Kaby_Lake">Kaby Lake</a></td><td>Intel Core i5-8250U</td><td>2017</td><td>x86_64 laptop</td></tr>
<tr><td>igelkott</td><td><a href="https://en.wikipedia.org/wiki/Zen_3">Zen 3</a></td><td>AMD Ryzen 5950X</td><td>2020</td><td>x86_64 desktop</td></tr>
<tr><td>deck</td><td><a href="https://en.wikipedia.org/wiki/Zen_2">Zen 2</a></td><td>AMD APU 0405</td><td>2022</td><td>x86_64 mobile</td></tr>
</tbody></table>
<p>Below is a plot of the performance ratio compared to the <code>exact</code> method, i.e.
the time of each method divided by the time of the <code>exact</code> method. A higher
ratio means higher performance, anything below 1 is slower than <code>exact</code> and
anything above is faster. We use the <code>-Ofast</code> flag here, as it is the fastest
option that can be used without sacrificing portability.</p>
<p><img src="https://hllmn.net/blog/2023-04-20_rsqrt/plot/bench.svg" alt=""/></p>
<p>The results are quite similar across all of the machines, the time of the
methods are approximately ranked in the order <code>rsqrtps</code> &lt;= <code>appr</code> &lt; <code>exact</code> &lt;=
<code>appr_nr</code>. Using the <code>appr_nr</code> method is either slower or the same as the
<code>exact</code> method, so it has no real benefit in this case.</p>
<p>The “jackalope” machine was not included in the above plot because it had an
extremely slow <code>exact</code> method. Especially when not using <code>-march=native</code> as the
compiler then resorted to using the antique <code>fsqrt</code> instruction.</p>
<p>Below is a table of the actual timings when using <code>-Ofast</code>, numbers in
parenthesis uses <code>-march=native</code>. Each number is how long a single operation
takes in femtoseconds.</p>
<table><thead><tr><th>Machine/Compiler</th><th>exact</th><th>appr</th><th>appr_nr</th><th>rsqrtps</th></tr></thead><tbody>
<tr><td>jackalope-clang</td><td>53634 (5363)</td><td>1500 (2733)</td><td>4971 (3996)</td><td>N/A</td></tr>
<tr><td>narwhal-gcc</td><td>419 (363)</td><td>443 (418)</td><td>601 (343)</td><td>396 (231)</td></tr>
<tr><td>narwhal-clang</td><td>389 (796)</td><td>340 (321)</td><td>445 (859)</td><td>349 (388)</td></tr>
<tr><td>silverback-gcc</td><td>422 (294)</td><td>179 (199)</td><td>543 (543)</td><td>178 (189)</td></tr>
<tr><td>bovinae-gcc</td><td>260 (127)</td><td>155 (81)</td><td>321 (119)</td><td>108 (105)</td></tr>
<tr><td>bovinae-clang</td><td>255 (132)</td><td>108 (78)</td><td>272 (112)</td><td>95 (96)</td></tr>
<tr><td>igelkott-gcc</td><td>141 (79)</td><td>111 (63)</td><td>168 (87)</td><td>58 (64)</td></tr>
<tr><td>igelkott-clang</td><td>152 (76)</td><td>63 (40)</td><td>149 (70)</td><td>61 (62)</td></tr>
<tr><td>deck-gcc</td><td>342 (160)</td><td>234 (114)</td><td>444 (172)</td><td>226 (120)</td></tr>
<tr><td>deck-clang</td><td>297 (166)</td><td>189 (123)</td><td>332 (140)</td><td>101 (126)</td></tr>
</tbody></table>
<p>The square root function yields slightly different results:</p>
<p><img src="https://hllmn.net/blog/2023-04-20_rsqrt/plot/bench_sqrt.svg" alt=""/></p>
<p>Oddly enough, the <code>sqrtps</code> built-in function is slower than the <code>exact</code> method,
and the <code>appr</code> without NR is now faster instead. The <code>appr_nr</code> method still
offers no advantage, it is instead consistently worse than <code>exact</code>.</p>
<p>Here are the original timings for the square root function as well, with
<code>-Ofast</code>. Again, numbers in parentheses use <code>-march=native</code>:</p>
<table><thead><tr><th>Machine/Compiler</th><th>exact</th><th>appr</th><th>appr_nr</th><th>sqrtps</th></tr></thead><tbody>
<tr><td>jackalope-clang</td><td>35197 (5743)</td><td>1494 (2738)</td><td>19191 (4308)</td><td>N/A</td></tr>
<tr><td>narwhal-gcc</td><td>505 (399)</td><td>399 (427)</td><td>659 (559)</td><td>796 (785)</td></tr>
<tr><td>narwhal-clang</td><td>448 (823)</td><td>327 (319)</td><td>638 (847)</td><td>803 (780)</td></tr>
<tr><td>silverback-gcc</td><td>625 (297)</td><td>271 (190)</td><td>958 (728)</td><td>1163 (1135)</td></tr>
<tr><td>bovinae-gcc</td><td>301 (148)</td><td>155 (81)</td><td>408 (200)</td><td>225 (226)</td></tr>
<tr><td>bovinae-clang</td><td>315 (244)</td><td>92 (60)</td><td>399 (159)</td><td>317 (227)</td></tr>
<tr><td>igelkott-gcc</td><td>173 (95)</td><td>119 (38)</td><td>233 (124)</td><td>288 (296)</td></tr>
<tr><td>igelkott-clang</td><td>168 (143)</td><td>63 (48)</td><td>234 (104)</td><td>170 (283)</td></tr>
<tr><td>deck-gcc</td><td>419 (205)</td><td>215 (108)</td><td>519 (252)</td><td>575 (574)</td></tr>
<tr><td>deck-clang</td><td>325 (244)</td><td>153 (88)</td><td>372 (180)</td><td>315 (458)</td></tr>
</tbody></table>
<h3 id="try-it-yourself">Try it yourself</h3>
<p>You can try to run the benchmarks on your machine and see if you get similar
results. There is a shell script <code>bench/run.sh</code> that will generate and run
benchmarks using the <code>bench/bench.c.m4</code> file. These files can be found in <a href="https://git.sr.ht/~nhellman/hllmn/tree/master/item/content/blog/2023-04-20_rsqrt">this
blog’s repo</a>. Simply run the script with no arguments and it will generate a
<code>.tsv</code> file with all results:</p>
<pre><code><span>$ cd bench
</span><span>$ sh run.sh
</span><span>$ grep rsqrt bench.tsv | sort -nk3 | head
</span><span>rsqrt	appr	40	1.91	0.0139	clang-Ofast-march=native
</span><span>rsqrt	rsqrtps	56	32.08	0.0000	clang-O3
</span><span>rsqrt	appr	58	31.08	0.0193	clang-O3
</span><span>rsqrt	rsqrtps	58	2.48	0.0000	clang-O3-fno-math-errno-funsafe-math-optimizations-ffinite-math-only
</span><span>rsqrt	rsqrtps	59	2.45	0.0000	gcc-Ofast
</span><span>rsqrt	rsqrtps	59	2.48	0.0000	clang-Ofast
</span><span>rsqrt	rsqrtps	59	31.07	0.0000	gcc-O3
</span><span>rsqrt	rsqrtps	59	7.83	0.0000	gcc-O3-fno-math-errno
</span><span>rsqrt	appr	60	2.41	0.0144	clang-O3-fno-math-errno-funsafe-math-optimizations-ffinite-math-only
</span><span>rsqrt	rsqrtps	60	8.09	0.0000	clang-O3-fno-math-errno
</span></code></pre>
<h2 id="final-thoughts">Final thoughts</h2>
<p>To summarize, using simply <code>1/sqrtf(x)</code> on modern x86 processors can be both
faster and more accurate than the <em>fast inverse square root</em> method from Quake
III’s <code>Q_rsqrt</code> function.</p>
<p>However, a key takeaway is that you have to order the compiler to make it
faster. When simply compiling using <code>-O3</code>, the fast inverse square root method
is actually <em>considerably</em> faster than the naive implementation. We have to
allow the compiler to violate some strict specification requirements in order
to make it emit a faster implementation (primarily to allow <a href="https://en.wikipedia.org/wiki/Automatic_vectorization">vectorization</a>).</p>
<p>Similarly can be said for the ordinary square root function as well, just using
<code>sqrtf(x)</code> and altering compiler flags allow for a very fast implementation.</p>
<p>If very low accuracy can be tolerated, it is possible to get a slightly faster
implementation by skipping the Newton-Raphson step from the fast inverse square
root method. Interestingly, the compiler also performs an NR step after using
approximate implementations of the inverse square root. This can also be made
slightly faster by skipping the NR step — by only emitting the approximate
instruction with the help of compiler intrinsics.</p>
<p>In this post, we focused on x86, but how about other instructions
sets? The fast inverse square root method could perhaps still be
useful for processors without dedicated square root instructions.</p>
<p>How are the hardware implementations of approximate square roots typically
implemented? Could an approximate hardware implementation potentially use
something similar to the first approximation of the fast inverse square root
method?</p>
<hr/>






</article></div>
  </body>
</html>
