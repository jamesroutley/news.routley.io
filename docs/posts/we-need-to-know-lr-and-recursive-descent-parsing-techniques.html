<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://tratt.net/laurie/blog/2023/why_we_need_to_know_lr_and_recursive_descent_parsing_techniques.html">Original</a>
    <h1>We Need to Know LR and Recursive Descent Parsing Techniques</h1>
    
    <div id="readability-page-1" class="page"><div id="article-body"><p>



A couple of people have asked me for my thoughts on part an article from Tiark Rompf
called (roughly) &#34;<a href="https://tiarkrompf.github.io/notes/?/just-write-the-parser/aside1">Just
Write the Parser</a>&#34; which advocates the use of recursive descent parsing over
more &#34;formal&#34; approaches to parsing, at least in the context of teaching
compilers. I&#39;m fairly sure I read this article a
couple of years ago, but it may have been updated, or simply have been
discovered anew. Either way, since I admire both Tiark and his work
a great deal, it was useful for me to engage with his ideas and
compare them to my own.

</p><p>In this post, I&#39;m going to summarise Tiark&#39;s argument as I see it, and try
to explain where I agree and disagree with that summary. I will inevitably
rehash many of the arguments from my 2020 &#34;<a href="https://tratt.net/laurie/blog/2020/which_parsing_approach.html">Which Parsing
Approach?</a><a>&#34; post, but I&#39;m going to try to focus on some factors which I now
realise were at best buried, and at worst inadequately explained, in that
previous post.


</a></p><h2><a>Summarising the argument</a></h2><a>
<p>In essence I believe Tiark is making two points:

</p><ol>
<li>Making parsing a major component of teaching compilers is misguided.
</li><li>Context-free grammars, and their associated parsing techniques, don&#39;t align
well with real-world compilers, and thus we should deemphasise CFGs (Context-Free
Grammars) and their associated parsing algorithms.
</li></ol>

I agree with the first point. Learning has to involve prioritisation: we can&#39;t
learn the underlying theory of <em>everything</em>. Parsing seems to me to be
one of those topics where a fairly superficial understanding of the underlying
theory is sufficient for most people to use it effectively. Personally I would
rather spend the time that a deep understanding of the underlying theory of parsing
requires elsewhere: there are many other parts of compilers that better reward
a deeper understanding </a><p>[1].

</p><p>However, I disagree with the second point — or, at least, I disagree
with part of it. It is true that many real-world languages&#39; syntaxes are not compatible
with <a href="https://en.wikipedia.org/wiki/LR_parser">LR parsing</a> (and the like) and must be parsed using
<a href="https://en.wikipedia.org/wiki/Recursive_descent_parser">recursive descent parsing</a>.
Without wishing to put words in his mouth, I suspect that Tiark sees this as a virtue of
recursive descent parsing.

</p><p>In contrast, I have come, over time, to see
languages that <em>require</em> recursive descent parsing as reflecting
a flaw in our approach to language design .
I believe it has led us to design languages
that are not just hard to parse, that don&#39;t just tend to have buggy parsers,
but which are harder than necessary to understand.


</p><h2>Differentiating LR and recursive descent parsing</h2>
<p>Let&#39;s start with some basic definitions. A <em>grammar</em> is a
specification of the valid sentences in a language. Note that grammars need not
be executable — indeed, they may be specified formally or informally,
explicitly or implicitly. A <em>parser</em> is an implementation of a grammar, which
checks whether an input is valid with respect to that grammar or not. Grammars
are <em>ambiguous</em> (e.g. a grammar such as <code>E: E &#34;+&#34; E | E &#34;-&#34;
E</code> is ambiguous because <code>2+3*4</code> can be parsed as equivalent to
either <code>(2+3)*4</code> or <code>2+(3*4)</code> with it) or <em>unambiguous</em>.

</p><p>For example, all of us share a common understanding of the grammar for basic
arithmetic expressions. We can then implement an LR or a recursive descent
parser which takes arithmetic expressions as inputs and checks whether they
conform to the arithmetic grammar.

</p><p>In practise, grammars and parsers are often closer in nature than their
abstract definitions suggest. In one direction, we can automatically derive
parsers from some grammars (e.g. those written to conform to the LR subset of
CFGs). In the other direction, a parser implicitly defines a grammar . There is
a subtle corollary to this: if I manually create a parser from the
specification of a grammar, I may introduce bugs, which means that the parser&#39;s
implicit grammar may not quite match the explicit grammar I thought I was
implementing!

</p><p>There are various ways of implementing parsers, but for me there are
generally only two which we really need to consider: LR parsing and recursive
descent parsing.

</p><p>LR parsing defines a subset of CFGs: every grammar in the LR subset is
guaranteed to be unambiguous . From an LR
grammar we can automatically derive an LR parser (e.g. using a tool like YACC
or <a href="https://crates.io/crates/lrpar/">lrpar</a>). A challenge with LR
parsing is that some grammars are difficult to express unambiguously (e.g.
arithmetic expressions): some are impossible to express unambiguously (e.g.
many real-world programming languages).

</p><p>In practise, a <em>recursive descent parser</em> can be thought of as meaning &#34;a
hand-written written parser&#34;: unless you go out of your way to do otherwise,
nearly all hand-written parsers are recursive descent parsers. As this suggests,
recursive descent parsers are easy to write. Since they&#39;re also just normal
programs, we can use them to parse any language we can conceive of.

</p><p>Presented like this there there doesn&#39;t seem to be much of a choice
between the two. Recursive descent parsing is easier and more flexible,
so let&#39;s just use that!

</p><p>However, recursive descent parsing has an inherent consequence: it produces
parsers which resolve ambiguities in the underlying grammar without telling you
that they have been resolved. For example, a recursive descent parser for
arithmetic expressions will parse <code>2+3*4</code> as equivalent to either
<code>(2+3)*4</code> or <code>2+(3*4)</code>, but not both. However, since a
recursive descent parser is just a normal program, you might not even realise
that you have written your parser in such a way that it has chosen to parse
just one of the possibilities — hopefully you chose to parse it as
equivalent to <code>2+(3*4)</code>!

</p><p>In contrast, a CFG can&#39;t be LR if it has such an ambiguity. If you give a
grammar such as <code>E: E &#34;+&#34; E | E &#34;-&#34; E</code> to YACC it will tell you it
is ambiguous (using the infamous, and in some ways unfortunate, terminology of &#34;shift/shift&#34;,
&#34;reduce/reduce&#34;, or &#34;accept/reduce&#34;). Although few people think of it as such,
LR is effectively a type checker for grammar ambiguity: if you don&#39;t get any
errors, your grammar is guaranteed to be unambiguous. One of the beauties of LR
parsing is that the same grammar that has been &#34;type checked&#34; to be unambiguous
can then be automatically turned into a parser: there is no way that you can
accidentally introduce ambiguity, or other bugs, into the resulting parser
.

</p><p>Fundamentally, recursive descent parsers have no equivalent &#34;type checker&#34;,
and we know that we can&#39;t build such an equivalent for the general case.
Recursive descent parsers thus parse the languages they parse, but we don&#39;t, in
general, know how to recover the grammar that such parsers are implicitly
parsing. That is not the tautology it sounds: my experience is that bugs in
recursive parsers are common. I have had the dubious pleasure of converting a
number of recursive descent parsers to LR grammars and none of the non-trivial
recursive descent parsers has parsed the language their authors expected.
Sometimes they accept inputs they shouldn&#39;t; sometimes they reject inputs they
shouldn&#39;t.

</p><p>This wouldn&#39;t be as much of a problem if it was easy to test parsers, but it turns out
to be surprisingly hard to create good parser suites. Most parser test suites
that I&#39;ve seen focus most of their effort on testing that &#34;good&#34; inputs are
accepted. Since the range of &#34;bad&#34; inputs is, in general, much more diverse,
test suites rarely cover much of the space of &#34;bad&#34; inputs to check that
they&#39;re rejected .

</p><p>There is an additional, easily over-looked, factor that comes from
relying on recursive descent parsers: learners cannot,
in any sensible time, understand from the recursive descent parser the
language&#39;s syntax. Many languages ignore this problem at first, and hope
that learners can guess the language&#39;s syntax from examples. This is inevitably
unsatisfactory, and it is then common over time for languages to try deriving
a CFG from the recursive descent parser, to help learners and tool authors.
However, these CFGs rarely fully match the recursive descent parser, leaving
learners confused, and tool authors frustrated.

</p><p>There are two other use-cases for recursive descent parsing. The first is
performance. Although there aren&#39;t, to the best of my knowledge, modern
performance numbers, it is reasonable to assume that recursive descent parsing
is generally faster than LR parsing. However, &#34;faster&#34; is relative: on modern
machines, even a naive LR parser on a huge grammar will happily parse thousands
of lines of code a second which is enough for the vast majority of use cases.
The second is error recovery. LR parsers have traditionally had terrible error
recovery. Immodestly, I now claim that <a href="https://tratt.net/laurie/blog/2020/automatic_syntax_error_recovery.html">some work I was
involved in</a> allows LR parsers to have error recovery that is better than
all but the best recursive descent parsers.


</p><h2>Summary</h2><p>

It seems to me that if you believe static type checking allows you to write
programs that are more reliable than if you used dynamic type checking then,
logically, you must also believe that LR parsing is more reliable than
recursive descent parsing. Similarly, if you believe that explicit static types
help readers better understand code, then you must also believe that
explicit LR grammars better convey to readers the grammar of a language than
the equivalent recursive descent parser.

</p><p>There is then a separate debate to be had about whether the costs of LR
parsing are worth the benefits. My personal experience is that, for unambiguous
languages, it takes relatively little practise to design LR grammars. It is
then more than just a bonus that we can automatically generate a parser
from the grammar. Indeed, and to my surprise, I&#39;ve found it markedly quicker to write an LR grammar
than to write, test, and debug the equivalent recursive descent parser.
My advice for writing LR grammars to those new to it boils down to,
in essence, &#34;write your grammar in
small bits, continually test it for LR-ness, and if you get a shift/reduce
(etc.) error, undo your last change, and try a variation.&#34; If that sounds
trite, or patronising, please bear in mind that it&#39;s exactly the approach I
take myself!

</p><p>If you agree with this line of thinking, then you might find the following
list of recommendations useful:

</p><ul>
<li>New languages (whether they be &#34;programming languages&#34; or mere
&#34;configuration languages&#34; and the like) should come with an LR grammar: you
then have a specification that users can understand, and people can automatically
derive parsers for their favourite use-case directly from that specification.
Specifying new languages in this style is not difficult, and the restrictions
on syntax rarely onerous. If writing a grammar in LR style is hard for you as
the language author, it will almost certainly be hard for users to understand!

</li><li>Existing languages have often evolved in a manner that makes it
difficult, or impossible, to specify an LR grammar. There&#39;s no point in
trying to fight this: just use recursive descent parsing.

</li><li>The more complex your recursive descent parser, the more effort you need to
put into testing. Without extensive testing, your parser will do things you
didn&#39;t expect: it is just normal code, after all, and all of us make mistakes
when coding!

</li><li>If you&#39;re writing a parser for a language which doesn&#39;t have an LR grammar,
it can be worth trying to create one. If you&#39;re lucky you&#39;ll find that your
language has an unambiguous grammar; if you&#39;re unlucky, you&#39;ll at least
understand where some of the points of ambiguity are . I find that this makes writing a recursive descent parser a less fraught
task, as at least I know where the likely bugs due to ambiguity will lie.

</li><li>If you&#39;re writing a parser for a language which is ambiguous, you
<em>might</em> be able to use the conflict resolution offered by YACC and
friends. However, in general, I suggest avoiding YACC&#39;s conflict resolution if
possible, as you have to at least somewhat understand the underlying LR
algorithm to understand how conflicts have been resolved.

</li><li>If you need the best possible performance or error recovery, recursive
descent parsing is the best choice. If you know you will need this <em>and</em>
you&#39;re designing a new language, at least create an LL grammar, since from
that you can derive a recursive descent parser. [Bear in mind that LL grammars
are more annoying to write than LR grammars which is why I suggest
avoiding LL unless you have this very specific use case.]

</li><li>LALR and SLR are subsets of LR parsing that are irritating to use and, on
any machine from the last 20 years, have no meaningful advantages. Avoid.

</li><li>Otherwise, just use LR parsing, whether it&#39;s YACC or the equivalent for
your favourite language (e.g. I am fond of <a href="https://crates.io/crates/lrpar/">lrpar</a> for Rust since I wrote it).

</li></ul><p>

Looking back to Tiark&#39;s article, my opinion on teaching parsing is two-fold.
First, parsing isn&#39;t generally worth much of student&#39;s time, so teach them the
minimum possible. Second, the minimum possible is, in my opinion, that which is
needed to write: recursive descent parsing, so that they can parse existing
languages which lack an unambiguous grammar; and LR parsing so that they don&#39;t
keep designing hard-to-parse languages in the future.


</p>



<h3>Footnotes</h3>
<p><a name="55770871">[1] I didn&#39;t have the opportunity to study compilers as an undergraduate. If I had, I
would not have wanted parsing to take up more than 1 week of such a course: I&#39;d
have wanted to know more about the many other parts that constitute a compiler!</a></p></div></div>
  </body>
</html>
