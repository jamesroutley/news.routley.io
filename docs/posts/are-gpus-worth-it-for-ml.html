<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://exafunction.com/blog/are-gpus-worth-it">Original</a>
    <h1>Are GPUs Worth It for ML?</h1>
    
    <div id="readability-page-1" class="page"><div><div><p>In theory, this sounds like a dumb question. Machine Learning models, particularly Deep Learning models, are (mostly) a bunch of arithmetic operations, especially parallelizable ones like matrix multiplications. GPUs, by design, are optimized for parallelizable arithmetic operations. It&#39;s a match made in heaven.</p><p><img src="https://exafunction.com/static/images/are-gpus-worth-it/gpu_cpu_throughput.png" alt="drawing"/></p><p><em>Source: <a target="_blank" rel="noopener noreferrer" href="https://developer.nvidia.com/blog/accelerating-wide-deep-recommender-inference-on-gpus/">NVIDIA</a> (2019)</em></p><hr/><p>But just like most things that sound great in theory, the story is different in practice. In reality, we almost always have to do some preprocessing of inputs and postprocessing of outputs on either end of an ML inference, or some other operations that aren&#39;t GPU-optimized. Ok, you might say, I&#39;ll just run those stages on a CPU and run the inference on a GPU - best of both worlds!<sup>(1)</sup></p><p>Unfortunately, these computations are often more time consuming and intensive than the inference itself. And CPUs are so much cheaper. <strong>It feels wasteful to have an expensive GPU sitting idle while we are executing the CPU portions of the ML workflow.</strong> Sure, latency is nice, but is the cost really worth it? Using the statistics from NVIDIA, should we just eat a 7x slowdown on the inference stage and run everything on CPU?</p><p><img src="https://exafunction.com/static/images/are-gpus-worth-it/gpu_cpu_cost.png" alt="drawing"/></p><p><em>Source: <a target="_blank" rel="noopener noreferrer" href="https://cloud.google.com/compute/all-pricing">GCP</a> (2022)</em></p><hr/><p>As shown above, GPU instances are an order of magnitude more expensive than that of CPUs, with similar results across cloud providers. And we haven&#39;t even mentioned the scarcity problem of GPUs.</p><p>So in reality, there are two camps:</p><ul><li><strong>Latency Conscious</strong>: Pay for both the CPU and GPU to get the quickest request turnaround.</li><li><strong>Cost Conscious</strong>: Take a hit on latency by running the ML model inference on CPU to get cheaper cost.</li></ul><p><img src="https://exafunction.com/static/images/are-gpus-worth-it/1-06.png" alt="drawing"/></p><p>This is not the end of the story though because of one big aspect of most ML workloads - lots of concurrent requests<sup>(2)</sup>. <strong>Modern batch ML workloads operate at much higher scale than a single inference at a time</strong>, and so with a little systems magic, neither of these two camps should have to sacrifice on cost or latency. The basic premise is that <strong>a bunch of &#34;clients&#34; can do their CPU computation on their own dedicated CPUs and do any GPU-optimized computation (i.e. model inference) on a shared GPU resource</strong>. Of course we don&#39;t have a preset schedule of requests, so all of this GPU resource sharing has to be done dynamically. Borrowing some terminology from the networking world, <strong>we refer to this approach as &#34;dynamic multiplexing.&#34;</strong></p><p><img src="https://exafunction.com/static/images/are-gpus-worth-it/dynamic_multiplexing.png" alt="drawing"/></p><hr/><p>Let&#39;s make this more concrete with some numbers. Say we have a workload where, per &#34;request,&#34; we need 90ms of CPU-only preprocess/postprocess and a model inference that would take 10ms on a GPU and 210ms on a CPU. Let&#39;s say we have 10 clients that each make a request every 100ms. Finally, let&#39;s say each CPU costs 1 unit and each GPU costs 5 units (per some standardized time interval). How does each option shape up?</p><ul><li><strong>Latency Conscious (dedicated GPU per client)</strong>: Each request from each client can be satisfied with 1 CPU and 1 GPU per client, with an optimal latency of 100ms and cost of 10 * (1 + 5) = <strong>60 units</strong>.</li><li><strong>Cost Conscious (CPU only)</strong>: Since each call will have latency of 300ms, we will need 3 CPUs per client to satisfy all requests. Even though we took a hit on latency, the cost is now just 10 * 3 = <strong>30 units</strong>.</li><li><strong>Dynamic Multiplexing (shared GPU)</strong>: With 10 clients, a call from each every 100ms, and 10ms of GPU time required per call, we can fully satisfy all calls with a single shared GPU resource!<sup>(3)</sup> Each client still needs a dedicated CPU, but we now have optimal 100ms latency per call and a cost of only 10 * 1 + 5 = <strong>15 units</strong>.</li></ul><p><strong>Dynamic multiplexing lets us have our cake and eat it too!</strong><sup>(4)</sup></p><p>Clearly this was a toy example, and dynamic multiplexing hosts its own set of challenges:</p><ul><li>Too few clients to a GPU resource and you are underutilizing it, but too many clients to a GPU resource and you start introducing a minimum &#34;wait latency&#34; when clients are blocked on another client&#39;s inference.</li><li>Some models don&#39;t batch well, such as language models with variable input sequence lengths.</li><li>Preprocessing times and model inference times may not be constant, either due to randomness or programmed to change over the course of the batch workload.</li><li>The shape of CPU/GPU resources needed may not match the available colocated nodes.</li></ul><p>We&#39;ll discuss in the future how we&#39;ve solved these challenges and how we&#39;ve taken dynamic multiplexing even further, such as executing multiple models on a GPU at the same time if the models have poor GPU utilization when running (ex. long kernel load times).</p><p>As discussed in our earlier <a target="_blank" rel="noopener noreferrer" href="https://exafunction.com/blog/ml-deployment-problems">post</a>, we at Exafunction want to remove all pains and concerns that ML practitioners feel when deploying their workloads. Cost and latency arguably top that list, so we knew that we needed to solve this dynamic multiplexing problem with ExaDeploy. For numbers, <strong>we have successfully been able to multiplex 30 clients to a single GPU for our customers&#39; workloads, a big reason why some of their GPU costs have dropped by ~97% by using ExaDeploy</strong>.</p><p>This all being said, there are even more things we can do to keep squeezing out cents and milliseconds, and we will explore these in future blog posts<sup>(5)</sup>. But just with dynamic multiplexing, we can answer this post&#39;s motivating question: <strong>yes, GPUs are absolutely worth it if utilized properly!</strong></p><p>If you are currently weighing this cost vs latency tradeoff for your ML workloads, we want to chat with you - send us a message <a target="_blank" rel="noopener noreferrer" href="https://exafunction.com/contact">here</a>!</p></div></div></div>
  </body>
</html>
