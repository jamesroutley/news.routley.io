<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://penghtyx.github.io/Era3D/">Original</a>
    <h1>Era3D: High-Resolution Multiview Diffusion Using Efficient Row-Wise Attention</h1>
    
    <div id="readability-page-1" class="page">

<nav role="navigation" aria-label="main navigation">
  
  

  
</nav>
<section>
  </section>
<section>
  <div>
    <!-- Teaser video. -->
    <div>
      <div>
        
      <div>
        <div>
          <p><img src="https://penghtyx.github.io/Era3D/static/images/teaser.jpg" alt="teaser"/></p><div>
            <p>Input</p>
            <p>Generated multiview image and normal maps</p>
            <p>Mesh</p>
          </div>
        </div>
      </div>
    </div>
  </div>
  <div>
    <!-- Abstract. -->
    <div>
      <div>
        <h2>Abstract</h2>
        <div>
          <p>
            In this paper, we introduce <b>Era3D</b>, a novel multiview diffusion method that generates high-resolution multiview images from a single image. Despite significant
            advancements in multiview generation, existing methods still suffer from camera prior mismatch, inefficacy, and low resolution resulting in poor-quality multiview
            images. Specifically, these methods assume that the input images should comply with a predefined camera type, e.g., a perspective camera with a fixed focal length,
            leading to distorted shapes when the assumption fails. Moreover, the full-image or dense multiview attention they employ leads to an exponential explosion of
            computational complexity as image resolution increases, resulting in prohibitively expensive training costs. To bridge the gap between assumption and reality, Era3D
            first proposes a diffusion-based camera prediction module to estimate the focal length and elevation degree of the input image, which allows our method to produce feasible images without shape distortions. Furthermore, a simple but efficient
            attention layer, named row-wise attention, is used to enforce epipolar priors in the
            multiview diffusion, facilitating efficient cross-view information fusion. Consequently, compared with state-of-the-art methods, Era3D generates high-quality
            multiview images with up to a 512×512 resolution while reducing computation complexity by 12x times. Comprehensive experiments demonstrate that Era3D can
            reconstruct high-quality and detailed 3D meshes from diverse single-view input images, significantly outperforming baseline multiview diffusion methods.
          </p><br/>
        </div>
      </div>
    </div>
  </div>
  <!--/ Abstract. -->
  <div>
    <div>
      <div>
        <p><img src="https://penghtyx.github.io/Era3D/static/images/pipeline.jpg" alt="pipeline"/>
        </p>
      </div>
    </div>
    <p><b>Pipeline.</b> Given a single-view image as input, Era3D applies multiview diffusion to
      generate multiview consistent images and normal maps in the canonical camera setting, which
      enables us to reconstruct 3D meshes using neural representations or transformer-based
      reconstruction methods.</p>
  </div>
  <div>
  <div>
    <div>
      <div>
        <figure>
          <img src="https://penghtyx.github.io/Era3D/static/images/camera_setting.jpg" alt="Custom setup"/>
          <figcaption>(a) General camera setting                                        (b) Canonical camera setting  </figcaption>
        </figure>
      </div>
    </div>
  </div>
  <div>
      <div>
        <figure>
          <img src="https://penghtyx.github.io/Era3D/static/images/attention_comp.jpg" alt="Custom setup"/>
          <figcaption>    
                      (c) Dense MV attention       
                      (d) Epipolar MV attention       
                      (e) Row-wise MV attention
          </figcaption>
          <p>    
            O(<i>N<sup>2</sup>S<sup>4</sup></i>)                                     
            O(<i>N<sup>2</sup>S<sup>2</sup>K</i>)                                       
            O(<i>N<sup>2</sup>S<sup>3</sup></i>)
         </p>
        </figure>
      </div>
  </div>
  <p> <b> Motivation.</b> 
    We analyze different types of multiview attention layers. In a dense multiview attention layer <b>(c)</b>, all
    feature vectors of multiview images are fed into an attention block. For a general camera setting
    <b>(a)</b> with arbitrary viewpoints and intrinsics, utilizing epipolar constraint to construct an epipolar
    attention <b>(d)</b> needs to correlate the features on the epipolar line. This means that we need to sample
    <i>K</i> points along each epipolar line to compute such an attention layer. In our canonical camera setting
    <b>(b)</b> with orthogonal cameras and viewpoints on an elevation of 0<sup>°</sup>, epipolar lines align with the
    row of the images across different views <b>(e)</b>, which eliminates the need to resample epipolar line
    to compute epipolar attention. We assume the latent feature map has a resolution of <i>H × W</i> and
    <i>H = W = S</i>. In such a <i>N</i>-view camera system, row-wise attention reduces the computational
    complexity to O(<i>N<sup>2</sup>S<sup>3</sup></i>).

  </p></div></div></section>

  <section>
    <!-- results -->
    
    </section>

    <section>
      <!-- results -->
      <div>
        <div>
          <div>
            <p>
              <h2>Text to 3D
              </h2>
            </p>
            
           
            <div>
              
              <p>A bulldog with a black pirate hat.</p>
            </div>
       
            </div>
        </div>
      </div>
      </section>

      <section>
        <!-- results -->
        </section>


        <section>
          <div>
            <!-- Concurrent Work. -->
            <div>
              <div>
                <h2>Acknowledgement</h2>
        
                <p>
                    This work is mainly supported by Hong Kong Generative AI Research &amp; Development Center (HKGAI) led by Prof. Yike Guo. We are grateful for the necessary GPU resources provided by both Hong Kong University of Science and Technology (HKUST) and DreamTech.
                  </p>
              </div>
            </div>
            <!--/ Concurrent Work. -->
          </div>
        </section>
        
<section id="BibTeX">
          <div>
            <h2>BibTeX</h2>

    <pre><code>@article{li2024era3d,   
  title={Era3D: High-Resolution Multiview Diffusion using Efficient Row-wise Attention},
  author={Li, Peng and Liu, Yuan and Long, Xiaoxiao and Zhang, Feihu and Lin, Cheng and Li, Mengfei and Qi, Xingqun and Zhang, Shanghang and Luo, Wenhan and Tan, Ping and others},
  journal={arXiv preprint arXiv:2405.11616},
  year={2024}
}</code></pre> 
        </div>
        </section>
        
        
        




</div>
  </body>
</html>
