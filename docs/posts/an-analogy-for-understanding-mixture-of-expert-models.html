<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="http://www.kolaayonrinde.com/blog/2023/10/22/moe-analogy.html">Original</a>
    <h1>An Analogy for Understanding Mixture of Expert Models</h1>
    
    <div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">
  
  <!-- Google tag (gtag.js) -->
  
  
  

  <div itemprop="articleBody"><!-- An Intuitive Way To Understand Mixture Of Expert Models (Sparse MoEs) -->

<h4 id="tldr-experts-are-doctors-routers-are-gps">TL;DR: Experts are Doctors, Routers are GPs</h4>

<h2 id="motivation">Motivation</h2>

<p><a href="https://en.wikipedia.org/wiki/Foundation_models">Foundation models</a> aim to
solve a wide range of tasks. In the days of yore, we would build a supervised
model for every individual use case; foundation models promise a single unified
solution.</p>

<p>There are challenges with this however. When two tasks need different skills,
trying to learn both can make you learn neither as well as if you had focused on
one<sup id="fnref:neg" role="doc-noteref"><a href="#fn:neg" rel="footnote">1</a></sup>. Storing information for many tasks can also be a challenge, even for
large models.</p>

<p>Moreover we might wonder if it make sense to use the same parameters for
computing the answer to a logic puzzle and for finding the perfect adjective to
describe the love interest in a romance fanfic.</p>

<p>We would like our models to have modular functions. We could then select and
even combine abilities when needed.</p>

<h2 id="moes-for-scale">MoEs For Scale</h2>

<p>Scaling up models offers various advantages. There are three main quantities to
scale: the number of model <code>parameters</code>, the amount of <code>data</code> and the amount of
<code>compute</code> applied at train time. With regular transformers, to scale up the
number of parameters, we must likewise scale the amount of compute applied.</p>

<blockquote>
  <p>Intuitively more parameters mean more <code>knowledge</code>, and more compute represents
additional <code>intelligence</code> <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">2</a></sup>.</p>
</blockquote>

<p>There are some use cases where having more knowledge can be traded off with
being more cognitively able. For example, you may choose to memorise rather than
re-derive the laws of physics to use them in a specific problem. Similarly we
can trade off the opposite way as well - if you know you’ll have access to a
textbook or Wikipedia then you might not want to memorise certain historical
facts. All you need to know is when and how to look up the facts you need.</p>

<p>So, dependent on whether we need more knowledge or more cognitive ability, we
also want to scale parameters and compute separately<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" rel="footnote">3</a></sup>.</p>

<h2 id="sparse-mixture-of-experts-models">Sparse Mixture of Experts Models</h2>

<p>In a vanilla transformer, each Transformer Block contains an attention layer for
<code>communication</code> between tokens and an MLP layer for <code>computation</code> within
tokens. The MLP layer contains most of the parameters of a large transformer and
transforms the individual tokens.</p>

<p>In <a href="https://arxiv.org/pdf/2101.03961.pdf">Sparse Mixture of Experts</a> (MoEs), we
swap out the <code>MLP layers</code> of the vanilla transformer for an <code>Expert Layer</code>. The
Expert Layer is made up of multiple MLPs called “Experts”. For each input we
select one expert to send that input to. In this way, each token it has
different parameters applied to it. A dynamic routing mechanism decides how to
map tokens to Experts<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" rel="footnote">4</a></sup>.</p>

<div>
  <figure>
    <img src="http://www.kolaayonrinde.com/blog/images/softmoe/moe.png" width="800" alt="Sparse MoE"/>
    <figcaption>Sparse Expert Layer (Switch Transformer) </figcaption>
    </figure>
</div>

<p>Sparse MoEs solve the problems we noted earlier:</p>

<ul>
  <li>MoEs allow their internal “Experts” to specialise in certain domains rather
than having to be all things to all tokens <sup id="fnref:0" role="doc-noteref"><a href="#fn:0" rel="footnote">5</a></sup> <sup id="fnref:m" role="doc-noteref"><a href="#fn:m" rel="footnote">6</a></sup>.</li>
  <li>And with MoEs, we are able to increase the number of parameters of models
without increasing how much training compute or inference time latency. This
decouples parameter scaling from compute scaling (i.e. we decouple knowledge
from intelligence)</li>
</ul>

<h2 id="the-analogy">The Analogy</h2>

<p>Imagine you’re feeling fatigued and you have no idea what’s causing this.
Suppose the problem is with your eyes but you don’t know this yet. Since your
friend is a cardiologist (doctor specialising in the heart), you ask them for
advice, which they freely give. You might ask yourself if you should follow
their advice blindly or if you should:</p>

<p><strong>Approach 1: Get a second opinion from another cardiologist.</strong></p>

<div>
  <figure>
    <img src="http://www.kolaayonrinde.com/blog/images/analogy-moe/two_cardiologists.png" width="600" alt="Two Cardiologists"/>
    <!-- <figcaption>Sparse Expert Layer (Switch Transformer) </figcaption> -->
    </figure>
</div>

<blockquote>
  <p>Averaging over multiple doctors who were trained in the same way increases
robustness by reducing variance (maybe the first doctor was tired that day or
something). But it doesn’t help with bias <sup id="fnref:stat" role="doc-noteref"><a href="#fn:stat" rel="footnote">7</a></sup> - all the cardiologists are
likely to be wrong in the same way, if they are wrong at all.</p>
</blockquote>



<p><strong>Approach 2: Go to a generalist doctor that has no specialism.</strong></p>

<div>
  <figure>
    <img src="http://www.kolaayonrinde.com/blog/images/analogy-moe/no_specialist.png" width="600" alt="One cardiologist and one doctor with no specialism"/>
    <!-- <figcaption>Sparse Expert Layer (Switch Transformer) </figcaption> -->
    </figure>
</div>

<blockquote>
  <p>It’s not clear whether this is better than asking another cardiologist. Sure
they might have different knowledge to the cardiologist which might be useful
if your problem isn’t about the heart. But there’s an awful lot of medical
knowledge out there and we can’t reasonably expect this one generalist to know
everything about all of them. They probably have cursory knowledge at best. We
need someone who specialises in the area that we’re struggling with. Problem
is we don’t know which area of specialism we need!</p>
</blockquote>



<p><strong>Approach 3: Ask multiple doctors who all specialise in different areas and do
the thing most of them suggest.</strong></p>

<div>
  <figure>
    <img src="http://www.kolaayonrinde.com/blog/images/analogy-moe/all_doctors.png" width="600" alt="Multiple Doctors with Different Specialisms"/>
    <!-- <figcaption>Sparse Expert Layer (Switch Transformer) </figcaption> -->
    </figure>
</div>

<blockquote>
  <p>This is much better. If you have a problem with your eyes, you know that the
eye doctor is being consulted so you have a much better chance of getting the
right treatment. But there are downsides here. Most notably, asking multiple
doctors is probably pretty inefficient. Now we have to see 50 specialists for
every problem even though most of them have no idea about our problem. What we
would prefer is to know which one specialist (or possibly couple of
specialists) we should see and only get advice from them.</p>
</blockquote>



<p><strong>Approach 4: Go to your GP, tell them about your ailment and ask them which
doctor you should go and see.</strong></p>

<div>
  <figure>
    <img src="http://www.kolaayonrinde.com/blog/images/analogy-moe/gp.png" width="600" alt="GP-Doctor System"/>
    <!-- <figcaption>Sparse Expert Layer (Switch Transformer) </figcaption> -->
    </figure>
</div>

<blockquote>
  <p>This is of course what we do in real life. Here we get the benefits of getting
advice from the most relevant specialised doctor without having to ask every
other doctor. This is both more accurate and time-efficient.</p>
</blockquote>



<p>In approach 4, the GP is the routing function. They know the strengths of the
different doctors and send you to one of them depending on your problem.</p>

<p>The Doctors are the Experts. We allow them to specialise knowing that the GP can
route us to the correct doctor for our problem.</p>

<p><strong>The GP-doctor system is exactly a Mixture of Experts layer.</strong></p>

<h3 id="what-are-moes-good-for">What Are MoEs Good For?</h3>

<p>Viewed this way we see that Mixture of Expert models will be effective whenever
we want a model to have access to large amounts of information - more than a
single Expert could hope to learn alone. Another use case is when our task can
be decomposed into one of a number of tasks.</p>

<p>In general we might imagine MoEs which when faced with more difficult problems
can send the input to a more powerful expert which has access to more resources.
This starts to move us increasingly towards
<a href="https://github.com/koayon/awesome-adaptive-computation">Adaptive Computation</a>.</p>


</div>
</article>

      </div>
    </div></div>
  </body>
</html>
