<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.louisbouchard.ai/editgan/">Original</a>
    <h1>Nvidia EditGAN: Image Editing with Full Control from Sketches</h1>
    
    <div id="readability-page-1" class="page"><article>

    <header>

        <section>
            <a href="https://www.louisbouchard.ai/tag/artificial-intelligence/">Artificial Intelligence</a>
        </section>

        

        <p>Control any feature from quick drafts, and it will only edit what you want keeping the rest of the image the same! SOTA Image Editing from sketches model by NVIDIA, MIT, UofT.</p>

        <div>
            <section>
                <ul>
                    <li>
                        <a href="https://www.louisbouchard.ai/author/louis/">
                            <img src="https://www.louisbouchard.ai/content/images/size/w100/2021/04/profile.png" alt="Louis Bouchard"/>
                        </a>
                    </li>
                </ul>
                <div>
                    
                    <p><time datetime="2021-12-04">Dec 4, 2021</time>
                        <span><span>•</span> 6 min read</span>
                    </p>
                </div>
            </section>
        </div>

        <figure>
            <img srcset="/content/images/size/w300/2021/12/Blue-Dynamic-Fitness-Youtube-Thumbnail--47-.png 300w,
                        /content/images/size/w600/2021/12/Blue-Dynamic-Fitness-Youtube-Thumbnail--47-.png 600w,
                        /content/images/size/w1000/2021/12/Blue-Dynamic-Fitness-Youtube-Thumbnail--47-.png 1000w,
                        /content/images/size/w2000/2021/12/Blue-Dynamic-Fitness-Youtube-Thumbnail--47-.png 2000w" sizes="(min-width: 1400px) 1400px, 92vw" src="https://www.louisbouchard.ai/content/images/size/w2000/2021/12/Blue-Dynamic-Fitness-Youtube-Thumbnail--47-.png" alt="NVIDIA EditGAN: Image Editing with Full Control From Sketches"/>
        </figure>
    </header>



    <section>
        <h4 id="watch-the-video">Watch the video!</h4><figure><iframe width="200" height="113" src="https://www.youtube.com/embed/bus4OGyMQec?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></figure><p>Have you ever dreamed of being able to edit any part of a picture with quick sketches or suggestions? Changing specific features like the eyes, eyebrows, or even the wheels of your car? Well, it is not only possible, but it has never been easier than now with this new model called EditGAN, and the results are really impressive! You can basically improve, or meme-ify, any pictures super quickly. Indeed, you can control whatever feature you want from quick drafts, and it will only edit the modifications keeping the rest of the image the same. Control has been sought and extremely challenging to obtain with image synthesis and image editing AI models like GANs.</p><figure><img src="https://cdn-images-1.medium.com/max/800/1*3ap4LbF74PT6UpqJJVNuhA.gif" alt="" loading="lazy"/><figcaption>Result examples of EditGAN. Image by <a href="https://nv-tlabs.github.io/editGAN/" data-href="https://nv-tlabs.github.io/editGAN/" rel="noopener" target="_blank">Ling et al., 2021, EditGAN</a>.</figcaption></figure><p>As we said, this fantastic new paper from NVIDIA, the University of Toronto, and MIT allows you to edit any picture with superb control over specific features from sketch inputs. Typically, controlling specific features requires huge datasets and experts to know which features to change within the model to have the desired output image with only the wanted features changed. Instead, EditGAN learns through only a handful of examples of labeled examples to match segmentations to images, allowing you to edit the images with segmentation, or in other words, with quick sketches. It preserves the full image quality while allowing a level of detail and freedom never achieved before. This is such a great jump forward, but what’s even cooler is how they achieved that, so let’s dive a bit deeper into their method!</p><figure><a href="http://eepurl.com/huGLT5"><img src="https://cdn-images-1.medium.com/max/800/0*yte4qK4EyDMJGPc4.png" alt="" loading="lazy"/></a></figure><p>First, the model uses StyleGAN2 to generate images, which is the best image generation model available at the time of the publication and is widely used in research. I won’t dive into the details of this model since I already covered it in <a href="https://pub.towardsai.net/how-ai-generates-new-images-gans-put-simply-674e413bc22a" rel="noopener">numerous articles</a> with different applications if you’d like to learn more about it. Instead, I will assume you have a basic knowledge of what StyleGAN2 does: take an image, encode it into a condensed sub-space, and use a type of model called a generator to transform this encoded subspace into another image. This also works using directly encoded information instead of encoding an image to obtain this information. What’s important here is the generator.</p><figure><img src="https://cdn-images-1.medium.com/max/800/1*cis786j4TaEi0d9omHznsw.gif" alt="" loading="lazy"/><figcaption>How the generator of a GAN network works.</figcaption></figure><p>As I said, it will take information from a sub-space, often referred to as latent space, where we have a lot of information about our image and its features, but this space is multidimensional, and we can hardly visualize it. The challenge is to identify which part of this sub-space is responsible for reconstructing which feature in the image. This is where EditGAN comes into play, not only telling you which part of the sub-space does what but also allowing you to edit them automatically using another input: a sketch that you can easily draw. Indeed, it will encode your image or simply take a specific latent code and generate both the segmentation map of the picture and the picture itself. This means that both the segmentation and images are in the same sub-space by training a model to do that, and it allows for the control of only the desired features without you having to do anything else as you simply need to change the segmentation image and the other will follow. The training will only be on this new segmentation generation, and the StyleGAN generator will stay fixed for the original image. This will allow the model to understand and link the segmentations to the same sub-space needed for the generator to reconstruct the image. Then, if trained correctly, you can simply edit this segmentation, and it will change the image accordingly!</p><figure><img src="https://cdn-images-1.medium.com/max/800/1*Vao24IjhKn6JsoQkd1akwg.png" alt="" loading="lazy"/><figcaption>EditGAN overview (1) and process (2,3,4). Image by <a href="https://nv-tlabs.github.io/editGAN/" data-href="https://nv-tlabs.github.io/editGAN/" rel="noopener" target="_blank">Ling et al., 2021, EditGAN</a>.</figcaption></figure><p>EditGAN will basically assign each pixel of your image to a specific class, such as head, ear, eye, etc., and control these classes independently using masks covering the pixels of other classes within the latent space.</p><figure><img src="https://cdn-images-1.medium.com/max/800/1*LxsUowNAzB4eGgbkwjxoyg.jpeg" alt="" loading="lazy"/><figcaption>Bird segmentation map. Image by <a href="https://nv-tlabs.github.io/editGAN/" data-href="https://nv-tlabs.github.io/editGAN/" rel="noopener" target="_blank">Ling et al., 2021, EditGAN</a>.</figcaption></figure><p>So each pixel will have its label, and EditGAN will decide which label to edit instead of which pixel directly in the latent space and reconstruct the image modifying only the editing region. And voilà! By connecting a generated image with a segmentation map, EditGAN allows you to edit this map as you wish and apply these modifications to the image, creating a new version!</p><figure><a href="https://www.louisbouchard.ai/learnai/"><img src="https://cdn-images-1.medium.com/max/800/0*NT2-SGQjCX3M3Wsu.png" alt="" loading="lazy"/></a></figure><p>Of course, after training with these examples, it works with unseen images.</p><h3 id="ai-ethics-segment-by-martina-todaro">AI ethics segment by Martina Todaro</h3><p>«Just recently, a new law in Norway makes it illegal for advertisers and social media influencers to share retouched promotional photos online when not disclosed. The amendment requires disclosures for edits made after the image was taken and before, such as Snapchat and Instagram filters that modify one&#39;s appearance. According to <a href="https://www.vice.com/en/article/g5gd99/norway-law-forces-influencers-to-label-retouched-photos-on-instagram">Vice</a>, examples of edits that people who are being paid for pictures are required to label include &#34;enlarged lips, narrowed waists, and exaggerated muscles,&#34; among other things <a href="https://www.insider.com/norway-law-social-media-influencers-advertisers-disclose-edited-images-2021-7">[1]</a>.» Can we predict if it will be a norm in other countries too in the future? </p><p>Thank you once again to <a href="https://wandb.ai/site" rel="noopener ugc nofollow noopener">Weights &amp; Biases</a> for sponsoring the video and article and to you that is still reading! See you next week with a very special and exciting video about a subject I love!</p><hr/><p>If you like my work and want to stay up-to-date with AI, you should definitely follow me on my other social media accounts (<a href="https://www.linkedin.com/in/whats-ai/" rel="noopener ugc nofollow noopener">LinkedIn</a>, <a href="https://twitter.com/Whats_AI" rel="noopener ugc nofollow noopener">Twitter</a>) and subscribe to my weekly AI <a href="http://eepurl.com/huGLT5" rel="noopener ugc nofollow noopener"><strong>newsletter</strong></a>!</p><h4 id="to-support-me">To support me:</h4><ul><li>The best way to support me is by being a member of this website<strong> </strong>or subscribing to my channel<strong> </strong>on <strong>YouTube </strong>if you like the video format.</li><li>Follow me here or on <a href="https://whats-ai.medium.com/" rel="noopener"><strong>medium</strong></a></li><li>Want to get into AI or improve your skills, <a href="https://www.louisbouchard.ai/learnai/" rel="noopener ugc nofollow noopener">read this</a>!</li></ul><h4 id="references">References</h4><ul><li>Ling, H., Kreis, K., Li, D., Kim, S.W., Torralba, A. and Fidler, S., 2021, May. EditGAN: High-Precision Semantic Image Editing. In <em>Thirty-Fifth Conference on Neural Information Processing Systems</em>.</li><li>Code and interactive tool (arriving soon): <a href="https://nv-tlabs.github.io/editGAN/" rel="nofollow noopener">https://nv-tlabs.github.io/editGAN/</a></li></ul>
    </section>
    

    

</article></div>
  </body>
</html>
