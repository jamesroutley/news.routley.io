<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/lucidrains/imagen-pytorch">Original</a>
    <h1>Imagen Open Source implementation from the same developer as the one for DALL-E2</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/lucidrains/imagen-pytorch/blob/main/imagen.png"><img src="https://github.com/lucidrains/imagen-pytorch/raw/main/imagen.png" width="450px"/></a></p>
<h2 dir="auto"><a id="user-content-imagen---pytorch-wip" aria-hidden="true" href="#imagen---pytorch-wip"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Imagen - Pytorch (wip)</h2>
<p dir="auto">Implementation of <a href="https://gweb-research-imagen.appspot.com/" rel="nofollow">Imagen</a>, Google&#39;s Text-to-Image Neural Network that beats DALL-E2, in Pytorch. It is the new SOTA for text-to-image synthesis.</p>
<p dir="auto">Architecturally, it is actually much simpler than DALL-E2. It composes of a cascading DDPM conditioned on text embeddings from a large pretrained T5 model (attention network). It also contains dynamic clipping for improved classifier free guidance, noise level conditioning, and a memory efficient unet design.</p>
<p dir="auto">It appears neither CLIP nor prior network is needed after all. And so research continues.</p>
<p dir="auto">Please join <a href="https://discord.gg/xBPBXfcFHd" rel="nofollow"><img alt="Join us on Discord" src="https://camo.githubusercontent.com/6752f150c04aa4fb06db9d04d900329ef595a2376ea06860a1e834c89e5ab725/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3832333831333135393539323030313533373f636f6c6f723d353836354632266c6f676f3d646973636f7264266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/discord/823813159592001537?color=5865F2&amp;logo=discord&amp;logoColor=white"/></a> if you are interested in helping out with the replication with the <a href="https://laion.ai/" rel="nofollow">LAION</a> community</p>
<h2 dir="auto"><a id="user-content-install" aria-hidden="true" href="#install"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Install</h2>
<div data-snippet-clipboard-copy-content="$ pip install imagen-pytorch"><pre>$ pip install imagen-pytorch</pre></div>
<h2 dir="auto"><a id="user-content-usage" aria-hidden="true" href="#usage"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Usage</h2>
<div data-snippet-clipboard-copy-content="import torch
from imagen_pytorch import Unet, Imagen

# unet for imagen

unet1 = Unet(
    dim = 32,
    cond_dim = 128,
    channels = 3,
    dim_mults = (1, 2, 4, 8),
    layer_attns = (False, True, True, True)
)

unet2 = Unet(
    dim = 32,
    cond_dim = 128,
    channels = 3,
    dim_mults=(1, 2, 4, 8),
    layer_attns = False
)

# imagen, which contains the unets above (base unet and super resoluting ones)

imagen = Imagen(
    unets = (unet1, unet2),
    image_sizes = (64, 256),
    beta_schedules = (&#39;cosine&#39;, &#39;linear&#39;),
    timesteps = 1000,
    cond_drop_prob = 0.5
).cuda()

# mock images (get a lot of this) and text encodings from large T5

text_embeds = torch.randn(4, 256, 768).cuda()
images = torch.randn(4, 3, 256, 256).cuda()

# feed images into imagen, training each unet in the cascade

for i in (1, 2):
    loss = imagen(images, text_embeds = text_embeds, unet_number = i)
    loss.backward()

# do the above for many many many many steps
# now you can sample an image based on the text embeddings from the cascading ddpm

images = imagen.sample(texts = [
    &#39;a whale breaching from afar&#39;,
    &#39;young girl blowing out candles on her birthday cake&#39;,
    &#39;fireworks with blue and green sparkles&#39;
], cond_scale = 2.)

images.shape # (3, 3, 256, 256)"><pre><span>import</span> <span>torch</span>
<span>from</span> <span>imagen_pytorch</span> <span>import</span> <span>Unet</span>, <span>Imagen</span>

<span># unet for imagen</span>

<span>unet1</span> <span>=</span> <span>Unet</span>(
    <span>dim</span> <span>=</span> <span>32</span>,
    <span>cond_dim</span> <span>=</span> <span>128</span>,
    <span>channels</span> <span>=</span> <span>3</span>,
    <span>dim_mults</span> <span>=</span> (<span>1</span>, <span>2</span>, <span>4</span>, <span>8</span>),
    <span>layer_attns</span> <span>=</span> (<span>False</span>, <span>True</span>, <span>True</span>, <span>True</span>)
)

<span>unet2</span> <span>=</span> <span>Unet</span>(
    <span>dim</span> <span>=</span> <span>32</span>,
    <span>cond_dim</span> <span>=</span> <span>128</span>,
    <span>channels</span> <span>=</span> <span>3</span>,
    <span>dim_mults</span><span>=</span>(<span>1</span>, <span>2</span>, <span>4</span>, <span>8</span>),
    <span>layer_attns</span> <span>=</span> <span>False</span>
)

<span># imagen, which contains the unets above (base unet and super resoluting ones)</span>

<span>imagen</span> <span>=</span> <span>Imagen</span>(
    <span>unets</span> <span>=</span> (<span>unet1</span>, <span>unet2</span>),
    <span>image_sizes</span> <span>=</span> (<span>64</span>, <span>256</span>),
    <span>beta_schedules</span> <span>=</span> (<span>&#39;cosine&#39;</span>, <span>&#39;linear&#39;</span>),
    <span>timesteps</span> <span>=</span> <span>1000</span>,
    <span>cond_drop_prob</span> <span>=</span> <span>0.5</span>
).<span>cuda</span>()

<span># mock images (get a lot of this) and text encodings from large T5</span>

<span>text_embeds</span> <span>=</span> <span>torch</span>.<span>randn</span>(<span>4</span>, <span>256</span>, <span>768</span>).<span>cuda</span>()
<span>images</span> <span>=</span> <span>torch</span>.<span>randn</span>(<span>4</span>, <span>3</span>, <span>256</span>, <span>256</span>).<span>cuda</span>()

<span># feed images into imagen, training each unet in the cascade</span>

<span>for</span> <span>i</span> <span>in</span> (<span>1</span>, <span>2</span>):
    <span>loss</span> <span>=</span> <span>imagen</span>(<span>images</span>, <span>text_embeds</span> <span>=</span> <span>text_embeds</span>, <span>unet_number</span> <span>=</span> <span>i</span>)
    <span>loss</span>.<span>backward</span>()

<span># do the above for many many many many steps</span>
<span># now you can sample an image based on the text embeddings from the cascading ddpm</span>

<span>images</span> <span>=</span> <span>imagen</span>.<span>sample</span>(<span>texts</span> <span>=</span> [
    <span>&#39;a whale breaching from afar&#39;</span>,
    <span>&#39;young girl blowing out candles on her birthday cake&#39;</span>,
    <span>&#39;fireworks with blue and green sparkles&#39;</span>
], <span>cond_scale</span> <span>=</span> <span>2.</span>)

<span>images</span>.<span>shape</span> <span># (3, 3, 256, 256)</span></pre></div>
<p dir="auto">With the <code>ImagenTrainer</code> wrapper class, the exponential moving averages for all of the U-nets in the cascading DDPM will be automatically taken care of when calling <code>update</code></p>
<div data-snippet-clipboard-copy-content="import torch
from imagen_pytorch import Unet, Imagen, ImagenTrainer

# unet for imagen

unet1 = Unet(
    dim = 32,
    cond_dim = 512,
    channels = 3,
    dim_mults = (1, 2, 4, 8),
    layer_attns = (False, True, True, True)
)

unet2 = Unet(
    dim = 32,
    cond_dim = 512,
    channels = 3,
    dim_mults = (1, 2, 4, 8),
    layer_attns = False
)

# imagen, which contains the unets above (base unet and super resoluting ones)

imagen = Imagen(
    unets = (unet1, unet2),
    text_encoder_name = &#39;t5-large&#39;,
    image_sizes = (64, 256),
    beta_schedules = (&#39;cosine&#39;, &#39;linear&#39;),
    timesteps = 1000,
    cond_drop_prob = 0.5
).cuda()

# wrap imagen with the trainer class

trainer = ImagenTrainer(imagen)

# mock images (get a lot of this) and text encodings from large T5

text_embeds = torch.randn(4, 256, 1024).cuda()
images = torch.randn(4, 3, 256, 256).cuda()

# feed images into imagen, training each unet in the cascade

for i in (1, 2):
    loss = trainer(images, text_embeds = text_embeds, unet_number = i)
    trainer.update(unet_number = i)

# do the above for many many many many steps
# now you can sample an image based on the text embeddings from the cascading ddpm

images = trainer.sample(texts = [
    &#39;a puppy looking anxiously at a giant donut on the table&#39;,
    &#39;the milky way galaxy in the style of monet&#39;
], cond_scale = 2.)

images.shape # (2, 3, 256, 256)"><pre><span>import</span> <span>torch</span>
<span>from</span> <span>imagen_pytorch</span> <span>import</span> <span>Unet</span>, <span>Imagen</span>, <span>ImagenTrainer</span>

<span># unet for imagen</span>

<span>unet1</span> <span>=</span> <span>Unet</span>(
    <span>dim</span> <span>=</span> <span>32</span>,
    <span>cond_dim</span> <span>=</span> <span>512</span>,
    <span>channels</span> <span>=</span> <span>3</span>,
    <span>dim_mults</span> <span>=</span> (<span>1</span>, <span>2</span>, <span>4</span>, <span>8</span>),
    <span>layer_attns</span> <span>=</span> (<span>False</span>, <span>True</span>, <span>True</span>, <span>True</span>)
)

<span>unet2</span> <span>=</span> <span>Unet</span>(
    <span>dim</span> <span>=</span> <span>32</span>,
    <span>cond_dim</span> <span>=</span> <span>512</span>,
    <span>channels</span> <span>=</span> <span>3</span>,
    <span>dim_mults</span> <span>=</span> (<span>1</span>, <span>2</span>, <span>4</span>, <span>8</span>),
    <span>layer_attns</span> <span>=</span> <span>False</span>
)

<span># imagen, which contains the unets above (base unet and super resoluting ones)</span>

<span>imagen</span> <span>=</span> <span>Imagen</span>(
    <span>unets</span> <span>=</span> (<span>unet1</span>, <span>unet2</span>),
    <span>text_encoder_name</span> <span>=</span> <span>&#39;t5-large&#39;</span>,
    <span>image_sizes</span> <span>=</span> (<span>64</span>, <span>256</span>),
    <span>beta_schedules</span> <span>=</span> (<span>&#39;cosine&#39;</span>, <span>&#39;linear&#39;</span>),
    <span>timesteps</span> <span>=</span> <span>1000</span>,
    <span>cond_drop_prob</span> <span>=</span> <span>0.5</span>
).<span>cuda</span>()

<span># wrap imagen with the trainer class</span>

<span>trainer</span> <span>=</span> <span>ImagenTrainer</span>(<span>imagen</span>)

<span># mock images (get a lot of this) and text encodings from large T5</span>

<span>text_embeds</span> <span>=</span> <span>torch</span>.<span>randn</span>(<span>4</span>, <span>256</span>, <span>1024</span>).<span>cuda</span>()
<span>images</span> <span>=</span> <span>torch</span>.<span>randn</span>(<span>4</span>, <span>3</span>, <span>256</span>, <span>256</span>).<span>cuda</span>()

<span># feed images into imagen, training each unet in the cascade</span>

<span>for</span> <span>i</span> <span>in</span> (<span>1</span>, <span>2</span>):
    <span>loss</span> <span>=</span> <span>trainer</span>(<span>images</span>, <span>text_embeds</span> <span>=</span> <span>text_embeds</span>, <span>unet_number</span> <span>=</span> <span>i</span>)
    <span>trainer</span>.<span>update</span>(<span>unet_number</span> <span>=</span> <span>i</span>)

<span># do the above for many many many many steps</span>
<span># now you can sample an image based on the text embeddings from the cascading ddpm</span>

<span>images</span> <span>=</span> <span>trainer</span>.<span>sample</span>(<span>texts</span> <span>=</span> [
    <span>&#39;a puppy looking anxiously at a giant donut on the table&#39;</span>,
    <span>&#39;the milky way galaxy in the style of monet&#39;</span>
], <span>cond_scale</span> <span>=</span> <span>2.</span>)

<span>images</span>.<span>shape</span> <span># (2, 3, 256, 256)</span></pre></div>
<h2 dir="auto"><a id="user-content-shoutouts" aria-hidden="true" href="#shoutouts"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Shoutouts</h2>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://stability.ai/" rel="nofollow">StabilityAI</a> for the generous sponsorship, as well as my other sponsors out there</p>
</li>
<li>
<p dir="auto"><a href="https://huggingface.co/" rel="nofollow"><g-emoji alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png">🤗</g-emoji> Huggingface</a> for their amazing transformers library. The text encoder portion is pretty much taken care of because of them</p>
</li>
<li>
<p dir="auto">You? It isn&#39;t done yet, chip in if you are a researcher or skilled ML engineer</p>
</li>
</ul>
<h2 dir="auto"><a id="user-content-todo" aria-hidden="true" href="#todo"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Todo</h2>
<ul>
<li> use huggingface transformers for T5-small text embeddings</li>
<li> add dynamic thresholding</li>
<li> add dynamic thresholding DALLE2 and video-diffusion repository as well</li>
<li> allow for one to set T5-large (and perhaps small factory method to take in any huggingface transformer)</li>
<li> add the lowres noise level with the pseudocode in appendix, and figure out what is this sweep they do at inference time</li>
<li> port over some training code from DALLE2</li>
<li> need to be able to use a different noise schedule per unet (cosine was used for base, but linear for SR)</li>
<li> just make one master-configurable unet</li>
<li> complete resnet block (biggan inspired? but with groupnorm) - complete self attention</li>
<li> complete conditioning embedding block (and make it completely configurable, whether it be attention, film etc)</li>
<li> add attention pooling option, in addition to cross attention and film</li>
<li> figure out if learned variance was used at all, and remove it if it was inconsequential</li>
<li> switch to continuous timesteps instead of discretized, as it seems that is what they used for all stages - first figure out the linear noise schedule case from the variational ddpm paper <a href="https://openreview.net/forum?id=2LdBqxc1Yv" rel="nofollow">https://openreview.net/forum?id=2LdBqxc1Yv</a></li>
<li> exercise efficient attention expertise + explore skip layer excitation</li>
</ul>
<h2 dir="auto"><a id="user-content-citations" aria-hidden="true" href="#citations"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Citations</h2>
<div data-snippet-clipboard-copy-content="@inproceedings{Saharia2022PhotorealisticTD,
    title   = {Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
    author  = {Chitwan Saharia and William Chan and Saurabh Saxena and Lala Li and Jay Whang and Emily L. Denton and Seyed Kamyar Seyed Ghasemipour and Burcu Karagol Ayan and Seyedeh Sara Mahdavi and Raphael Gontijo Lopes and Tim Salimans and Jonathan Ho and David Fleet and Mohammad Norouzi},
    year    = {2022}
}"><pre><span>@inproceedings</span>{<span>Saharia2022PhotorealisticTD</span>,
    <span>title</span>   = <span><span>{</span>Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding<span>}</span></span>,
    <span>author</span>  = <span><span>{</span>Chitwan Saharia and William Chan and Saurabh Saxena and Lala Li and Jay Whang and Emily L. Denton and Seyed Kamyar Seyed Ghasemipour and Burcu Karagol Ayan and Seyedeh Sara Mahdavi and Raphael Gontijo Lopes and Tim Salimans and Jonathan Ho and David Fleet and Mohammad Norouzi<span>}</span></span>,
    <span>year</span>    = <span><span>{</span>2022<span>}</span></span>
}</pre></div>
</article>
          </div></div>
  </body>
</html>
