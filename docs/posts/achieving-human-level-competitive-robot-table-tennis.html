<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://sites.google.com/view/competitive-robot-table-tennis/home?pli=1">Original</a>
    <h1>Achieving Human Level Competitive Robot Table Tennis</h1>
    
    <div id="readability-page-1" class="page"><div tabindex="-1" dir="ltr"><section id="h.4a9d518ede350020_0"><div><div tabindex="-1"><div><div><div jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId"><div><div id="h.2f7a5920284bf616_16"><div><div role="main" tabindex="0"><p dir="ltr"><span>David B. D&#39;Ambrosio</span><span>¹*</span><span>, Saminda Abeyruwan</span><span>¹*</span><span>, Laura Graesser</span><span>¹*</span><span>, Atil Iscen</span><span>¹</span><span>, Heni Ben Amor², Alex Bewley², Barney J. Reed²^, Krista Reymann², Leila Takayama²+, Yuval Tassa², Krzysztof Choromanski, Erwin Coumans, Deepali Jain, Navdeep Jaitly, Natasha Jaques, Satoshi Kataoka, Yuheng Kuang, Nevena Lazic, Reza Mahjourian, Sherry Moore, Kenneth Oslund, Anish Shankar, Vikas Sindhwani, Vincent Vanhoucke,</span><span><br/></span><span>Grace Vesom, Peng Xu, and Pannag R. Sanketi</span><span>¹</span></p><p dir="ltr"><span>Google DeepMind</span><span><br/></span><span>*: </span><span>C</span><span>orresponding authors (equal contribution, order randomized), ¹: Primary contributors, ²: Core contributors (alphabetized)</span><span><br/></span><span>^: work done at Google DeepMind via Stickman Studios LLC, +: work done at Google DeepMind via Hoku Labs</span></p><p dir="ltr"><a href="https://arxiv.org/abs/2408.03906" target="_blank"><span>Paper</span></a><span> |  </span><a href="https://youtu.be/EqQl-JQxToE" target="_blank"><span>Highlights</span></a><span> | </span><a href="https://sites.google.com/view/competitive-robot-table-tennis/videos"><span>Full Length Match Videos</span></a></p></div></div></div></div></div></div></div></div></div></section><section id="h.59bc57a00cd0dade_172"><div><div tabindex="-1"><div><div><div jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId"><div><div id="h.579e97048230af95_14"><div><div><p dir="ltr"><span>Achieving human-level speed and performance on real world tasks is a north star for the robotics research community. This work takes a step towards that goal and presents the first learned robot agent that reaches amateur human-level performance in competitive table tennis. Table tennis is a physically demanding sport which requires human players to undergo years of training to achieve an advanced level of proficiency. In this paper, we contribute (1) a hierarchical and modular policy architecture consisting of (i) low level controllers with their detailed skill descriptors which model the agent&#39;s capabilities and help to bridge the sim-to-real gap and (ii) a high level controller that chooses the low level skills, (2) techniques for enabling zero-shot sim-to-real including an iterative approach to defining the task distribution that is grounded in the real-world and defines an automatic curriculum, and (3) real time adaptation to unseen opponents. Policy performance was assessed through 29 robot vs. human matches of which the robot won 45% (13/29). All humans were unseen players and their skill level varied from beginner to tournament level. Whilst the robot lost all matches vs. the most advanced players it won 100% matches vs. beginners and 55% matches vs. intermediate players, demonstrating solidly amateur human-level performance.</span></p><br/></div></div></div></div></div></div></div><div><div><div jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId"><div><div id="h.4a02c7177784aef6_0"><div><div><p><img src="https://lh6.googleusercontent.com/r75ueHaA7MFYKb4Xz8p5w45qcryA0UoLT2gDrw8NjXR3mKyGi9XN4UE8JlPz6R8H5H7XuY-_3XJvMzpc59d8svwsKZa4KUzJN5VGWHWtr19FKHLTZdd_0-vU-ATHqxY8Ug=w1280" role="img"/></p></div></div></div></div></div></div></div></div></div></section><section id="h.579e97048230af95_15"><div><div tabindex="-1"><div><div><div jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId"><div><div id="h.31dc94627cafd6cb_11"><div><div><div jscontroller="VYKRW" jsaction="rcuQ6b:rcuQ6b;"><div><iframe jsname="L5Fo6c" sandbox="allow-scripts allow-popups allow-forms allow-same-origin allow-popups-to-escape-sandbox allow-downloads allow-modals allow-storage-access-by-user-activation" frameborder="0" aria-label="YouTube Video, TT demonstrations" src="https://www.youtube.com/embed/abi84lnjNV4?embed_config=%7B%22enc%22:%22AXH1ezk0Hmbd1obvwrn_6iRTlZMGNzegPdNf3iOJghCoA4vtQ_PlnJgpsGVuebQ8Yo81jbhK91gSEUhWC5_1Xi7NMqj6ybOhHgBnB0F8XjInCs2Bs2apdlrT8YtcFsjPTXVlRnHKNMhJIctRYeWAamqJ0vPRvk5Tz3eU8USckSTTWHga%22%7D&amp;errorlinks=1" allowfullscreen=""></iframe></div></div></div></div></div></div><div><div id="h.579e97048230af95_20"><div><div><h2 id="h.783is43nw5l0_l" dir="ltr"><div jscontroller="Ae65rd" jsaction="touchstart:UrsOsc; click:KjsqPd; focusout:QZoaZ; mouseover:y0pDld; mouseout:dq0hvd;fv1Rjc:jbFSOd;CrfLRd:SzACGe;"><p><span>Coach Barney Demonstrates Capabilities</span></p></div></h2></div></div></div></div></div></div></div><div><div><div jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId"><div><div id="h.31dc94627cafd6cb_19"><div><div><div jscontroller="VYKRW" jsaction="rcuQ6b:rcuQ6b;"><div><iframe jsname="L5Fo6c" sandbox="allow-scripts allow-popups allow-forms allow-same-origin allow-popups-to-escape-sandbox allow-downloads allow-modals allow-storage-access-by-user-activation" frameborder="0" aria-label="YouTube Video, Toaster Highlights still wip" src="https://www.youtube.com/embed/EqQl-JQxToE?embed_config=%7B%22enc%22:%22AXH1ezlrSaBCemQRLtbBvfjkrZbqSWL16ZXxrLwDyLNcVjCxGvcdL5ZN4P72vY5Ii7w8flvdZNBuzCjXKutj1s9n6EyFHQLSP-Ul6lRGak8sj8DZEF7o1fpBTEc_KYE2K1M2UQqSYzSQwgRIVWdKAAYlACoqacqxmJTc-5XgH4dyzhpz%22%7D&amp;errorlinks=1" allowfullscreen=""></iframe></div></div></div></div></div></div></div></div></div></div></div></section><section id="h.42fbba6cc2ee6e20_19"><div><div tabindex="-1"><div><div><div jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId"><div><div id="h.42fbba6cc2ee6e20_16"><div><div><p dir="ltr"><span>Truly awesome to watch the robot play players of all levels and styles. Going in our aim was to have the robot be at an intermediate level. Amazingly it did just that, all the hard work paid off.</span></p><p dir="ltr"><span>I feel the robot exceeded even my expectations. It was a true honor and pleasure to be a part of this research. I have learned so much and am very thankful for everyone I had the pleasure of working with on this.</span></p><p dir="ltr"><span>- Barney J. Reed, Professional Table Tennis Coach</span></p></div></div></div></div></div></div></div></div></div></section><section id="h.19d8a2d7f2b570e7_27"></section><section id="h.19d8a2d7f2b570e7_3"><div><div tabindex="-1"><div><div><div jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId"><div><div id="h.19d8a2d7f2b570e7_0"><div><div><ul><li dir="ltr"><p dir="ltr" role="presentation"><span>Achieving human-level performance in terms of accuracy, speed, and adaptability remains a challenge for robotics in spite of inspiring recent progress.</span></p></li><li dir="ltr"><p dir="ltr" role="presentation"><span>Table Tennis requires years of training for humans to master due to its complex low level skills and strategic gameplay. </span><span>A strategically suboptimal - but confidently executable - low</span><span> </span><span>level skill might be a better choice. This sets </span><span>Table Tennis</span><span> apart from purely strategic games such as Chess, Go, etc. </span></p></li><li dir="ltr"><p dir="ltr" role="presentation"><span>Table Tennis is thus a valuable benchmark for advancing robotic capabilities, including high-speed motion, real-time precise and strategic decision making, system design and </span><span>enabling direct competition with a human opponent</span><span>.</span></p></li><li dir="ltr"><p dir="ltr" role="presentation"><span>No prior research in robotic Table Tennis has addressed the challenge of a robot playing a full competitive game against previously unseen humans.</span></p></li></ul></div></div></div></div></div></div></div><div><div><div jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId"><div><div id="h.39d7b4ec8f9134af_3"><div><div><p><img src="https://lh3.googleusercontent.com/4N8u65x3q7shqzkZCuQDzpBVKYxsAxW30Ro74Vzt317iLPXy-AZxKLnjADeobpgp-REvlvmPGv4yjeCsMrNY3r2KPuE0np-4yFxIZGCMM5M9vbsrXLCjxA7Rv8pZCvRiaA=w1280" role="img"/></p></div></div></div></div></div></div></div></div></div></section><section id="h.42fbba6cc2ee6e20_11"></section><section id="h.42fbba6cc2ee6e20_15"><div><div tabindex="-1"><div><div><div jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId"><div><div id="h.42fbba6cc2ee6e20_12"><div><div><p dir="ltr"><span>Our approach lead to</span><span> competitive play at human level and </span><span>a robot agent </span><span>that humans actually enjoy playing with</span><span>. To achieve this we </span><span>make</span><span> </span><span>four</span><span> technical contributions:</span></p><ol><li dir="ltr"><p dir="ltr" role="presentation"><span> </span><span>A</span><span> hierarchical and modular policy architecture</span></p></li><li dir="ltr"><p dir="ltr" role="presentation"><span> </span><span>T</span><span>echniques to enable zero-shot sim-to-real including an iterative approach to defining the training task distribution that is grounded in the real-world </span></p></li><li dir="ltr"><p dir="ltr" role="presentation"><span> </span><span>R</span><span>eal time adaptation to unseen opponents</span></p></li><li dir="ltr"><p dir="ltr" role="presentation"><span> </span><span>A</span><span> user-study to test our model playing actual matches against unseen humans in physical environments</span></p></li></ol></div></div></div></div></div></div></div></div></div></section><section id="h.19d8a2d7f2b570e7_35"></section><section id="h.19d8a2d7f2b570e7_11"><div><div tabindex="-1"><div><div><div jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId"><div><div id="h.19d8a2d7f2b570e7_8"><div><div><p dir="ltr"><span>The</span><span> </span><span>agent</span><span> consists of a </span><span>library of </span><span>low-level skill</span><span>s</span><span> and high-level controller that selects </span><span>the most effective skill</span><span>. Each low-level skill policy specializes in a specific aspect of table tennis, such as forehand topspin, backhand targeting, or forehand serve. In addition to </span><span>training</span><span> the policy itself, we collect and store information both offline and online about the strengths, weaknesses, and limitations of each low-level skill. The resulting </span><span>skill descriptors</span><span> provide the robot with important information regarding its abilities and shortcomings. In turn, a high-level controller, responsible for orchestrating the low-level skills, selects the optimal skill given the current game statistics, skill descriptors and the opponent&#39;s capabilities.</span></p><p dir="ltr"><span>We collect a small amount of human-human play data to seed the initial task conditions. We then train an agent in simulation using RL and employ a number of techniques (known and novel) to deploy the policy zero-shot to real hardware. This agent plays with humans to generate more training task conditions and the training-deployment cycle is repeated. As the robot improves, the standard of play becomes progressively more complex whilst remaining grounded in real-world task conditions.</span><span> </span><span>This hybrid sim-real cycle creates an automatic task curriculum and enables the robot&#39;s skills to improve over time.</span></p></div></div></div></div></div></div></div></div></div></section><section id="h.2db562aa0a8dbe88_0"><div><div tabindex="-1"><div><div><div jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId"><div><div id="h.19d8a2d7f2b570e7_12"><div><div><p><img src="https://lh5.googleusercontent.com/f33GPXukFTzcuBi2sMJkJ4h92BlO2MAv2nxTtQBGnvXSHUXmX80usEsRMwX6ILZjqyjHT6Pmm3ODjuRvpsPqPtXgEsDZZII1KGTqfRnNxWeiC0GWIbaUwRc7Ac50WwnfYg=w1280" role="img"/></p></div></div></div></div></div></div></div></div></div></section><section id="h.59bc57a00cd0dade_156"><div><div tabindex="-1"><div><div><div jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId"><div><div id="h.bcb601a725a66d3_0"><div><div><p><img src="https://lh4.googleusercontent.com/BxBtxKVbO3MIJ5fAiULHx1bb1Sx9OSLU_uQQQpFObM8j_xNh9jbXQIRbALnKnDgtEk7reAVxuAzHcy3NPYsKZT8x_RXHU_fmscqJtY9zs3BVF6b2uvy6M2ikhi-D8I4vbw=w1280" role="img"/></p></div></div></div></div></div></div></div><div><div><div jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId"><div><div id="h.59bc57a00cd0dade_168"><div><div><h3 id="h.to9dvlc5q722_l" dir="ltr"></h3><ul><li dir="ltr"><p dir="ltr" role="presentation"><span>Select Style</span><span>: T</span><span>he high-level controller (HLC) first decides which style to use (Forehand / Backhand).</span></p></li><li dir="ltr"><p dir="ltr" role="presentation"><span>Adapt</span><span>: </span></p><ul><li dir="ltr"><p dir="ltr" role="presentation"><span>Shortlist</span><span> LLCs using their skill descriptors using tree-search, heuristics and the Opponent&#39;s Stats (strengths / weaknesses).</span></p></li><li dir="ltr"><p dir="ltr" role="presentation"><span>Maintain online </span><span>preferences (H-values) </span><span>for each LLC based on the game stats against the opponent.</span></p></li></ul></li><li dir="ltr"><p dir="ltr" role="presentation"><span>Pick the most effective skill:</span><span> </span><span>HLC samples the shortlisted LLCs according to the adapted H-values. </span></p></li><li dir="ltr"><p dir="ltr" role="presentation"><span>Update</span><span>: The H-values and Opponent Stats are continuously updated until the match is over.</span></p></li></ul><br/></div></div></div></div></div></div></div></div></div></section><section id="h.59bc57a00cd0dade_69"><div><div tabindex="-1"><div><div><div jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId"><div><div id="h.59bc57a00cd0dade_74"><div><p dir="ltr"><span>To evaluate the skill level of our agent, we ran competitive matches  against  29  table  tennis  players  of  varying  skill levels  –  beginner,  intermediate,  advanced,  and  advanced+ as determined by a professional table tennis coach.  The humans played 3 games against the robot following standard table tennis rules with some modifications because the robot is physically unable to serve the ball.  Against all opponents, the robot won 45% of matches and 46% of games.  Broken down by skill level, we see the robot won all matches against beginners, lost all matches against the advanced and advanced+ players, and won 55% of matches against intermediate players.  This strongly suggests our agent achieved intermediate level human play on rallies.</span></p></div></div></div></div></div></div></div></div></section><section id="h.2f7a5920284bf616_71"><div><div tabindex="-1"><div><div><div jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId"><div><div id="h.2f7a5920284bf616_72"><div><div><p><img src="https://lh4.googleusercontent.com/4eHj0BJD60Z7L0BJFSZ6mNft3Qzp97dOyswWwWpoH5nsnn8bFVk87dpn_0ZWxOl56eGC0cCCLo8eSFscnWOIASy5WD6imE7w2YeJVD5KinS2tQzQ_pPLJBkLz4V3e23V=w1280" role="img"/></p></div></div></div></div></div></div></div><div><div><div jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId"><div><div id="h.2f7a5920284bf616_68"><div><div><p><img src="https://lh6.googleusercontent.com/osLIscLx0iodtfRycFkxqmg4QhB8T67BdDzF4PgGBH3IG8jYt2qxCYDonuEGFImY7uLqDO3Gz9wtsDX4eu66YNznG0OeD7IP3TTnlq7enmjTYMDxkGk7HahGvCw6St5zBA=w1280" role="img"/></p></div></div></div></div></div></div></div></div></div></section><section id="h.5c6ee3130405cd15_3"><div><div tabindex="-1"><div><div><div jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId"><div><div id="h.5c6ee3130405cd15_8"><div><div><h3 id="h.h2gnapn0ze60_l" dir="ltr"></h3><p dir="ltr"><span>Study participants enjoyed playing with the robot, </span><span>rating it highly on &#34;fun&#34; and &#34;engaging&#34;</span><span>.  This rating held true across different skill levels and whether the participant won or lost.  They also overwhelmingly responded </span><span>&#34;definitely yes&#34; to wanting to play again</span><span> with the robot. When given free play time with the robot they played for an average of 4:06 out of 5 minutes.</span></p><p dir="ltr"><span>Advanced players were able to exploit weaknesses in the robot&#39;s policies, but they still had fun playing with it. In post-match interviews saw the potential for it as a </span><span>more dynamic practice partner</span><span> than ball throwers.</span></p></div></div></div></div></div></div></div><div><div><div jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId"><div><div id="h.5c6ee3130405cd15_11"><div><div><p><img src="https://lh6.googleusercontent.com/DdLILushWdg9V2ht_wEMmN8MZRGnrrJShh6rpKpnCtWeK3qhLlQi8BMUOGEkxgT7KfTpY9TCLgYEtepwfQTPyGgrSx3ENmyfi48i8UTYfVPXjrRHf0gmkI8lTP2bLEKa5w=w1280" role="img"/></p></div></div></div></div></div></div></div><div><div><div jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId"><div><div id="h.5c6ee3130405cd15_4"><div><div><p><img src="https://lh4.googleusercontent.com/AHeT88a9jFWBao0AkEQGg3K58Ys0zBmYRfDZHphg5BWCy81-v8mzCSX4DNkGl-V4vv1-G8iT_SmPBuT-R3aRFgyB_jsVmy-r4ZCHtJ4VkRvY0bVgDO6eiAT6jTQEeVMRyA=w1280" role="img"/></p></div></div></div></div></div></div></div></div></div></section><section id="h.2f7a5920284bf616_102"><div><div tabindex="-1"><div><div><div jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId"><div><div id="h.2f7a5920284bf616_99"><div><div><p><img src="https://lh3.googleusercontent.com/hfTye4DofXW6pn5PsdDeoR2vgyQbBwZFiWA3LOSquGS-BEHQKOneVOrHKnNH_dXsYkkwByTlXwkgHxEJJ78JLOHQESKdz4u37gXl-8vspjt-kDpGnZsjc--VmjTh7Cs9Gg=w1280" role="img"/></p></div></div></div></div></div></div></div><div><div><div jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId"><div><div id="h.2f7a5920284bf616_103"><div><div><h3 id="h.u0f1zevi9vq6_l" dir="ltr"><div jscontroller="Ae65rd" jsaction="touchstart:UrsOsc; click:KjsqPd; focusout:QZoaZ; mouseover:y0pDld; mouseout:dq0hvd;fv1Rjc:jbFSOd;CrfLRd:SzACGe;"><p><span>Human Strategies and Policy Weaknesses</span></p></div></h3><p dir="ltr"><span>The most skilled players mentioned that the robot was not good at handling underspin.  To test this observation we plotted the robot&#39;s landing rate by the estimated spin on the ball and indeed saw a large dropoff as more underspin was faced.  This deficiency is partially due to the difficulty of handling low balls to avoid collision with the table and secondly determining ball spin in real time. Fortunately this provides clear feedback for additional training in our flywheel.</span></p></div></div></div></div></div></div></div></div></div></section><section id="h.59bc57a00cd0dade_85"></section><section id="h.5c6ee3130405cd15_18"><div><div tabindex="-1"><div><div><div jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId"><div><div id="h.453f4483d0aaa17b_4"><div><div><p><img src="https://lh4.googleusercontent.com/nmVNCvS_n2x3mXM4anSSDtyuGPUMoStptrRwNhzpVGtQCY_HKkx_Pu6vYPSD7qAhdudduWwgVbmPZdYoThDSaFfoBFUqyicGL-CJpVK_DU4-wYPOjUsdXrmK2PkEUVIcjg=w1280" role="img"/></p></div></div></div></div></div></div></div></div></div></section></div></div>
  </body>
</html>
