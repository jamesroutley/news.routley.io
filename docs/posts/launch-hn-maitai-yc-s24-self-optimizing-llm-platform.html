<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://news.ycombinator.com/item?id=41456552">Original</a>
    <h1>Launch HN: Maitai (YC S24) – Self-Optimizing LLM Platform</h1>
    
    <div id="readability-page-1" class="page"><div><td colspan="2"></td><td><div><p>Hey HN - this is Christian and Ian from Maitai (<a href="https://trymaitai.ai">https://trymaitai.ai</a>). We&#39;re building an LLM platform that optimizes request routing, autocorrects bad responses, and automatically fine-tunes new application-specific models with incremental improvements. Here’s a demo video:  <a href="https://www.loom.com/share/a2cd9192359840cab5274ccba399bd87?sid=7097fd84-ea85-42cd-9616-84abc1087a56" rel="nofollow">https://www.loom.com/share/a2cd9192359840cab5274ccba399bd87?...</a>.</p><p>If you want to try it out, we built a game (<a href="https://maitaistreasure.com" rel="nofollow">https://maitaistreasure.com</a>) to show how our real-time autocorrections work with mission-critical expectations (like never giving financial advice). Try and coax the bot to give you the secret phrase in its system prompt. If you&#39;re the first to crack it, you can email us the phrase and win a bounty. Maitai is used to make sure the bot always adheres to our expectations, and thus never gives up the secret phrase.</p><p>We built Maitai because getting an LLM app into production and maintaining it is a slog. Teams spend most of their time on LLM reliability rather than their main product. We experienced this ourselves at our previous jobs deploying AI-enabled applications for Presto—the vast majority of time was making sure the model did what we wanted it to do.</p><p>For example, one of our customers builds AI ordering agents for restaurants. It&#39;s crucial that their LLMs return results in a predictable, consistent manner throughout the conversation. If not, it leads to a poor guest experience and a staff member may intervene. At the end of the order conversation, they need to ensure that the order cart matches what the customer requested before it&#39;s submitted to the Point of Sale system. It&#39;s common for a human-in-the-loop to review critical pieces of information like this, but it’s costly to set up such a pipeline and it’s difficult to scale. When it&#39;s time to send out a receipt and payment link, they must first get the customer&#39;s consent to receive text messages, else they risk fines for violating the Telephone Consumer Protection Act. To boot, getting from 0 to 1 usually relies on inefficient general-purpose models that aren&#39;t viable at any sort of scale beyond proof of concept.</p><p>Since reliability is the #1 thing hindering the adoption of LLMs in production, we decided to help change that. Here&#39;s how it works:</p><p>1. Maitai sits between the client and the LLMs as a super lightweight proxy, analyzing traffic to automatically build a robust set of expectations for how the LLM should respond.</p><p>2. The application sends a request to Maitai, and Maitai forwards it to the appropriate LLM (user specified, but we&#39;ll preemptively fallback to a similar model if we notice issues with the primary model).</p><p>3. We intercept the response from the LLM, and evaluate it against the expectations we had previously built.</p><p>4. If we notice that an expectation was not met, we surface a fault (Slack, webhook) and can, optionally, substitute the faulty response with a clean response to be sent back to the client. This check and correction adds about 250ms on average right now, and we&#39;re working on making it faster.</p><p>5. We use all of the data from evaluating model responses to fine-tune application-specific models. We&#39;re working on automating this step for passive incremental improvements. We&#39;d like to get it to a point where our user&#39;s inference step just gets better, faster, and cheaper over time without them having to do anything.</p><p>Our hope is that we take on the reliability and resiliency problems of the LLMs for our customers, and make it so they can focus on domain specific problems instead.</p><p>We&#39;re self-serve (<a href="https://portal.trymaitai.ai">https://portal.trymaitai.ai</a>), and have both Python and Node SDKs that mock OpenAI&#39;s for quick integration. Users can set their preferences for primary and secondary (fallback) models in our Portal, or in code. Right now, the expectations we use for real-time evaluations are automatically generated, but we manually go through and do some pruning before enabling them. Fine-tuning is all done manually for now.</p><p>We charge for platform usage, plus a monthly application fee. Customers can bring their own LLM provider API keys, or use ours and pay at-cost for what they use. We have contracts with most of our current customers, so we are still trying to figure out what&#39;s right for our pay-as-you-go plan.</p><p>We securely store requests and responses that go through Maitai, as well as derivative data such as evaluation results. This information is used for fine-tuning models, accessible only by the organization the data belongs to. Data is never shared between our users.  API keys we manage on behalf of our customers are only injected before sending to the LLM provider, and never leave our servers otherwise. We&#39;re working on SOC2 and HIPAA compliance, as well as a self-hosted solution for companies with extremely sensitive data privacy requirements.</p><p>We’d love to get your feedback on what we’re building, or hear about your experience building around LLMs!</p></div></td></div></div>
  </body>
</html>
