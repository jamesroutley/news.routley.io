<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://bigdata.2minutestreaming.com/p/why-was-apache-kafka-created">Original</a>
    <h1>Why was Apache Kafka created?</h1>
    
    <div id="readability-page-1" class="page"><div><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!iezf!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ee51ab5-5a4e-4267-bf8a-dbce3dc2091b_1600x900.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!iezf!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ee51ab5-5a4e-4267-bf8a-dbce3dc2091b_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!iezf!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ee51ab5-5a4e-4267-bf8a-dbce3dc2091b_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!iezf!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ee51ab5-5a4e-4267-bf8a-dbce3dc2091b_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!iezf!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ee51ab5-5a4e-4267-bf8a-dbce3dc2091b_1600x900.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!iezf!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ee51ab5-5a4e-4267-bf8a-dbce3dc2091b_1600x900.png" width="728" height="409.5" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/5ee51ab5-5a4e-4267-bf8a-dbce3dc2091b_1600x900.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:false,&#34;imageSize&#34;:&#34;normal&#34;,&#34;height&#34;:819,&#34;width&#34;:1456,&#34;resizeWidth&#34;:728,&#34;bytes&#34;:1317126,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:false,&#34;topImage&#34;:true,&#34;internalRedirect&#34;:&#34;https://bigdata.2minutestreaming.com/i/170964904?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ee51ab5-5a4e-4267-bf8a-dbce3dc2091b_1600x900.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:&#34;center&#34;,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!iezf!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ee51ab5-5a4e-4267-bf8a-dbce3dc2091b_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!iezf!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ee51ab5-5a4e-4267-bf8a-dbce3dc2091b_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!iezf!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ee51ab5-5a4e-4267-bf8a-dbce3dc2091b_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!iezf!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ee51ab5-5a4e-4267-bf8a-dbce3dc2091b_1600x900.png 1456w" sizes="100vw" fetchpriority="high"/></picture><div><div></div></div></div></a></figure></div><p><em>Reading Time</em><span>: </span><strong>13 minutes</strong></p><p>We talk all the time about what Kafka is, but not so much about why it is the way it is.</p><p>What better way than to dive into the original motivation for creating Kafka?</p><p>Circa 2012, LinkedIn’s original intention with Kafka was to solve a data integration problem.</p><p><span>LinkedIn used site activity data </span><em><span>(e.g. someone liked this, someone posted this)</span></em><span> for many things - tracking fraud/abuse, matching jobs to users, training ML models, basic features of the website (e.g who viewed your profile, the newsfeed), warehouse ingestion for offline analysis/reporting and etc.</span></p><p><span>The big takeaway is that many of these activity data feeds are not simply used for reporting, they’re </span><strong>a dependency to the website’s core functionality</strong><span>.</span></p><p><span>As such, they require </span><strong>very robust infrastructure</strong><span>.</span></p><p>It mainly consisted of two pipelines:</p><p>One was an hourly batch-oriented system designed purely to load data into a data warehouse.</p><p>Applications would directly publish XML messages of the events (e.g profile view) to an HTTP server. The system would then write these to aggregate files, copy them to ETL servers, parse &amp; transform the XML and finally load it into the warehouse infrastructure consisting of a relational Oracle database and Hadoop clusters.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!47Pa!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20e18e49-118e-41d9-8914-13dcc4f7acca_1600x1100.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!47Pa!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20e18e49-118e-41d9-8914-13dcc4f7acca_1600x1100.png 424w, https://substackcdn.com/image/fetch/$s_!47Pa!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20e18e49-118e-41d9-8914-13dcc4f7acca_1600x1100.png 848w, https://substackcdn.com/image/fetch/$s_!47Pa!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20e18e49-118e-41d9-8914-13dcc4f7acca_1600x1100.png 1272w, https://substackcdn.com/image/fetch/$s_!47Pa!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20e18e49-118e-41d9-8914-13dcc4f7acca_1600x1100.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!47Pa!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20e18e49-118e-41d9-8914-13dcc4f7acca_1600x1100.png" width="1456" height="1001" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/20e18e49-118e-41d9-8914-13dcc4f7acca_1600x1100.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:1001,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:null,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!47Pa!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20e18e49-118e-41d9-8914-13dcc4f7acca_1600x1100.png 424w, https://substackcdn.com/image/fetch/$s_!47Pa!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20e18e49-118e-41d9-8914-13dcc4f7acca_1600x1100.png 848w, https://substackcdn.com/image/fetch/$s_!47Pa!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20e18e49-118e-41d9-8914-13dcc4f7acca_1600x1100.png 1272w, https://substackcdn.com/image/fetch/$s_!47Pa!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20e18e49-118e-41d9-8914-13dcc4f7acca_1600x1100.png 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a></figure></div><p><span>They used a separate more real-time pipeline for observability. It contained regular server metrics (CPU, errors, etc.), structured logs and distributed tracing events, all flowing to </span><a href="https://en.wikipedia.org/wiki/Zenoss" rel="">Zenoss</a><span>.</span></p><p>It was a cumbersome manual process to add new metrics there. This pipeline’s data was not available anywhere else besides Zenoss, so it couldn’t be freely processed or joined with other data.</p><p>There were a few commonalities between both of these pipelines:</p><ol><li><p><strong>manual work</strong><span> - both systems required a lot of manual maintenance, both to keep the lights on and add new data to.</span></p></li><li><p><strong>large backlogs</strong><span> - both systems had large work backlogs of missing data that needed to be added by the resource-constrained central teams responsible for them.</span></p></li><li><p><strong>no integration</strong><span> - both of these systems were effectively </span><strong>point-to-point</strong><span> data pipelines that delivered data to </span><strong>a single destination</strong><span> with </span><em>no integration</em><span> between them or other systems.</span></p></li></ol><p>Along the line, LinkedIn figured out that they gained massive value from the simple act of integration - joining previously-siloed data together. Just getting data into Hadoop would unlock new features for them.</p><p>As such, the demand for more pipelines to move data to Hadoop grew. But data coverage wasn’t there - only a very small percentage of data was available in Hadoop.</p><p>The current way was unsustainable - the pipeline team would never be able to catch up with their ever-growing backlog as more teams realized the value and demanded more features.</p><p>There were too many problems - both individual within the pipeline and fundamental throughout the architecture - that made it impossible to scale:</p><ul><li><p><em><strong>Schema Parsing</strong></em><span>: there were hundreds of XML schemas. XML did not map to the downstream</span><span> systems (e.g. Hadoop), so it required custom parsing and mapping.</span></p><ul><li><p><span>This was time consuming, error prone and computationally expensive - they would often fail and were hard to test with production releases. This then led to delays in adding new types of activity data, which forced them to do </span><strong>hacks</strong><span> like shoe-horning new data into already-existing inappropriate types in order to avoid extra work but ship on time.</span></p></li></ul></li><li><p><em><strong>Brittle</strong></em><span>: The pipeline(s) were critical, because any problem in it would break the downstream system (i.e website feature). Running fancy algorithms on bad data simply produced more bad data.</span></p><ul><li><p>Reliability had to be prioritized.</p></li></ul></li><li><p><em><strong>Schema Evolution</strong></em><span>: adding/removing fields from the schemas without breaking downstream systems in the pipeline was hard.</span></p><ul><li><p>The app generating the data would own the XML format - testing and being aware of all downstream uses was difficult for them, especially due to the asynchronous nature of the pipeline.</p></li><li><p><span>There wasn’t good communication between teams either, so sometimes the schema would change unexpectedly.</span></p></li></ul></li><li><p><em><strong>Lag</strong></em><span>: it was impossible to inspect the activity metrics in real time. They would only be available hours later (via the batch process).</span></p><ul><li><p>This led to a long lag time in understanding and resolving problems with the website.</p></li></ul></li><li><p><em><strong>Separation of Data</strong></em><span>: the fact that operational server metrics can’t be joined with the activity data was problematic - it further prevented them from correlating, detecting and understanding problems (e.g. decrease in page views can’t always be seen in server CPU metrics)</span></p></li><li><p><em><strong>No Deep Operational Metrics Analysis</strong></em><span>: can’t run long-form longitudinal queries on the operational metrics because they’re only present in a real-time system that doesn’t support such queries.</span></p></li></ul><ul><li><p><em><strong>Single Destination</strong></em><span>: the use cases for the activity data were growing by the day. But the clean data was solely present in the warehouse. Even if it had 100% data coverage, it wasn’t enough for the data to just be there. It had to go to other destinations too.</span></p></li></ul><p>Reading these problems, LinkedIn’s requirements are pretty clear.</p><p><span>They needed to have </span><strong>robust pipeline</strong><span> (1/7) infrastructure (i.e a standardized, well-maintained API). This pipeline had to be </span><strong>scalable </strong><span>(2/7). </span></p><p><span>They needed to </span><strong>handle schemas</strong><span> (3/7) properly - commit to a backwards-compatible contract.</span></p><p><span>They needed the system to handle high </span><strong>fan-out</strong><span> (4/7) - the activity data, for example, needed to go to a lot of destinations.</span></p><p><span>They needed it to be </span><strong>real-time</strong><span> (5/7) - measured in seconds, not hours.</span></p><p><span>They needed </span><strong>plug-and-play integration </strong><span>capabilities</span><strong> </strong><span>(6/7). Ideally new sources/sinks would be very easy to load, without any manual work. For this to work, they needed structured schemas (the 3/7) with </span><strong>clean data </strong><span>(6/7), so no extra processing would be required.</span></p><p><span>And they needed to </span><strong>switch the ownership</strong><span> (7/7) of some of this work, because the two small teams handling the pipelines would have never caught up on their large backlogs while the rest of the organization kept coming up with new use cases.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!cXVA!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66ab8c3a-feac-4b84-8c36-d566e5b42163_2048x768.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!cXVA!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66ab8c3a-feac-4b84-8c36-d566e5b42163_2048x768.png 424w, https://substackcdn.com/image/fetch/$s_!cXVA!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66ab8c3a-feac-4b84-8c36-d566e5b42163_2048x768.png 848w, https://substackcdn.com/image/fetch/$s_!cXVA!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66ab8c3a-feac-4b84-8c36-d566e5b42163_2048x768.png 1272w, https://substackcdn.com/image/fetch/$s_!cXVA!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66ab8c3a-feac-4b84-8c36-d566e5b42163_2048x768.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!cXVA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66ab8c3a-feac-4b84-8c36-d566e5b42163_2048x768.png" width="1456" height="546" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/66ab8c3a-feac-4b84-8c36-d566e5b42163_2048x768.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:546,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:216674,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://bigdata.2minutestreaming.com/i/170964904?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66ab8c3a-feac-4b84-8c36-d566e5b42163_2048x768.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!cXVA!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66ab8c3a-feac-4b84-8c36-d566e5b42163_2048x768.png 424w, https://substackcdn.com/image/fetch/$s_!cXVA!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66ab8c3a-feac-4b84-8c36-d566e5b42163_2048x768.png 848w, https://substackcdn.com/image/fetch/$s_!cXVA!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66ab8c3a-feac-4b84-8c36-d566e5b42163_2048x768.png 1272w, https://substackcdn.com/image/fetch/$s_!cXVA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66ab8c3a-feac-4b84-8c36-d566e5b42163_2048x768.png 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a><figcaption>Something like this had to happen</figcaption></figure></div><p><strong>PS:</strong><span> While evaluating solutions, they figured out that the system had to support data backlogs - i.e </span><strong>decouple writers from readers </strong><span>(8/8). Some systems they tested (ActiveMQ) would collapse in performance when readers fell behind and a significant data backlog accumulated.</span></p><p>They created Apache Kafka. This solved for 5/8 of the problems:</p><ul><li><p><strong>Robustness </strong><span>(1/7)</span><strong> </strong><span>- being a distributed system with built-in replication, failover and durability guarantees meant single machine hiccups would not disrupt the pipelines</span></p></li><li><p><strong>Scalability</strong><span> (2/7) - the distributed nature and sharding of topics via partitions made it horizontally scalable on commodity hardware </span></p></li><li><p><em><strong>High read fan-out</strong></em><strong> </strong><span>(4/7) - the lock-free design of </span><a href="https://topicpartition.io/definitions/the-log" rel="">the Log data structure</a><span> made high-scale reading trivial</span></p></li><li><p><em><strong>Real-time</strong></em><span> (5/7) - the system was real-time, although funnily initially </span><em>average</em><span> latency of their large multi-cluster pipeline was 10 seconds. Nowadays this’d be a lot lower.</span></p></li><li><p><em><strong>Decouple writers from readers</strong></em><span> (8/8)- the fact that the data was buffered to disk by design allowed them to set longer retention and not tie the retention to whether the message was consumed or not. This meant slow readers would never impact the system.</span></p></li></ul><p>The other three problems were schemas, data integration and ownership.</p><p>Let’s dive into them, because they’re pretty interesting.</p><p>They moved from XML to Apache Avro as both the schema and serialization language for all the activity data records, as well as downstream in Hadoop.</p><p><span>This led to significantly </span><strong>less data size</strong><span> - Avro messages were 7x smaller than XML. They additionally compressed them 3x down later when producing to Kafka.</span></p><p><span>They developed what seems like the precursor to </span><a href="https://github.com/confluentinc/schema-registry" rel="">Confluent’s Schema Registry</a><span>: a service to serve as the source of truth for the schema in a Kafka topic, as well as maintain a history of all schema versions ever associated with the topic.</span></p><p><span>Kafka messages carried an id to refer to the exact schema version it was written with. This versioning made it impossible to break deserialization of messages, because you could always know the right schema to read a message with.</span></p><p>This was a much needed improvement, but it wasn’t everything they needed.</p><p><span>While messages were individually understandable, Hadoop applications could still be broken by toying with the schema - e.g. removing a field in the record they needed to use. They generally expected a single schema to describe a data set.</span></p><p><span>To solve this </span><strong>backwards compatibility</strong><span> issue, LinkedIn developed a compatibility model to programmatically check schema changes for backwards compatibility against existing production schemas. The model only allowed changes that maintained compatibility with historical data - something that sounds just like schema registry’s </span><a href="https://github.com/confluentinc/schema-registry/blob/05cd4f59d2a11c1b7475f2eec2cbac39f83e55b4/client/src/main/java/io/confluent/kafka/schemaregistry/CompatibilityLevel.java#L21" rel="">`</a><code>BACKWARD</code><a href="https://github.com/confluentinc/schema-registry/blob/05cd4f59d2a11c1b7475f2eec2cbac39f83e55b4/client/src/main/java/io/confluent/kafka/schemaregistry/CompatibilityLevel.java#L21" rel="">` compatibility level</a><span>.</span></p><p><span>To make new data very easy to integrate without any extra manual work, you need to settle on </span><strong>a single schema </strong><span>between the upstream and downstream system.</span></p><p>Having even slight differences means you’ll always have to translate them between systems - i.e., an extra JIRA ticket for somebody to manually write, test &amp; deploy code to transform the data into whatever the downstream system’s requirements.</p><p><span>But if the schema is the same, then that work doesn’t exist.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!z1xN!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1797fdd-34be-4af1-bd35-39d6439666ef_1600x1100.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!z1xN!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1797fdd-34be-4af1-bd35-39d6439666ef_1600x1100.png 424w, https://substackcdn.com/image/fetch/$s_!z1xN!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1797fdd-34be-4af1-bd35-39d6439666ef_1600x1100.png 848w, https://substackcdn.com/image/fetch/$s_!z1xN!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1797fdd-34be-4af1-bd35-39d6439666ef_1600x1100.png 1272w, https://substackcdn.com/image/fetch/$s_!z1xN!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1797fdd-34be-4af1-bd35-39d6439666ef_1600x1100.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!z1xN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1797fdd-34be-4af1-bd35-39d6439666ef_1600x1100.png" width="1456" height="1001" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/b1797fdd-34be-4af1-bd35-39d6439666ef_1600x1100.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:1001,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:143824,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://bigdata.2minutestreaming.com/i/170964904?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1797fdd-34be-4af1-bd35-39d6439666ef_1600x1100.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!z1xN!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1797fdd-34be-4af1-bd35-39d6439666ef_1600x1100.png 424w, https://substackcdn.com/image/fetch/$s_!z1xN!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1797fdd-34be-4af1-bd35-39d6439666ef_1600x1100.png 848w, https://substackcdn.com/image/fetch/$s_!z1xN!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1797fdd-34be-4af1-bd35-39d6439666ef_1600x1100.png 1272w, https://substackcdn.com/image/fetch/$s_!z1xN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1797fdd-34be-4af1-bd35-39d6439666ef_1600x1100.png 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a></figure></div><p><span>Another big problem with the different schema approach was the time at which problems would surface. Hadoop used a </span><strong>Schema on Read</strong><span> model, where data would be stored in files, unstructured</span><span>, and the structure would only be defined within each script that reads the files. This meant that things would break much later, at query time, not at the time of ingestion.</span></p><p>Traditionally, at the time, the data warehouse was the only location with a clean version of the data. The dirty, unstructured data would land in Hadoop, and then get cleansed/curated through various scripts (if they didn’t break).</p><p>Access to such clean &amp; complete data was of utmost importance to any data-centric company, so having it locked up in the warehouse didn’t scale to meet the organization’s needs. Not to mention it only arrived in the warehouse after hours of delay - very problematic for anybody who wants to access it sooner.</p><p><span>The solution was very straightforward - “simply” move the clean data upstream to a real-time source. Clean it as it lands into Kafka - i.e adopt a </span><strong>Schema on Write</strong><span> model.</span></p><p>This not only allows plug and play integration with the warehouse (e.g creating a new table from a new topic is a piece of cake), but also makes it available for other types of consumers - be it real-time or batch.</p><p>Similarly, schema changes like adding a new field could be automatically handled.</p><p>That’s exactly what LinkedIn did - define a single uniform schema with the canonical, cleansed format of the particular message. But this required organizational change to execute:</p><p>Previously, it was the pipeline team’s responsibility to match the schema to the downstream system. LinkedIn needed to move this ownership away from them if they wanted to ever get to 100% data coverage. This was the final step in solving the problem.</p><p>The best team to drive this? The team(s) that created the data, of course! They best know what the cleanest representation of the data should be.</p><p>Agreeing on a uniform schema amongst downstream systems was still a joint effort though.</p><p><span>LinkedIn established a mandatory code review process between all involved teams - whenever schema code was changed, there would need to be LGTMs from stakeholders before the next production release</span><span>.</span></p><p>A side-effect of this was that it helped document the thousands of fields in their hundreds of schemas.</p><p>This was a very long-winded way of saying that:</p><ul><li><p>data needs to be integrated between many systems</p></li><li><p>schemas play a very crucial role in this, make sure you define them well and establish good ownership</p></li><li><p>Kafka, with schemas, was literally invented to solve this problem at scale.</p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!-eGU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85f4196b-7e14-4102-a57d-578ef8fdb119_1600x1100.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!-eGU!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85f4196b-7e14-4102-a57d-578ef8fdb119_1600x1100.png 424w, https://substackcdn.com/image/fetch/$s_!-eGU!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85f4196b-7e14-4102-a57d-578ef8fdb119_1600x1100.png 848w, https://substackcdn.com/image/fetch/$s_!-eGU!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85f4196b-7e14-4102-a57d-578ef8fdb119_1600x1100.png 1272w, https://substackcdn.com/image/fetch/$s_!-eGU!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85f4196b-7e14-4102-a57d-578ef8fdb119_1600x1100.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!-eGU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85f4196b-7e14-4102-a57d-578ef8fdb119_1600x1100.png" width="1456" height="1001" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/85f4196b-7e14-4102-a57d-578ef8fdb119_1600x1100.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:1001,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:142676,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://bigdata.2minutestreaming.com/i/170964904?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85f4196b-7e14-4102-a57d-578ef8fdb119_1600x1100.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!-eGU!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85f4196b-7e14-4102-a57d-578ef8fdb119_1600x1100.png 424w, https://substackcdn.com/image/fetch/$s_!-eGU!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85f4196b-7e14-4102-a57d-578ef8fdb119_1600x1100.png 848w, https://substackcdn.com/image/fetch/$s_!-eGU!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85f4196b-7e14-4102-a57d-578ef8fdb119_1600x1100.png 1272w, https://substackcdn.com/image/fetch/$s_!-eGU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85f4196b-7e14-4102-a57d-578ef8fdb119_1600x1100.png 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a></figure></div><p>In the end, this proved very effective for LinkedIn. A single engineer was able to implement and maintain the process that does data loads for all topics with no incremental work or co-ordination needed as teams add new topics or change schemas.</p><p><em>Source</em><span>: </span><a href="http://sites.computer.org/debull/A12june/pipeline.pdf" rel="">this 2012 paper</a><span> and </span><a href="https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying" rel="">this blog</a><span>.</span></p><p><span>One thing that surprised me while reading this paper was </span><strong>the emphasis on schemas</strong><span>.</span></p><p>I’ve been vocal before that I believe the lack of first-class schema support was Kafka’s biggest mistake.</p><p><span>The paper’s solution shares a lot similarities to Buf’s </span><a href="https://buf.build/blog/kafka-schema-driven-development" rel="">schema-driven development vision</a><span>. Buf has been talking about universal schemas for the longest time, and built </span><a href="https://www.linkedin.com/posts/stanislavkozlovski_bufstream-activity-7296172586965635072-zfIp" rel="">a diskless Kafka system</a><span> that prioritizes schemas as a first-class citizen. I haven’t seen any other Kafka provider focus on schemas that much.</span></p><p><span>What surprised me </span><em>precisely</em><span> in this paper is that it, </span><strong>13+ years ago</strong><span>, described a problem for which LinkedIn literally invented Kafka and auxiliary systems to solve, then took their time to write out the universal schema solution really well:</span></p><blockquote><p><em><span>why am I hearing about this schema-centric vision </span><strong>a full decade later</strong><span> from a Protobuf/Kafka startup?</span></em></p></blockquote><p>I’ve posted before on how Kafka lacks first-class schemas and how Buf seems to be the only one in the space beating the drum on their importance:</p><p><strong>❌ Every valuable use-case requires schemas</strong><span>. You can’t use Kafka Connect to integrate data between upstream system A, Kafka and downstream system B without the existence of schemas — because chances are both systems A &amp; B require some structure. Ditto for stream processing - you can’t do joins and aggregations on 1s and 0s.</span></p><p><strong>❌ Every message has a schema</strong><span> - it’s either explicit and defined in a single place, or implicit and scattered throughout your application’s code.</span></p><p><strong>❌ Fragmentation</strong><span> - absent of an “official” schema registry that ships with Kafka, we have dozens of options to choose from - Confluent’s Schema Registry, Karaspace, AWS Glue, Apicurio, Buf’s Schema Registry, etc.</span></p><p><strong>❌ No Server-Side Validation</strong><span> - the Producer client is the </span><strong>only</strong><span> one who validates the schema prior to writing. There’s no way to defend against buggy (or malicious) clients, hence no way to enforce a uniform schema under all circumstances.</span></p><p><span>Again, I really commend Buf’s </span><a href="https://buf.build/blog/semantic-validation" rel="">server-side validation</a><span>. When you embrace the fundamental idea that messages must have schemas and enforce it on the server, a lot of things open up:</span></p><ul><li><p><strong>Native Iceberg integration</strong><span> - writing into an open table format becomes a trivial job of translating one schema language (e.g Avro or Protobuf) to another’s (Parquet’s)</span></p></li><li><p><strong>Semantic validation</strong><span> - ensuring message fields match a specific format (e.g email validation, age validation)</span></p></li><li><p><strong>Filtering fields based on policies</strong><span> (RBAC) - certain sensitive fields ought to not be readable by certain groups.</span></p></li><li><p><strong>Debugging</strong><span> - if a bad message somehow makes it in there (e.g configuration wasn’t right), the server can immediately pin-point it.</span></p></li><li><p><strong>Filtering bad messages directly on the server</strong><span> - avoiding the need to have custom poison pill handling code (e.g send into a DLQ topic) in each consumer application.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!lDiT!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8977eee2-b5c2-4168-9c23-22a57d93b48d_1024x558.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!lDiT!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8977eee2-b5c2-4168-9c23-22a57d93b48d_1024x558.jpeg 424w, https://substackcdn.com/image/fetch/$s_!lDiT!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8977eee2-b5c2-4168-9c23-22a57d93b48d_1024x558.jpeg 848w, https://substackcdn.com/image/fetch/$s_!lDiT!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8977eee2-b5c2-4168-9c23-22a57d93b48d_1024x558.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!lDiT!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8977eee2-b5c2-4168-9c23-22a57d93b48d_1024x558.jpeg 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!lDiT!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8977eee2-b5c2-4168-9c23-22a57d93b48d_1024x558.jpeg" width="575" height="313.330078125" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/8977eee2-b5c2-4168-9c23-22a57d93b48d_1024x558.jpeg&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:558,&#34;width&#34;:1024,&#34;resizeWidth&#34;:575,&#34;bytes&#34;:null,&#34;alt&#34;:&#34;Image&#34;,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="Image" title="Image" srcset="https://substackcdn.com/image/fetch/$s_!lDiT!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8977eee2-b5c2-4168-9c23-22a57d93b48d_1024x558.jpeg 424w, https://substackcdn.com/image/fetch/$s_!lDiT!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8977eee2-b5c2-4168-9c23-22a57d93b48d_1024x558.jpeg 848w, https://substackcdn.com/image/fetch/$s_!lDiT!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8977eee2-b5c2-4168-9c23-22a57d93b48d_1024x558.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!lDiT!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8977eee2-b5c2-4168-9c23-22a57d93b48d_1024x558.jpeg 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a><figcaption>poison pills in action. IYKYK</figcaption></figure></div></li></ul><p>Lack of schemas continues to be my biggest gripe with Kafka.</p><p>The fact that LinkedIn treated schemas as first-class citizens all the way back in 2012 really has left me scratching my head…</p><p>They knew this was a major problem, yet never baked it into the product. Neither did Confluent after branching out of LinkedIn.</p><p>I’m uncertain how much of the schema decision was a business decision versus a technical one.</p><p><span>Back in the days, Confluent made their money off of their on-premise Confluent Platform package which priced things </span><em><strong>per node</strong></em><span>, hence a financial incentive existed to deploy more nodes (i.e. a schema registry cluster with three nodes for HA versus bundling it in the broker).</span></p><p><span>Perhaps there was investor pressure to preserve pricing optionality? Any business has to make money, and you can’t give away your most valuable features. It’s pretty common in the </span><a href="https://en.wikipedia.org/wiki/Open-core_model" rel="">open-core business model</a><span> to gate enterprise features behind paywalls.</span></p><p><span>Schema Registry is source-available though. In fact, it’s pretty much free to use </span><a href="https://news.ycombinator.com/item?id=18681234" rel="">unless you plan on selling it as a SaaS</a><span>. But basic features like </span><a href="https://docs.confluent.io/platform/current/confluent-security-plugins/schema-registry/introduction.html" rel="">ACLs</a><span> require a paid license. Something open-source Schema Registries like Karaspace </span><a href="https://aiven.io/docs/products/kafka/karapace/concepts/acl-definition" rel="">offer for free</a><span>. And hence my point on </span><strong>fragmentation</strong><span> in the space. It’s got to the point where we have to have </span><a href="https://kroxylicious.io/use-cases/#schema-validation-and-enforcement" rel="">proxies that enforce schemas</a><span>.</span></p><p>At the same time, there are some technical reasons why no-schema can be preferred:</p><ul><li><p>Enforcement on the client scales easier, as no bottleneck exists in the broker.</p><ul><li><p><span>Having the broker be agnostic to the data allows for </span><a href="https://blog.2minutestreaming.com/p/apache-kafka-zero-copy-operating-system-optimization" rel="">zero-copy</a><span> to the disk - although I’ve mentioned why that optimization doesn’t move the needle at all nowadays with SSL. Perhaps it was a legacy optimization?</span></p></li></ul></li><li><p><span>Any state management on the broker could require extra resource usage (parsing schemas</span><span>, validating) and worse off — it could block the stream as it’d require additional locking. That’s contrary to the value prop of Kafka for slinging lots of bytes fast.</span></p></li></ul><p><span>But I believe it </span><strong>didn’t have to be an either-or decision</strong><span>. Kafka could have shipped with first-class schema support built into topics - just an optional toggle.</span></p><p>And I don’t think performance would be hurt to the point of being unusable. Few people push Kafka to its true limits anyway.</p><p><span>In hindsight, it’s clear that the schemaless wave </span><em>(“it’s just byte arrays!”)</em><span> was a fad that went away. We see which model won out in the SQL vs NoSQL wars - Postgres is not eating the world today by accident.</span></p><p>I keep asking - why doesn’t Apache Kafka have schemas?</p><div data-attrs="{&#34;url&#34;:&#34;https://bigdata.2minutestreaming.com/p/why-was-apache-kafka-created?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&#34;,&#34;text&#34;:&#34;Share&#34;}" data-component-name="CaptionedButtonToDOM"><p>Share with a colleague in Slack</p><p data-attrs="{&#34;url&#34;:&#34;https://bigdata.2minutestreaming.com/p/why-was-apache-kafka-created?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&#34;,&#34;text&#34;:&#34;Share&#34;}" data-component-name="ButtonCreateButton"><a href="https://bigdata.2minutestreaming.com/p/why-was-apache-kafka-created?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div><blockquote><p><em><span>Additionally, if you enjoy this letter and its writing - support our growth by reposting this to your network in LinkedIn - </span><a href="https://www.linkedin.com/posts/stanislavkozlovski_why-was-apache-kafka-created-i-wrote-about-activity-7364651793995173889-BLhM" rel="">https://www.linkedin.com/posts/stanislavkozlovski_why-was-apache-kafka-created-i-wrote-about-activity-7364651793995173889-BLhM</a><span>. ✌️</span></em></p><p>Or on Twitter/X -</p><p><a href="https://x.com/BdKozlovski/status/1958900478470967702" rel="">https://x.com/BdKozlovski/status/1958900478470967702</a><span> </span></p></blockquote></div></div></div>
  </body>
</html>
