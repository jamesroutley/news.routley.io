<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://sygi.xyz/posts/2025-02-09-multiagent-llms.html">Original</a>
    <h1>Small Multiagent Vision for Large Language Models</h1>
    
    <div id="readability-page-1" class="page"><div id="content">
            <p>
    Posted on February  9, 2025
    
</p>

<p>Multiagent algorithms are not in the mainstream focus nowadays, in particular, LLM development rarely incorporates them.
This post explains why I think the characteristics of LLMs make them ideal candidates for multiagent algorithms.</p>
<h2 id="what-makes-an-algorithm-multiagent">What makes an algorithm multiagent?</h2>
<p>This post focuses on RL algorithms (i.e. agents acting in an environment optimizing some reward).</p>
<p>Multi-agent RL (MARL) can be seen as a syntactic sugar: we can always model the behavior of other agents as part of the environment. It is sometimes a useful mental model for designing learning systems.</p>
<p>Multi-agent algorithms fall into two categories<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a>:</p>
<ol type="1">
<li>Analyzing what properties emerge in a system where there are multiple independent agents with particular behaviors, eg. analyzing versions of Tragedy of the commons</li>
<li>Using the diversity of the agents’ behavior to create a curriculum to faster/better train agent(s) toward their goal, eg. <a href="https://www.nature.com/articles/s41586-019-1724-z.epdf?author_access_token=lZH3nqPYtWJXfDA10W0CNNRgN0jAjWel9jnR3ZoTv0PSZcPzJFGNAZhOlk4deBCKzKm70KfinloafEF1bCCXL6IIHHgKaDkaTkBcTEv7aT-wqDoG1VeO9-wO3GEoAMF9bAOt7mJ0RWQnRVMbyfgH9A%3D%3D">Starcraft league</a></li>
</ol>
<p>In this write-up, I’m going to use the second definition.</p>
<h2 id="general-strategy">General strategy</h2>
<p>If MARL is a <em>method</em> for improving agents’ performance, how we apply it to an RL problem? It is the easiest when the RL problem is expressed as a game between players.</p>
<p>In a typical RL approach, an agent will play against the same environment, collecting the rewards and updating its behavior based on them. In a sense, even if there is a fixed opponent (eg. a bot against which to play in chess), from the perspective of the learning agent, the environment looks like they are playing a single-player game.</p>
<p>The multi-agent method tells us to vary the opponents in the game according to the current needs of the player: there may be a particular weakness the current agent has that can be exploited (and, in effect, trained against), or a particular collaborator to adapt to. This helps the agent to learn faster than if it were to explore various strategies with a limited set of co-players.</p>
<p>The other (usually fixed) players to be put in the environment may be hard-coded (in which case their utility is limited, as there is only as much you can learn from a single co-player), or generated through an automated process (eg. old versions of yourself in fictitious self-play<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a>).</p>
<details>
<summary>
On RL &amp; math of the gradient estimator
</summary>
<p>In RL, as opposed to supervised learning, the agent controls what data it trains on. While, in theory, making parameter updates using an unbiased estimator of the gradient of the objective:</p>
<p><span>\[
\theta \leftarrow \theta + \alpha \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) R_t
\]</span></p>
<p>will lead to convergence to the (locally) optimal policy<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a>, the rate of convergence will vary widely depending on the variance of the estimator.</p>
<p>One mechanism to decrease the variance, explored in the context of multiagent algorithms, is to consistently put the agent in situations with varying rewards<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a>, to teach it to distinguish what to do from what to avoid.</p>
<p>To understand how increasing the variance of the rewards helps agents to learn, consider the following mind experiment:
Imagine that an agent is placed in a room with two doors: red and blue. If it goes through the red door, it gets the reward of 1 and the episode terminates. If it goes through the blue door, it gets to a new room, where it is playing a proper game, and, depending on its score, it gets 0 or 100.</p>
<p>Now, if the agent consistently chooses the red (boring) door 99% of the time, its proper learning experience is limited to only 1% of the time and is affected by the noise from the uninteresting episodes.</p>
</details>
<h2 id="when-to-use-marl">When to use MARL?</h2>
<p>Many of the RL problems can be solved without resolving to multiagent methods using standard techniques like policy gradient, planning in model-based RL, trust-region methods like PPO, and others.</p>
<p>I believe there are two properties of a problem that make it particularly amenable to MARL:</p>
<ol type="1">
<li>The environment being collaborative (as opposed to competitive), and</li>
<li>a large non-transitive dimension amongst average/typical policies</li>
</ol>
<h3 id="collaboration">Collaboration</h3>
<p>In a collaborative environment, a positive/high reward for one player correlates with a positive reward for another player.</p>
<p>In collaborative environments, the players often need to coordinate their actions and adapt to each other strategies: if one player makes a bad move, the other has to compensate instead of penalizing the first player. An agent needs to learn to cooperate with a wide range of policies, instead of finding the best action assuming optimal play from the opponent.</p>
<p>As MARL involves agent training with a range of co-players, handling a wide diversity of collaborators may become easier.</p>
<h3 id="non-transitivity">Non-transitivity</h3>
<p>Let’s define non-transitivity as the presence of a cycle long of strategies <span>\(\pi_0, \ldots, \pi_{n-1}\)</span> such that <span>\(\pi_i\)</span> beats <span>\(\pi_{(i+1)\bmod n}\)</span> for each <span>\(i\)</span>.</p>
<p>For the training of an RL model to progress, it needs to see examples of things it does right and ones it does badly. As the non-transitive cycle is long, it is difficult to provide the right set of challenges to a policy <span>\(\pi\)</span> without additional information about it. We would like to ask <span>\(\pi\)</span> to play with its neighbors in the cycle to learn from them, but we don’t, a priori, have a way to find them.</p>
<p>In this situation it feels more natural to learn as a population, keeping up-to-date win-rate statistics to be able to provide the right opponents at the right time.</p>
<p>Note: one can argue that interesting games of skill have large non-transitive components (<a href="https://arxiv.org/pdf/2004.09468">ref: spinning tops paper</a>).</p>
<h2 id="large-language-models">Large Language Models</h2>
<p>What does it have to do with LLMs? One can model LLMs answering people’s questions as solving an RL environment with two players: the human (H) who asks the question and the model (LLM) who answers it. The reward the model receives correlates with how much a human likes the answer<a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a>.</p>
<p>This RL problem has the properties suggesting MARL techniques will be successful:</p>
<ol type="1">
<li>The game is collaborative: the goal of H is to phrase the question in such a way as to receive the response from LLM it likes (high reward for LLM = high reward for H).</li>
<li>The problem is highly non-transitive:
<ol type="a">
<li>there is no common agreement of what an “answer a human likes” looks like: within humans, there is non-transitivity in preferences where one human may like A more than B whereas another prefers B from A.</li>
<li>even a single human isn’t always consistent, has a non-zero variance in establishing preference, and might genuinely have non-transitive preferences<a href="#fn6" id="fnref6" role="doc-noteref"><sup>6</sup></a>.</li>
</ol></li>
<li>Due to the vast range of topics (should they be called subenvironments?) that the problem consists of, it is easy to construct a varying pool of agents, both on H and LLM sides:
<ol type="a">
<li>for H, one may consider:
<ul>
<li>humans with varying expertise/interests</li>
<li>different LLMs posing as such humans</li>
<li>writing a program generating a large set of questions from a well-defined pool</li>
<li>using a program like a compiler or a spell-checker to evaluate the quality of an answer</li>
<li>combining any of the above, splitting the responsibility for asking a question and evaluating an answer to separate entities</li>
</ul></li>
<li>for LLM, it seems natural to:
<ul>
<li>finetune LLMs on data relating to different topics</li>
<li>include some data (evaluation’s test split, safety alignment data) in the training or not</li>
<li>allow to use tools or not</li>
<li>adjust the LLM with system prompts</li>
<li>align the LLM to follow a grammar during inference<a href="#fn7" id="fnref7" role="doc-noteref"><sup>7</sup></a></li>
</ul></li>
</ol></li>
</ol>
<h3 id="somewhat-concrete-proposal">Somewhat? concrete proposal</h3>
<p>There are millions of ways of implementing the multiagent concept in LLMs.</p>
<p>One, conceptually simple, I would try time allowing, follows this:</p>
<ol type="1">
<li>Let’s fine-tune (potentially with LoRA) an agent on a number of different datasets from different domains: one on math, one on programming, one on biology, etc.</li>
<li>Keep using the “experts” in their respective domains to teach the other (“student”) agents by:
<ul>
<li>taking a problem from the expert domain</li>
<li>generate the chain of thought/explanation of a correct solution</li>
<li>use it as a few-shot prompt for the student</li>
<li>evaluate the student on a similar problem using an independent expert/ground truth</li>
<li>reward the student based on the correctness of the answer and the teacher on the improvement the few-shot example gave</li>
</ul></li>
<li>Keep training the models on the reward / useful examples and see them improve as a population.</li>
</ol>
<p>This is not a sophisticated project, but it can help assess the promise of the idea compared to just training the agents on all the available data.</p>
<h2 id="outro">Outro</h2>
<p>Multiagent algorithms have been there long before LLMs and, while still being present in research (eg. see a bit outdated <a href="https://arxiv.org/pdf/2402.01680">survey</a>), they didn’t yet enter mainstream<a href="#fn8" id="fnref8" role="doc-noteref"><sup>8</sup></a>.</p>
<p>I expect and hope to see more of them in the coming months and years!</p>

 



            
        </div></div>
  </body>
</html>
