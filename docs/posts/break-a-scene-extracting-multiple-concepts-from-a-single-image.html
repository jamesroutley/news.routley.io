<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://omriavrahami.com/break-a-scene/">Original</a>
    <h1>Break-a-Scene: Extracting Multiple Concepts from a Single Image</h1>
    
    <div id="readability-page-1" class="page">
  <!-- Authors, institutions and links -->
  <section>
    <div>
      <div>
        <div>
          <div>
            
            

            <p><span><sup>1</sup>Google Research,</span>
              <span><sup>2</sup>The Hebrew University of Jerusalem,</span>
              <span><sup>3</sup>Tel Aviv University,</span>
              <span><sup>4</sup>Reichman University</span>
            </p>

            <p>SIGGRAPH Asia 2023</p>

            
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Teaser -->
  <section>
    <div>
      <div>
        <p><img src="https://omriavrahami.com/break-a-scene/static/images/teaser.gif" alt="Break-A-Scene teaser."/>
        </p>

        <!-- <video id="teaser" autoplay="" controls="" muted="" loop="" playsinline="" height="100%">
          <source src="static/videos/SpaText_Teaser.mp4" type="video/mp4">
        </video> -->

        <h2>
          <span>Break-A-Scene</span>: Given a <i>single</i> image with <i>multiple</i> concepts,
          annotated by loose segmentation masks, our method can learn a distinct token for each concept, and
          use natural language guidance to re-synthesize the individual concepts or combinations of them in various
          contexts.
        </h2>
      </div>
    </div>
  </section>

  <!-- Abstract. -->
  <section>
    <div>
      <div>
        <div>
          <h2>Abstract</h2>
          <p>
              Text-to-image model personalization aims to introduce a user-provided concept to the model, allowing its
              synthesis in diverse contexts. However, current methods primarily focus on the case of learning a
              <i>single</i> concept from <i>multiple</i> images with variations in backgrounds and poses, and struggle
              when adapted to a different scenario. In this work, we introduce the task of textual scene decomposition:
              given a <i>single</i> image of a scene that may contain <i>several</i> concepts, we aim to extract a
              distinct text token for each concept, enabling fine-grained control over the generated scenes. To this
              end, we propose augmenting the input image with masks that indicate the presence of target concepts. These
              masks can be provided by the user or generated automatically by a pre-trained segmentation model. We then
              present a novel two-phase customization process that optimizes a set of dedicated textual embeddings
              (handles), as well as the model weights, striking a delicate balance between accurately capturing the
              concepts and avoiding overfitting. We employ a masked diffusion loss to enable handles to generate their
              assigned concepts, complemented by a novel loss on cross-attention maps to prevent entanglement. We also
              introduce union-sampling, a training strategy aimed to improve the ability of combining multiple concepts
              in generated images. We use several automatic metrics to quantitatively compare our method against several
              baselines, and further affirm the results using a user study. Finally, we showcase several applications of
              our method.
            </p>
        </div>
      </div>
    </div>
  </section>

  <!-- Paper video. -->
  <section>
    <div>
      <div>
        <div>
          <div>
            <div>
              <div>
                <h2>Video</h2>
                <p>
                  <iframe src="https://www.youtube.com/embed/-9EA-BhizgM?rel=0&amp;showinfo=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Method explanation -->
  <section>
    <div>
      <div>
        <div>
          <h2>Method</h2>

          <div>

            <p>
              Humans have a natural ability to decompose complex scenes into their constituent parts and envision them
              in diverse contexts. For instance, given a photo of a ceramic artwork depicting a creature seated on a
              bowl, one can effortlessly imagine the <i>same creature</i> in a variety of different
              poses and locations, or envision the <i>same bowl</i> in a new setting. However, today&#39;s generative models
              struggle when confronted with this type of task.
            </p>

            <p><img src="https://omriavrahami.com/break-a-scene/static/images/method.jpg"/>
              <br/>
            </p>

            <p>
              In this work, we propose a novel customization pipeline that effectively balances the preservation of
              learned concept identity with the avoidance of overfitting. Our pipeline, depicted above, consists of
              <strong>two phases</strong>. In the first phase, we designate a set of dedicated text tokens (handles),
              freeze the model weights, and optimize the handles to reconstruct the input image. In the second phase, we
              switch to fine-tuning the model weights, while continuing to optimize the handles.
            </p>

            <p>
              We also recognize that in order to generate images exhibiting combinations of concepts, the customization
              process cannot be carried out separately for each concept. This observation leads us to introduce
              <strong>union-sampling</strong>, a training strategy that addresses this requirement and enhances the
              generation of concept combinations.
            </p>

            <p>
              A crucial focus of our approach is on disentangled concept extraction, i.e., ensuring that each handle is
              associated with only a single target concept. To achieve this, we employ a <strong>masked version</strong>
              of the standard diffusion loss, which guarantees that each custom handle can generate its designated
              concept; however, this loss does not penalize the model for associating a handle with multiple concepts.
              Our main insight is that we can penalize such entanglement by additionally imposing a loss on the
              <strong> cross-attention maps</strong>, known to correlate with the scene layout. This additional loss
              ensures that each handle attends only to the areas covered by its target concept.
            </p>

          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Scene Breaking Examples -->
  <section>
    <div>
      <div>
        <div>
          <h2>Results</h2>
          <h3>Scene Breaking Examples</h3>
          <p>
              Here are some results of scene breaking and re-synthesis in different contexts and combinations:
            </p>

          <section>
            <div>
              <div>
                <div id="results-carousel">
                  <div>
                    <p><img src="https://omriavrahami.com/break-a-scene/static/images/examples/scene_breaking/mask_overlay.jpg"/></p><p>Input scene</p>
                  </div>
                </div>

                </div>
            </div>
          </section>
        </div>
      </div>
    </div>
  </section>

  <!-- Entangled Scene Decomposition -->
  <section>
    <div>
      <div>
        <div>
          <h3>Entangled Scene Decomposition</h3>
          <p>
              Our method is also capable of breaking entangled scenes:
            </p>

          <!-- Dog example -->
          <section>
            <div>
              <div>
                <div id="results-carousel">
                  <div>
                    <p><img src="https://omriavrahami.com/break-a-scene/static/images/examples/entangled_scene/dog_shirt/mask_overlay.jpg"/></p><p>Input scene</p>
                  </div>
                </div>
                </div>
            </div>
          </section>

          <!-- Cat example -->
          

          
          <!-- Bear example -->
          
        </div>
      </div>
    </div>
  </section>

  <!-- Local image editing -->
  <section>
    <div>
      <div>
        <div>
          <h3>Local Image editing</h3>
          <p>
              Our incorporate our method with <a href="https://omriavrahami.com/blended-latent-diffusion-page/">Blended
                Latent Diffusion</a> to achieve local image editing by example:
            </p>

          <!-- Creature -->
          <section>
            <div>
              <div>
                <div id="results-carousel">
                  <div>
                    <p><img src="https://omriavrahami.com/break-a-scene/static/images/examples/bld/empty_paintings/mask_overlay.jpg"/></p><p>Input scene</p>
                  </div>
                </div>

                </div>
            </div>
          </section>

          <!-- Panda -->
          

          
          <!-- Chicken -->
          

          
          <!-- Sheep -->
          
        </div>
      </div>
    </div>
  </section>

  <!-- Background extraction -->
  <section>
    <div>
      <div>
        <div>
          <h3>Background Extraction</h3>
          <p>
              Our method is also capable of extracting the background of a scene for further edits:
            </p>

          <section>
            <div>
              <div>
                <div id="results-carousel">
                  <div>
                    <p><img src="https://omriavrahami.com/break-a-scene/static/images/examples/background_extraction/mask_overlay.jpg"/></p><p>Input scene</p>
                  </div>
                </div>

                </div>
            </div>
          </section>
        </div>
      </div>
    </div>
  </section>

  <!-- Image variations -->
  <section>
    <div>
      <div>
        <div>
          <h3>Image variations</h3>
          <p>
              Given a single image containing multiple concepts of interest, once these are extracted using our method,
              they can be used to generate multiple variations of the image. The arrangement of the objects in the
              scene, as well as the background, are different in each generation.
            </p>

          <!-- Dog -->
          <section>
            <div>
              <div>
                <div id="results-carousel">
                  <div>
                    <p><img src="https://omriavrahami.com/break-a-scene/static/images/examples/image_variations/dog_ball/mask_overlay.jpg"/></p><p>Input scene</p>
                  </div>
                </div>

                </div>
            </div>
          </section>

          <!-- Cat -->
          <!-- Bear -->
          
          

          <!-- Panda -->
          
          
        </div>
      </div>
    </div>
  </section>

  <section id="BibTeX">
    <div>
      <h2>BibTeX</h2>
      <p>If you find this project useful for your research, please cite the following:</p>

      <pre><code>@article{avrahami2023break,
        title={Break-A-Scene: Extracting Multiple Concepts from a Single Image},
        author={Avrahami, Omri and Aberman, Kfir and Fried, Ohad and Cohen-Or, Daniel and Lischinski, Dani},
        journal={arXiv preprint arXiv:2305.16311},
        year={2023}
}</code></pre>
    </div>
  </section>

  



</div>
  </body>
</html>
