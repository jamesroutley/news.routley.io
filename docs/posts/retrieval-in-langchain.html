<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.langchain.dev/retrieval/">Original</a>
    <h1>Retrieval in LangChain</h1>
    
    <div id="readability-page-1" class="page"><div id="site-main">
<article>

    <header>

        

        


        

            <figure>
                <img srcset="https://images.unsplash.com/photo-1576201836106-db1758fd1c97?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEwfHxnb2xkZW4lMjByZXRyaWV2ZXJ8ZW58MHx8fHwxNjc5NjMyNDA2&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=300 300w,
                            https://images.unsplash.com/photo-1576201836106-db1758fd1c97?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEwfHxnb2xkZW4lMjByZXRyaWV2ZXJ8ZW58MHx8fHwxNjc5NjMyNDA2&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=600 600w,
                            https://images.unsplash.com/photo-1576201836106-db1758fd1c97?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEwfHxnb2xkZW4lMjByZXRyaWV2ZXJ8ZW58MHx8fHwxNjc5NjMyNDA2&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1000 1000w,
                            https://images.unsplash.com/photo-1576201836106-db1758fd1c97?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEwfHxnb2xkZW4lMjByZXRyaWV2ZXJ8ZW58MHx8fHwxNjc5NjMyNDA2&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000 2000w" sizes="(min-width: 1400px) 1400px, 92vw" src="https://images.unsplash.com/photo-1576201836106-db1758fd1c97?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEwfHxnb2xkZW4lMjByZXRyaWV2ZXJ8ZW58MHx8fHwxNjc5NjMyNDA2&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Retrieval"/>
                    <figcaption>Photo by <a href="https://unsplash.com/@castillcc?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">Cristian Castillo</a> / <a href="https://unsplash.com/?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">Unsplash</a></figcaption>
            </figure>

    </header>

    <section>
        <p>TL;DR: We are adjusting our abstractions to make it easy for other retrieval methods besides the LangChain <code>VectorDB</code> object to be used in LangChain. This is done with the goals of (1) allowing retrievers <a href="https://github.com/openai/chatgpt-retrieval-plugin">constructed elsewhere</a> to be used more easily in LangChain, (2) encouraging more experimentation with alternative retrieval methods (like <a href="https://www.pinecone.io/learn/hybrid-search-intro/">hybrid search</a>). This is backwards compatible, so all existing chains should continue to work as before. However, we recommend updating from <code>VectorDB</code> chains to the new <code>Retrieval</code> chains as soon as possible, as those will be the ones most fully supported going forward.</p><p><a href="https://langchain.readthedocs.io/en/latest/modules/indexes/how_to_guides.html#retrievers">Python Docs</a></p><p><a href="https://hwchase17.github.io/langchainjs/docs/modules/indexes/retrievers/vectorstore">JS Docs</a></p><hr/><h2 id="introduction">Introduction</h2><p>Ever since ChatGPT came out, people have been building a personalized ChatGPT for their data. We even wrote <a href="https://blog.langchain.dev/tutorial-chatgpt-over-your-data/">a tutorial on this</a>, and then <a href="https://blog.langchain.dev/chat-your-data-challenge/">ran a competition</a> about this a few months ago. The desire and demand for this highlights an important limitation of ChatGPT - it doesn&#39;t know about YOUR data, and most people would find it more useful if it did. So how do you go about building a chatbot that knows about your data?</p><p>The main way of doing this is through a process commonly referred to as &#34;Retrieval Augmented Generation&#34;. In this process, rather than just passing a user question directly to a language model, the system &#34;retrieves&#34; any documents that could be relevant in answering the question, and then passes those documents (along with the original question) to the language model for a &#34;generation&#34; step.</p><p>The main way most people - including us at LangChain - have been doing retrieval is by using semantic search. In this process, a numerical vector (an embedding) is calculated for all documents, and those vectors are then stored in a vector database (a database optimized for storing and querying vectors). Incoming queries are then vectorized as well, and the documents retrieved are those who are closest to the query in embedding space. We&#39;re not going to go into too much detail on that here - but <a href="https://blog.langchain.dev/tutorial-chatgpt-over-your-data/">here</a> is a more in depth tutorial on the topic, and below is a diagram which nicely summarizes this.</p><figure><img src="https://blog.langchain.dev/content/images/2023/03/image-1.png" alt="" loading="lazy" width="641" height="267" srcset="https://blog.langchain.dev/content/images/size/w600/2023/03/image-1.png 600w, https://blog.langchain.dev/content/images/2023/03/image-1.png 641w"/><figcaption>Diagram of typical retrieval step</figcaption></figure><h2 id="problems">Problems</h2><p>This process works pretty well, and a lot of the components and abstractions we&#39;ve built (embeddings, vectorstores) are aimed at facilitating this process.</p><p>But we&#39;ve noticed two problems.</p><p><strong>First:</strong> there a lot of different variations in how you do this retrieval step. People want to do things beyond semantic search. To be concrete:</p><ul><li>We support two different query methods: one that just optimizes similarity, another with optimizes for <a href="https://www.cs.cmu.edu/~jgc/publication/The_Use_MMR_Diversity_Based_LTMIR_1998.pdf">maximal marginal relevance</a>.</li><li>Users often want to specify metadata filters to filter results before doing semantic search</li><li>Other types of indexes, <a href="https://langchain.readthedocs.io/en/latest/modules/indexes/chain_examples/graph_qa.html">like graphs</a>, have piqued user&#39;s interests</li></ul><p><strong>Second:</strong> we also realized that people may construct a retriever outside of LangChain - for example OpenAI released their <code><a href="https://github.com/openai/chatgpt-retrieval-plugin">ChatGPT Retrieval Plugin</a></code>. We want to make it as easy as possible for people to use whatever retriever they created within LangChain.</p><p>We realized we made a mistake - by making our abstractions centered around VectorDBQA we were limiting to use of our chains, making them hard to use (1) for users who wanted to experiment with other retrieval methods, (2) for users who created a retriever outside the LangChain ecosystem.</p><h2 id="solution">Solution</h2><p>So how did we fix this?</p><p>In our most recent Python and TypeScript releases, we&#39;ve:</p><ol><li>Introduced the concept of a <code>Retriever</code>. Retrievers are expected to expose a <code>get_relevant_documents</code> method with the following signature: <code>def get_relevant_documents(self, query: str) -&gt; List[Document]</code>. That&#39;s the only assumption we make about Retrievers. See more about this interface below.</li><li>Changed all our chains that used VectorDBs to now use Retrievers. <code>VectorDBQA</code> is now <code>RetrievalQA</code>, <code>ChatVectorDBChain</code> is now <code>ConversationalRetrievalChain</code>, etc. <em>Note that, moving forward, we are intentionally using the <code>Conversational</code> prefix to indicate that the chain is using memory and the <code>Chat</code> prefix to indicate the chain is using a chat model.</em></li><li>Added the first instance of a non-LangChain Retriever - the <code><a href="https://github.com/openai/chatgpt-retrieval-plugin">ChatGPT Retrieval Plugin</a></code>. This was a module open-sourced yesterday by OpenAI to help companies expose retrieval endpoints to hook into ChatGPT. NB: for all intents and purposes, the inner workings of the <code>ChatGPT Retrieval Plugin</code> are extremely similar to our VectorStores, but we are still extremely excited to integrate this as a way highlighting the new flexibility that exists.</li></ol><p>Expanding on the <code>Retriever</code> interface:</p><ul><li>We purposefully only require one method (<code>get_relevant_documents</code>) in order to be as permissive as possible. We do not (yet) require any uniform methods around construction of these retrievers.</li><li>We purposefully enforce <code>query: str</code> as the only argument. For all other parameters - including metadata filtering - this should be stored as parameters on the retriever itself. This is because we anticipate the retrievers often being used nested inside chains, and we do not want to have plumb around other parameters.</li></ul><p><strong><em>This is all done with the end goal of making it easier for alternative retrievers (besides the LangChain VectorStore) to be used in chains and agents, and encouraging innovation in alternative retrieval methods. </em></strong></p><h2 id="qa">Q&amp;A</h2><p><strong>Q: What&#39;s the difference between an index and a retriever?</strong></p><p><strong>A: </strong>An index is a data structure that supports efficient searching, and a retriever is the component that uses the index to find and return relevant documents in response to a user&#39;s query. The index is a key component that the retriever relies on to perform its function.</p><p><strong>Q: If I was using a VectorStore before in <code>VectorDBQA</code> chain (or other <code>VectorDB</code>-type chains), what do I now use in <code>RetrievalQA</code> chain?</strong></p><p><strong>A:</strong> You can use a <code>VectorStoreRetriever</code>, which you can create from an existing vectorstore by doing <code>vectorstore.as_retriever()</code></p><p><strong>Q: Does <code>VectorDBQA</code> chain (or other <code>VectorDB</code>-type chains) still exist?</strong></p><p><strong>A:</strong> Yes, although we will be no be focusing on it any more. Expect any future development to be done on <code>RetrievalQA</code> chain.</p><p><strong>Q: Can I contribute a new retrieval method to the library?</strong></p><p><strong>A:</strong> Yes! We started a new <code>langchain/retrievers</code> module exactly for this purpose</p><p><strong>Q: What are real world examples this enables?</strong></p><p><strong>A: </strong>The main one is better question-answering over your documents. However, if start to ingest and then retrieve previous messages, this can then be thought of as better long term memory for AI.</p>
    </section>


</article>
</div></div>
  </body>
</html>
