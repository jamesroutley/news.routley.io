<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://openai.com/blog/introducing-superalignment">Original</a>
    <h1>Introducing Superalignment</h1>
    
    <div id="readability-page-1" class="page"><div id="content"><!--[--><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><div><p>Superintelligence will be the most impactful technology humanity has ever invented, and could help us solve many of the world’s most important problems. But the vast power of superintelligence could also be very dangerous, and could lead to the disempowerment of humanity or even human extinction.</p><p>While superintelligence<span><sup><span>[^superintelligence]</span></sup><!----></span> seems far off now, we believe it could arrive this decade.</p><p>Managing these risks will require, among <a href="https://openai.com/blog/how-should-ai-systems-behave" rel="noopener noreferrer" target="_blank">other things</a>, <a href="https://openai.com/blog/governance-of-superintelligence" rel="noopener noreferrer" target="_blank">new institutions for governance</a> and solving the problem of superintelligence alignment:</p></div></div></div></div></div></div><!----><!----><!----><!----></div><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><div><p>Our goal is to build a roughly human-level <a href="https://openai.com/blog/our-approach-to-alignment-research" rel="noopener noreferrer" target="_blank">automated alignment researcher</a>. We can then use vast amounts of compute to scale our efforts, and iteratively align superintelligence.</p><p>To align the first automated alignment researcher, we will need to 1) develop a scalable training method, 2) validate the resulting model, and 3) stress test our entire alignment pipeline:</p><ol><li>To provide a training signal on tasks that are difficult for humans to evaluate, we can leverage AI systems to <a href="https://openai.com/research/critiques" rel="noopener noreferrer" target="_blank">assist evaluation of other AI systems</a> (<em>scalable oversight). </em>In addition, we want to understand and control how our models generalize our oversight to tasks we can’t supervise (<em>generalization)</em>.</li><li>To validate the alignment of our systems, we <a href="https://www.deepmind.com/blog/red-teaming-language-models-with-language-models" rel="noopener noreferrer" target="_blank">automate search for problematic behavior</a> (<em>robustness) </em>and problematic internals (<a href="https://openai.com/research/language-models-can-explain-neurons-in-language-models" rel="noopener noreferrer" target="_blank"><em>automated interpretability</em></a>).</li><li>Finally, we can test our entire pipeline by deliberately training misaligned models, and confirming that our techniques detect the worst kinds of misalignments (<em>adversarial testing</em>).</li></ol><p>We expect our research priorities will evolve substantially as we learn more about the problem and we’ll likely add entirely new research areas. We are planning to share more on our roadmap in the future.<br/></p></div></div></div></div></div></div><!----><!----><!----><!----></div><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><div><p>We are assembling a team of top machine learning researchers and engineers to work on this problem. </p><p>We are dedicating 20% of the compute we’ve secured to date over the next four years to solving the problem of superintelligence alignment. Our chief basic research bet is our new Superalignment team, but getting this right is critical to achieve our mission and we expect many teams to contribute, from developing new methods to scaling them up to deployment.<br/></p></div></div></div></div></div></div><!----><!----><!----><!----></div><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><figure><blockquote><p><span>Our goal is to solve the core technical challenges of superintelligence alignment in four years.</span></p></blockquote><!----></figure></div></div></div></div><!----><!----><!----><!----><!----><!----><!----><!----><!----></div><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><div><p>While this is an incredibly ambitious goal and we’re not guaranteed to succeed, we are optimistic that a focused, concerted effort can solve this problem:<span><sup><span>[^problem]</span></sup><!----></span> There are many ideas that have shown promise in preliminary experiments, we have increasingly useful metrics for progress, and we can use today’s models to study many of these problems empirically. </p><p>Ilya Sutskever (cofounder and Chief Scientist of OpenAI) has made this his core research focus, and will be co-leading the team with Jan Leike (Head of Alignment). Joining the team are researchers and engineers from our previous alignment team, as well as researchers from other teams across the company.</p><p>We’re also looking for outstanding new researchers and engineers to join this effort. Superintelligence alignment is fundamentally a machine learning problem, and we think great machine learning experts—even if they’re not already working on alignment—will be critical to solving it.</p><p>We plan to share the fruits of this effort broadly and view contributing to alignment and safety of non-OpenAI models as an important part of our work.</p><p>This new team’s work is in addition to existing work at OpenAI aimed at improving the <a href="https://openai.com/blog/our-approach-to-ai-safety" rel="noopener noreferrer" target="_blank">safety of current models</a> like ChatGPT, as well as understanding and mitigating other risks from AI such as misuse, economic disruption, disinformation, bias and discrimination, addiction and overreliance, and others. While this new team will focus on the machine learning challenges of aligning superintelligent AI systems with human intent, there are related sociotechnical problems on which we are <a href="https://openai.com/blog/democratic-inputs-to-ai" rel="noopener noreferrer" target="_blank">actively engaging with interdisciplinary experts</a> to make sure our technical solutions consider broader human and societal concerns.<br/></p></div></div></div></div></div></div><!----><!----><!----><!----></div><div><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><div><div><div><div><div><div><p>Superintelligence alignment is one of the most important unsolved technical problems of our time. We need the world’s best minds to solve this problem.</p><p>If you’ve been successful in machine learning, but you haven’t worked on alignment before, this is your time to make the switch! We believe this is a tractable machine learning problem, and you could make enormous contributions.</p><p><em></em><a href="https://openai.com/careers/research-engineer-superalignment" rel="noopener noreferrer" target="_blank"><em>research engineer</em></a><em>, </em><a href="https://openai.com/careers/research-scientist-superalignment" rel="noopener noreferrer" target="_blank"><em>research scientist</em></a><em>, and </em><a href="https://openai.com/careers/research-manager-superalignment" rel="noopener noreferrer" target="_blank"><em>research manager</em></a><em> positions.</em><br/></p></div></div></div></div></div></div><!----><!----><!----><!----></div><!--]--></div></div>
  </body>
</html>
