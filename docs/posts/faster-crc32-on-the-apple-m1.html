<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://dougallj.wordpress.com/2022/05/22/faster-crc32-on-the-apple-m1/">Original</a>
    <h1>Faster CRC32 on the Apple M1</h1>
    
    <div id="readability-page-1" class="page"><div>
		
<p>CRC32 is a checksum first proposed in 1961, and now used in a wide variety of performance sensitive contexts, from file formats (zip, png, gzip) to filesystems (ext4, btrfs) and protocols (like ethernet and SATA). So, naturally, a lot of effort has gone into optimising it over the years. However, I discovered a simple update to a widely used technique that makes it possible to run twice as fast as existing solutions on the Apple M1.</p>



<p>Searching for the state-of-the-art, I found a lot of outdated posts, which is unsurprising for a sixty year old problem. Eventually I found a <a href="https://dev.mysql.com/blog-archive/faster-crc32-c-computation-in-mysql-8027/">MySQL blog post from November 2021</a> that presents the following graph, including M1 figures, and gives us some idea that 30GB/s is considered fast:</p>



<figure><a href="https://dougallj.files.wordpress.com/2022/05/crc32_1.png"><img data-attachment-id="1327" data-permalink="https://dougallj.wordpress.com/2022/05/22/faster-crc32-on-the-apple-m1/crc32_1/" data-orig-file="https://dougallj.files.wordpress.com/2022/05/crc32_1.png" data-orig-size="710,392" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="crc32_1" data-image-description="" data-image-caption="" data-medium-file="https://dougallj.files.wordpress.com/2022/05/crc32_1.png?w=300" data-large-file="https://dougallj.files.wordpress.com/2022/05/crc32_1.png?w=628" src="https://dougallj.files.wordpress.com/2022/05/crc32_1.png?w=710" alt="" srcset="https://dougallj.files.wordpress.com/2022/05/crc32_1.png 710w, https://dougallj.files.wordpress.com/2022/05/crc32_1.png?w=150 150w, https://dougallj.files.wordpress.com/2022/05/crc32_1.png?w=300 300w" sizes="(max-width: 710px) 100vw, 710px"/></a></figure>



<p>In fact, in my own testing of the zlib <strong>crc32</strong> function, I saw that it performs at around 30GB/s on the M1, so a little better than the graph, which is promising. Possibly that version has been optimised by Apple?</p>



<p>I wanted to try to implement my own version. So, I started at the obvious place, with a special ARM64 instruction designed for calculating CRC32 checksums: <strong>CRC32X</strong>. This can produce a checksum of 8-bytes, with a latency of 3 cycles. So, theoretically, using this instruction, we could get 3.2GHz / 3 * 8B = 8.5GB/s. On the other hand, <strong>CRC32X</strong> has a throughput of one per cycle, so supposing we can avoid being latency bound (e.g. by calculating bits of the CRC in chunks, and then combining them) we could get 3.2GHz / 1 * 8B = 25.6GB/s. That’s <em>maybe</em> a little better than the numbers in the MySQL chart, but this is a theoretical best case, not accounting for the overhead of combining the results.</p>



<p>So, can we do better than <strong>CRC32X</strong>? The M1 can run eight instructions per cycle, and our best idea so far only runs at one instruction per cycle, so maybe we can. Besides, I already tested zlib, and it already performs at 30GB/s, so I <em>know</em> there’s a better way.</p>



<p>The better way was published by Intel in the 2009 paper <em><a href="https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/fast-crc-computation-generic-polynomials-pclmulqdq-paper.pdf">Fast CRC Computation for Generic Polynomials Using PCLMULQDQ Instruction</a></em>. This algorithm has been widely implemented, and has been ported to use equivalent ARM64 instructions, <strong>PMULL</strong> and <strong>PMULL2</strong>, e.g. <a href="https://chromium.googlesource.com/chromium/src/+/a0771caebe87477558454cc6d793562e3afe74ac/third_party/zlib/crc32_simd.c#321">in Chromium</a> (coincidentally committed only hours ago at the time of this writing).</p>



<p>I won’t delve into the maths – I don’t even properly understand it – but the main loop has four independent latency chains that look something like this:</p>



<pre><code>x5 = (uint64x2_t) pmull_lo(x1, x0);
y7 = vld1q_u64((const uint64_t *)(buf));
x1 = (uint64x2_t) pmull_hi(x1, x0);
x1 = veorq_u64(x1, x5);
x1 = veorq_u64(x1, y5);</code></pre>



<p>At a glance, I’d say the latency chain was <strong>PMULL2</strong> (3) + <strong>EOR</strong> (2) + <strong>EOR</strong> (2) = 7 cycles. However, the M1 can fuse <strong>PMULL</strong> / <strong>PMULL2</strong> instructions with subsequent <strong>EOR</strong> instructions, giving a single uop with three cycle latency. So we can reshuffle this to [<strong>PMULL</strong> + <strong>EOR</strong>] (3) + [<strong><strong>PMULL</strong>2 + <strong>EOR</strong></strong>] (3) = 6 cycles. (This is ideal for maximising throughput, as it minimises fused uop count, but if you want to minimise latency you can take the PMULL2 off the critical path, giving [<strong>PMULL</strong> + <strong>EOR</strong>] (3) +<strong> <strong>EOR</strong></strong> (2) = 5 cycles.)</p>



<p>So we know each chain will have a latency of 6 cycles and have 2 (fused) uops. These uops have a throughput of 4 per cycle, so we can calculate how many independent chains we could benefit from. Over 6 cycles, we can run 6*4 = 24 uops. Since each chain needs 2 uops, we should benefit from having as many as 24/2=12 independent chains – three times more than the 2009 paper used, but also three times more than the more recent implementations I’ve seen.</p>



<p>To be wildly optimistic, if SIMD throughput were the bottleneck, this could run at 0.5 cycles per 16-byte register. 3.2GHz / 0.5 * 16B = 102GB/s. However, that SIMD throughput requires we sustain the maximum of eight unfused instructions per cycle, which would leave no time to load values from memory. Since we’ll need 1 load uop for every four unfused uops (for a total of five out of the eight possible unfused uops per cycle), a more realistic estimate of the frontend limit is 3.2GHz / (5/8) * 16B = 82GB/s.</p>



<p>(By contrast, if we only process 4*16B = 64B per cycle and have a 6 cycle critical path, we could at most reach 3.2GHz / 6 * 64B = 34GB/s.)</p>



<p>Implementing this is mostly fairly straightforward – stepping in 192 byte increments and copying code to add more latency chains, but it does requires computing new values for <strong>k1</strong> and <strong>k2</strong>, which I did by calling the private zlib function <a href="https://github.com/madler/zlib/blob/21767c654d31d2dccdde4330529775c6c5fd5389/crc32.c#L562-L580">x2nmodp</a>:</p>



<pre><code>uint64_t k1 = (uint64_t)x2nmodp(12*128+32, 0) &lt;&lt; 1; // 0x1821d8bc0
uint64_t k2 = (uint64_t)x2nmodp(12*128-32, 0) &lt;&lt; 1; // 0x12e968ac4</code></pre>



<p>The resulting code runs at about 70GB/s on the M1, reaching up to 75GB/s if the assembly is adjusted to always have valid fusion pairs. There’s probably still some room for improvement, but I’m pretty happy with that.</p>



<p>My <a href="https://gist.github.com/dougallj/a15a0feac92e2126932b0fbe1de7863d">test code is available</a>, although it is not suitable for real-world use as-is.</p>
			</div></div>
  </body>
</html>
