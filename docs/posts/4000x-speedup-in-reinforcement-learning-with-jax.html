<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://chrislu.page/blog/meta-disco/">Original</a>
    <h1>4000x Speedup in Reinforcement Learning with Jax</h1>
    
    <div id="readability-page-1" class="page"><div>
    <h2>TL;DR</h2>
    <p>
      We can leverage recent advancements in <a href="https://github.com/google/jax">JAX</a> to train parallelised RL
      agents over 4000x faster entirely on GPUs. Unlike past RL implementations, ours is written end-to-end in
      Jax. This enables RL researchers to do things like:
    </p>
    <ul>
      <li>üèÉ Efficiently run tons of seeds in parallel on one GPU</li>
      <li>üíª Perform rapid hyperparameter tuning</li>
      <li>ü¶é Discover new RL algorithms with meta-evolution</li>
    </ul>
    <p>
      The simple, self-contained code is here: <a href="https://github.com/luchris429/purejaxrl">https://github.com/luchris429/purejaxrl</a>.
    </p>

    <h2>Overview</h2>

    <p>
      This blog post is about a computational and experimental paradigm that powers many recent and ongoing works at
      the <a href="https://foersterlab.com/">Foerster Lab for AI Research (FLAIR)</a> that efficiently utilises GPU
      resources to meta-evolve new discoveries in Deep RL. The techniques that power this paradigm have the potential
      radically accelerate the rate of progress in Deep Reinforcement Learning (RL) research by more heavily utilising
      GPU resources, enabled by recent advancements in JAX. The codebase, <a href="https://github.com/luchris429/purejaxrl">PureJaxRL</a>, vastly lowers the computational
      barrier of entry to Deep RL research, enabling academic labs to perform research using trillions of frames
      (closing the gap with industry research labs) and enabling independent researchers to get orders of magnitude more
      mileage out of a single GPU.
    </p>

    <p>
      This blog post will be split into two parts. The first will be about the computational techniques that
      enable this paradigm. The second will discuss how we can effectively leverage these techniques to deepen our
      understanding of RL agents and algorithms with evolutionary meta-learning. We will then briefly describe three
      recent papers from our lab that heavily utilised this framework:
    </p>
    <ul>
      <li><a href="https://arxiv.org/abs/2205.01447">Model-Free Opponent Shaping</a>
        (ICML 2022)
      </li>
      <li><a href="https://arxiv.org/abs/2210.05639">Discovered Policy Optimisation</a>
        (NeurIPS 2022)
      </li>
      <li><a href="https://arxiv.org/abs/2211.11030">Adversarial Cheap Talk</a>
        (Preprint)</li>
    </ul>
    <h2>Part 1: Over 4000x Speedups with PureJaxRL</h2>

    <h3>Section 1.1: Run Everything on the GPU!</h3>

    <p>
      Most Deep RL implementations run on a combination of CPU and GPU resources. Usually, the environments run on the
      CPU while the policy neural network and algorithms run on the GPU. To increase the wallclock time,
      practitioners run multiple environments in parallel using multiple threads. We will look at
      Costa Huang&#39;s amazing <a href="https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo.py">CleanRL library</a>
      as an example of a well-benchmarked implementation of PPO under this &#34;standard&#34; paradigm.
    </p>

    <p>
      Instead of using multiple threads for our environment, we can use Jax to <i>vectorise</i> the environment and run
      it on the GPU! Not only does this allow us to avoid having to transfer data between the CPU and GPU, but if we
      program our environment using Jax primitives, we can use Jax&#39;s powerful <a href="https://jax.readthedocs.io/en/latest/jax-101/03-vectorization.html"><i>vmap</i></a> function to
      instantly create a vectorised version of the environment. While re-writing RL environments in Jax can be time
      consuming, luckily for us, a few libraries have already done this for us for a variety of environments.</p>

    <p>
      There are a few complementary libraries that we recommend:

    </p><ul>
      <li><a href="https://github.com/RobertTLange/gymnax">Gymnax</a> is an awesome
        library that contains several well-known environments such as Classic Control tasks, Bsuite tasks, and Minatar
        (Atari-like) environments. I use it as the go-to library for testing and evaluating my code, and we will use it
        for this blog post. It has many other really neat features and is very easy to use.
        <a href="https://github.com/RobertTLange/gymnax">
          

          
        </a>

      </li>
      <li><a href="https://github.com/google/brax">Brax</a> is <i>the</i> way to run
        Mujoco-like continuous control environments using Jax. The library has many RL environments directly analogous
        to popular continuous control environments, such as HalfCheetah and Humanoid. It&#39;s also
        differentiable!</li>

      <a href="https://github.com/google/brax">
        
      </a>

      <li><a href="https://github.com/instadeepai/jumanji">Jumanji</a> contains a lot of
        incredibly cool, simple, and industry-driven environments. Many of the environments in this library come
        directly from industry settings, ensuring that the environments presented here are practical and relevant for
        the real world! These include combinatorial problems such as the famous Traveling Salesman Problem or 3D Bin
        Packing.</li>

      <a href="https://github.com/instadeepai/jumanji">
        
      </a>
      <li><a href="https://github.com/sotetsuk/pgx">Pgx</a> Has many popular board games
        and other environments implemented in Jax. This includes Connect 4, Go, and Poker!</li>

      <a href="https://github.com/sotetsuk/pgx">
        
      </a>
    </ul>
    

    <p>
      Let&#39;s look at some of the <a href="https://github.com/RobertTLange/gymnax-blines">reported speedups for Gymnax
        below.</a> CartPole-v1 in numpy, with 10 environments running in parallel, takes 46 seconds to reach one
      million frames. Using Gymnax on an A100, with 2k environments in parallel takes <i>0.05 seconds</i>.
      That&#39;s a <i>1000x</i> speedup. This applies to environments more complicated than CartPole-v1 as well. For
      example, Minatar-Breakout, which takes 50 seconds to reach one million frames on CPU only takes <i>0.2 seconds</i>
      in Gymnax. These results show an improvement of <i>several orders of magnitude,</i> enabling academic researchers
      to efficiently run experiments involving <i>trillions</i> of frames on limited hardware.
    </p>
    <a href="https://github.com/RobertTLange/gymnax-blines/blob/main/docs/speed.png">
      <img src="https://chrislu.page/blog/meta-disco/dist/img/speed.png" alt="Gymnax Speedups"/>
    </a>

    <!-- <p>
      Due to <a href="https://en.wikipedia.org/wiki/Amdahl%27s_law">Amdahl's Law</a>, this unfortunately does not equate
      to a 1000x speedup in training time. However, it does mean that we can run many more environments in parallel,
      which can lead to alternative training paradigms for RL. For example, <a
        href="https://github.com/google/brax">Brax's PPO implementation</a> runs <i>2048</i> environments in parallel
      with a short rollout length of 5. This is nearly the exact opposite of OpenAI Baseline's impelementation that runs
      a single environment with a rollout length of 2048. While Brax's PPO ultimately ends up being less sample
      efficient, it runs <i>significantly faster.</i> However, this type of wallclock speedup is not what this blog post
      is mainly about.
    </p> -->

    <p>
      There are many advantages to doing everything end-to-end in Jax. To name a
      few:

      </p><li>Vectorising environments on the accelerator allows us to run them quickly.</li>
      <li>By keeping the computation entirely on the GPU, we avoid the overhead of
        copying data back and forth between the CPU and GPU, which is often a significant bottleneck.</li>
      <li>By <a href="https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html">JIT
          compiling</a> our implementation, we avoid the overhead of Python, which sometimes block GPU
        computation between sending commands. </li>
      <li>JIT compilation can lead to significant speedups through operator fusion. In
        other words, it optimises memory usage on the GPU.</li>
      <li>It is fully synchronous. Multi-processing for running environments in
        parallel is notoriously difficult to debug and leads to complicated infrastructure.</li>
    

    <!-- CODEX WROTE THE ENTIRE PARAGRAPH BELOW...JUST KEEPING IT IN HERE AS A COMMENT -->
    <!-- <p>
      However, there are some disadvantages to doing everything in Jax. For one, it is not as easy to debug as
      traditional Python. For example, if you make a mistake in your Jax code, you will get a Jax error, which is
      often not very helpful. Secondly, it is not as easy to write custom environments. For example, if you want to
      write a custom environment, you will have to write it in Jax. This is not a problem if you are already familiar
      with Jax, but it can be a significant barrier to entry for new users. Thirdly, it is not as easy to write custom
      algorithms. For example, if you want to write a custom algorithm, you will have to write it in Jax. This is not a
      problem if you are already familiar with Jax, but it can be a significant barrier to entry for new users.
    </p> -->

    <p>
      To demonstrate this, we closely replicated CleanRL&#39;s PyTorch PPO baseline implementation in pure Jax and jitted it
      end-to-end. We used the same number of parallel environments and the same hyperparameter settings, so we&#39;re not
      taking advantage of the massive environment vectorisation. We show the training plots below across 5 runs in
      CartPole-v1 and MinAtar-Breakout.
    </p>

    <figure>
      <div>
        <p><img src="https://chrislu.page/blog/meta-disco/dist/plots/cartpole_plot_frames.png" alt="CartPole-Frames"/>
        </p>
        <p><img src="https://chrislu.page/blog/meta-disco/dist/plots/minatar_plot_frames.png" alt="MinAtar-Frames"/>
        </p>
      </div>
      <figcaption>Figure 1: CleanRL vs. Our Jax PPO on CartPole-v1 and MinAtar-Breakout. We
        achieve nearly identical results given the same hyperparameters and number of frames.</figcaption>
    </figure>

    <p>
      Now, let&#39;s swap out the x-axis for the wall-clock time instead of frames. It&#39;s over 10x faster without any extra
      parallel environments.
    </p>

    <figure>
      <div>
        <p><img src="https://chrislu.page/blog/meta-disco/dist/plots/cartpole_plot_seconds.png" alt="CartPole-Frames"/>
        </p>
        <p><img src="https://chrislu.page/blog/meta-disco/dist/plots/minatar_plot_seconds.png" alt="MinAtar-Frames"/>
        </p>
      </div>
      <figcaption>Figure 2: CleanRL vs. Our Jax PPO on CartPole-v1 and MinAtar-Breakout. We
        achieve the same results but over 10x faster!</figcaption>
    </figure>

    <p>
      The code for this is shown below and is available at <a href="https://github.com/luchris429/purejaxrl">this
        repository.</a> It&#39;s all within a single readable file, so it is easy to use!
    </p>

    

    <h3>Section 1.2: Running Many Agents in Parallel</h3>

    <p>
      We got a pretty good speedup from the above tricks. However, it&#39;s far from the 4000x speedup in the headline. How
      do we get there? By <u>vectorising the entire RL training loop.</u> This is really easy to do! Just use Jax&#39;s
      <a href="https://jax.readthedocs.io/en/latest/jax-101/03-vectorization.html"><i>vmap</i></a> we mentioned before!
      Now we can train many agents in parallel.
    </p>

    

    <p>
      (Furthermore, we can use Jax&#39;s convenient <a href="https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html"><i>pmap</i></a> function to run on
      multiple GPU&#39;s! Previously, this type of parallelisation and vectorisation both across and <i>especially</i>
      within devices would have been a massive headache to write.)
    </p>

    <figure>
      <div>
        <p><img src="https://chrislu.page/blog/meta-disco/dist/plots/cartpole_plot_parallel.png" alt="CartPole-Frames"/>
        </p>
        <p><img src="https://chrislu.page/blog/meta-disco/dist/plots/minatar_plot_parallel.png" alt="MinAtar-Frames"/>
        </p>
      </div>
      <figcaption>Figure 3: CleanRL vs. Our Jax PPO on CartPole-v1 and MinAtar-Breakout. We
        can parallelise the agent training itself! On CartPole-v1 we can train 2048 agents in about half the time it
        takes to train a single CleanRL agent!</figcaption>
    </figure>

    <p>
      That&#39;s more like it! If you&#39;re developing a new RL algorithm, you can quickly train on a statistically-significant
      number of seeds <i>simultaneously on a single GPU</i>. Beyond that, we can train <i>thousands of independent
        agents at the same time!</i> In the <a href="https://github.com/luchris429/purejaxrl/blob/main/examples/example_0.ipynb">notebook we provide</a>, we
      show how to use this for rapid hyperparameter search. However, we can also use this for evolutionary
      meta-learning!
    </p>

    <h2>Part 2: Meta-Evolving Discoveries for Deep RL</h2>

    <p>
      Meta-learning, or &#34;learning to learn,&#34; has the potential to revolutionize the field of reinforcement learning (RL)
      by discovering general principles and algorithms that can be applied across a broad range of tasks. At FLAIR,
      we use the above computational technique to power new discoveries with Meta-RL by using evolution. This
      approach promises to enhance our understanding of RL algorithms and agents, and the advantages it offers are well
      worth exploring.
    </p>

    <p>
      Traditional meta-learning techniques, which often use meta-gradients or higher-order derivatives, focus on quickly
      adapting to similar but unseen tasks using only a small number of samples. While this works well within specific
      domains, it falls short of achieving general-purpose learning algorithms that can tackle diverse tasks across many
      updates. This limitation becomes even more pronounced when attempting to meta-learn across millions of timesteps
      and thousands of updates, as gradient-based methods often result in high variance updates that compromise
      performance. For more information on the limitations of gradient-based meta-learning, we recommended reading <a href="http://lukemetz.com/exploring-hyperparameter-meta-loss-landscapes-with-jax/">Luke Metz&#39;s blog post</a> on
      the topic.
    </p>

    <p>
      Evolutionary methods, on the other hand, offer a promising alternative. By treating the underlying problem as a
      black box and avoiding explicitly calculating derivatives, they can efficiently and effectively meta-learn across
      long horizons. For a comprehensive introduction to these methods, we recommend <a href="https://blog.otoro.net/2017/10/29/visual-evolution-strategies/">David Ha&#39;s blog post</a>. The
      key advantages of evolutionary strategies (ES) include:
    </p>

    <li> Agnosticism to the number of learning timesteps</li>
    <li> No concerns with vanishing or exploding gradients</li>
    <li> Unbiased updates</li>
    <li> Often Lower variance</li>
    <li> Highly parallelisability</li>

    <p>At a high level, this approach mirrors the emergence of learning in nature, where animals have genetically
      evolved to perform reinforcement learning in their brains
    </p>

    <p>
      The main criticism of evolutionary methods is that they can be slow and sample-inefficient, often requiring
      thousands of parameters to be evaluated simultaneously. This framework addresses these
      concerns by enabling rapid parallel evaluation with limited hardware, making the use of evolution in meta-RL an
      attractive and practical option.
    </p>

    <p>
      A good library for doing this is <a href="https://github.com/RobertTLange/evosax">Robert Lange&#39;s evosax
        library</a> (He&#39;s also the creator of Gymnax!). We can easily hook up our RL training loop to this library and
      use it to perform extremely fast meta-evolution entirely on the GPU. Here&#39;s a simple example from an upcoming
      project. (Keep your eyes peeled for our paper on this!) In this example, we meta-learn the value loss
      function of a PPO agent on CartPole-v1. While L2 loss is the most popular choice for the value loss in PPO, we
      can instead parameterise this with a neural network and evolve it! On the outer loop, we sample the parameters for
      this neural network (which we will call meta-parameters), and on the inner loop we train RL agents from scratch
      using those meta-parameters for the value loss function. You can view the code and follow along in our <a href="https://github.com/luchris429/purejaxrl/blob/main/examples/example_0.ipynb">provided notebook</a>.
    </p>

    <p>
      On a single Nvidia A40, we train 512 agents for 1024 generations, churning through over one hundred billion
      frames. In other words, we trained over half a million agents in ~9 hours on a single GPU! The performance of the
      resulting meta-learned value loss function is shown below.
    </p>

    <figure>
      <div>
        <p><img src="https://chrislu.page/blog/meta-disco/dist/plots/meta_value_cartpole_plot_frames.png" alt="CartPole-Frames"/>
        </p>
        <p><img src="https://chrislu.page/blog/meta-disco/dist/plots/meta_value_cartpole_meta.png" alt="MinAtar-Frames"/>
        </p>
      </div>
      <figcaption>Figure 4: Meta-Learning the Value Distance function. The resulting learned
        distance function outperforms L2.</figcaption>
    </figure>

    <p>
      Finally, we visualise, interpret, and understand what the learned meta-parameters are doing. In this
      case, we plot the loss function below.
    </p>

    <figure>
      <div>
        <p><img src="https://chrislu.page/blog/meta-disco/dist/plots/meta_value_cartpole_vis.png" alt="MinAtar-Frames"/></p><figcaption>Figure 5: Meta-Learned Value Distance Function</figcaption>
    </div></figure>

    <p>That looks interesting -- it looks nothing like the standard L2 loss! It&#39;s not symmetric and it isn&#39;t even
      convex. This is currently ongoing preliminary work from an upcoming project. To summarise, the meta-evolving
      discovery framework involves:
      </p><li>Running everything on the GPU by using Jax.</li>
      <li>Meta-Learning across entire training trajectories with evolutionary methods.
      </li>
      <li>Interpreting the learned meta-parameters to &#34;discover&#34; new insights about
        learning algorithms.</li>
    

    <h2>Part 3: Case Studies</h2>
    <p>
      This is a very powerful framework that we use at FLAIR to better understand the behavior of RL algorithms and have
      used it in several published papers, which you can read below. We will also be releasing future blog posts going
      more in-depth into these works.
    </p>

    <h3><a href="https://arxiv.org/abs/2210.05639">Discovered Policy Optimisation (NeurIPS 2022)</a></h3>

    <p><a href="https://arxiv.org/abs/2210.05639"><b>https://arxiv.org/abs/2210.05639</b></a></p>

    <blockquote>
      <p lang="en" dir="ltr">Deep RL has been driven by improvements in handcrafted algorithms. Our NeurIPS 2022 paper,
        ‚ÄúDiscovered Policy Optimisation‚Äù instead meta-learns in a space of theoretically-sound algorithms and beats PPO
        on unseen tasks! w/ <a href="https://twitter.com/kuba_AI?ref_src=twsrc%5Etfw">@kuba_AI</a> <a href="https://twitter.com/_aletcher?ref_src=twsrc%5Etfw">@_aletcher</a> <a href="https://twitter.com/Luke_Metz?ref_src=twsrc%5Etfw">@Luke_Metz</a> <a href="https://twitter.com/casdewitt?ref_src=twsrc%5Etfw">@casdewitt</a> <a href="https://twitter.com/j_foerst?ref_src=twsrc%5Etfw">@j_foerst</a> üßµ <a href="https://t.co/H4Zp3siuZH">pic.twitter.com/H4Zp3siuZH</a></p>‚Äî Chris Lu (@_chris_lu_) <a href="https://twitter.com/_chris_lu_/status/1595388750330155010?ref_src=twsrc%5Etfw">November 23, 2022</a>
    </blockquote>
    

    <h3><a href="https://arxiv.org/abs/2205.01447">Model-Free Opponent Shaping (ICML 2022)</a></h3>

    <p><a href="https://arxiv.org/abs/2205.01447"><b>https://arxiv.org/abs/2205.01447</b></a></p>

    <blockquote>
      <p lang="en" dir="ltr">General-sum games describe many scenarios, from negotiations to autonomous driving. How
        should an AI act in the presence of other learning agents? Our <a href="https://twitter.com/icmlconf?ref_src=twsrc%5Etfw">@icmlconf</a> 2022 paper, ‚ÄúModel-Free Opponent
        Shaping‚Äù(M-FOS) approaches this as a meta-game. <a href="https://twitter.com/_chris_lu_?ref_src=twsrc%5Etfw">@_chris_lu_</a> <a href="https://twitter.com/TimonWilli?ref_src=twsrc%5Etfw">@TimonWilli</a> <a href="https://twitter.com/casdewitt?ref_src=twsrc%5Etfw">@casdewitt</a> üßµ <a href="https://t.co/wshwxpTNTP">pic.twitter.com/wshwxpTNTP</a></p>‚Äî Jakob Foerster (@j_foerst) <a href="https://twitter.com/j_foerst/status/1547245398825345025?ref_src=twsrc%5Etfw">July 13, 2022</a>
    </blockquote>
    

    <h3><a href="https://arxiv.org/abs/2211.11030">Adversarial Cheap Talk (Preprint)</a></h3>

    <p><a href="https://arxiv.org/abs/2211.11030"><b>https://arxiv.org/abs/2211.11030</b></a></p>

    <p>Tweet Thread TBD</p>

    <!-- <h3>Adversarial Cheap Talk</h3>
    <a href="https://arxiv.org/abs/2211.11030">
      <p><b>Link</b></p>
    </a>
    <p><b>What we meta-learned:</b> We append deterministic features to existing RL environments and show that we can
      cause RL algorithms to completely fail or perform better, even though simply appending features should not
      influence the underlying dynamics or optimal policy. In theory the RL agent should be able to ignore the appended
      inputs!</p>
    <a href="https://arxiv.org/abs/2211.11030">
      <div class="row">
        <div class="column">
          <img src="dist/img/act_cartpole_curves.png" alt="ACT-CartPole" style="width:100.0%">
        </div>
        <div class="column">
          <img src="dist/img/act_pendulum_curves.png" alt="ACT-Pendulum" style="width:100.0%">
        </div>
        <div class="column">
          <img src="dist/img/act_reacher_curves.png" alt="ACT-Reacher" style="width:100.0%">
        </div>
      </div>
    </a>
    <a href="https://arxiv.org/abs/2211.11030">
      <div class="row">
        <div class="column" style="width:50.0%">
          <img src="dist/gifs/act/pendulum_ally_fast.gif" alt="ACT-Pendulum Ally" style="width:100.0%">
        </div>
        <div class="column" style="width:50.0%">
          <img src="dist/gifs/act/pendulum_adversary_fast.gif" alt="ACT-Pendulum Adversary" style="width:100.0%">
        </div>
      </div>
    </a>
    <p><b>What we visualised:</b> We visualised the cosine distance between gradient updates in different parts of the
      environment. This has previously been used to measure notions of catastrophic interference.</p>
    <img src="dist/img/ACT-Grads.png" alt="ACT Visualisation" style="width:100.0%">
    </p>
    <p><b>What we discovered:</b>Adversarial ACT works by learning to induce catastrophic interference in the learning
      agent's function approximator. In other words, gradient updates taken at the beginning steps of the environment
      interfere and cancel out with gradient steps taken at the ending steps of the environment.
      Allied ACT works by doing the exact opposite way: The gradient steps taken at different parts of the environment
      are positively correlated, meaning that gradients for steps taken in the beginning help performance at the steps
      at the end of the episode!
    </p>

    <p><b>Something Extra in the Paper:</b> We also meta-learned an adversary that learns to encode spurious
      correlations over the cheap talk channel during training, and then instantly leverage the cheap talk channel at
      test time to achieve an arbitrary goal! In the gif below, the agent is trained to reach for the blue circle with a
      given cheap talk channel. At test time, when the cheap talk channel is swapped out, the agent ends up reaching for
      the yellow one!</p>
    <a href="https://arxiv.org/abs/2211.11030">
      <img src="dist/gifs/act/reacher_zeroshot_fast.gif" alt="ACT-Reacher Fast" style="width:100.0%">
    </a> -->

    <h2>Part 4: Related Works</h2>
    <p>
      The ideas described in this blog post builds upon the work of many others. We mentioned some of these above, but
      would like to provide further links to existing works that we believe would be relevant for readers of this blog.

      In particular, we would like to highlight the following papers:
      </p><li> <a href="https://arxiv.org/abs/2211.11260">Lange, Robert Tjarko, et al. &#34;Discovering Evolution Strategies via
          Meta-Black-Box Optimization.&#34; The Eleventh International Conference on Learning Representations. 2023.</a>
      </li>
      <li><a href="https://arxiv.org/abs/1802.04821">Houthooft, Rein, et al. &#34;Evolved policy gradients.&#34; Advances in
          Neural Information Processing Systems 31 (2018). </a>
      </li>
      <li><a href="https://arxiv.org/abs/2111.05803">Metz, Luke, et al. &#34;Gradients are not all you need.&#34; arXiv preprint
          arXiv:2111.05803 (2021). </a>
      </li>
      <li><a href="https://arxiv.org/abs/2206.08888">Flajolet, Arthur, et al. &#34;Fast population-based reinforcement
          learning on a single machine.&#34; International Conference on Machine Learning. PMLR, 2022.
        </a>
      </li>
      <li>
        <a href="https://arxiv.org/abs/2104.06272">Hessel, Matteo, et al. &#34;Podracer architectures for scalable
          reinforcement learning.&#34; arXiv preprint arXiv:2104.06272 (2021).
        </a>
      </li>
    

  </div></div>
  </body>
</html>
