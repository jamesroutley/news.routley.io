<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://dagshub.com/blog/yolov6/#">Original</a>
    <h1>YOLOv6: Redefine state-of-the-art for object detection</h1>
    
    <div id="readability-page-1" class="page"><div>
              
              <p>The field of computer vision has rapidly evolved in recent years and achieved results that seemed like science fiction a few years back. From <a href="https://dagshub.com/nirbarazida/pneumonia-Classification">analyzing X-ray images</a> and diagnosing patients to (semi-)autonomous cars, we&#39;re witnessing a revolution in the making. These breakthroughs have many causes – building better, more accessible compute resources, but also the fact that they are the closest thing we have to <a href="https://dagshub.com/blog/a-case-for-open-source-data-science/">Open Source Data Science</a> (OSDS). Revealing the source code to the community unlocks the &#34;wisdom of the crowd&#34; and enables innovation and problem-solving at scale.</p><p>One of the most popular OS projects in computer vision is YOLO (<strong>Y</strong>ou <strong>O</strong>nly <strong>L</strong>ook <strong>O</strong>nce). YOLO is an efficient real-time object detection algorithm, first described in the seminal 2015 <a href="https://arxiv.org/abs/1506.02640">paper</a> by Joseph Redmon et al. YOLO divides an image into a grid system, and each grid detects objects within itself. It can be used for real-time inference and require very few computational resources.</p><p>Today, 7 years after the first version of YOLO was released, the research group at Meituan published the new YOLOv6 model - and it&#39;s here to kick a**!</p><h2 id="the-history-of-yolo">The History of YOLO</h2><h3 id="object-detection-before-yolo">Object detection before YOLO</h3><p>Before YOLO, the two-stage object detection architecture dominated the field. It used region-based classifiers to locate areas and then pass them to a more robust classifier. While this method gives accurate results, with a high mean Average Precision (mAP), it is very resource-intensive, requiring many iterations in its operation.</p><figure><img src="https://dagshub.com/blog/content/images/2022/06/image-1.png" alt="" srcset="https://dagshub.com/blog/content/images/size/w600/2022/06/image-1.png 600w, https://dagshub.com/blog/content/images/2022/06/image-1.png 936w" sizes="(min-width: 720px) 720px"/><figcaption>Two-stage object detection architecture</figcaption></figure><h3 id="how-does-yolo-work"><strong><strong>How does YOLO work?</strong></strong></h3><p>YOLO suggested a different methodology where both stages are conducted in the same neural network. First, the image is divided into cells, each having an equal dimensional region of SxS. Then, each cell detects and locates the objects it contains with bounding box coordinates (relative to its coordinates) with the object label and probability of the thing being present in the cell.</p><figure><img src="https://dagshub.com/blog/content/images/2022/06/image-2.png" alt="" srcset="https://dagshub.com/blog/content/images/size/w600/2022/06/image-2.png 600w, https://dagshub.com/blog/content/images/size/w1000/2022/06/image-2.png 1000w, https://dagshub.com/blog/content/images/2022/06/image-2.png 1204w" sizes="(min-width: 720px) 720px"/><figcaption>YOLOv1 architecture</figcaption></figure><p>Because each cell &#34;works on its own&#34; it can process the grid simultaneously and reduces the required computing powers and time needed to train and infer. In fact, YOLO achieves state-of-the-art results, beating other real-time object detection algorithms.</p><h3 id="what-versions-does-yolo-have">What versions does YOLO have?</h3><ul><li><strong>YOLOv1</strong> (Jun, 2015): <a href="https://arxiv.org/abs/1506.02640">You Only Look Once: Unified, Real-Time Object Detection</a></li><li><strong>YOLOv2</strong> (Dec, 2016): <a href="https://arxiv.org/abs/1612.08242v1">YOLO9000:Better, Faster, Stronger</a></li><li><strong>YOLOv3</strong> (Apr, 2018): <a href="https://arxiv.org/abs/1804.02767v1">YOLOv3: An Incremental Improvement</a></li><li><strong>YOLOv4</strong> (Apr, 2020): <a href="https://arxiv.org/abs/2004.10934v1">YOLOv4: Optimal Speed and Accuracy of Object Detection</a></li><li><strong>YOLOv5</strong> (May, 2020): <a href="https://github.com/ultralytics/yolov5">Github repo</a> (No paper was released yet)</li></ul><h2 id="yolov6-is-here-to-kick-a-and-take-names">YOLOv6 is here to kick A** and Take Names</h2><p>MT-YOLOv6 was inspired by the original one-stage YOLO architecture and thus <a href="https://github.com/meituan/YOLOv6/blob/main/docs/About_naming_yolov6.md">was (bravely) named</a> YOLOv6 by its authors. Though it provides outstanding results, it&#39;s important to note that MT-YOLOv6 is not part of the official YOLO series.</p><p>YOLOv6 is a single-stage object detection framework dedicated to industrial applications, with hardware-friendly efficient design and high performance. It outperforms YOLOv5 in detection accuracy and inference speed, making it the best OS version of YOLO architecture for production applications.</p><h3 id="yolov6-achievements">YOLOv6 Achievements</h3><ul><li><strong>YOLOv6-nano</strong> - achieves 35.0 mAP on COCO val2017 dataset with 1242 FPS on T4 using TensorRT FP16 for bs32 inference</li><li><strong>YOLOv6-s</strong> - achieves 43.1 mAP on COCO val2017 dataset with 520 FPS on T4 using TensorRT FP16 for bs32 inference.</li></ul><h3 id="single-image-inference">Single Image Inference</h3><figure><img src="https://dagshub.com/blog/content/images/2022/06/Untitled--5-.png" alt="" srcset="https://dagshub.com/blog/content/images/size/w600/2022/06/Untitled--5-.png 600w, https://dagshub.com/blog/content/images/size/w1000/2022/06/Untitled--5-.png 1000w, https://dagshub.com/blog/content/images/2022/06/Untitled--5-.png 1364w" sizes="(min-width: 720px) 720px"/><figcaption>YOLOv6 - single image inference</figcaption></figure><p>YOLOv6s (red) provide a better mean Average Precision (mAP) than all the previous versions of YOLOv5, with approximately 2x faster inference time. We can also see a huge performance gap between YOLO-based architecture and EfficientDet, which is based on two-stage object detection.</p><h3 id="video-inference">Video Inference</h3><figure><img src="https://dagshub.com/blog/content/images/2022/06/Untitled--6-.png" alt="" srcset="https://dagshub.com/blog/content/images/size/w600/2022/06/Untitled--6-.png 600w, https://dagshub.com/blog/content/images/size/w1000/2022/06/Untitled--6-.png 1000w, https://dagshub.com/blog/content/images/2022/06/Untitled--6-.png 1388w" sizes="(min-width: 720px) 720px"/><figcaption>YOLOv6 - video inference</figcaption></figure><p>Same as in the single image inference, YOLOv6 provides better results for video on all the FPS spectrum. It’s interesting to note the change in the curve for ~550-620 FPS. I wonder if it has anything to do with hardware performance and whether or not the maintainers reduce the bias of hardware when conducting their experiments.</p><h3 id="benchmark">Benchmark</h3><figure><img src="https://dagshub.com/blog/content/images/2022/06/Untitled--7-.png" alt="" srcset="https://dagshub.com/blog/content/images/size/w600/2022/06/Untitled--7-.png 600w, https://dagshub.com/blog/content/images/size/w1000/2022/06/Untitled--7-.png 1000w, https://dagshub.com/blog/content/images/size/w1600/2022/06/Untitled--7-.png 1600w, https://dagshub.com/blog/content/images/2022/06/Untitled--7-.png 2003w" sizes="(min-width: 720px) 720px"/></figure><ul><li>Comparisons of the mAP and speed of different object detectors are tested on <a href="https://cocodataset.org/#download">COCO val2017</a> dataset.</li><li>Speed results of other methods were tested in the maintainers’ environment using the official codebase and model if not found from the corresponding official release.</li></ul><p><strong>Disclaimer</strong>: The above review is based on the authors’ claims, and we have yet to verify them.</p><h2 id="yolov5-vs-yolov6">YOLOv5 vs. YOLOv6</h2><h3 id="benchmark-comparison-between-yolov5-and-yolov6">Benchmark comparison between YOLOv5 and YOLOv6</h3><p>While looking into the benchmarks of both models, I found it hard to compare apples to apples. YOLOv6 has fewer types of models (lacking m/l/x) and doesn&#39;t have any information about images larger than 640 pixels. For the benchmarks both projects reported, we can clearly see the improvement in mAP for YOLOv6. However, v6 has 2x the number of parameters and Flops from v5, making me want to dive into the training process myself and double-check the results below.</p><figure><img src="https://dagshub.com/blog/content/images/2022/06/image-7.png" alt="" srcset="https://dagshub.com/blog/content/images/size/w600/2022/06/image-7.png 600w, https://dagshub.com/blog/content/images/size/w1000/2022/06/image-7.png 1000w, https://dagshub.com/blog/content/images/size/w1600/2022/06/image-7.png 1600w, https://dagshub.com/blog/content/images/2022/06/image-7.png 2002w" sizes="(min-width: 720px) 720px"/><figcaption>Benchmark comparison between YOLOv5 and YOLOv6</figcaption></figure><h3 id="qualitative-comparison-between-yolov5-and-yolov6">Qualitative comparison between YOLOv5 and YOLOv6</h3><p>I used the s version of both models to detect objects on the following images</p><!--kg-card-begin: html--><div>
<div>
<p><img src="https://dagshub.com/nirbarazida/YOLOv6/raw/b749ede942b667999d7b6b290ec8c0c8ab7c27c8/assets/YOLOv6_Res/zidane.jpg" alt="YOLOv6s Results"/></p><center><figcaption>YOLOv6s Results
</figcaption></center></div>
<div>
<p><img src="https://dagshub.com/nirbarazida/YOLOv6/raw/94b5322464bba0a4b6188cc6438f45bcca3cc8e7/assets/YOLOv5_Res/zidane.jpeg" alt="YOLOv5s Results
"/></p><center><figcaption>YOLOv5s Results
</figcaption></center></div>
</div><!--kg-card-end: html--><!--kg-card-begin: html--><div>
<div>
<p><img src="https://dagshub.com/nirbarazida/YOLOv6/raw/94b5322464bba0a4b6188cc6438f45bcca3cc8e7/assets/YOLOv6_Res/bus.jpeg" alt="YOLOv6s Results"/></p><center><figcaption>YOLOv6s Results
</figcaption></center></div>
<div>
<p><img src="https://dagshub.com/nirbarazida/YOLOv6/raw/94b5322464bba0a4b6188cc6438f45bcca3cc8e7/assets/YOLOv5_Res/bus.png" alt="YOLOv5s Results
"/></p><center><figcaption>YOLOv5s Results
</figcaption></center></div>
</div><!--kg-card-end: html--><!--kg-card-begin: html--><div>
<div>
<p><img src="https://dagshub.com/nirbarazida/YOLOv6/raw/94b5322464bba0a4b6188cc6438f45bcca3cc8e7/assets/YOLOv6_Res/street.jpeg" alt="YOLOv6s Results"/></p><center><figcaption>YOLOv6s Results
</figcaption></center></div>
<div>
<p><img src="https://dagshub.com/nirbarazida/YOLOv6/raw/94b5322464bba0a4b6188cc6438f45bcca3cc8e7/assets/YOLOv5_Res/street.jpeg" alt="YOLOv5s Results
"/></p><center><figcaption>YOLOv5s Results
</figcaption></center></div>
</div><!--kg-card-end: html--><p>We can clearly see that YOLOv6s detects more objects in the image and has higher confidence about their label.</p><h3 id="flexibility">Flexibility</h3><p>Both projects have similar approaches to creating different model sizes. The biggest difference is that <a href="https://github.com/ultralytics/yolov5/tree/master/models">YOLOv5 uses YAML</a>, whereas <a href="https://dagshub.com/nirbarazida/YOLOv6/src/main/configs">YOLOv6 defines the model parameters directly in Python</a>. A precursory glance also indicates that YOLOv5 might be a bit more customizable to a certain extent.</p><p>However, the fact that YOLOv6 is so flexible means that we could see larger versions of YOLOv6 in the future with even higher accuracy predictions!</p><p><strong>If you create a larger YOLOv6 model, <a href="https://discord.com/invite/9gU36Y6" rel="noopener noreferrer">let us know on Discord</a>! We’d love to see it!</strong></p><h2 id="usage">Usage</h2><p><a href="https://yolov6.dagshubusercontent.com/">You can interact with the latest version of YOLOv6 using DagsHub’s application</a>. If you want to use it on your local machine follow these steps:</p><p><strong>Installation </strong></p><pre><code>git clone https://dagshub.com/nirbarazida/YOLOv6 cd
YOLOv6 pip install -r requirements.txt 
dvc pull</code></pre><p><strong>Inference</strong></p><ul><li>Use YOLOv6s</li></ul><p><code>python tools/infer.py --weights yolov6s.pt --source &lt;path to image/directory&gt;</code></p><ul><li>Use YOLOv6n</li></ul><p><code>python tools/infer.py --weights yolov6n.pt --source &lt;path to image/directory&gt;</code></p><h2 id="conclusion">Conclusion</h2><p>YOLOv6 is one of the most exciting OSDS projects recently released. It provides state-of-the-art results and a significant improvement on all fronts compared to previous YOLO versions. The maintainers are currently focused on enriching the types of models, deployment options, and quantization tools. Still, as with any open source project, the community can greatly impact its roadmap and progress curve.</p><p>Although the project is still in its early days, it looks very promising, and I&#39;m intrigued to see what other benchmarks it will break in the future.</p>
                <section>
                  
                  <ul>
                      <li>
                        <a href="https://dagshub.com/blog/tag/data-science/" title="Data Science">Data Science</a>
                      </li>
                      <li>
                        <a href="https://dagshub.com/blog/tag/machine-learning-production/" title="Machine Learning Production">Machine Learning Production</a>
                      </li>
                      <li>
                        <a href="https://dagshub.com/blog/tag/machine-learning/" title="Machine Learning">Machine Learning</a>
                      </li>
                      <li>
                        <a href="https://dagshub.com/blog/tag/open-source-data-science/" title="Open Source Data Science">Open Source Data Science</a>
                      </li>
                      <li>
                        <a href="https://dagshub.com/blog/tag/project/" title="Project">Project</a>
                      </li>
                      <li>
                        <a href="https://dagshub.com/blog/tag/reproducibility/" title="Reproducibility">Reproducibility</a>
                      </li>
                  </ul>
                </section>
            </div></div>
  </body>
</html>
