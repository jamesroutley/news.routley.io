<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://eugeneyan.com/writing/obsidian-copilot/">Original</a>
    <h1>Obsidian-Copilot: A Prototype Assistant for Writing and Thinking</h1>
    
    <div id="readability-page-1" class="page"><div>
<p>What would a copilot for writing and thinking look like? To try answering this question, I built a prototype: Obsidian-Copilot. Given a section header, it helps draft a few paragraphs via <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html" target="_blank">retrieval-augmented generation</a>. Also, if you write a daily journal, it can help you reflect on the past week and plan for the week ahead.</p>
<p><img src="https://eugeneyan.com/assets/copilot.jpg" loading="lazy" title="Obsidian Copilot: Helping to write drafts and reflect on the week" alt="Obsidian Copilot: Helping to write drafts and reflect on the week"/></p>
<p>Obsidian Copilot: Helping to write drafts and reflect on the week</p>
<p>Here’s a short 2-minute demo. The code is available at <a href="https://github.com/eugeneyan/obsidian-copilot" target="_blank">obsidian-copilot</a>.</p>
<iframe width="800" height="405" src="https://www.youtube.com/embed/QRJW5jT5VRA?cc_load_policy=1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
<h2 id="how-does-it-work">How does it work?</h2>
<p><strong>We start by parsing documents into chunks.</strong> A sensible default is to <a href="https://github.com/hwchase17/langchain/blob/master/langchain/text_splitter.py#L58" target="_blank">chunk documents by token length</a>, typically 1,500 to 3,000 tokens per chunk. However, I found that this <a href="https://eugeneyan.com/writing/llm-experiments/#documents-may-be-inadequately-chunked" target="_blank">didn’t work very well</a>. A better approach might be to chunk by paragraphs (e.g., split on <code>\n\n</code>).</p>
<p>Given that my notes are mostly in bullet form, I <a href="https://github.com/eugeneyan/obsidian-copilot/blob/main/src/prep/build_vault_dict.py#L73" target="_blank">chunk by top-level bullets</a>: Each chunk is made up of a single top-level bullet and its sub-bullets. There are usually 5 to 10 sub-bullets per top-level bullet making each chunk similar in length to a paragraph.</p>
<div><div><pre><code><span>chunks</span> <span>=</span> <span>defaultdict</span><span>()</span>
<span>current_chunk</span> <span>=</span> <span>[]</span>
<span>chunk_idx</span> <span>=</span> <span>0</span>
<span>current_header</span> <span>=</span> <span>None</span>

<span>for</span> <span>line</span> <span>in</span> <span>lines</span><span>:</span>

    <span>if</span> <span>&#39;##&#39;</span> <span>in</span> <span>line</span><span>:</span>  <span># Chunk header = Section header
</span>        <span>current_header</span> <span>=</span> <span>line</span>
    
    <span>if</span> <span>line</span><span>.</span><span>startswith</span><span>(</span><span>&#39;- &#39;</span><span>):</span>  <span># Top-level bullet
</span>        <span>if</span> <span>current_chunk</span><span>:</span>  <span># If chunks accumulated, add it to chunks
</span>            <span>if</span> <span>len</span><span>(</span><span>current_chunk</span><span>)</span> <span>&gt;=</span> <span>min_chunk_lines</span><span>:</span>
                <span>chunks</span><span>[</span><span>chunk_idx</span><span>]</span> <span>=</span> <span>current_chunk</span>
                <span>chunk_idx</span> <span>+=</span> <span>1</span>
            <span>current_chunk</span> <span>=</span> <span>[]</span>  <span># Reset current chunk
</span>            <span>if</span> <span>current_header</span><span>:</span>
                <span>current_chunk</span><span>.</span><span>append</span><span>(</span><span>current_header</span><span>)</span>
    
    <span>current_chunk</span><span>.</span><span>append</span><span>(</span><span>line</span><span>)</span>
</code></pre></div></div>
<p>Next, we build an OpenSearch index and a semantic index on these chunks. In a previous experiment, I found that <a href="https://eugeneyan.com/writing/llm-experiments/#embedding-based-retrieval-alone-might-be-insufficient" target="_blank">embedding-based retrieval alone might be insufficient</a> and thus added classical search (i.e., <a href="https://en.wikipedia.org/wiki/Okapi_BM25" target="_blank">BM25</a> via OpenSearch) in this prototype.</p>
<p><strong>For OpenSearch, we start by configuring filters and fields.</strong> We include <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/analysis-tokenfilters.html" target="_blank">filters</a> such as <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/analysis-htmlstrip-charfilter.html" target="_blank">stripping HTML</a>, <a href="https://lucene.apache.org/core/9_6_0/analysis/common/org/apache/lucene/analysis/en/EnglishPossessiveFilter.html" target="_blank">removing possessives</a> (i.e., the trailing ‘s in words), removing stopwords, and basic stemming. These filters are applied on both documents (during indexing) and queries. We also specify the fields we want to index and their respective types. Types matter because filters are applied on text fields (e.g., title, chunk) but not on keyword fields (e.g., path, document type). We don’t apply preprocessing on file paths to keep them as they are.</p>
<div><div><pre><code><span>&#39;mappings&#39;</span><span>:</span> <span>{</span>
	<span>&#39;properties&#39;</span><span>:</span> <span>{</span>
		<span>&#39;title&#39;</span><span>:</span> <span>{</span><span>&#39;type&#39;</span><span>:</span> <span>&#39;text&#39;</span><span>,</span> <span>&#39;analyzer&#39;</span><span>:</span> <span>&#39;english_custom&#39;</span><span>},</span>
		<span>&#39;type&#39;</span><span>:</span> <span>{</span><span>&#39;type&#39;</span><span>:</span> <span>&#39;keyword&#39;</span><span>},</span>
		<span>&#39;path&#39;</span><span>:</span> <span>{</span><span>&#39;type&#39;</span><span>:</span> <span>&#39;keyword&#39;</span><span>},</span>
		<span>&#39;chunk_header&#39;</span><span>:</span> <span>{</span><span>&#39;type&#39;</span><span>:</span> <span>&#39;text&#39;</span><span>,</span> <span>&#39;analyzer&#39;</span><span>:</span> <span>&#39;english_custom&#39;</span><span>},</span>
		<span>&#39;chunk&#39;</span><span>:</span> <span>{</span><span>&#39;type&#39;</span><span>:</span> <span>&#39;text&#39;</span><span>,</span> <span>&#39;analyzer&#39;</span><span>:</span> <span>&#39;english_custom&#39;</span><span>},</span>
	<span>}</span>
<span>}</span>
</code></pre></div></div>
<p>When querying, we apply <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/mapping-boost.html" target="_blank">boosts</a> to make some fields count more towards the relevance score. In this prototype, I arbitrarily <a href="https://github.com/eugeneyan/obsidian-copilot/blob/main/src/prep/build_opensearch_index.py#L163" target="_blank">boosted</a> titles by 5x and chunk headers (i.e., top-level bullets) by 2x. Retrieval can be improved by tweaking these boosts as well as <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-rank-feature-query.html" target="_blank">other features</a>.</p>
<p><strong>For semantic search, we start by picking an embedding model.</strong> I referred to the <a href="https://huggingface.co/spaces/mteb/leaderboard" target="_blank">Massive Text Embedding Benchmark Leaderboard</a>, sorted it on descending order of retrieval score, and picked a model that had a good balance of embedding dimension and score.</p>
<p>This led me to <a href="https://huggingface.co/intfloat/e5-small-v2" target="_blank">e5-small-v2</a>. Currently, it’s ranked a respectable 7th, right below text-embedding-ada-002. What’s impressive is its embedding size of 384 which is far smaller than what most models have (768 - 1,536). And while it supports a maximum sequence length of only 512, this is sufficient given my shorter chunks. (More details in the paper <a href="https://arxiv.org/abs/2212.03533" target="_blank">Text Embeddings by Weakly-Supervised Contrastive Pre-training</a>.) After embedding these documents, we store them in a <code>numpy</code> array.</p>
<p>During query time, we tokenize and embed the query, do a dot product with the document embedding array, and take the top <em>n</em> results (in this case, 10).</p>
<div><div><pre><code><span>def</span> <span>query_semantic</span><span>(</span><span>query</span><span>,</span> <span>tokenizer</span><span>,</span> <span>model</span><span>,</span> <span>doc_embeddings_array</span><span>,</span> <span>n_results</span><span>=</span><span>10</span><span>):</span>
    <span>query_tokenized</span> <span>=</span> <span>tokenizer</span><span>(</span><span>f</span><span>&#39;query: </span><span>{</span><span>query</span><span>}</span><span>&#39;</span><span>,</span> <span>max_length</span><span>=</span><span>512</span><span>,</span> <span>padding</span><span>=</span><span>False</span><span>,</span> <span>truncation</span><span>=</span><span>True</span><span>,</span> <span>return_tensors</span><span>=</span><span>&#39;pt&#39;</span><span>)</span>
    <span>outputs</span> <span>=</span> <span>model</span><span>(</span><span>**</span><span>query_tokenized</span><span>)</span>
    <span>query_embedding</span> <span>=</span> <span>average_pool</span><span>(</span><span>outputs</span><span>.</span><span>last_hidden_state</span><span>,</span> <span>query_tokenized</span><span>[</span><span>&#39;attention_mask&#39;</span><span>])</span>
    <span>query_embedding</span> <span>=</span> <span>F</span><span>.</span><span>normalize</span><span>(</span><span>query_embedding</span><span>,</span> <span>p</span><span>=</span><span>2</span><span>,</span> <span>dim</span><span>=</span><span>1</span><span>).</span><span>detach</span><span>().</span><span>numpy</span><span>()</span>

    <span>cos_sims</span> <span>=</span> <span>np</span><span>.</span><span>dot</span><span>(</span><span>doc_embeddings_array</span><span>,</span> <span>query_embedding</span><span>.</span><span>T</span><span>)</span>
    <span>cos_sims</span> <span>=</span> <span>cos_sims</span><span>.</span><span>flatten</span><span>()</span>

    <span>top_indices</span> <span>=</span> <span>np</span><span>.</span><span>argsort</span><span>(</span><span>cos_sims</span><span>)[</span><span>-</span><span>n_results</span><span>:][::</span><span>-</span><span>1</span><span>]</span>

    <span>return</span> <span>top_indices</span>
</code></pre></div></div>
<p>If you’re thinking of using the e5 models, remember to add the necessary prefixes during preprocessing. For documents, you’ll have to prefix them with “<code>passage:‎ </code>” and for queries, you’ll have to prefix them with “<code>query:‎ </code>”</p>
<p><strong>The retrieval service is a FastAPI app.</strong> Given an input query, it <a href="https://github.com/eugeneyan/obsidian-copilot/blob/main/src/app.py#L144" target="_blank">performs both BM25 and semantic search</a>, deduplicates the results, and returns the documents’ text and associated title. The latter is used to <a href="https://www.youtube.com/watch?v=QRJW5jT5VRA&amp;t=72s" target="_blank">link source documents</a> when generating the draft.</p>
<p>To start the OpenSearch node and semantic search + FastAPI server, we use a <a href="https://github.com/eugeneyan/obsidian-copilot/blob/main/docker-compose.yml" target="_blank">simple-docker compose file</a>. They each run in their own containers, bridged by a common network. For convenience, we also define common commands in a <a href="https://github.com/eugeneyan/obsidian-copilot/blob/main/Makefile" target="_blank">Makefile</a>.</p>
<p><strong>Finally, we integrate with Obsidian via a TypeScript plugin.</strong> The <a href="https://github.com/obsidianmd/obsidian-sample-plugin" target="_blank">obsidian-plugin-sample</a> made it easy to get started and I added functions to display retrieved documents in a new tab, query APIs, and stream the output. (I’m new to TypeScript so feedback appreciated!)</p>
<h2 id="what-else-can-we-apply-this-to">What else can we apply this to?</h2>
<p>While this prototype uses local notes and journal entries, it’s not a stretch to imagine the copilot retrieving from other documents (online). For example, team documents such as product requirements and technical design docs, internal wikis, and even code. I’d guess that’s what Microsoft, Atlassian, and Notion are working on right now.</p>
<p>It also extends beyond personal productivity. Within my field of recommendations and search, researchers and practitioners are excited about <a href="https://arxiv.org/abs/2306.02887" target="_blank">layering LLM-based generation</a> on top of existing systems and products to improve the customer experience. (I expect we’ll see some of this in production by the end of the year.)</p>
<h2 id="ideas-for-improvement">Ideas for improvement</h2>
<p>One idea is to try LLMs with larger context sizes that allow us to feed in entire documents instead of chunks. (This may help with retrieval recall but puts more onus on the LLM to identify the relevant context for generation.) Currently, I’m using gpt-3.5-turbo which is a good balance of speed and cost. Nonetheless, I’m excited to try claude-1.3-100k and provide entire documents as context.</p>
<p>Another idea is to augment retrieval with web or internal search when necessary. For example, when documents and notes go stale (e.g., based on last updated timestamp), we can look up the web or internal documents for more recent information.</p>
<p>• • •</p>
<p>Here’s the <a href="https://github.com/eugeneyan/obsidian-copilot" target="_blank">GitHub repo</a> if you’re keen to try. Start by cloning the repo and updating the path to your obsidian-vault and huggingface hub cache. The latter saves us from downloading the tokenizer and model each time you start the containers.</p>
<div><div><pre><code>git clone https://github.com/eugeneyan/obsidian-copilot.git

<span># Open Makefile and update the following paths</span>
<span>export </span>OBSIDIAN_PATH <span>=</span> /Users/eugene/obsidian-vault/
<span>export </span>TRANSFORMER_CACHE <span>=</span> /Users/eugene/.cache/huggingface/hub
</code></pre></div></div>
<p>Then, build the image and indices before starting the retrieval app.</p>
<div><div><pre><code><span># Build the docker image</span>
make build

<span># Start the opensearch container and wait for it to start. </span>
<span># You should see something like this: [c6587bf83572] Node &#39;c6587bf83572&#39; initialized</span>
make opensearch

<span># In ANOTHER terminal, build your artifacts (this can take a while)</span>
make build-artifacts

<span># Start the app. You should see this: Uvicorn running on http://0.0.0.0:8000</span>
make run
</code></pre></div></div>
<p>Finally, install the copilot plugin, enable it in community plugin settings, and update the API key. You’ll have to restart your Obsidian app if you had it open before installation.</p>

<p>If you tried it, I would love to hear how it went, especially where it didn’t work well and how it can be improved. Or if you’ve been working with retrieval-augmented generation, I’d love to hear about your experience so far!</p>
</div></div>
  </body>
</html>
