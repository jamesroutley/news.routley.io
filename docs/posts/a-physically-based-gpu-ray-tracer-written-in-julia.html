<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://makie.org/website/blogposts/raytracing/">Original</a>
    <h1>Show HN: A physically-based GPU ray tracer written in Julia</h1>
    
    <div id="readability-page-1" class="page"><div data-jscall-id="29"><div data-jscall-id="30"><p data-jscall-id="32"><img alt="BOMEX cumulus clouds rendered with volumetric path tracing" data-jscall-id="33" src="https://makie.org/website/bonito/png/breeze7235631823897871785.png"/></p><p data-jscall-id="34">We&#39;re excited to announce <a href="https://github.com/MakieOrg/Makie.jl/pull/5532" data-jscall-id="35">RayMakie</a> and <a href="https://github.com/JuliaGraphics/Hikari.jl" data-jscall-id="36">Hikari</a>, a physically-based GPU ray tracing pipeline integrated directly into <a href="https://docs.makie.org" data-jscall-id="37">Makie</a>. Any Makie scene can now be rendered with photorealistic path tracing: just swap out the backend and get global illumination, volumetric media, spectral rendering, and physically-based materials, all running on the GPU.</p><p data-jscall-id="38">All showcase scripts and demo scenes from this post are available at <a href="https://github.com/SimonDanisch/RayDemo" data-jscall-id="39">github.com/SimonDanisch/RayDemo</a>.</p><p data-jscall-id="40"><strong data-jscall-id="41">Note:</strong> RayMakie, Hikari, and Raycore are not fully released yet â€” we plan to publish official releases in the coming weeks. In the meantime, the <code data-jscall-id="42">Project.toml</code> in <a href="https://github.com/SimonDanisch/RayDemo" data-jscall-id="43">RayDemo</a> will be kept up to date so you can already try things out.</p><h2 id="Why Ray Tracing in Julia?" data-jscall-id="44">Why Ray Tracing in Julia?</h2><p data-jscall-id="45">Research groups across many fields (climate science, structural biology, fluid dynamics, particle physics) produce complex 3D data that needs to be communicated clearly and compellingly. Photorealistic rendering can transform dense simulation output into images that reveal structure and tell a story. But getting research data into a traditional ray tracer usually means exporting meshes, learning new tools, and losing the interactive workflow.</p><p data-jscall-id="46">By building ray tracing directly into Makie, we eliminate that gap. The same scene you explore interactively with GLMakie can be rendered photorealistically with RayMakie, no export step, no new API to learn.</p><p data-jscall-id="47">Writing the implementation in Julia gives us:</p><ul data-jscall-id="48"><li data-jscall-id="49"><p data-jscall-id="50"><strong data-jscall-id="51">Performance</strong>: Julia compiles to efficient GPU code, competitive with C++ ray tracers</p></li><li data-jscall-id="52"><p data-jscall-id="53"><strong data-jscall-id="54">Cross-vendor GPU support</strong>: A single codebase runs on AMD, NVIDIA, and CPU via <a href="https://github.com/JuliaGPU/KernelAbstractions.jl" data-jscall-id="55">KernelAbstractions.jl</a></p></li><li data-jscall-id="56"><p data-jscall-id="57"><strong data-jscall-id="58">Makie integration</strong>: The familiar Makie API for scene construction, camera control, and lighting, with ray tracing as a drop-in rendering option</p></li><li data-jscall-id="59"><p data-jscall-id="60"><strong data-jscall-id="61">Modular architecture</strong>: Custom materials, media, and integrators run directly on the GPU through Julia&#39;s multiple dispatch, opening up novel visualization and simulation use cases</p></li><li data-jscall-id="62"><p data-jscall-id="63"><strong data-jscall-id="64">Hackability</strong>: A state-of-the-art spectral path tracer in a high-level language makes ray tracing research and experimentation accessible to a much wider audience</p></li></ul><h2 id="How It Works" data-jscall-id="65">How It Works</h2><p data-jscall-id="66">Hikari is a Julia port of <a href="https://pbrt.org/" data-jscall-id="67">pbrt-v4</a>, the reference implementation from <em data-jscall-id="68">Physically Based Rendering</em> (Pharr, Jakob, Humphreys). It implements a wavefront volumetric path tracer with spectral rendering, supporting participating media (NanoVDB, grid-based volumes), physically-based materials (metals, dielectrics, coated surfaces), and environment/sun-sky lighting. The ray intersection engine lives in <a href="https://github.com/JuliaGeometry/Raycore.jl" data-jscall-id="69">Raycore.jl</a>, a standalone package factored out of Hikari and based on AMD&#39;s <a href="https://github.com/GPUOpen-LibrariesAndSDKs/RadeonRays_SDK" data-jscall-id="70">Radeon Rays SDK</a> and <a href="https://gpuopen.com/hiprt/" data-jscall-id="71">HIPRT</a>.</p><p data-jscall-id="72">RayMakie connects Hikari to Makie&#39;s scene graph. You build a scene with standard Makie calls (<code data-jscall-id="73">mesh!</code>, <code data-jscall-id="74">surface!</code>, <code data-jscall-id="75">volume!</code>, <code data-jscall-id="76">meshscatter!</code>), set materials and lights, then render:</p><pre data-jscall-id="77"><code data-jscall-id="78">
using RayMakie, Hikari

scene = Scene(size=(1920, 1080), lights=[SunSkyLight(Vec3f(1, 2, 8))])
cam3d!(scene)
mesh!(scene, my_mesh; material=Hikari.Gold(roughness=0.1))

img = colorbuffer(scene;
    device=AMDGPU.ROCBackend(),  # or CUDABackend(), CPU()
    integrator=Hikari.VolPath(samples=100, max_depth=12),
    sensor=Hikari.FilmSensor(iso=100, white_balance=6500),
)</code></pre><h2 id="Showcases" data-jscall-id="79">Showcases</h2><h3 id="Breeze: Volumetric Cloud Rendering" data-jscall-id="80">Breeze: Volumetric Cloud Rendering</h3><p data-jscall-id="81"><a href="https://github.com/NumericalEarth/Breeze.jl/" data-jscall-id="82">Breeze.jl</a> generates cloud fields from Large Eddy Simulations powered by <a href="https://github.com/CliMA/Oceananigans.jl" data-jscall-id="83">Oceananigans.jl</a>, a GPU-accelerated ocean and atmosphere simulator developed at MIT&#39;s Climate Modeling Alliance (CliMA). The demo renders BOMEX cumulus clouds as photorealistic volumetric media, using NanoVDB sparse grids for efficient storage and Hikari&#39;s volumetric path tracing for physically-accurate light scattering through the cloud medium.</p><p data-jscall-id="84"><img alt="BOMEX cumulus clouds rendered with volumetric path tracing" data-jscall-id="85" src="https://makie.org/website/bonito/png/breeze7235631823897871785.png"/></p><p data-jscall-id="86">The terrain demo combines real elevation data from ArcGIS (via <a href="https://github.com/MakieOrg/Tyler.jl" data-jscall-id="87">Tyler.jl</a>) with BOMEX cloud volumes, inspired by <a href="https://www.rayshader.com/" data-jscall-id="88">rayshader</a>.</p><p data-jscall-id="89"><img alt="Alpine terrain with volumetric clouds" data-jscall-id="90" src="https://makie.org/website/bonito/png/terrain794547408289448833.png"/></p><blockquote data-jscall-id="91"><p data-jscall-id="92">Makie-Oceananigans integration has been a major initiative over the past year. The ray tracing work with Breeze is truly groundbreaking in my opinion. It may be crucial for science, for inspecting the structure of clouds and thunderstorms. It also has the potential to lead to commercial products.</p><p data-jscall-id="93"><strong data-jscall-id="94">Gregory Wagner</strong>, Research Scientist at MIT and CliMA (Climate Modeling Alliance), lead developer of Oceananigans.jl and Breeze.jl</p></blockquote><h3 id="PlantGeom: Digital Twins for Agriculture" data-jscall-id="95">PlantGeom: Digital Twins for Agriculture</h3><p data-jscall-id="96"><a href="https://github.com/VEZY/PlantGeom.jl" data-jscall-id="97">PlantGeom.jl</a> renders biophysically accurate 3D plant models for agricultural research. The ray-traced oil palm uses coated diffuse transmission materials to simulate the waxy leaf cuticle and subsurface light transport through leaf tissue.</p><p data-jscall-id="98"><img alt="Ray-traced oil palm with translucent leaves" data-jscall-id="99" src="https://makie.org/website/bonito/png/plants9877348102989772360.png"/></p><blockquote data-jscall-id="100"><p data-jscall-id="101">Makie has become a central part of my daily scientific workflow. The fact that a single plotting ecosystem can cover the entire spectrum from high-performance 3D scenes to publication-ready figures is just exceptional. Several of the packages I develop would simply not exist in their current form without Makie. My research focuses on developing plant-scale digital twins to study complex agricultural systems (agroforestry, species mixtures, agrivoltaics) with the goal of understanding crop responses to climate and contributing to food security and climate-change adaptation. The recent work on ray tracing is especially exciting: beyond rendering, I am planning to use RayCore.jl for scientific applications such as computing radiation interception in plant canopies, which enables organ-scale energy balance calculations (latent and sensible heat fluxes) and photosynthesis modeling. Understanding how plant shadows affect heat transfer is crucial for adapting agriculture to extreme heat events. Makie has been a genuine force multiplier for my research.</p><p data-jscall-id="102"><strong data-jscall-id="103">Remi Vezy</strong>, CIRAD, developer of PlantGeom.jl</p></blockquote><h3 id="ProtPlot: Protein Structure Visualization" data-jscall-id="104">ProtPlot: Protein Structure Visualization</h3><p data-jscall-id="105"><a href="https://github.com/MurrellGroup/ProtPlot.jl" data-jscall-id="106">ProtPlot.jl</a> renders protein ribbon diagrams with physically-based materials. Different materials reveal different aspects of the structure: glass refractions show the interior ribbon path, while a coated diffuse surface with shallow depth of field draws attention to specific regions of the fold. Specular highlights, ambient occlusion, and defocus blur make spatial relationships immediately clear in a way that flat shading cannot.</p><table data-jscall-id="107"><tbody><tr data-jscall-id="108"><th data-jscall-id="109">Glass</th><th data-jscall-id="110">Physical based Gold with Depth of Field</th></tr><tr data-jscall-id="111"><td data-jscall-id="112"><img alt="Protein ribbon with glass material" data-jscall-id="113" src="https://makie.org/website/bonito/png/protplot_glass2422419635124639137.png"/></td><td data-jscall-id="114"><img alt="Protein ribbon with a gold material and depth of field" data-jscall-id="115" src="https://makie.org/website/bonito/png/protplot_7pkz4446811177816820644.png"/></td></tr></tbody></table><p data-jscall-id="116">ProtPlot also powers animated visualizations of protein folding trajectories, the transport process that generative models learn to perform. Ray tracing makes the spatial relationships in these animations immediately legible. The code to produce this is almost entirely standard Makie and ProtPlot, only the imports and a few rendering parameters change:</p><pre data-jscall-id="117"><code data-jscall-id="118">
using RayMakie, ProtPlot, Hikari, AMDGPU

device = AMDGPU.ROCBackend()

tracks = readdir(&#34;Trajectories/samp_00015&#34;, join=true)
set_theme!(lights=[
    Makie.SunSkyLight(Vec3f(0.4, -0.3, 0.7);
        intensity=1.0f0, turbidity=3.0f0, ground_enabled=false),
])
ProtPlot.animate_molecule_dir(&#34;trajectory.mp4&#34;, tracks)</code></pre><p data-jscall-id="119">The <code data-jscall-id="120">animate_molecule_dir</code> function is pure ProtPlot code, building Makie figures with <code data-jscall-id="121">Axis3</code>, <code data-jscall-id="122">atomplot!</code>, and <code data-jscall-id="123">ribbon!</code> as usual. Then when using the RayMakie backend, Makie will use it to render the video without requiring any additional changes.</p><blockquote data-jscall-id="127"><p data-jscall-id="128">My lab is working on new kinds of generative models, primarily for protein structures. These work by specifying a process that transports randomly sampled noise to a biophysically plausible protein structure, and training a deep neural network to learn how to do this. But the details of this transport process are critical and, while you can to some extent mathematically reason about it, there is no substitute to being able to actually see it in action, both when you&#39;re designing it and when investigating how well the model has actually learned it. We work with visualization tools, like ProtPlot.jl, primarily developed by Anton Oresten in my lab, which rely entirely on Makie for this. These visualizations are not just for our internal use, but can be valuable for communicating how the methods work. Developments like raytracing will allow us to configure these to show exactly the details we seek to communicate without them getting buried in the complexity.</p><p data-jscall-id="129"><strong data-jscall-id="130">Ben Murrell</strong>, Karolinska Institutet</p></blockquote><h3 id="TrixiParticles: Fluid Simulation" data-jscall-id="131">TrixiParticles: Fluid Simulation</h3><p data-jscall-id="132"><a href="https://github.com/trixi-framework/TrixiParticles.jl" data-jscall-id="133">TrixiParticles.jl</a> simulates fluid dynamics with smoothed particle hydrodynamics. The water splash demo renders SPH surface meshes with a dielectric material (IOR 1.33) simulating water. Fresnel reflections and refractions create the characteristic look of water without any post-processing. The animation shows a gold ball dropping into water, with each frame independently path-traced:</p><blockquote data-jscall-id="137"><p data-jscall-id="138">Native raytracing in Makie would let us move some of our workflows entirely into Julia, from simulation to publication-ready figures, without relying on external tools.</p><p data-jscall-id="139"><strong data-jscall-id="140">Dr. Michael Schlottke-Lakemper</strong>, University of Stuttgart, lead developer of TrixiParticles.jl</p></blockquote><h3 id="Volumetric Rendering: Clouds, Smoke, and Exhaust Plumes" data-jscall-id="141">Volumetric Rendering: Clouds, Smoke, and Exhaust Plumes</h3><p data-jscall-id="142">Participating media are first-class citizens in Hikari&#39;s path tracer. Light scatters through volumes realistically, no billboard tricks or screen-space approximations. Hikari supports two volume representations: <strong data-jscall-id="143">NanoVDB</strong> sparse grids for large-scale data (like the cloud simulations above), and <strong data-jscall-id="144">RGBGridMedium</strong> for dense grids with per-voxel color, emission, and scattering properties. Both integrate directly with Makie&#39;s <code data-jscall-id="145">volume!</code> plot or can be attached to any mesh as a <code data-jscall-id="146">MediumInterface</code>.</p><p data-jscall-id="147">The <strong data-jscall-id="148">Stanford bunny cloud</strong> from the <a href="https://github.com/mmp/pbrt-v4-scenes" data-jscall-id="149">pbrt-v4 test scenes</a>, rendered with NanoVDB and Henyey-Greenstein phase function scattering:</p><p data-jscall-id="150"><img alt="Stanford bunny cloud" data-jscall-id="151" src="https://makie.org/website/bonito/png/bunny3536803747307745313.png"/></p><p data-jscall-id="152">A <strong data-jscall-id="153">sphere wake</strong> from <a href="https://github.com/WaterLily-jl/WaterLily.jl" data-jscall-id="154">WaterLily.jl</a> LES simulation, visualized as volumetric smoke. The volume density is updated in-place each frame via <code data-jscall-id="155">RGBGridMedium</code> with a colormap, so the scene never needs to be rebuilt:</p><p data-jscall-id="159">The <strong data-jscall-id="160">HL-20 spacecraft</strong> (model provided by <a href="https://juliahub.com" data-jscall-id="161">JuliaHub</a>) with a procedurally generated emissive rocket exhaust plume. <code data-jscall-id="162">RGBGridMedium</code> with per-voxel emission creates the blue-white core fading to orange at the edges:</p><p data-jscall-id="163"><img alt="HL-20 spacecraft with volumetric exhaust" data-jscall-id="164" src="https://makie.org/website/bonito/png/spacecraft12604446927484047855.png"/></p><h3 id="GLTF Models: Emissive Textures and Scene Composition" data-jscall-id="165">GLTF Models: Emissive Textures and Scene Composition</h3><p data-jscall-id="166">RayMakie loads GLTF/GLB models and automatically maps their PBR materials to Hikari equivalents. Emissive texture maps create area lights that both glow and illuminate the scene. The Christmas tree decorations below are lit entirely by their own emissive maps; the quadcopter model was provided by <a href="https://juliahub.com" data-jscall-id="167">JuliaHub</a>:</p><p data-jscall-id="168"><img alt="Christmas tree with glowing decorations and drone" data-jscall-id="169" src="https://makie.org/website/bonito/png/drone_christmas13347553003851564844.png"/></p><h3 id="Geant4: Particle Detector Visualization" data-jscall-id="170">Geant4: Particle Detector Visualization</h3><p data-jscall-id="171"><a href="https://github.com/JuliaHEP/Geant4.jl" data-jscall-id="172">Geant4.jl</a> provides Julia bindings to the CERN Geant4 particle physics simulation toolkit. The CMS (Compact Muon Solenoid) detector at CERN is one of the largest and most complex scientific instruments ever built. RayMakie can load the full detector geometry from GDML files, apply a quadrant cut to reveal internal structure, and render the result with physically-based metal materials (gold, copper, silver, aluminum, iron).</p><table data-jscall-id="173"><tbody><tr data-jscall-id="174"><th data-jscall-id="175"></th><th data-jscall-id="176"></th></tr><tr data-jscall-id="177"><td data-jscall-id="178"><img alt="CMS detector close-up" data-jscall-id="179" src="https://makie.org/website/bonito/png/cms_detector24219475611935801176.png"/></td><td data-jscall-id="180"><img alt="CMS detector full view" data-jscall-id="181" src="https://makie.org/website/bonito/png/cms_detector5584426331694323319.png"/></td></tr></tbody></table><p data-jscall-id="182">The detector meshes are generated from Geant4&#39;s tessellated polyhedra with crease-angle vertex splitting for smooth normals on curved barrel surfaces while preserving hard edges at structural boundaries.</p><h3 id="Black Hole: Custom GPU Materials" data-jscall-id="183">Black Hole: Custom GPU Materials</h3><p data-jscall-id="184">One of the most powerful aspects of building a ray tracer in Julia is that users can define entirely new physics by subtyping <code data-jscall-id="185">Hikari.Medium</code>. The black hole demo demonstrates this: a custom <code data-jscall-id="186">SpacetimeMedium</code> struct implements gravitational lensing by overriding <code data-jscall-id="187">Hikari.apply_deflection</code> to bend light rays according to the Schwarzschild metric. The accretion disk is an emissive volumetric density field with temperature-dependent coloring, hotter near the event horizon, cooler at the outer edge.</p><p data-jscall-id="188">Because Julia&#39;s multiple dispatch compiles these custom methods directly into the GPU kernels alongside the built-in materials, there is no performance penalty for user-defined physics. The same wavefront path tracer handles standard glass and metals alongside relativistic spacetime curvature.</p><p data-jscall-id="189"><img alt="Black hole with accretion disk and gravitational lensing" data-jscall-id="190" src="https://makie.org/website/bonito/png/blackhole2083336987488631976.png"/></p><p data-jscall-id="191">The black hole can also be explored interactively. RayMakie&#39;s interactive mode progressively refines the path-traced image while you move the camera in real time:</p><p data-jscall-id="195">An overlay renderer composites rasterized elements (lines, text, wireframes) on top of the ray-traced image, so Makie&#39;s standard 2D overlays like legends, colorbars, and annotations work alongside path tracing. The interactive window can also target individual subscenes and axes, so you can place a ray-traced view next to other plots in a GLMakie figure. Looking ahead, this overlay renderer has the potential to grow into a full pure-Julia GPU-accelerated rasterizer, which could eventually replace OpenGL as Makie&#39;s primary rendering backend.</p><h2 id="Getting Started" data-jscall-id="196">Getting Started</h2><p data-jscall-id="197">RayMakie is available as a Makie backend. To render any existing Makie scene with ray tracing:</p><pre data-jscall-id="198"><code data-jscall-id="199">
using RayMakie, Hikari

# Build your scene with standard Makie
scene = Scene(size=(800, 600), lights=[SunSkyLight(Vec3f(1, 2, 8))])
cam3d!(scene)
mesh!(scene, my_geometry; material=Hikari.Gold())

# Render with path tracing
img = colorbuffer(scene;
    device=AMDGPU.ROCBackend(),
    integrator=Hikari.VolPath(samples=100, max_depth=12),
)</code></pre><p data-jscall-id="200">For interactive exploration, use <code data-jscall-id="201">RayMakie.interactive_window</code> to progressively refine the image while adjusting the camera in real time.</p><h2 id="Future Work" data-jscall-id="202">Future Work</h2><ul data-jscall-id="203"><li data-jscall-id="204"><p data-jscall-id="205"><strong data-jscall-id="206">Memory optimizations</strong>: GPU memory is currently allocated eagerly and not released early, making it easy to run out of VRAM on complex scenes. We plan to implement early release of intermediate GPU buffers and more careful memory management. We also can still improve packing and our struct sizes and test that we dont do unecessary allocations.</p></li><li data-jscall-id="207"><p data-jscall-id="208"><strong data-jscall-id="209">Performance optimizations</strong>: While rendering is already fast, there is significant room for optimization in BVH construction, batching ray intersections, kernel occupancy, and data layout.</p></li><li data-jscall-id="210"><p data-jscall-id="211"><strong data-jscall-id="212">Code quality and Makie integration</strong>: The current implementation is a solid prototype, but needs cleanup and full integration of experimental features. Some internal code paths and the Makie backend interface need improvement.</p></li><li data-jscall-id="213"><p data-jscall-id="214"><strong data-jscall-id="215">SPPM for caustics</strong>: Stochastic Progressive Photon Mapping was removed and deprioritized, but we would like to bring caustic rendering back in the future.</p></li><li data-jscall-id="216"><p data-jscall-id="217"><strong data-jscall-id="218">GPU backend testing</strong>: Currently only AMD GPUs (via AMDGPU.jl) are fully tested and guaranteed to work well. NVIDIA support via CUDA.jl should work but needs more testing. The CPU backend still has allocation issues that are likely a Julia-level bug requiring further investigation.</p></li><li data-jscall-id="219"><p data-jscall-id="220"><strong data-jscall-id="221">Makie tests</strong> We still need to port the whole Makie test suite with reference tests, right now tests are minimal and no visual regression tests.</p></li></ul><h2 id="Acknowledgments" data-jscall-id="222">Acknowledgments</h2><p data-jscall-id="223">Hikari is a Julia port of <a href="https://pbrt.org/" data-jscall-id="224">pbrt-v4</a> by Matt Pharr, Wenzel Jakob, and Greg Humphreys. It builds on <a href="https://github.com/JuliaGraphics/Trace.jl" data-jscall-id="225">Trace.jl</a>, originally created by <a href="https://github.com/pxl-th" data-jscall-id="226">Anton Smirnov</a>, and on the <a href="https://github.com/JuliaGeometry/Raycore.jl" data-jscall-id="227">Raycore.jl</a> intersection engine. GPU support is powered by <a href="https://github.com/JuliaGPU/KernelAbstractions.jl" data-jscall-id="228">KernelAbstractions.jl</a> and the Julia GPU ecosystem. The quadcopter and HL-20 spacecraft GLTF models were provided by <a href="https://juliahub.com" data-jscall-id="229">JuliaHub</a>.</p><p data-jscall-id="230">This work was made possible by an investment of the <a href="https://www.sovereign.tech" data-jscall-id="231">Sovereign Tech Agency</a>.</p><p data-jscall-id="232">Some of the optimization and GPU work on Raycore has been funded by <a href="https://www.muonspace.com/" data-jscall-id="233">Muon Space</a>.</p></div></div></div>
  </body>
</html>
