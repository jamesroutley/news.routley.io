<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/SonyResearch/micro_diffusion">Original</a>
    <h1>Diffusion training from scratch on a micro-budget</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">This repository provides a minimalistic implementation of our approach to training large-scale diffusion models from scratch on an extremely low budget. In particular, using only 37M publicly available real and synthetic images, we train a 1.16 billion parameter sparse transformer with a total cost of only $1,890 and achieve an FID of 12.7 in zero-shot generation on the COCO dataset. Please find further details in the official paper: <a href="https://arxiv.org/abs/2407.15811" rel="nofollow">https://arxiv.org/abs/2407.15811</a>.</p>

  <p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/SonyResearch/micro_diffusion/blob/main/assets/demo.jpg"><img src="https://github.com/SonyResearch/micro_diffusion/raw/main/assets/demo.jpg" alt="Alt text"/></a></p>
  Prompt: <em>&#39;Image of an astronaut riding a horse in {} style&#39;.</em> Styles: Origami, Pixel art, Line art, Cyberpunk, Van Gogh Starry Night, Animation, Watercolor, Stained glass

<p dir="auto">The current codebase enables the reproduction of our models, as it provides both the training code and the dataset code used in the paper. We also provide pre-trained model checkpoints for off-the-shelf generation.</p>

<p dir="auto">Clone the repository and install micro_diffusion as a Python package using the following command.</p>


<p dir="auto">First, set up the necessary directories to store the datasets and training artifacts.</p>
<div dir="auto" data-snippet-clipboard-copy-content="ln -s path_to_dir_that_stores_data ./datadir
ln -s path_to_dir_that_stores_trained_models ./trained_models"><pre>ln -s path_to_dir_that_stores_data ./datadir
ln -s path_to_dir_that_stores_trained_models ./trained_models</pre></div>
<p dir="auto">Next, download the training dataset by following <a href="https://github.com/SonyResearch/micro_diffusion/blob/main/micro_diffusion/datasets/README.md">datasets.md</a>. It provides instructions on downloading and precomputing the image and caption latents for all five datasets used in the paper. As downloading and precomputing latents for entire datasets can be time-consuming, it supports downloading and using only a small (~1%) fraction of each dataset, instead of the full dataset. We strongly recommend working with this small subset for initial experimentation.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Training Stages for Micro-Budget Models</h2><a id="user-content-training-stages-for-micro-budget-models" aria-label="Permalink: Training Stages for Micro-Budget Models" href="#training-stages-for-micro-budget-models"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">We progressively train the models from low resolution to high resolution. We first train the model on 256×256 resolution images for 280K steps and then fine-tune the model for 55K steps on 512×512 resolution images. The estimated training time for the end-to-end model on an 8×H100 machine is 2.6 days. We provide the training configuration for each stage in the ./yamls/ directory.</p>
<p dir="auto"><strong>Patch masking</strong>: Our DiT model by default uses a patch-mixer before the backbone transformer architecture. Using the patch-mixer significantly reduces performance degradation with masking while providing a large reduction in training time. We mask 75% of the patches after the patch mixer across both resolutions. After training with masking, we perform a follow-up fine-tuning with a mask ratio of 0 to slightly improve performance. Overall, the training includes four steps.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Step-1: Pre-training at 256x256 image resolution with 75% patch masking</h3><a id="user-content-step-1-pre-training-at-256x256-image-resolution-with-75-patch-masking" aria-label="Permalink: Step-1: Pre-training at 256x256 image resolution with 75% patch masking" href="#step-1-pre-training-at-256x256-image-resolution-with-75-patch-masking"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto" data-snippet-clipboard-copy-content="composer train.py --config-path ./configs --config-name res_256_pretrain.yaml exp_name=MicroDiTXL_mask_75_res_256_pretrain model.train_mask_ratio=0.75"><pre>composer train.py --config-path ./configs --config-name res_256_pretrain.yaml exp_name=MicroDiTXL_mask_75_res_256_pretrain model.train_mask_ratio=0.75</pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Step-2: Finetuning at 256x256 image resolution with no patch masking</h3><a id="user-content-step-2-finetuning-at-256x256-image-resolution-with-no-patch-masking" aria-label="Permalink: Step-2: Finetuning at 256x256 image resolution with no patch masking" href="#step-2-finetuning-at-256x256-image-resolution-with-no-patch-masking"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto" data-snippet-clipboard-copy-content="composer train.py --config-path ./configs --config-name res_256_finetune.yaml exp_name=MicroDiTXL_mask_0_res_256_finetune model.train_mask_ratio=0.0 trainer.load_path=./trained_models/MicroDiTXL_mask_75_res_256_pretrain/latest-rank0.pt"><pre>composer train.py --config-path ./configs --config-name res_256_finetune.yaml exp_name=MicroDiTXL_mask_0_res_256_finetune model.train_mask_ratio=0.0 trainer.load_path=./trained_models/MicroDiTXL_mask_75_res_256_pretrain/latest-rank0.pt</pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Step-3: Finetuning at 512x512 image resolution with 75% patch masking</h3><a id="user-content-step-3-finetuning-at-512x512-image-resolution-with-75-patch-masking" aria-label="Permalink: Step-3: Finetuning at 512x512 image resolution with 75% patch masking" href="#step-3-finetuning-at-512x512-image-resolution-with-75-patch-masking"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto" data-snippet-clipboard-copy-content="composer train.py --config-path ./configs --config-name res_512_pretrain.yaml exp_name=MicroDiTXL_mask_75_res_512_pretrain model.train_mask_ratio=0.75 trainer.load_path=./trained_models/MicroDiTXL_mask_0_res_256_finetune/latest-rank0.pt"><pre>composer train.py --config-path ./configs --config-name res_512_pretrain.yaml exp_name=MicroDiTXL_mask_75_res_512_pretrain model.train_mask_ratio=0.75 trainer.load_path=./trained_models/MicroDiTXL_mask_0_res_256_finetune/latest-rank0.pt</pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Step-4: Finetuning at 512x512 image resolution with no patch masking</h3><a id="user-content-step-4-finetuning-at-512x512-image-resolution-with-no-patch-masking" aria-label="Permalink: Step-4: Finetuning at 512x512 image resolution with no patch masking" href="#step-4-finetuning-at-512x512-image-resolution-with-no-patch-masking"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto" data-snippet-clipboard-copy-content="composer train.py --config-path ./configs --config-name res_512_finetune.yaml exp_name=MicroDiTXL_mask_0_res_512_finetune model.train_mask_ratio=0.0 trainer.load_path=./trained_models/MicroDiTXL_mask_75_res_512_pretrain/latest-rank0.pt"><pre>composer train.py --config-path ./configs --config-name res_512_finetune.yaml exp_name=MicroDiTXL_mask_0_res_512_finetune model.train_mask_ratio=0.0 trainer.load_path=./trained_models/MicroDiTXL_mask_75_res_512_pretrain/latest-rank0.pt</pre></div>
<p dir="auto">Across all steps, we use a batch size of 2,048, apply center cropping, and do not horizontally flip the images.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Pre-trained Model Checkpoints</h2><a id="user-content-pre-trained-model-checkpoints" aria-label="Permalink: Pre-trained Model Checkpoints" href="#pre-trained-model-checkpoints"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">We release four pre-trained models (<a href="https://huggingface.co/VSehwag24/MicroDiT" rel="nofollow">HF</a>). The table below provides download links and a description of each model.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model Description</th>
<th>VAE (channels)</th>
<th>FID</th>
<th>GenEval Score</th>
<th>Download</th>
</tr>
</thead>
<tbody>
<tr>
<td>MicroDiT_XL_2 trained on 22M real images</td>
<td>SDXL-VAE (4 channel)</td>
<td>12.72</td>
<td>0.46</td>
<td><a href="https://huggingface.co/VSehwag24/MicroDiT/resolve/main/ckpts/dit_4_channel_22M_real_only_data.pt" rel="nofollow">link</a></td>
</tr>
<tr>
<td>MicroDiT_XL_2 trained on 37M images (22M real, 15M synthetic)</td>
<td>SDXL-VAE (4 channel)</td>
<td><strong>12.66</strong></td>
<td>0.46</td>
<td><a href="https://huggingface.co/VSehwag24/MicroDiT/resolve/main/ckpts/dit_4_channel_37M_real_and_synthetic_data.pt" rel="nofollow">link</a></td>
</tr>
<tr>
<td>MicroDiT_XL_2 trained on 37M images (22M real, 15M synthetic)</td>
<td>Ostris-VAE (16 channel)</td>
<td>13.04</td>
<td>0.40</td>
<td><a href="https://huggingface.co/VSehwag24/MicroDiT/resolve/main/ckpts/dit_16_channel_37M_real_and_synthetic_data.pt" rel="nofollow">link</a></td>
</tr>
<tr>
<td>MicroDiT_XL_2 trained on 490M synthetic images</td>
<td>SDXL-VAE (4 channel)</td>
<td>13.26</td>
<td><strong>0.52</strong></td>
<td><a href="https://huggingface.co/VSehwag24/MicroDiT/resolve/main/ckpts/dit_4_channel_0.5B_synthetic_data.pt" rel="nofollow">link</a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">All four models are trained with nearly identical training configurations and computational budgets.</p>

<p dir="auto">Use the following straightforward steps to generate images from the final model at 512×512 resolution.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from micro_diffusion.models.model import create_latent_diffusion
model = create_latent_diffusion(latent_res=64, in_channels=4, pos_interp_scale=2.0).to(&#39;cuda&#39;)
model.dit.load_state_dict(torch.load(final_ckpt_path_on_local_disk)) # use model.load_state_dict if ckpt includes vae and text-encoder
gen_images = model.generate(prompt=[&#39;An elegant squirrel pirate on a ship&#39;]*4, num_inference_steps=30, 
                                    guidance_scale=5.0, seed=2024)"><pre><span>from</span> <span>micro_diffusion</span>.<span>models</span>.<span>model</span> <span>import</span> <span>create_latent_diffusion</span>
<span>model</span> <span>=</span> <span>create_latent_diffusion</span>(<span>latent_res</span><span>=</span><span>64</span>, <span>in_channels</span><span>=</span><span>4</span>, <span>pos_interp_scale</span><span>=</span><span>2.0</span>).<span>to</span>(<span>&#39;cuda&#39;</span>)
<span>model</span>.<span>dit</span>.<span>load_state_dict</span>(<span>torch</span>.<span>load</span>(<span>final_ckpt_path_on_local_disk</span>)) <span># use model.load_state_dict if ckpt includes vae and text-encoder</span>
<span>gen_images</span> <span>=</span> <span>model</span>.<span>generate</span>(<span>prompt</span><span>=</span>[<span>&#39;An elegant squirrel pirate on a ship&#39;</span>]<span>*</span><span>4</span>, <span>num_inference_steps</span><span>=</span><span>30</span>, 
                                    <span>guidance_scale</span><span>=</span><span>5.0</span>, <span>seed</span><span>=</span><span>2024</span>)</pre></div>

<p dir="auto">We would like to thank previous open-source efforts that we utilize in our code, in particular, the Composer training framework, streaming dataloaders, the Diffusers library, and the original Diffusion Transformers (DiT) implementation.</p>

<p dir="auto">The code and model weights are released under Apache 2.0 License.</p>

<div dir="auto" data-snippet-clipboard-copy-content="@article{Sehwag2024MicroDiT,
  title={Stretching Each Dollar: Diffusion Training from Scratch on a Micro-Budget},
  author={Sehwag, Vikash and Kong, Xianghao and Li, Jingtao and Spranger, Michael and Lyu, Lingjuan},
  journal={arXiv preprint arXiv:2407.15811},
  year={2024}
}"><pre><span>@article</span>{<span>Sehwag2024MicroDiT</span>,
  <span>title</span>=<span><span>{</span>Stretching Each Dollar: Diffusion Training from Scratch on a Micro-Budget<span>}</span></span>,
  <span>author</span>=<span><span>{</span>Sehwag, Vikash and Kong, Xianghao and Li, Jingtao and Spranger, Michael and Lyu, Lingjuan<span>}</span></span>,
  <span>journal</span>=<span><span>{</span>arXiv preprint arXiv:2407.15811<span>}</span></span>,
  <span>year</span>=<span><span>{</span>2024<span>}</span></span>
}</pre></div>
</article></div></div>
  </body>
</html>
