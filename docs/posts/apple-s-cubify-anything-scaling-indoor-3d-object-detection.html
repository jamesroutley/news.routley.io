<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/apple/ml-cubifyanything">Original</a>
    <h1>Apple&#39;s Cubify Anything: Scaling Indoor 3D Object Detection</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">This repository includes the public implementation of Cubify Transformer and the
associated CA-1M dataset.</p>

<p dir="auto"><strong>Apple</strong></p>
<p dir="auto"><a href="https://arxiv.org/abs/2412.04458" rel="nofollow">Cubify Anything: Scaling Indoor 3D Object Detection</a></p>
<p dir="auto">Justin Lazarow, David Griffiths, Gefen Kohavi, Francisco Crespo, Afshin Dehghan</p>
<p dir="auto"><strong>CVPR 2025</strong></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/apple/ml-cubifyanything/blob/main/teaser.jpg?raw=true"><img src="https://github.com/apple/ml-cubifyanything/raw/main/teaser.jpg?raw=true" alt="Teaser" title="Teaser"/></a></p>

<p dir="auto">This repository includes:</p>
<ol dir="auto">
<li>Links to the underlying data and annotations of the CA-1M dataset.</li>
<li>Links to released models of the Cubify Transformer (CuTR) model from the Cubify Anything paper.</li>
<li>Basic readers and inference code to run CuTR on the provided data.</li>
<li>Basic support for using images captured from own device using the NeRF Capture app.</li>
</ol>

<p dir="auto">We recommend Python 3.10 and a recent 2.x build of PyTorch. We include a <code>requirements.txt</code> which should encapsulate
all necessary dependencies. Please make sure you have <code>torch</code> installed first, e.g.,:</p>
<div data-snippet-clipboard-copy-content="pip install torch torchvision"><pre><code>pip install torch torchvision
</code></pre></div>
<p dir="auto">Then, within the root of the repository:</p>
<div data-snippet-clipboard-copy-content="pip install -r requirements.txt
pip install -e ."><pre><code>pip install -r requirements.txt
pip install -e .
</code></pre></div>
<div dir="auto"><h2 tabindex="-1" dir="auto">CA-1M versus ARKitScenes?</h2><a id="user-content-ca-1m-versus-arkitscenes" aria-label="Permalink: CA-1M versus ARKitScenes?" href="#ca-1m-versus-arkitscenes"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">This work is related to <a href="https://machinelearning.apple.com/research/arkitscenes" rel="nofollow">ARKitScenes</a>. We generally share
the same underlying captures. Some notable differences in CA-1M:</p>
<ol dir="auto">
<li>Each scene has been exhaustively annotated with class-agnostic 3D boxes. We release these in the laser scanner&#39;s coordinate frame.</li>
<li>For each frame in each capture, we include &#34;per-frame&#34; 3D box ground-truth which was produced using the rendering
process outlined in the Cubify Anything paper. These annotations are, therefore, <em>independent</em> of any pose.</li>
</ol>
<p dir="auto">Some other nice things:</p>
<ol dir="auto">
<li>We release the GT poses (registered to laser scanner) for every frame in each capture.</li>
<li>We release the GT depth (rendered from laser scanner) at 512 x 384 for every frame in each capture.</li>
<li>Each frame has been already oriented into an upright position.</li>
</ol>
<p dir="auto"><strong>NOTE:</strong> CA-1M will only include captures which were successfully registered to the laser scanner. Therefore
not every capture including in ARKitScenes will be present in CA-1M.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Downloading and using the CA-1M data</h2><a id="user-content-downloading-and-using-the-ca-1m-data" aria-label="Permalink: Downloading and using the CA-1M data" href="#downloading-and-using-the-ca-1m-data"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>

<p dir="auto"><strong>All data is released under the <a href="https://github.com/apple/ml-cubifyanything/blob/main/data/LICENSE_DATA">CC-by-NC-ND</a>.</strong></p>
<p dir="auto">All links to the data are contained in <code>data/train.txt</code> and <code>data/val.txt</code>. You can use <code>curl</code> to download all files
listed. If you don&#39;t need the whole dataset in advance, you can either explicitly pass these
links explicitly or pass the split&#39;s <code>txt</code> file itself and use the <code>--video-ids</code> argument to filter the desired videos.</p>
<p dir="auto">If you pass the <code>txt</code> file, please note that file will be cached under <code>data/[split]</code>.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Understanding the CA-1M data</h2><a id="user-content-understanding-the-ca-1m-data" aria-label="Permalink: Understanding the CA-1M data" href="#understanding-the-ca-1m-data"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">CA-1M is released in WebDataset format. Therefore, it is essentially a fancy tar archive
<em>per</em> capture (i.e., a video). Therefore, a single archive <code>ca1m-[split]-XXXXXXX.tar</code> corresponds to all data
of capture XXXXXXXX.</p>
<p dir="auto">Both splits are released at full frame rate.</p>
<p dir="auto">All data should be neatly loaded by <code>CubifyAnythingDataset</code>. Please refer to <code>dataset.py</code> for more
specifics on how to read/parse data on disk. Some general pointers:</p>
<div dir="auto" data-snippet-clipboard-copy-content="[video_id]/[integer_timestamp].wide/image.png               # A 1024x768 RGB image corresponding to the main camera.
[video_id]/[integer_timestamp].wide/depth.png               # A 256x192 depth image stored as a UInt16 (as millimeters) derived from the capture device&#39;s onboard LiDAR (ARKit depth).
[video_id]/[integer_timestamp].wide/depth/confidence.tiff   # A 256x192 confidence image storing the [0, 1] confidence value of each depth measurement (currently unused).
[video_id]/[integer_timestamp].wide/instances.json          # A list of GT instances alongside their 3D boxes (i.e., the resulting of the GT rendering process).
[video_id]/[integer_timestamp].wide/T_gravity.json          # A rotation matrix which encodes the pitch/roll of the camera, which we assume is known (e.g., IMU).

[video_id]/[integer_timestamp].gt/RT.json                   # A 4x4 (row major) JSON-encoded matrix corresponding to the registered pose in the laser-scanner space.
[video_id]/[integer_timestamp].gt/depth.png                 # A 512x384 depth image stored as a UInt16 (as millimeters) derived from the FARO laser scanner registration.
"><pre>[<span>video_id</span>]<span>/</span>[<span>integer_timestamp</span>].<span>wide</span><span>/</span><span>image</span>.<span>png</span>               <span># A 1024x768 RGB image corresponding to the main camera.</span>
[<span>video_id</span>]<span>/</span>[<span>integer_timestamp</span>].<span>wide</span><span>/</span><span>depth</span>.<span>png</span>               <span># A 256x192 depth image stored as a UInt16 (as millimeters) derived from the capture device&#39;s onboard LiDAR (ARKit depth).</span>
[<span>video_id</span>]<span>/</span>[<span>integer_timestamp</span>].<span>wide</span><span>/</span><span>depth</span><span>/</span><span>confidence</span>.<span>tiff</span>   <span># A 256x192 confidence image storing the [0, 1] confidence value of each depth measurement (currently unused).</span>
[<span>video_id</span>]<span>/</span>[<span>integer_timestamp</span>].<span>wide</span><span>/</span><span>instances</span>.<span>json</span>          <span># A list of GT instances alongside their 3D boxes (i.e., the resulting of the GT rendering process).</span>
[<span>video_id</span>]<span>/</span>[<span>integer_timestamp</span>].<span>wide</span><span>/</span><span>T_gravity</span>.<span>json</span>          <span># A rotation matrix which encodes the pitch/roll of the camera, which we assume is known (e.g., IMU).</span>

[<span>video_id</span>]<span>/</span>[<span>integer_timestamp</span>].<span>gt</span><span>/</span><span>RT</span>.<span>json</span>                   <span># A 4x4 (row major) JSON-encoded matrix corresponding to the registered pose in the laser-scanner space.</span>
[<span>video_id</span>]<span>/</span>[<span>integer_timestamp</span>].<span>gt</span><span>/</span><span>depth</span>.<span>png</span>                 <span># A 512x384 depth image stored as a UInt16 (as millimeters) derived from the FARO laser scanner registration.</span></pre></div>
<p dir="auto">Note that since we have already oriented the images, these dimensions may be transposed. GT depth may have 0 values which corresponding to unregistered points.</p>
<p dir="auto">An additional file is included as <code>[video_id]/world.gt/instances.json</code> which corresponds to the full world set of 3D annotations from which
the per-frame labels are generated from. These instances include some structural labels: <code>wall</code>, <code>floor</code>, <code>ceiling</code>, <code>door_frame</code> which
might aid in rendering.</p>

<p dir="auto">We include visualization support using <a href="https://rerun.io" rel="nofollow">rerun</a>. Visualization should happen
automatically. If you wish to not run any models, but only visualize the data, use <code>--viz-only</code>.</p>
<p dir="auto">During inference, you may wish to inspect the 3D accuracy of the predictions. We support
visualizing the predictions on the GT point cloud (derived from Faro depth) when using
the <code>--viz-on-gt-points</code> flag.</p>

<div dir="auto" data-snippet-clipboard-copy-content="python tools/demo.py [path_to_downloaded_data]/ca1m-val-42898570.tar --viz-only"><pre>python tools/demo.py [path_to_downloaded_data]/ca1m-val-42898570.tar --viz-only</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="python tools/demo.py data/train.txt --viz-only --video-ids 45261548"><pre>python tools/demo.py data/train.txt --viz-only --video-ids 45261548</pre></div>

<p dir="auto">The data is provided at a high frame rate, so using <code>--every-nth-frame N</code> will only
process every N frames.</p>

<p dir="auto"><strong>All models are released under the Apple ML Research Model Terms of Use in <a href="https://github.com/apple/ml-cubifyanything/blob/main/LICENSE_MODEL">LICENSE_MODEL</a>.</strong></p>
<ol dir="auto">
<li><a href="https://ml-site.cdn-apple.com/models/cutr/cutr_rgbd.pth" rel="nofollow">RGB-D</a></li>
<li><a href="https://ml-site.cdn-apple.com/models/cutr/cutr_rgb.pth" rel="nofollow">RGB</a></li>
</ol>
<p dir="auto">Models can be provided to <code>demo.py</code> using the <code>--model-path</code> argument. We detect whether this is an RGB
or RGB-D model and disable depth accordingly.</p>

<p dir="auto">The first variant of CuTR expects an RGB image and a metric depth map. We train on ARKit depth,
although you may find it works with other metric depth estimators as well.</p>

<p dir="auto">If your computer is MPS enabled:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python tools/demo.py data/val.txt --video-ids 42898570 --model-path [path_to_models]/cutr_rgbd.pth --viz-on-gt-points --device mps"><pre>python tools/demo.py data/val.txt --video-ids 42898570 --model-path [path_to_models]/cutr_rgbd.pth --viz-on-gt-points --device mps</pre></div>
<p dir="auto">If your computer is CUDA enabled:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python tools/demo.py data/val.txt --video-ids 42898570 --model-path [path_to_models]/cutr_rgbd.pth --viz-on-gt-points --device cuda"><pre>python tools/demo.py data/val.txt --video-ids 42898570 --model-path [path_to_models]/cutr_rgbd.pth --viz-on-gt-points --device cuda</pre></div>
<p dir="auto">Otherwise:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python tools/demo.py data/val.txt --video-ids 42898570 --model-path [path_to_models]/cutr_rgbd.pth --viz-on-gt-points --device cpu"><pre>python tools/demo.py data/val.txt --video-ids 42898570 --model-path [path_to_models]/cutr_rgbd.pth --viz-on-gt-points --device cpu</pre></div>

<p dir="auto">The second variant of CuTR expects an RGB image alone and attempts to derive the metric scale of
the scene from the image itself.</p>

<p dir="auto">If your device is MPS enabled:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python tools/demo.py data/val.txt --video-ids 42898570 --model-path [path_to_models]/cutr_rgb.pth --viz-on-gt-points --device mps"><pre>python tools/demo.py data/val.txt --video-ids 42898570 --model-path [path_to_models]/cutr_rgb.pth --viz-on-gt-points --device mps</pre></div>
<div dir="auto"><h2 tabindex="-1" dir="auto">Run on captures from your own device</h2><a id="user-content-run-on-captures-from-your-own-device" aria-label="Permalink: Run on captures from your own device" href="#run-on-captures-from-your-own-device"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">We also have basic support for running on RGB/Depth captured from your own device.</p>
<ol dir="auto">
<li>Make sure you have <a href="https://apps.apple.com/au/app/nerfcapture/id6446518379" rel="nofollow">NeRF Capture</a> installed on your device</li>
<li>Start the NeRF Capture app <em>before</em> running <code>demo.py</code> (force quit and reopen if for some reason things stop working or a connection is not made).</li>
<li>Run the normal commands but pass &#34;stream&#34; instead of the usual tar/folder path.</li>
<li>Hit &#34;Send&#34; in the app to send a frame for inference. This will be visualized in the rerun window.</li>
</ol>
<p dir="auto">We will continue to print &#34;Still waiting&#34; to show liveliness.</p>
<p dir="auto">If you have a device equipped with LiDAR, you can use this combined with the RGB-D models, otherwise, you can
only use the RGB only model.</p>

<div dir="auto" data-snippet-clipboard-copy-content="python tools/demo.py stream --model-path [path_to_models]/cutr_rgbd.pth --device mps"><pre>python tools/demo.py stream --model-path [path_to_models]/cutr_rgbd.pth --device mps</pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="python tools/demo.py stream --model-path [path_to_models]/cutr_rgb.pth --device mps"><pre>python tools/demo.py stream --model-path [path_to_models]/cutr_rgb.pth --device mps</pre></div>

<p dir="auto">If you use CA-1M or CuTR in your research, please use the following entry:</p>
<div data-snippet-clipboard-copy-content="@article{lazarow2024cubify,
  title={Cubify Anything: Scaling Indoor 3D Object Detection},
  author={Lazarow, Justin and Griffiths, David and Kohavi, Gefen and Crespo, Francisco and Dehghan, Afshin},
  journal={arXiv preprint arXiv:2412.04458},
  year={2024}
}"><pre><code>@article{lazarow2024cubify,
  title={Cubify Anything: Scaling Indoor 3D Object Detection},
  author={Lazarow, Justin and Griffiths, David and Kohavi, Gefen and Crespo, Francisco and Dehghan, Afshin},
  journal={arXiv preprint arXiv:2412.04458},
  year={2024}
}
</code></pre></div>

<p dir="auto">The sample code is released under <a href="https://github.com/apple/ml-cubifyanything/blob/main/LICENSE">Apple Sample Code License</a>.</p>
<p dir="auto">The data is released under <a href="https://github.com/apple/ml-cubifyanything/blob/main/data/LICENSE_DATA">CC-by-NC-ND</a>.</p>
<p dir="auto">The models are released under <a href="https://github.com/apple/ml-cubifyanything/blob/main/LICENSE_MODEL">Apple ML Research Model Terms of Use</a>.</p>

<p dir="auto">We use and acknowledge contributions from multiple open-source projects in <a href="https://github.com/apple/ml-cubifyanything/blob/main/ACKNOWLEDGEMENTS">ACKNOWLEDGEMENTS</a>.</p>
</article></div></div>
  </body>
</html>
