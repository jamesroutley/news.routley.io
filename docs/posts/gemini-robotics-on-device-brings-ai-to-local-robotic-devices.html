<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://deepmind.google/discover/blog/gemini-robotics-on-device-brings-ai-to-local-robotic-devices/">Original</a>
    <h1>Gemini Robotics On-Device brings AI to local robotic devices</h1>
    
    <div id="readability-page-1" class="page"><div id="content">
      
  <article>
    
    
  
  
  
    
      

      
      
        
          
            <div>
              
                
                
                  
                  
<div>
    <div>
      <p>Models</p>
      

      
    <dl>
      
        <dt>Published</dt>
        <dd><time datetime="2025-06-24">24 June 2025</time></dd>
      
      
        <dt>Authors</dt>
        
      
    </dl>
  

      
    </div>

    
      
    
    
    <picture>
      <source media="(min-width: 1024px)" type="image/webp" width="1072" height="603" srcset="https://lh3.googleusercontent.com/Jt_Vw7PIJEZtXcMIKM1HbWbBCLxv7RUyjyf07eHp-YOfxMCUZA6mPI9kSCaz65UkMoGcZ8CwlD3dNBvy7bnnYchjSkWyN-SugglT3dmg1A9KdoDqdQM=w1072-h603-n-nu-rw 1x, https://lh3.googleusercontent.com/Jt_Vw7PIJEZtXcMIKM1HbWbBCLxv7RUyjyf07eHp-YOfxMCUZA6mPI9kSCaz65UkMoGcZ8CwlD3dNBvy7bnnYchjSkWyN-SugglT3dmg1A9KdoDqdQM=w2144-h1206-n-nu-rw 2x"/><source media="(min-width: 600px)" type="image/webp" width="928" height="522" srcset="https://lh3.googleusercontent.com/Jt_Vw7PIJEZtXcMIKM1HbWbBCLxv7RUyjyf07eHp-YOfxMCUZA6mPI9kSCaz65UkMoGcZ8CwlD3dNBvy7bnnYchjSkWyN-SugglT3dmg1A9KdoDqdQM=w928-h522-n-nu-rw 1x, https://lh3.googleusercontent.com/Jt_Vw7PIJEZtXcMIKM1HbWbBCLxv7RUyjyf07eHp-YOfxMCUZA6mPI9kSCaz65UkMoGcZ8CwlD3dNBvy7bnnYchjSkWyN-SugglT3dmg1A9KdoDqdQM=w1856-h1044-n-nu-rw 2x"/><source type="image/webp" width="528" height="297" srcset="https://lh3.googleusercontent.com/Jt_Vw7PIJEZtXcMIKM1HbWbBCLxv7RUyjyf07eHp-YOfxMCUZA6mPI9kSCaz65UkMoGcZ8CwlD3dNBvy7bnnYchjSkWyN-SugglT3dmg1A9KdoDqdQM=w528-h297-n-nu-rw 1x, https://lh3.googleusercontent.com/Jt_Vw7PIJEZtXcMIKM1HbWbBCLxv7RUyjyf07eHp-YOfxMCUZA6mPI9kSCaz65UkMoGcZ8CwlD3dNBvy7bnnYchjSkWyN-SugglT3dmg1A9KdoDqdQM=w1056-h594-n-nu-rw 2x"/>
      <img alt="Collage of imagery demonstrating Gemini Robotics On-Device. A central abstract image of a toy busy box shaped like a human head alludes to neural functions and problem-solving." height="603" src="https://lh3.googleusercontent.com/Jt_Vw7PIJEZtXcMIKM1HbWbBCLxv7RUyjyf07eHp-YOfxMCUZA6mPI9kSCaz65UkMoGcZ8CwlD3dNBvy7bnnYchjSkWyN-SugglT3dmg1A9KdoDqdQM=w1072-h603-n-nu" width="1072"/>
    </picture>
    
  
    
  </div>
                
              
                
                
                  
                  <div>
  <h4 data-block-key="fslsq">We’re introducing an efficient, on-device robotics model with general-purpose dexterity and fast task adaptation.</h4><p data-block-key="cj9eq">In March, we introduced <a href="https://deepmind.google/discover/blog/gemini-robotics-brings-ai-into-the-physical-world/" rel="noopener" target="_blank">Gemini Robotics</a>, our most advanced VLA (vision language action) model, bringing Gemini 2.0’s multimodal reasoning and real-world understanding into the physical world.</p><p data-block-key="ecdhr">Today, we’re introducing Gemini Robotics On-Device, our most powerful VLA model optimized to run locally on robotic devices. Gemini Robotics On-Device shows strong general-purpose dexterity and task generalization, and it’s optimized to run efficiently on the robot itself.</p><p data-block-key="bv9r5">Since the model operates independent of a data network, it’s helpful for latency sensitive applications, and ensures robustness in environments with intermittent or zero connectivity.</p><p data-block-key="954qn">We’re also sharing a <a href="https://github.com/google-deepmind/gemini-robotics-sdk" rel="noopener" target="_blank">Gemini Robotics SDK</a> to help developers easily evaluate Gemini Robotics On-Device on their tasks and environments, test our model in our <a href="https://github.com/google-deepmind/aloha_sim" rel="noopener" target="_blank">MuJoCo</a> physics simulator, and quickly adapt it to new domains, with as few as 50 to 100 demonstrations. Developers can access the SDK by signing up to our trusted tester program.</p><h2 data-block-key="s070">Model capabilities and performance</h2><p data-block-key="2f1eg">Gemini Robotics On-Device is a robotics foundation model for bi-arm robots, engineered to require minimal computational resources. It builds on the task generalization and dexterity capabilities of Gemini Robotics and is:</p><ul><li data-block-key="6usj8">Designed for rapid experimentation with dexterous manipulation.</li><li data-block-key="52so1">Adaptable to new tasks through fine-tuning to improve performance.</li><li data-block-key="cpmu">Optimized to run locally with low-latency inference.</li></ul><p data-block-key="6034e">Gemini Robotics On-Device achieves strong visual, semantic and behavioral generalization across a wide range of testing scenarios, follows natural language instructions, and completes highly-dexterous tasks like unzipping bags or folding clothes — all while operating directly on the robot.</p>
</div>
                
              
                
                
                  
                  





<figure>
  

  
</figure>
                
              
                
                
                  
                  <p data-block-key="fje3d">In our evaluations, our On-Device mode exhibits strong generalization performance while running entirely locally.</p>
                
              
                
                
                  
                  





<figure aria-labelledby="caption-456f78ab-7f92-4940-985f-1d15556c9775">
  

  <figcaption>
      <p data-block-key="qu0k9">Chart evaluating Gemini Robotics On-Device’s generalization performance, compared to our flagship Gemini Robotics model and the previous best on-device model.</p>
    </figcaption>
</figure>
                
              
                
                
                  
                  <p data-block-key="bfdk2">Gemini Robotics On-Device also outperforms other on-device alternatives on more challenging out-of-distribution tasks and complex multi-step instructions. For developers seeking state-of-the-art results in these settings, without on-device limitations, we also offer the Gemini Robotics model.</p>
                
              
                
                
                  
                  





<figure aria-labelledby="caption-52dcd6d1-0509-4a0f-ba66-7a7ca7de6c1a">
  

  <figcaption>
      <p data-block-key="qu0k9">Chart evaluating Gemini Robotics On-Device’s instruction following performance, compared to our flagship Gemini Robotics model and the previous best on-device model.</p>
    </figcaption>
</figure>
                
              
                
                
                  
                  <div>
  <p data-block-key="bfdk2">To learn more about our evaluations, read our <a href="https://arxiv.org/pdf/2503.20020" rel="noopener" target="_blank">Gemini Robotics tech report</a>.</p>
</div>
                
              
                
                
                  
                  <div>
  <h2 data-block-key="bfdk2">Adaptable to new tasks, generalizable across embodiments</h2><p data-block-key="bdslo">Gemini Robotics On-Device is the first VLA model we&#39;re making available for fine-tuning. While many tasks will work out of the box, developers can also choose to adapt the model to achieve better performance for their applications. Our model quickly adapts to new tasks, with as few as 50 to 100 demonstrations — indicating how well this on-device model can generalize its foundational knowledge to new tasks.</p><p data-block-key="bvcef">Here, we show how Gemini Robotics On-Device outperforms the current, best on-device VLA on tasks involving fine-tuning to newer models. We tested the model on seven dexterous manipulation tasks of varying degrees of difficulty, including zipping a lunch-box, drawing a card and pouring salad dressing.</p>
</div>
                
              
                
                
                  
                  





<figure aria-labelledby="caption-bea01efa-d837-4abb-9e3a-98a583f023de">
  

  <figcaption>
      <p data-block-key="qu0k9">Chart showing Gemini Robotics On-Device’s task adaptation performance, with fewer than 100 examples.</p>
    </figcaption>
</figure>
                
              
                
                
                  
                  <div>
  <p data-block-key="bfdk2">We further adapted the Gemini Robotics On-Device model to different robot embodiments. While we trained our model only for <a href="https://aloha-2.github.io/" rel="noopener" target="_blank">ALOHA robots</a>, we were able to further adapt it to a bi-arm <a href="https://franka.de/franka-research-3" rel="noopener" target="_blank">Franka FR3 robot</a> and the <a href="https://apptronik.com/apollo" rel="noopener" target="_blank">Apollo humanoid robot</a> by Apptronik.</p><p data-block-key="ft9gu">On the bi-arm Franka, the model performs general-purpose instruction following, including handling previously unseen objects and scenes, completing dexterous tasks like folding a dress, or executing <a href="https://www.nist.gov/el/intelligent-systems-division-73500/robotic-grasping-and-manipulation-assembly/assembly" rel="noopener" target="_blank">industrial belt assembly tasks</a> that require precision and dexterity.</p>
</div>
                
              
                
                
                  
                  





<figure>
  

  
</figure>
                
              
                
                
                  
                  <p data-block-key="bfdk2">On the Apollo humanoid, we adapt the model to a significantly different embodiment. The same generalist model can follow natural language instructions and manipulate different objects, including previously unseen objects, in a general manner.</p>
                
              
                
                
                  
                  





<figure>
  

  
</figure>
                
              
                
                
                  
                  <div>
  <h2 data-block-key="bfdk2">Responsible development and safety</h2><p data-block-key="659ds">We’re developing all Gemini Robotics models in alignment with our <a href="https://ai.google/principles/" rel="noopener" target="_blank">AI Principles</a> and applying a <a href="https://sites.google.com/corp/view/safe-robots" rel="noopener" target="_blank">holistic safety approach</a> spanning semantic and physical safety.</p><p data-block-key="735t">In practice, we capture semantic and content safety using the <a href="https://ai.google.dev/gemini-api/docs/live" rel="noopener" target="_blank">Live API</a>, and interface our models with low-level safety critical controllers to execute the actions. We recommend evaluating the end-to-end system on our recently developed <a href="https://asimov-benchmark.github.io/" rel="noopener" target="_blank">semantic safety benchmark</a> and performing <a href="https://predictive-red-team.github.io/" rel="noopener" target="_blank">red-teaming exercises</a> at all levels to expose the model’s safety vulnerabilities.</p><p data-block-key="61g0f">Our Responsible Development &amp; Innovation (ReDI) team continues to analyze and advise on the real-world impact of all Gemini Robotics models, finding ways to maximize their societal impact and minimize risk. Then our Responsibility &amp; Safety Council (RSC) reviews these assessments, providing feedback to integrate into model development to help further maximize benefits and minimize risk.</p><p data-block-key="3v220">To gain a deeper understanding of Gemini Robotics On-Device’s usage and safety profile and to gather feedback, we’re initially releasing it to a select group of trusted testers.</p><h2 data-block-key="7efg8">Accelerating innovation in robotics</h2><p data-block-key="6r8fc">Gemini Robotics On-Device marks a step forward in making powerful robotics models more accessible and adaptable — and our on-device solution will help the robotics community tackle important latency and connectivity challenges.</p><p data-block-key="abc0t">The Gemini Robotics SDK will further accelerate innovation by allowing developers to adapt the model to their specific needs. Sign up for model and SDK access via our <a href="https://docs.google.com/forms/d/1sM5GqcVMWv-KmKY3TOMpVtQ-lDFeAftQ-d9xQn92jCE/edit?ts=67cef986" rel="noopener" target="_blank">trusted tester program</a>.</p><p data-block-key="4erv7">We’re excited to see what the robotics community will build with these new tools as we continue to explore the future of bringing AI into the physical world.</p>
</div>
                
              
                
                
                  
                  

<section>
  

  <ul>
    
      <li>
            <gemini-button data-in-view="">
              <a data-gtm-tag="cta-selection" href="https://docs.google.com/forms/d/1sM5GqcVMWv-KmKY3TOMpVtQ-lDFeAftQ-d9xQn92jCE/edit?ts=67cef986" rel="noopener" target="_blank">
      <span>Sign up for our trusted tester program</span>
      <svg aria-hidden="true" role="presentation">
    <use href="/static/glue-icons.87e996bc684c.svg#open-in-new"></use>
  </svg>
    </a>
            </gemini-button>
        </li>
        
    
      <li>
            <gemini-button data-in-view="">
              <a data-gtm-tag="cta-selection" href="https://arxiv.org/pdf/2503.20020" rel="noopener" target="_blank">
      <span>Read the Gemini Robotics tech report</span>
      <svg aria-hidden="true" role="presentation">
    <use href="/static/glue-icons.87e996bc684c.svg#open-in-new"></use>
  </svg>
    </a>
            </gemini-button>
        </li>
        
    
      <li>
            <gemini-button data-in-view="">
              <a data-gtm-tag="cta-selection" href="https://github.com/google-deepmind/aloha_sim" rel="noopener" target="_blank">
      <span>Test ALOHA robots in simulation</span>
      <svg aria-hidden="true" role="presentation">
    <use href="/static/glue-icons.87e996bc684c.svg#open-in-new"></use>
  </svg>
    </a>
            </gemini-button>
        </li>
        
    
  </ul>
</section>
                
              
                
                
                  
                  <section>
  <div>
    <div>
      <p data-block-key="nfym9"><strong>Acknowledgements</strong></p><p data-block-key="eg2vo">We gratefully acknowledge contributions, advice, and support from Abbas Abdolmaleki, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Maria Attarian, Ashwin Balakrishna, Yanan Bao, Clara Barbu, Catarina Barros, Robert Baruch, Nathan Batchelor, Maria Bauza, Lucas Beyer, Michael Bloesch, Michiel Blokzijl, Steven Bohez, Konstantinos Bousmalis, Demetra Brady, Philemon Brakel, Anthony Brohan, Thomas Buschmann, Arunkumar Byravan, Kendra Byrne, Serkan Cabi, Ken Caluwaerts, Federico Casarini, Christine Chan, Oscar Chang, Jose Enrique Chen, Xi Chen, Huizhong Chen, Hao-Tien Lewis Chiang, Krzysztof Choromanski, Adrian Collister, Kieran Connell, David D&#39;Ambrosio, Sudeep Dasari, Todor Davchev, Coline Devin, Norman Di Palo, Tianli Ding, Adil Dostmohamed, Anca Dragan, Yilun Du, Debidatta Dwibedi, Michael Elabd, Tom Erez, Claudio Fantacci, Cody Fong, Erik Frey, Chuyuan Fu, Frankie Garcia, Ashley Gibb, Marissa Giustina, Keerthana Gopalakrishnan, Laura Graesser, Simon Green, Oliver Groth, Roland Hafner, Leonard Hasenclever, Sam Haves, Nicolas Heess, Brandon Hernaez, Tim Hertweck, Alexander Herzog, R. Alex Hofer, Sandy H Huang, Jan Humplik , Atil Iscen, Mithun George Jacob, Deepali Jain, Sally Jesmonth, Ryan Julian, Dmitry Kalashnikov, M. Emre Karagozler, Stefani Karp, Chase Kew, Jerad Kirkland, Sean Kirmani, Yuheng Kuang, Thomas Lampe, Antoine Laurens, Isabel Leal, Alex X. Lee, Tsang-Wei Edward Lee, Jennie Lees, Jacky Liang, Yixin Lin, Li-Heng Lin, Caden Lu, Sharath Maddineni, Anirudha Majumdar, Kevis-Kokitsi Maninis, Siobhan Mcloughlin, Assaf Hurwitz Michaely, Joss Moore, Robert Moreno, Thomas Mulc, Michael Neunert, Francesco Nori, Dave Orr, Carolina Parada, Emilio Parisotto, Peter Pastor, André Susano Pinto, Acorn Pooley, Grace Popple, Thomas Power, Alessio Quaglino, Haroon Qureshi, Kanishka Rao, Dushyant Rao, Krista Reymann, Martin Riedmiller, Francesco Romano, Keran Rong, Dorsa Sadigh, Stefano Saliceti, Daniel Salz, Pannag Sanketi, Mili Sanwalka, Kevin Sayed, Pierre Sermanet, Dhruv Shah, Mohit Sharma, Kathryn Shea, Mohit Shridhar, Charles Shu, Vikas Sindhwani, Sumeet Singh, Radu Soricut, Andreas Steiner, Rachel Sterneck, Ian Storz, Razvan Surdulescu, Ben Swanson, Mitri Syriani, Jie Tan, Yuval Tassa, Alan Thompson, Dhruva Tirumala, Jonathan Tompson, Karen Truong, Jake Varley, Siddharth Verma, Grace Vesom, Giulia Vezzani, Oriol Vinyals, Ayzaan Wahid, Zhicheng Wang, Stefan Welker, Paul Wohlhart, Chengda Wu, Markus Wulfmeier, Fei Xia, Ted Xiao, Annie Xie, Jinyu Xie, Peng Xu, Sichun Xu, Ying Xu, Zhuo Xu, Yuxiang Yang, Rui Yao, Sergey Yaroshenko, Matt Young, Wenhao Yu, Wentao Yuan, Martina Zambelli, Xiaohua Zhai, Jingwei Zhang, Tingnan Zhang, Allan Zhou, Yuxiang Zhou, Guangyao (Stannis) Zhou, Howard Zhou.</p><p data-block-key="416qn">We also thank the operations and support staff that performed data collection and robot evaluations for this project.</p>
    </div>
  </div>
</section>
                
              
            </div>
          
        
      

      
    
  
  

  

  </article>

    </div></div>
  </body>
</html>
