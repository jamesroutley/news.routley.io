<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/lstein/stable-diffusion/commit/62863ac586194a43ff952eba17a83cecf9956500">Original</a>
    <h1>Speedup from switch to &#43;=</h1>
    
    <div id="readability-page-1" class="page"><p dir="auto">Changing this back to the original implementation fixed an error I was getting when doing textual inversion on Windows:</p><div data-snippet-clipboard-copy-content="  | Name              | Type               | Params
---------------------------------------------------------
0 | model             | DiffusionWrapper   | 859 M
1 | first_stage_model | AutoencoderKL      | 83.7 M
2 | cond_stage_model  | FrozenCLIPEmbedder | 123 M
3 | embedding_manager | EmbeddingManager   | 1.5 K
---------------------------------------------------------
768       Trainable params
1.1 B     Non-trainable params
1.1 B     Total params
4,264.947 Total estimated model params size (MB)
Global seed set to 23
C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\callbacks\lr_monitor.py:112: RuntimeWarning: You are using `LearningRateMonitor` callback with models that have no learning rate schedulers. Please see documentation for `configure_optimizers` method.
  rank_zero_warn(
Epoch 0:   0%|                                                                                                                                                                                               | 0/1414 [00:00&lt;?, ?it/s][W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\reducer.cpp:1289] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator ())
C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\autograd\__init__.py:173: UserWarning: Error detected in NativeLayerNormBackward0. Traceback of forward call that caused the error:
  File &#34;main.py&#34;, line 945, in &lt;module&gt;
    trainer.fit(model, data)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\trainer\trainer.py&#34;, line 553, in fit
    self._run(model)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\trainer\trainer.py&#34;, line 918, in _run
    self._dispatch()
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\trainer\trainer.py&#34;, line 986, in _dispatch
    self.accelerator.start_training(self)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\accelerators\accelerator.py&#34;, line 92, in start_training
    self.training_type_plugin.start_training(trainer)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\plugins\training_type\training_type_plugin.py&#34;, line 161, in start_training
    self._results = trainer.run_stage()
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\trainer\trainer.py&#34;, line 996, in run_stage
    return self._run_train()
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\trainer\trainer.py&#34;, line 1045, in _run_train
    self.fit_loop.run()
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\base.py&#34;, line 111, in run
    self.advance(*args, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\fit_loop.py&#34;, line 200, in advance
    epoch_output = self.epoch_loop.run(train_dataloader)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\base.py&#34;, line 111, in run
    self.advance(*args, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\epoch\training_epoch_loop.py&#34;, line 130, in advance
    batch_output = self.batch_loop.run(batch, self.iteration_count, self._dataloader_idx)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\batch\training_batch_loop.py&#34;, line 101, in run
    super().run(batch, batch_idx, dataloader_idx)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\base.py&#34;, line 111, in run
    self.advance(*args, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\batch\training_batch_loop.py&#34;, line 148, in advance
    result = self._run_optimization(batch_idx, split_batch, opt_idx, optimizer)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\batch\training_batch_loop.py&#34;, line 202, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\batch\training_batch_loop.py&#34;, line 396, in _optimizer_step
    model_ref.optimizer_step(
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\core\lightning.py&#34;, line 1618, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\core\optimizer.py&#34;, line 209, in step
    self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\core\optimizer.py&#34;, line 129, in __optimizer_step
    trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\accelerators\accelerator.py&#34;, line 296, in optimizer_step
    self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\accelerators\accelerator.py&#34;, line 303, in run_optimizer_step
    self.training_type_plugin.optimizer_step(optimizer, lambda_closure=lambda_closure, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\plugins\training_type\training_type_plugin.py&#34;, line 226, in optimizer_step
    optimizer.step(closure=lambda_closure, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\optim\optimizer.py&#34;, line 88, in wrapper
    return func(*args, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\autograd\grad_mode.py&#34;, line 27, in decorate_context
    return func(*args, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\optim\adamw.py&#34;, line 100, in step
    loss = closure()
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\batch\training_batch_loop.py&#34;, line 236, in _training_step_and_backward_closure
    result = self.training_step_and_backward(split_batch, batch_idx, opt_idx, optimizer, hiddens)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\batch\training_batch_loop.py&#34;, line 537, in training_step_and_backward
    result = self._training_step(split_batch, batch_idx, opt_idx, hiddens)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\batch\training_batch_loop.py&#34;, line 307, in _training_step
    training_step_output = self.trainer.accelerator.training_step(step_kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\accelerators\accelerator.py&#34;, line 193, in training_step
    return self.training_type_plugin.training_step(*step_kwargs.values())
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\plugins\training_type\ddp.py&#34;, line 383, in training_step
    return self.model(*args, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\nn\modules\module.py&#34;, line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\nn\parallel\distributed.py&#34;, line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\nn\modules\module.py&#34;, line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\overrides\base.py&#34;, line 82, in forward
    output = self.module.training_step(*inputs, **kwargs)
  File &#34;A:\stable-diffusion\stable-diffusion\ldm\models\diffusion\ddpm.py&#34;, line 498, in training_step
    loss, loss_dict = self.shared_step(batch)
  File &#34;A:\stable-diffusion\stable-diffusion\ldm\models\diffusion\ddpm.py&#34;, line 1253, in shared_step
    loss = self(x, c)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\nn\modules\module.py&#34;, line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File &#34;A:\stable-diffusion\stable-diffusion\ldm\models\diffusion\ddpm.py&#34;, line 1270, in forward
    return self.p_losses(x, c, t, *args, **kwargs)
  File &#34;A:\stable-diffusion\stable-diffusion\ldm\models\diffusion\ddpm.py&#34;, line 1475, in p_losses
    model_output = self.apply_model(x_noisy, t, cond)
  File &#34;A:\stable-diffusion\stable-diffusion\ldm\models\diffusion\ddpm.py&#34;, line 1440, in apply_model
    x_recon = self.model(x_noisy, t, **cond)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\nn\modules\module.py&#34;, line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File &#34;A:\stable-diffusion\stable-diffusion\ldm\models\diffusion\ddpm.py&#34;, line 2148, in forward
    out = self.diffusion_model(x, t, context=cc)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\nn\modules\module.py&#34;, line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File &#34;A:\stable-diffusion\stable-diffusion\ldm\modules\diffusionmodules\openaimodel.py&#34;, line 811, in forward
    h = module(h, emb, context)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\nn\modules\module.py&#34;, line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File &#34;A:\stable-diffusion\stable-diffusion\ldm\modules\diffusionmodules\openaimodel.py&#34;, line 88, in forward
    x = layer(x, context)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\nn\modules\module.py&#34;, line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File &#34;A:\stable-diffusion\stable-diffusion\ldm\modules\attention.py&#34;, line 298, in forward
    x = block(x, context=context)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\nn\modules\module.py&#34;, line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File &#34;A:\stable-diffusion\stable-diffusion\ldm\modules\attention.py&#34;, line 232, in forward
    return checkpoint(
  File &#34;A:\stable-diffusion\stable-diffusion\ldm\modules\diffusionmodules\util.py&#34;, line 155, in checkpoint
    return func(*inputs)
  File &#34;A:\stable-diffusion\stable-diffusion\ldm\modules\attention.py&#34;, line 240, in _forward
    x += self.ff(self.norm3(x))
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\nn\modules\module.py&#34;, line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\nn\modules\normalization.py&#34;, line 189, in forward
    return F.layer_norm(
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\nn\functional.py&#34;, line 2486, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
 (Triggered internally at  C:\cb\pytorch_1000000000000\work\torch\csrc\autograd\python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Summoning checkpoint.
Traceback (most recent call last):
  File &#34;main.py&#34;, line 945, in &lt;module&gt;
    trainer.fit(model, data)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\trainer\trainer.py&#34;, line 553, in fit
    self._run(model)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\trainer\trainer.py&#34;, line 918, in _run
    self._dispatch()
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\trainer\trainer.py&#34;, line 986, in _dispatch
    self.accelerator.start_training(self)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\accelerators\accelerator.py&#34;, line 92, in start_training
    self.training_type_plugin.start_training(trainer)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\plugins\training_type\training_type_plugin.py&#34;, line 161, in start_training
    self._results = trainer.run_stage()
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\trainer\trainer.py&#34;, line 996, in run_stage
    return self._run_train()
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\trainer\trainer.py&#34;, line 1045, in _run_train
    self.fit_loop.run()
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\base.py&#34;, line 111, in run
    self.advance(*args, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\fit_loop.py&#34;, line 200, in advance
    epoch_output = self.epoch_loop.run(train_dataloader)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\base.py&#34;, line 111, in run
    self.advance(*args, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\epoch\training_epoch_loop.py&#34;, line 130, in advance
    batch_output = self.batch_loop.run(batch, self.iteration_count, self._dataloader_idx)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\batch\training_batch_loop.py&#34;, line 101, in run
    super().run(batch, batch_idx, dataloader_idx)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\base.py&#34;, line 111, in run
    self.advance(*args, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\batch\training_batch_loop.py&#34;, line 148, in advance
    result = self._run_optimization(batch_idx, split_batch, opt_idx, optimizer)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\batch\training_batch_loop.py&#34;, line 202, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\batch\training_batch_loop.py&#34;, line 396, in _optimizer_step
    model_ref.optimizer_step(
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\core\lightning.py&#34;, line 1618, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\core\optimizer.py&#34;, line 209, in step
    self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\core\optimizer.py&#34;, line 129, in __optimizer_step
    trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\accelerators\accelerator.py&#34;, line 296, in optimizer_step
    self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\accelerators\accelerator.py&#34;, line 303, in run_optimizer_step
    self.training_type_plugin.optimizer_step(optimizer, lambda_closure=lambda_closure, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\plugins\training_type\training_type_plugin.py&#34;, line 226, in optimizer_step
    optimizer.step(closure=lambda_closure, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\optim\optimizer.py&#34;, line 88, in wrapper
    return func(*args, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\autograd\grad_mode.py&#34;, line 27, in decorate_context
    return func(*args, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\optim\adamw.py&#34;, line 100, in step
    loss = closure()
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\batch\training_batch_loop.py&#34;, line 236, in _training_step_and_backward_closure
    result = self.training_step_and_backward(split_batch, batch_idx, opt_idx, optimizer, hiddens)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\batch\training_batch_loop.py&#34;, line 549, in training_step_and_backward
    self.backward(result, optimizer, opt_idx)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\batch\training_batch_loop.py&#34;, line 590, in backward
    result.closure_loss = self.trainer.accelerator.backward(result.closure_loss, optimizer, *args, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\accelerators\accelerator.py&#34;, line 276, in backward
    self.precision_plugin.backward(self.lightning_module, closure_loss, *args, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\plugins\precision\precision_plugin.py&#34;, line 78, in backward
    model.backward(closure_loss, optimizer, *args, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\core\lightning.py&#34;, line 1481, in backward
    loss.backward(*args, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\_tensor.py&#34;, line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\autograd\__init__.py&#34;, line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [1, 4096, 320]], which is output 0 of AsStridedBackward0, is at version 3; expected version 2 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"><pre lang="text"><code>  | Name              | Type               | Params
---------------------------------------------------------
0 | model             | DiffusionWrapper   | 859 M
1 | first_stage_model | AutoencoderKL      | 83.7 M
2 | cond_stage_model  | FrozenCLIPEmbedder | 123 M
3 | embedding_manager | EmbeddingManager   | 1.5 K
---------------------------------------------------------
768       Trainable params
1.1 B     Non-trainable params
1.1 B     Total params
4,264.947 Total estimated model params size (MB)
Global seed set to 23
C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\callbacks\lr_monitor.py:112: RuntimeWarning: You are using `LearningRateMonitor` callback with models that have no learning rate schedulers. Please see documentation for `configure_optimizers` method.
  rank_zero_warn(
Epoch 0:   0%|                                                                                                                                                                                               | 0/1414 [00:00&lt;?, ?it/s][W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\reducer.cpp:1289] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator ())
C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\autograd\__init__.py:173: UserWarning: Error detected in NativeLayerNormBackward0. Traceback of forward call that caused the error:
  File &#34;main.py&#34;, line 945, in &lt;module&gt;
    trainer.fit(model, data)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\trainer\trainer.py&#34;, line 553, in fit
    self._run(model)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\trainer\trainer.py&#34;, line 918, in _run
    self._dispatch()
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\trainer\trainer.py&#34;, line 986, in _dispatch
    self.accelerator.start_training(self)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\accelerators\accelerator.py&#34;, line 92, in start_training
    self.training_type_plugin.start_training(trainer)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\plugins\training_type\training_type_plugin.py&#34;, line 161, in start_training
    self._results = trainer.run_stage()
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\trainer\trainer.py&#34;, line 996, in run_stage
    return self._run_train()
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\trainer\trainer.py&#34;, line 1045, in _run_train
    self.fit_loop.run()
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\base.py&#34;, line 111, in run
    self.advance(*args, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\fit_loop.py&#34;, line 200, in advance
    epoch_output = self.epoch_loop.run(train_dataloader)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\base.py&#34;, line 111, in run
    self.advance(*args, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\epoch\training_epoch_loop.py&#34;, line 130, in advance
    batch_output = self.batch_loop.run(batch, self.iteration_count, self._dataloader_idx)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\batch\training_batch_loop.py&#34;, line 101, in run
    super().run(batch, batch_idx, dataloader_idx)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\base.py&#34;, line 111, in run
    self.advance(*args, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\batch\training_batch_loop.py&#34;, line 148, in advance
    result = self._run_optimization(batch_idx, split_batch, opt_idx, optimizer)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\batch\training_batch_loop.py&#34;, line 202, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\batch\training_batch_loop.py&#34;, line 396, in _optimizer_step
    model_ref.optimizer_step(
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\core\lightning.py&#34;, line 1618, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\core\optimizer.py&#34;, line 209, in step
    self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\core\optimizer.py&#34;, line 129, in __optimizer_step
    trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\accelerators\accelerator.py&#34;, line 296, in optimizer_step
    self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\accelerators\accelerator.py&#34;, line 303, in run_optimizer_step
    self.training_type_plugin.optimizer_step(optimizer, lambda_closure=lambda_closure, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\plugins\training_type\training_type_plugin.py&#34;, line 226, in optimizer_step
    optimizer.step(closure=lambda_closure, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\optim\optimizer.py&#34;, line 88, in wrapper
    return func(*args, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\autograd\grad_mode.py&#34;, line 27, in decorate_context
    return func(*args, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\optim\adamw.py&#34;, line 100, in step
    loss = closure()
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\batch\training_batch_loop.py&#34;, line 236, in _training_step_and_backward_closure
    result = self.training_step_and_backward(split_batch, batch_idx, opt_idx, optimizer, hiddens)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\batch\training_batch_loop.py&#34;, line 537, in training_step_and_backward
    result = self._training_step(split_batch, batch_idx, opt_idx, hiddens)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\batch\training_batch_loop.py&#34;, line 307, in _training_step
    training_step_output = self.trainer.accelerator.training_step(step_kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\accelerators\accelerator.py&#34;, line 193, in training_step
    return self.training_type_plugin.training_step(*step_kwargs.values())
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\plugins\training_type\ddp.py&#34;, line 383, in training_step
    return self.model(*args, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\nn\modules\module.py&#34;, line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\nn\parallel\distributed.py&#34;, line 963, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\nn\modules\module.py&#34;, line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\overrides\base.py&#34;, line 82, in forward
    output = self.module.training_step(*inputs, **kwargs)
  File &#34;A:\stable-diffusion\stable-diffusion\ldm\models\diffusion\ddpm.py&#34;, line 498, in training_step
    loss, loss_dict = self.shared_step(batch)
  File &#34;A:\stable-diffusion\stable-diffusion\ldm\models\diffusion\ddpm.py&#34;, line 1253, in shared_step
    loss = self(x, c)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\nn\modules\module.py&#34;, line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File &#34;A:\stable-diffusion\stable-diffusion\ldm\models\diffusion\ddpm.py&#34;, line 1270, in forward
    return self.p_losses(x, c, t, *args, **kwargs)
  File &#34;A:\stable-diffusion\stable-diffusion\ldm\models\diffusion\ddpm.py&#34;, line 1475, in p_losses
    model_output = self.apply_model(x_noisy, t, cond)
  File &#34;A:\stable-diffusion\stable-diffusion\ldm\models\diffusion\ddpm.py&#34;, line 1440, in apply_model
    x_recon = self.model(x_noisy, t, **cond)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\nn\modules\module.py&#34;, line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File &#34;A:\stable-diffusion\stable-diffusion\ldm\models\diffusion\ddpm.py&#34;, line 2148, in forward
    out = self.diffusion_model(x, t, context=cc)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\nn\modules\module.py&#34;, line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File &#34;A:\stable-diffusion\stable-diffusion\ldm\modules\diffusionmodules\openaimodel.py&#34;, line 811, in forward
    h = module(h, emb, context)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\nn\modules\module.py&#34;, line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File &#34;A:\stable-diffusion\stable-diffusion\ldm\modules\diffusionmodules\openaimodel.py&#34;, line 88, in forward
    x = layer(x, context)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\nn\modules\module.py&#34;, line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File &#34;A:\stable-diffusion\stable-diffusion\ldm\modules\attention.py&#34;, line 298, in forward
    x = block(x, context=context)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\nn\modules\module.py&#34;, line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File &#34;A:\stable-diffusion\stable-diffusion\ldm\modules\attention.py&#34;, line 232, in forward
    return checkpoint(
  File &#34;A:\stable-diffusion\stable-diffusion\ldm\modules\diffusionmodules\util.py&#34;, line 155, in checkpoint
    return func(*inputs)
  File &#34;A:\stable-diffusion\stable-diffusion\ldm\modules\attention.py&#34;, line 240, in _forward
    x += self.ff(self.norm3(x))
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\nn\modules\module.py&#34;, line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\nn\modules\normalization.py&#34;, line 189, in forward
    return F.layer_norm(
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\nn\functional.py&#34;, line 2486, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
 (Triggered internally at  C:\cb\pytorch_1000000000000\work\torch\csrc\autograd\python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Summoning checkpoint.
Traceback (most recent call last):
  File &#34;main.py&#34;, line 945, in &lt;module&gt;
    trainer.fit(model, data)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\trainer\trainer.py&#34;, line 553, in fit
    self._run(model)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\trainer\trainer.py&#34;, line 918, in _run
    self._dispatch()
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\trainer\trainer.py&#34;, line 986, in _dispatch
    self.accelerator.start_training(self)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\accelerators\accelerator.py&#34;, line 92, in start_training
    self.training_type_plugin.start_training(trainer)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\plugins\training_type\training_type_plugin.py&#34;, line 161, in start_training
    self._results = trainer.run_stage()
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\trainer\trainer.py&#34;, line 996, in run_stage
    return self._run_train()
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\trainer\trainer.py&#34;, line 1045, in _run_train
    self.fit_loop.run()
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\base.py&#34;, line 111, in run
    self.advance(*args, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\fit_loop.py&#34;, line 200, in advance
    epoch_output = self.epoch_loop.run(train_dataloader)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\base.py&#34;, line 111, in run
    self.advance(*args, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\epoch\training_epoch_loop.py&#34;, line 130, in advance
    batch_output = self.batch_loop.run(batch, self.iteration_count, self._dataloader_idx)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\batch\training_batch_loop.py&#34;, line 101, in run
    super().run(batch, batch_idx, dataloader_idx)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\base.py&#34;, line 111, in run
    self.advance(*args, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\batch\training_batch_loop.py&#34;, line 148, in advance
    result = self._run_optimization(batch_idx, split_batch, opt_idx, optimizer)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\batch\training_batch_loop.py&#34;, line 202, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\batch\training_batch_loop.py&#34;, line 396, in _optimizer_step
    model_ref.optimizer_step(
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\core\lightning.py&#34;, line 1618, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\core\optimizer.py&#34;, line 209, in step
    self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\core\optimizer.py&#34;, line 129, in __optimizer_step
    trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\accelerators\accelerator.py&#34;, line 296, in optimizer_step
    self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\accelerators\accelerator.py&#34;, line 303, in run_optimizer_step
    self.training_type_plugin.optimizer_step(optimizer, lambda_closure=lambda_closure, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\plugins\training_type\training_type_plugin.py&#34;, line 226, in optimizer_step
    optimizer.step(closure=lambda_closure, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\optim\optimizer.py&#34;, line 88, in wrapper
    return func(*args, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\autograd\grad_mode.py&#34;, line 27, in decorate_context
    return func(*args, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\optim\adamw.py&#34;, line 100, in step
    loss = closure()
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\batch\training_batch_loop.py&#34;, line 236, in _training_step_and_backward_closure
    result = self.training_step_and_backward(split_batch, batch_idx, opt_idx, optimizer, hiddens)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\batch\training_batch_loop.py&#34;, line 549, in training_step_and_backward
    self.backward(result, optimizer, opt_idx)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\loops\batch\training_batch_loop.py&#34;, line 590, in backward
    result.closure_loss = self.trainer.accelerator.backward(result.closure_loss, optimizer, *args, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\accelerators\accelerator.py&#34;, line 276, in backward
    self.precision_plugin.backward(self.lightning_module, closure_loss, *args, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\plugins\precision\precision_plugin.py&#34;, line 78, in backward
    model.backward(closure_loss, optimizer, *args, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\pytorch_lightning\core\lightning.py&#34;, line 1481, in backward
    loss.backward(*args, **kwargs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\_tensor.py&#34;, line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File &#34;C:\Users\xiohe\.conda\envs\ldm\lib\site-packages\torch\autograd\__init__.py&#34;, line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [1, 4096, 320]], which is output 0 of AsStridedBackward0, is at version 3; expected version 2 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
</code></pre></div></div>
  </body>
</html>
