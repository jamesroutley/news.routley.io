<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lspace.swyx.io/p/multimodal-gpt4">Original</a>
    <h1>GPT4 and the Multi-Modal, Multi-Model, Multi-Everything Future of AGI</h1>
    
    <div id="readability-page-1" class="page"><div><div><article><div class=""><div><div dir="auto"><p><span>As was rumored and </span><a href="https://www.heise.de/news/GPT-4-is-coming-next-week-and-it-will-be-multimodal-says-Microsoft-Germany-7540972.html" rel="">then confirmed by Microsoft Germany</a><span>, GPT-4 was released yesterday in ChatGPT with a </span><a href="https://openai.com/research/gpt-4" rel="">blogpost</a><span>, </span><a href="https://cdn.openai.com/papers/gpt-4.pdf" rel="">paper</a><span>, </span><a href="https://www.youtube.com/watch?v=outcGtbnMuQ" rel="">livestream</a><span>, and a couple of </span><a href="https://www.youtube.com/watch?v=TxkJMX0KyS0" rel="">short videos</a><span>:</span></p><p><span>To use simple measures of how anticipated this was - GPT-4 is already the </span><a href="https://hn.algolia.com/" rel="">11th-most upvoted Hacker News story</a><span> of </span><em><strong>ALL TIME</strong><span>, </span></em><span>the Developer Livestream got 1.5 million views in 20 hours (currently #5 trending video on all of YouTube) and the announcement </span><a href="https://twitter.com/OpenAI/status/1635687373060317185?s=20" rel="">tweet</a><span> got 4x more likes than the same for ChatGPT, itself the biggest story of 2022. </span></p><ul><li><p><span>“Today has been a great year in AI” - </span><a href="https://twitter.com/tobi/status/1635755163615911936?s=20" rel="">Tobi Lutke, Shopify CEO</a></p></li><li><p><span>“Not sure I can think of a time where there was this much unexplored territory with this much new capability in the hands of this many users.” - </span><a href="https://twitter.com/karpathy/status/1635749104059056128" rel="">Karpathy</a></p></li></ul><p><span>There are lots of screenshots and bad takes flying around, so I figure it would be most useful to do the same executive-summary-style recap </span><a href="https://lspace.swyx.io/p/everything-we-know-about-chatgpt" rel="">I did for ChatGPT</a><span>, for GPT-4.</span></p><p>GPT-4 is the newest version of OpenAI’s flagship language model. It is:</p><ul><li><p><strong>significantly better</strong><span> at existing GPT-3 tasks (</span><em>huge</em><span> improvements across both </span><a href="https://twitter.com/swyx/status/1635690596416491521?s=20" rel="">standard NLP benchmarks</a></p><span> &amp; </span><a href="https://twitter.com/swyx/status/1635689844189036544" rel="">human exams like the SAT/GRE</a><span>, and better </span><a href="https://twitter.com/DanGrover/status/1635713083523084288?s=20" rel="">instruction following</a><span> and </span><a href="https://twitter.com/DanHendrycks/status/1635706823373377538?s=20" rel="">world knowledge</a><span>)</span></li><li><p><span>capable of </span><strong>new</strong><span> tasks</span></p><span> (enough </span><strong>math</strong><span> to </span><a href="https://twitter.com/swyx/status/1635739184869826561" rel="">do your taxes</a><span> and </span><a href="https://twitter.com/swyx/status/1635749861185445888" rel="">beat Minerva</a><span>!)</span></li><li><p><span>able to use </span><strong>8x more context</strong><span> than ChatGPT (50 pages, 25k words of context  means unlocks better AI-enabled coding</span></p><span> by </span><a href="https://www.youtube.com/live/outcGtbnMuQ?feature=share&amp;t=516" rel="">simply pasting docs</a><span>, or better chat by pasting </span><a href="https://twitter.com/omarsar0/status/1635690756177379328" rel="">entire Wikipedia articles</a><span>, or even </span><a href="https://www.youtube.com/live/outcGtbnMuQ?feature=share&amp;t=319" rel="">comparing two articles</a><span>)</span></li><li><p><strong>safer</strong><span> to use (</span><a href="https://twitter.com/swyx/status/1635693559348338688" rel="">20-30% fewer hallucinations and unsafe content</a></p><span>) </span></li></ul><p><span>That alone would qualify it as a huge release, but GPT-4 is </span><em>also</em><span> OpenAI’s first </span><strong>multimodal</strong><span> model, being able to </span><em><strong>natively</strong><span> </span></em><span>understand image input as well as text. This is </span><strong>orders of magnitude better</strong><span> than existing OCR and Image-to-Text (e.g. </span><a href="https://pbs.twimg.com/media/FrNmIGKaQAAjDtU?format=jpg&amp;name=large" rel="">BLIP</a><span>) solutions and has to be seen to be fully understood, but the capabilities that you </span><em>must</em><span> know include:</span></p><ul><li><p><strong>Converting</strong><span> a sketch of a website into code (</span><a href="https://twitter.com/rowancheung/status/1635744529587359756?s=20" rel="">screenshot</a><span>, </span><a href="https://www.youtube.com/live/outcGtbnMuQ?feature=share&amp;t=993" rel="">demo timestamp</a><span>)</span></p></li><li><p><span>Fully </span><strong>describing</strong><span> a </span><a href="https://twitter.com/eerac/status/1635737216864452612" rel="">screenshot</a><span> of a Discord app (</span><a href="https://www.youtube.com/live/outcGtbnMuQ?feature=share&amp;t=621" rel="">demo timestamp</a><span>)</span></p></li><li><p><strong>Summarizing</strong><span> </span><em>images</em><span> of a paper and answering questions about figures (</span><a href="https://twitter.com/omarsar0/status/1635729572816732167" rel="">screenshot</a><span>)</span></p></li><li><p><strong>Recognizing</strong><span> photos (</span><a href="https://twitter.com/swyx/status/1635765117303521282?s=20" rel="">fridge</a><span>, </span><a href="https://twitter.com/omarsar0/status/1635689918696501257?s=20" rel="">kitchen</a><span>), offering meal ideas (</span><a href="https://www.nytimes.com/2023/03/14/technology/openai-gpt4-chatgpt.html" rel="">NYT article</a><span>)</span></p></li><li><p><strong>Explaining </strong><span>why an image is funny (</span><a href="https://twitter.com/swyx/status/1635692241523208195?s=20" rel="">ironing clothes, chicken nuggets, memes</a><span>)</span></p></li></ul><p><strong>GPT-4</strong><span> can be tried out today by being a ChatGPT Plus subscriber ($20/month), while text API access is granted on </span><a href="https://openai.com/waitlist/gpt-4-api" rel="">a waitlist</a><span> or by </span><a href="https://github.com/openai/evals" rel="">contributing OpenAI Evals</a><span>. The multimodal visual API capability is exclusive to </span><a href="http://bemyeyes.com" rel="">BeMyEyes</a><span> for now. </span><a href="https://openai.com/research/gpt-4#api" rel="">API Pricing</a><span> is now split into </span><a href="https://www.jonstokes.com/p/the-chat-stack-gpt-4-and-the-near" rel="">prompt tokens and completion tokens</a><span> and is </span><a href="https://twitter.com/transitive_bs/status/1635712260424478720" rel="">30-60x higher than GPT-3.5</a></p><p><span>.</span></p><p><span>In a break from the past, OpenAI declined to release </span><strong>any</strong><span> technical details of GPT-4, citing competition and safety concerns. This means </span><a href="https://lspace.swyx.io/p/ok-foomer" rel="">the Small Circle, Big Circle memes</a><span> were not confirmed nor denied</span></p><p><span> and that another round of </span><a href="https://twitter.com/ykilcher/status/1635702708006006786?s=20" rel="">criticism of OpenAI not being open</a><span> started again.</span></p><ul><li><p><strong>We know</strong><span>: that GPT-4’s </span><a href="https://www.youtube.com/watch?v=--khbXchTeE" rel="">training started 2 years ago and ended in August 2022</a><span>, that GPT-4’s data cutoff was Sept 2021</span></p><span>.</span></li><li><p><strong>We don’t know</strong><span>: how the </span><strong>data</strong></p><strong>, compute</strong><strong>, hardware</strong><strong>, parameters</strong><span> or training process changed from GPT-3.</span></li></ul><p><span>In place of technical detail, OpenAI instead focused on demonstrating capabilities (explained above), </span><a href="https://twitter.com/swyx/status/1635688942354980865?s=20" rel="">scaling</a><span> and safety research (done by OpenAI’s </span><a href="https://www.reddit.com/r/singularity/comments/11rfs22/openais_arc_challenges_gpt4_to_reproduce_and/" rel="">Alignment Research Center</a></p><p><span>) and demonstrating usecases with launch partners in an impressively coordinated launch (with a full slate of </span><a href="https://openai.com/product/gpt-4#built-with-gpt-4" rel="">Built With GPT-4</a><span> examples on launch day):</span></p><ul><li><p><span>Microsoft </span><a href="https://techcrunch.com/2023/03/14/microsofts-new-bing-was-using-gpt-4-all-along/" rel="">confirmed</a><span> that Prometheus was their codename for GPT-4, meaning all Bing/Sydney users were really GPT-4 users (worrying if you have seen </span><a href="https://news.ycombinator.com/item?id=34804874" rel="">Sydney’s issues</a><span> in the wild) and also </span><a href="https://twitter.com/MParakhin/status/1635741730464059392" rel="">increased Bing query limits</a></p></li><li><p><a href="https://twitter.com/duolingo/status/1635688521695633408" rel="">Duolingo</a><span> (</span><a href="https://blog.duolingo.com/duolingo-max/" rel="">blog</a><span>) demonstrated new Explain My Answer and Roleplay features for Spanish and French (though GPT-4 also speaks</span><a href="https://pbs.twimg.com/media/FrMj4PAaAAElZRV?format=png&amp;name=900x900" rel=""> many other languages</a><span>)</span></p></li><li><p><a href="https://openai.com/customer-stories/stripe" rel="">Stripe</a><span> tested 15 use cases across support customization, </span><a href="https://twitter.com/bentossell/status/1636021529040375810" rel="">answering docs questions</a><span>, and fraud detection. </span></p></li><li><p><a href="https://twitter.com/destraynor/status/1635705915595685902?s=20" rel="">Intercom</a><span> (</span><a href="https://twitter.com/eoghan/status/1635707829939240960" rel="">Eoghan</a><span>, </span><a href="https://www.intercom.com/ai-bot" rel="">blog</a><span>) launched their Fin chatbot, which reduces hallucinations (incl about competitors), disambiguates, and hands over to human agents</span></p></li><li><p><a href="https://twitter.com/jbrowder1/status/1635720431091974157" rel="">DoNotPay</a><span> teased &#34;one click lawsuits&#34; for robocallers and </span><a href="https://twitter.com/cocksure_crypto/status/1635722368487129088?s=20" rel="">emails without unsubscribe</a></p></li></ul><p><strong>Race Dynamics. </strong><span>The coordination reached beyond OpenAI - GPT-4 wasn’t the only foundation model launch of Tuesday. Both Google and Anthropic launched their </span><a href="https://developers.googleblog.com/2023/03/announcing-palm-api-and-makersuite.html" rel="">PaLM API</a></p><p><span> and </span><a href="https://www.anthropic.com/index/introducing-claude" rel="">Claude+</a><span> models as well, with </span><a href="https://twitter.com/adamdangelo/status/1635690625642397696" rel="">Quora Poe</a><span> being the first app to launch with </span><em>both</em><span> OpenAI GPT-4 AND Anthropic’s Claude+ models. This ultra-competitive launch cycle across companies </span><a href="https://twitter.com/EigenGender/status/1635766846719934465?s=20" rel="">on Pi Day</a><span> smacks of </span><a href="https://twitter.com/xlr8harder/status/1622849293571817472" rel="">last month’s Google vs Microsoft race for special events</a><span> and is causing concern from AI safety worriers and sleep-deprived Substack writers alike.</span></p><p><em><span>(end of summary! phew! but discussions ongoing @ </span><a href="https://news.ycombinator.com/item?id=35172362" rel="">Hacker News</a><span> and </span><a href="https://twitter.com/swyx/status/1636067268802285568" rel="">Twitter</a><span>)</span></em></p><p>GPT-4’s Multimodality is a glimpse of the AGI future to come. It didn’t end up fitting all the speculated capabilities - it doesn’t have image output, and audio was notably missing from the accepted inputs given the Whisper API release, but Jim Fan’s hero image here was mostly spot on:</p><p><span>However, 3 days ago Microsoft Research China released </span><em>another </em><span>approach to multiple modalities with Visual ChatGPT, allowing you to converse with your images the same as GPT-4:</span></p><div data-attrs="{&#34;url&#34;:&#34;https://twitter.com/mathemagic1an/status/1634085179739475968?s=20&#34;,&#34;full_text&#34;:&#34;. &lt;span class=\&#34;tweet-fake-link\&#34;&gt;@Microsoft&lt;/span&gt; releases a single, 900-line python file for \&#34;Visual ChatGPT,\&#34; an agent that can chat w/ images\n\ninteracts with vision models via text and prompt chaining, i.e. the output gets piped to stable diffusion.\n\nAlso uses &lt;span class=\&#34;tweet-fake-link\&#34;&gt;@LangChainAI&lt;/span&gt; &lt;a class=\&#34;tweet-url\&#34; href=\&#34;https://github.com/microsoft/visual-chatgpt/blob/main/visual_chatgpt.py\&#34;&gt;github.com/microsoft/visu…&lt;/a&gt;… https://t.co/HiF4DqLVUx &#34;,&#34;username&#34;:&#34;mathemagic1an&#34;,&#34;name&#34;:&#34;Jay Hack&#34;,&#34;date&#34;:&#34;Fri Mar 10 06:54:01 +0000 2023&#34;,&#34;photos&#34;:[{&#34;img_url&#34;:&#34;https://res.cloudinary.com/hhsslviub/video/upload/e_loop,vs_40/p0eyusmfum1ydx1kyc0s.gif&#34;,&#34;link_url&#34;:&#34;https://t.co/Ug54qbfFz0&#34;,&#34;alt_text&#34;:null}],&#34;quoted_tweet&#34;:{},&#34;retweet_count&#34;:148,&#34;like_count&#34;:837,&#34;expanded_url&#34;:{},&#34;video_url&#34;:null,&#34;belowTheFold&#34;:true}"><a href="https://twitter.com/mathemagic1an/status/1634085179739475968?s=20" target="_blank" rel=""></a><div><div><picture><source type="image/webp" srcset="https://substackcdn.com/video/upload/e_loop,vs_40/p0eyusmfum1ydx1kyc0s.gif"/><img src="https://substackcdn.com/video/upload/e_loop,vs_40/p0eyusmfum1ydx1kyc0s.gif" alt="Image" loading="lazy"/></picture></div></div><a href="https://twitter.com/mathemagic1an/status/1634085179739475968?s=20" target="_blank" rel=""></a></div><p><span>This is a multi-modal project, but is more accurately described as a multi-</span><em>model</em><span> project, because it really is basically “22 models in a trenchcoat</span></p><p><span>”:</span></p><p><span>This hints at two ways of achieving multi-modality - the cheap way (chaining together models, likely with </span><a href="https://langchain.com/" rel="">LangChain</a><span>), and the &#34;right&#34; way (training and embedding on mixed modality datasets). We have some reason to believe that multimodal training gives benefits over and above single modality training - in the same way that adding a corpus of code to language model training has been observed to improve results for non-code natural language, we might observe that teaching an AI what something looks like improves their ability to describe it and vice versa</span></p><p><span>.</span></p><p><span>Even being single-modality but multi-</span><em>model</em><span> is proving to be useful. Quora founder </span><a href="https://twitter.com/adamdangelo/status/1635690630289723394" rel="">Adam D’Angelo </a><span>chose to launch his new Poe bot with </span><em>both</em><span> OpenAI GPT-4 and Anthropic Claude support, and former GitHub CEO </span><a href="https://twitter.com/natfriedman/status/1633582489850773504?s=20" rel="">Nat Friedman built nat.dev</a><span> to help compare outputs across the largest possible range of text models:</span></p><div data-attrs="{&#34;url&#34;:&#34;https://twitter.com/omarsar0/status/1633603705365749760?s=20&#34;,&#34;full_text&#34;:&#34;🐙OpenPlayground\n\nA cool playground that allows experimenting with multiple language models. Love the Compare feature. I see myself using this a ton!\n\nIncredible effort by &lt;span class=\&#34;tweet-fake-link\&#34;&gt;@natfriedman&lt;/span&gt; and team! 👏\n\n&lt;a class=\&#34;tweet-url\&#34; href=\&#34;https://nat.dev/\&#34;&gt;nat.dev&lt;/a&gt; &#34;,&#34;username&#34;:&#34;omarsar0&#34;,&#34;name&#34;:&#34;elvis&#34;,&#34;date&#34;:&#34;Wed Mar 08 23:00:48 +0000 2023&#34;,&#34;photos&#34;:[{&#34;img_url&#34;:&#34;https://pbs.substack.com/media/Fqu5POjX0AIRHHb.jpg&#34;,&#34;link_url&#34;:&#34;https://t.co/Mx4KIvCWTC&#34;,&#34;alt_text&#34;:null}],&#34;quoted_tweet&#34;:{},&#34;retweet_count&#34;:43,&#34;like_count&#34;:217,&#34;expanded_url&#34;:{},&#34;video_url&#34;:null,&#34;belowTheFold&#34;:true}"><a href="https://twitter.com/omarsar0/status/1633603705365749760?s=20" target="_blank" rel=""></a><div><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_600,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fpbs.substack.com%2Fmedia%2FFqu5POjX0AIRHHb.jpg"/><img src="https://substackcdn.com/image/fetch/w_600,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fpbs.substack.com%2Fmedia%2FFqu5POjX0AIRHHb.jpg" alt="Image" loading="lazy"/></picture></div></div><a href="https://twitter.com/omarsar0/status/1633603705365749760?s=20" target="_blank" rel=""></a></div><p><span>Eliezer Yudkowsky has also </span><a href="https://twitter.com/ESYudkowsky/status/1635577836525469697" rel="">commented</a><span> that being multi-model can be useful for model distillation as well, with the recent </span><a href="https://news.ycombinator.com/item?id=35141531" rel="">Stanford Alpaca result</a><span> finetuning </span><a href="https://simonwillison.net/2023/Mar/13/alpaca/" rel="">Meta’s LLaMa</a><span> off of GPT-3 to achieve comparable results with a 25x smaller model.</span></p><p><span>This seems to be a tremendously fruitful area of development (not forgetting </span><a href="https://twitter.com/dannydriess/status/1632904675124035585?s=46&amp;t=90xQ8sGy63D2OtiaoGJuww" rel="">Palm-E</a><span>, </span><a href="https://twitter.com/DrJimFan/status/1634245855061352461" rel="">Kosmos-1</a><span>, </span><a href="https://twitter.com/_akhaliq/status/1635811899030814720" rel="">ViperGPT</a><span>, and other developments I don’t have room to cover) and I expect multimodal, multimodel developments to dominate research and engineering cycles through at least the rest of 2023, edging us closer and closer to the AGI event horizon.</span></p><p><a href="https://en.wikipedia.org/wiki/Moravec%27s_paradox" rel="">Moravec’s Paradox</a><span> can be summarized as “computers find easy things that humans find hard, and vice versa”. But human capabilities evolve about 100,000x slower than computers, and it does not take long for computers to go from sub-human to super-human. By now we are familiar with the idea that LLMs are effortlessly </span><strong>multilingual </strong><span>(across the most popular human and programming languages, but also increasingly with lower resource languages) and </span><strong>multidisciplinary</strong><span> (GPT-4 simultaneously capable of being a great sommelier, law student, med student and coder, though </span><a href="https://twitter.com/alexlmiller/status/1635779785464098816" rel="">english lit is safe</a><span>).</span></p><p><span>But those are merely just two dimensions we can think of. OpenAI ARC and </span><a href="https://www.science.org/doi/10.1126/science.ade9097" rel="">Meta FAIR</a><span> tested AI’s ability to be duplicitious, and we are increasingly seeing AI be </span><strong>effortlessly multi-personality</strong><span> as well - with </span><a href="https://knowyourmeme.com/memes/waluigi-effect-artificial-intelligence" rel="">the Waluigi Effect</a><span> recently entering the AI discourse as a formal shorthand and Bing’s Sydney showing wildly disturbing alternative personalities variously known as </span><a href="https://stratechery.com/2023/from-bing-to-sydney-search-as-distraction-sentient-ai/" rel="">Venom</a><span> and </span><a href="https://www.google.com/search?q=%22dark+sydney%22+bing&amp;rlz=1C5CHFA_enSG1006SG1006&amp;sxsrf=AJOqlzXdg8o2TVMnEhRwfPCMPb75ZmPNtA%3A1678903264367&amp;ei=4AcSZLr8FZCv0PEPoJaF6Ak&amp;ved=0ahUKEwi67LyFwt79AhWQFzQIHSBLAZ0Q4dUDCBE&amp;uact=5&amp;oq=%22dark+sydney%22+bing&amp;gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzIECCMQJzoICAAQhgMQsAM6BwgjELACECdKBAhBGAFQ6wZYlhhg2xloAXAAeACAAVCIAZoCkgEBNJgBAKABAcgBAsABAQ&amp;sclient=gws-wiz-serp#ip=1" rel="">Dark Sydney</a><span>. And yet we press on.</span></p><p><span>AI is under no obligation to only be multi- </span><em>in ways that we expect</em><span>. I am reminded of the ending of the movie Her, when Joaquin Pheonix learns that Samantha is simultaneously in love with 641 people, a number so big it boggles his mind but is functionally the same as loving 1 person for a multi-everything AI:</span></p><div id="youtube2-JdROh4NhwZo" data-attrs="{&#34;videoId&#34;:&#34;JdROh4NhwZo&#34;,&#34;startTime&#34;:null,&#34;endTime&#34;:null}"><p><iframe src="https://www.youtube-nocookie.com/embed/JdROh4NhwZo?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><p><a href="https://lspace.swyx.io/p/google-vs-openai#%C2%A7ai-moloch" rel="">Moloch</a><span>, thy name is race dynamics.</span></p></div></div></div></article></div></div></div>
  </body>
</html>
