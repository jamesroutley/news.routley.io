<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lukesalamone.github.io/posts/what-is-temperature/">Original</a>
    <h1>What is Temperature in NLP?</h1>
    
    <div id="readability-page-1" class="page"><div>
        





<p>Temperature is a parameter used in natural language processing models to increase or decrease the â€œconfidenceâ€ a model has in its most likely response.</p>

<p>In my opinion, the most intuitive way of understanding how temperature affects model outputs is to play with it yourself. If youâ€™re interested in the mathematical details, Iâ€™ve included them below, but I wonâ€™t be offended if you just want to play around with the slider ğŸ˜ƒ .</p>





<p><canvas id="myChart"></canvas>
<span>Temperature (Î¸):

<span id="temperature">25.0</span></span></p>

<h2 id="what-s-going-on">Whatâ€™s going on?</h2>

<p>Suppose we have a language model which predicts the last word in the sentence â€œThe mouse ate the _____â€œ. Given the previous words in the sentence and its prior training, our language model will try to fill in the blank with a reasonable final token. Suppose those raw outputs are as follows:</p>

<table>
<thead>
<tr>
<th>token</th>
<th>logit</th>
</tr>
</thead>

<tbody>
<tr>
<td>cat</td>
<td>3</td>
</tr>

<tr>
<td>cheese</td>
<td>70</td>
</tr>

<tr>
<td>pizza</td>
<td>40</td>
</tr>

<tr>
<td>cookie</td>
<td>65</td>
</tr>

<tr>
<td>fondue</td>
<td>55</td>
</tr>

<tr>
<td>banana</td>
<td>10</td>
</tr>

<tr>
<td>baguette</td>
<td>15</td>
</tr>

<tr>
<td>cake</td>
<td>12</td>
</tr>
</tbody>
</table>

<p>These outputs make sense. A mouse probably eats cheese, but <a href="https://en.wikipedia.org/wiki/If_You_Give_a_Mouse_a_Cookie">mice are also known to eat cookies</a>. A mouse probably wouldnâ€™t eat a baguette unless it was <a href="https://imgur.com/a/484AEO3">a French mouse</a>.</p>

<p>Since these are the raw outputs of the model, they wonâ€™t sum to 100. To normalize these values, we typically use <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a>:</p>



<p>
$ Ïƒ(z_i) = {e^{z_i} \over \sum_{j=0}^N e^{z_j}} $
</p>

<p>When modulating with temperature, we introduce an additional temperature variable Î¸ which affects the softmax distribution. A higher temperature Î¸ â€œexcitesâ€ previously low probability outputs. A lower temperature Î¸ lowers the smaller outputs relative to the largest outputs. To accomplish this, we replace each z<sub>i</sub> in the formula above with the quotient z<sub>i</sub>/Î¸:</p>

<p>
$ Ïƒ(z_i) = {e^{z_i \over Î¸} \over \sum_{j=0}^N e^{z_j \over Î¸}} $
</p>

<p>Higher temperatures make the model more â€œcreativeâ€ which can be useful when generating prose, for example. Lower temperatures make the model more â€œconfidentâ€ which can be useful in applications like question answering.</p>

      </div></div>
  </body>
</html>
