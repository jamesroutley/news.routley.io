<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lukesalamone.github.io/posts/what-is-temperature/">Original</a>
    <h1>What is Temperature in NLP?</h1>
    
    <div id="readability-page-1" class="page"><div>
        





<p>Temperature is a parameter used in natural language processing models to increase or decrease the “confidence” a model has in its most likely response.</p>

<p>In my opinion, the most intuitive way of understanding how temperature affects model outputs is to play with it yourself. If you’re interested in the mathematical details, I’ve included them below, but I won’t be offended if you just want to play around with the slider 😃 .</p>





<p><canvas id="myChart"></canvas>
<span>Temperature (θ):

<span id="temperature">25.0</span></span></p>

<h2 id="what-s-going-on">What’s going on?</h2>

<p>Suppose we have a language model which predicts the last word in the sentence “The mouse ate the _____“. Given the previous words in the sentence and its prior training, our language model will try to fill in the blank with a reasonable final token. Suppose those raw outputs are as follows:</p>

<table>
<thead>
<tr>
<th>token</th>
<th>logit</th>
</tr>
</thead>

<tbody>
<tr>
<td>cat</td>
<td>3</td>
</tr>

<tr>
<td>cheese</td>
<td>70</td>
</tr>

<tr>
<td>pizza</td>
<td>40</td>
</tr>

<tr>
<td>cookie</td>
<td>65</td>
</tr>

<tr>
<td>fondue</td>
<td>55</td>
</tr>

<tr>
<td>banana</td>
<td>10</td>
</tr>

<tr>
<td>baguette</td>
<td>15</td>
</tr>

<tr>
<td>cake</td>
<td>12</td>
</tr>
</tbody>
</table>

<p>These outputs make sense. A mouse probably eats cheese, but <a href="https://en.wikipedia.org/wiki/If_You_Give_a_Mouse_a_Cookie">mice are also known to eat cookies</a>. A mouse probably wouldn’t eat a baguette unless it was <a href="https://imgur.com/a/484AEO3">a French mouse</a>.</p>

<p>Since these are the raw outputs of the model, they won’t sum to 100. To normalize these values, we typically use <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a>:</p>



<p>
$ σ(z_i) = {e^{z_i} \over \sum_{j=0}^N e^{z_j}} $
</p>

<p>When modulating with temperature, we introduce an additional temperature variable θ which affects the softmax distribution. A higher temperature θ “excites” previously low probability outputs. A lower temperature θ lowers the smaller outputs relative to the largest outputs. To accomplish this, we replace each z<sub>i</sub> in the formula above with the quotient z<sub>i</sub>/θ:</p>

<p>
$ σ(z_i) = {e^{z_i \over θ} \over \sum_{j=0}^N e^{z_j \over θ}} $
</p>

<p>Higher temperatures make the model more “creative” which can be useful when generating prose, for example. Lower temperatures make the model more “confident” which can be useful in applications like question answering.</p>

      </div></div>
  </body>
</html>
