<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/wgryc/phasellm">Original</a>
    <h1>PhaseLLM: Standardized Chat LLM API (Cohere, Claude, GPT) &#43; Evaluation Framework</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">Large language model evaluation and workflow framework from <a href="https://phaseai.com/" rel="nofollow">Phase AI</a>.</p>
<p dir="auto">The coming months and years will bring thousands of new products and experienced powered by large language models (LLMs) like ChatGPT or its increasing number of variants. Whether you&#39;re using OpenAI&#39;s ChatGPT, Anthropic&#39;s Claude, or something else all together, you&#39;ll want to test how well your models and prompts perform against user needs. As more models are launched, you&#39;ll also have a bigger range of options.</p>
<p dir="auto">PhaseLLM is a framework designed to help manage and test LLM-driven experiences -- products, content, or other experiences that product and brand managers might be driving for their users.</p>
<p dir="auto">Here&#39;s what PhaseLLM does:</p>
<ol dir="auto">
<li>We standardize API calls so you can plug and play models from OpenAI, Cohere, Anthropic, or other providers.</li>
<li>We&#39;ve built evaluation frameworks so you can compare outputs and decide which ones are driving the best experiences for users.</li>
<li>We&#39;re adding automations so you can use advanced models (e.g., GPT-4) to evaluate simpler models (e.g., GPT-3) to determine what combination of prompts yield the best experiences, especially when taking into account costs and speed of model execution.</li>
</ol>
<p dir="auto">PhaseLLM is open source and we envision building more features to help with model understanding. We want to help developers, data scientists, and others launch new, robust products as easily as possible.</p>
<p dir="auto">If you&#39;re working on an LLM product, please reach out. We&#39;d love to help out.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-example-evaluating-travel-chatbot-prompts-with-gpt-35-claude-and-more" aria-hidden="true" href="#example-evaluating-travel-chatbot-prompts-with-gpt-35-claude-and-more"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example: Evaluating Travel Chatbot Prompts with GPT-3.5, Claude, and more</h2>
<p dir="auto">PhaseLLM makes it incredibly easy to plug and play LLMs and evaluate them, in some cases with <em>other</em> LLMs. Suppose you&#39;re building a travel chatbot, and you want to test Claude and Cohere against each other, using GPT-3.5.</p>
<p dir="auto">What&#39;s awesome with this approach is that (1) you can plug and play models and prompts as needed, and (2) the entire workflow takes a small amount of code. This simple example can easily be scaled to much more complex workflows.</p>
<p dir="auto">So, time for the code... First, load your API keys.</p>
<div data-snippet-clipboard-copy-content="from dotenv import load_dotenv

load_dotenv()
openai_api_key = os.getenv(&#34;OPENAI_API_KEY&#34;)
anthropic_api_key = os.getenv(&#34;ANTHROPIC_API_KEY&#34;)
cohere_api_key = os.getenv(&#34;COHERE_API_KEY&#34;)"><pre lang="import"><code>from dotenv import load_dotenv

load_dotenv()
openai_api_key = os.getenv(&#34;OPENAI_API_KEY&#34;)
anthropic_api_key = os.getenv(&#34;ANTHROPIC_API_KEY&#34;)
cohere_api_key = os.getenv(&#34;COHERE_API_KEY&#34;)
</code></pre></div>
<p dir="auto">We&#39;re going to set up the <em>Evaluator</em>, which takes two LLM model outputs and decides which one is better for the objective at hand.</p>
<div data-snippet-clipboard-copy-content="
# We&#39;ll use GPT-3.5 as the evaluator.
e = llms.GPT35Evaluator(openai_api_key)"><pre lang="import"><code>
# We&#39;ll use GPT-3.5 as the evaluator.
e = llms.GPT35Evaluator(openai_api_key)
</code></pre></div>
<p dir="auto">Now it&#39;s time to set up the experiment. In this case, we&#39;ll set up an <code>objective</code> which describes what we&#39;re trying to achieve with our chatbot. We&#39;ll also provide 5 examples of starting chats that we&#39;ve seen with our users.</p>
<div data-snippet-clipboard-copy-content="objective = &#34;We&#39;re building a chatbot to discuss a user&#39;s travel preferences and provide advice.&#34;

# Chats that have been launched by users.
travel_chat_starts = [
    &#34;I&#39;m planning to visit Poland in spring.&#34;,
    &#34;I&#39;m looking for the cheapest flight to Europe next week.&#34;,
    &#34;I am trying to decide between Prague and Paris for a 5-day trip&#34;,
    &#34;I want to visit Europe but can&#39;t decide if spring, summer, or fall would be better.&#34;,
    &#34;I&#39;m unsure I should visit Spain by flying via the UK or via France.&#34;
]"><pre lang="#"><code>objective = &#34;We&#39;re building a chatbot to discuss a user&#39;s travel preferences and provide advice.&#34;

# Chats that have been launched by users.
travel_chat_starts = [
    &#34;I&#39;m planning to visit Poland in spring.&#34;,
    &#34;I&#39;m looking for the cheapest flight to Europe next week.&#34;,
    &#34;I am trying to decide between Prague and Paris for a 5-day trip&#34;,
    &#34;I want to visit Europe but can&#39;t decide if spring, summer, or fall would be better.&#34;,
    &#34;I&#39;m unsure I should visit Spain by flying via the UK or via France.&#34;
]
</code></pre></div>
<p dir="auto">Now we set up our Cohere and Claude models.</p>
<div data-snippet-clipboard-copy-content="claude_model = llms.ClaudeWrapper(anthropic_api_key)"><pre lang="cohere_model"><code>claude_model = llms.ClaudeWrapper(anthropic_api_key)
</code></pre></div>
<p dir="auto">Finally, we launch our test. We run an experiments where both models generate a chat response and then we have GPT-3.5 evaluate the response.</p>
<div data-snippet-clipboard-copy-content="for tcs in travel_chat_starts:

    messages = [{&#34;role&#34;:&#34;system&#34;, &#34;content&#34;:objective},
            {&#34;role&#34;:&#34;user&#34;, &#34;content&#34;:tcs}]

    response_cohere = cohere_model.complete_chat(messages, &#34;assistant&#34;)
    response_claude = claude_model.complete_chat(messages, &#34;assistant&#34;)

    pref = e.choose(objective, tcs, response_cohere, response_claude)
    print(f&#34;{pref}&#34;)"><pre lang="print(&#34;Running"><code>for tcs in travel_chat_starts:

    messages = [{&#34;role&#34;:&#34;system&#34;, &#34;content&#34;:objective},
            {&#34;role&#34;:&#34;user&#34;, &#34;content&#34;:tcs}]

    response_cohere = cohere_model.complete_chat(messages, &#34;assistant&#34;)
    response_claude = claude_model.complete_chat(messages, &#34;assistant&#34;)

    pref = e.choose(objective, tcs, response_cohere, response_claude)
    print(f&#34;{pref}&#34;)
</code></pre></div>
<p dir="auto">In this case, we simply print which of the two models was preferred.</p>
<p dir="auto">Voila! You&#39;ve got a suite to test your models and can plug-and-play three major LLMs.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-contact-us" aria-hidden="true" href="#contact-us"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Contact Us</h2>
<p dir="auto">If you have questions, requests, ideas, etc. please reach out at w (at) phaseai (dot) com.</p>
</article>
          </div></div>
  </body>
</html>
