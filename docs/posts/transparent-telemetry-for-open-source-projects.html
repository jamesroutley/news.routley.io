<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://research.swtch.com/telemetry-intro">Original</a>
    <h1>Transparent telemetry for open-source projects</h1>
    
    <div id="readability-page-1" class="page"><div>
      <div>
        
        

<p>
How do software developers understand which parts of their software
are being used and whether they are performing as expected?
The modern answer is <i>telemetry</i>, which means software sending
data to answer those questions back to a collection server.
This post is about why I believe telemetry is important for open-source projects,
and what it might look like to approach telemetry
in an open-source-friendly way.
That leads to a new design I call <i>transparent telemetry</i>.
If you are impatient, skip to the <a href="#summary">summary at the end</a>.
Other posts in the series <a href="https://research.swtch.com/telemetry-design">detail the design</a>
and <a href="https://research.swtch.com/telemetry-uses">present various uses</a>.
<a href="#why"></a></p><h2 id="why"><a href="#why">Why Telemetry?</a></h2>


<p>
Without telemetry, developers rely on bug reports and surveys to find out
when their software isn’t working or how it is being used. Both of these
techniques are too limited in their effectiveness. Let’s look at each in turn.

</p><p>
<b>Bug reports are not enough.</b>
Users only file bug reports when they think something is broken.
If a function is not behaving as documented, that’s a clear bug to report.
But if a program is misbehaving in a way that doesn’t affect correctness,
users are much less likely to notice.
Statistics gathered by transparent telemetry make it possible for
developers to notice that something is going wrong even when users do not.

</p><p>
For example, during the Go 1.14 release process in early 2020 we made a change
to the way macOS Go distributions are built, as part of keeping them acceptable
to Apple’s signing tools.
Unfortunately, the way we made the change also made all
the pre-compiled <code>.a</code> files shipped in the distribution appear
stale to builds.
The effect was that the <code>go</code> command rebuilt and cached the
standard library on first run, which meant that compiling any
program using package <code>net</code> (which uses <code>cgo</code>) required Xcode to be installed.
So Go 1.14 and later unintentionally required Xcode to compile even
trivial demo Go programs like a basic HTTP server.
This is not the way we want Go to work on macOS.
On systems without Xcode, when <code>go</code> tried to invoke <code>clang</code>,
macOS popped up a box explaining how to install it.
Users simply accepted that this was necessary,
perhaps even thinking <code>go</code> had displayed the popup.
No one reported the bug over three years of Go releases.
We didn’t notice and fix the problem until late 2022 while investigating something else.
With telemetry for the miss rate in the cache of pre-compiled standard library packages,
the impact would have been obvious: all Macs running Go 1.14 or later
would have a pre-installed package miss rate of 100%.
This bug wasn’t caught by our unit tests because it was caused
by the distribution build machines having a modified environment
different from actual user machines.
The unit tests ran in the same modified environment
as the build and worked fine.
These kinds of unexpected differences between developer machines
and user machines are inevitable at scale.
Instrumenting the software on user machines is the most reliable
way to understand how well it is working.

</p><p>
<b>Surveys are not enough.</b>
Surveys help us understand what users want to do with Go,
but they are only a small sample and have limited resolution.
Asking about usage of infrequently-used features on a survey wastes
time for a majority of respondents, and it requires large response
counts to get an accurate measurement.

</p><p>
For example, we announced in the Go 1.13 release notes
that future releases would drop support for Native Client (<code>GOOS=nacl</code>).
Similarly, we announced in the Go 1.15 release notes
that future releases would drop support for hardware floating point
on 32-bit Intel CPUs without SSE2 instructions (<code>GO386=387</code>).
Both of those removals went off okay, retroactively proving that our
instincts about how few people would be affected were correct.
On the other hand, we drafted an announcement for Go 1.18
removing <code>-buildmode=shared</code>, because it had essentially been
broken since the introduction of modules,
but when we issued Go 1.18 beta 1 we got feedback
from at least a few people who were using it in some form.
We still don’t know how many people are using it or whether it is
worth the maintenance costs, so <a href="https://github.com/golang/go/issues/47788">it lingers on</a>.
Another question is how long to keep supporting ARMv5 (<code>GOARM=5</code>),
which doesn’t have modern atomic instructions.
More recently, we announced that Go 1.20 will be the last
release to support macOS High Sierra and were
<a href="https://github.com/golang/go/issues/57125#issuecomment-1416277589">promptly asked to keep it around</a>.
Usage information would help us make more informed decisions.
It’s important to note the limitations of this usage information:
if telemetry is disabled on all the machines that use the
feature in question, or if it is only used in machines
that don’t stay up long enough to report anything,
then we won’t observe the usage.
Telemetry is never perfect, but it’s a useful input to the decision
and much better than guessing.
A survey is not any better and usually worse:
there is a limit to how many questions we can
reasonably ask in a survey,
and asking a question where 99% of people answer “no I don’t use that”
is a waste of most people’s time.
<a href="#why-open-source"></a></p><h2 id="why-open-source"><a href="#why-open-source">Why Telemetry For Open Source?</a></h2>


<p>
When you hear the word telemetry, if you’re like me, you may have
a visceral negative reaction to a mental image of intrusive, detailed traces
of your every keystroke and mouse click headed back to the developers
of the software you’re using.
And for good reason! That mental image sounds like it must be an exaggeration
but turns out to be fairly accurate.
(Citations:
<a href="https://www.theverge.com/2020/1/31/21117217/amazon-kindle-tracking-page-turn-taps-e-reader-privacy-policy-security-whispersync">Kindle tracking individual page turns</a>,
<a href="https://www.roboleary.net/tools/2022/04/20/vscode-telemetry.html">VS Code telemetry logs</a>,
and
<a href="https://learn.microsoft.com/en-us/dotnet/core/tools/telemetry">.NET telemetry events</a>.)

</p><p>
Open-source software projects have tended to avoid this kind of telemetry, for two reasons.
The first is the significant privacy cost to users of collecting and storing detailed activity traces.
The second is the fact that access to this data must be restricted,
which would make the project less open than most strive to be.
When the choice is between this kind of invasive tracking or doing nothing,
doing nothing seems like an easy call.
Still, doing nothing has real disadvantages.
It means open-source developers like me tend not to understand as well
how our software is used or how it performs.
Then, because we lack that knowledge,
we end up wasting time by maintaining features that aren’t used,
hurting users by removing features that are still being used,
and delivering a poorer user experience by failing to notice
when our software is underperforming in real-world usage.

</p><p>
Some open-source projects have adopted traditional telemetry,
with mixed success and varying levels of user pushback.
For example: <a href="https://www.theregister.com/2021/05/07/audacity_telemetry/">Audacity</a>,
<a href="https://www.zdnet.com/article/gitlab-backs-down-on-planned-telemetry-changes-forced-tracking/">GitLab</a>,
and
<a href="https://news.ycombinator.com/item?id=11566720">Homebrew</a>.
Homebrew’s telemetry seems to be generally accepted by users,
and VS Code’s detailed telemetry has not stopped
it from being used by 74% of developers,
as reported by the <a href="https://survey.stackoverflow.co/2022/#integrated-development-environment">2022 StackOverflow survey</a>.
It could even be that the benefits from telemetry are
part of how VS Code’s developers have been able to build a tool that users like so much.
Even so, the vast majority of projects, even large ones that would benefit,
stay away from telemetry.

</p><p>
I believe that the choice between invasive tracking
and doing nothing at all is a false dichotomy,
and it’s harming open source.
Not having basic information
about how their software is used and how well it is performing
puts open-source developers at a disadvantage compared
to commercial software developers.
Not having this information makes it more difficult to understand
what’s important and what isn’t working,
making prioritization that much harder.
Not having clear prioritization in turn exacerbates
the pre-existing problems with maintainer burnout.

</p><p>
Eric Raymond famously declared that
“given enough eyeballs, all bugs are shallow,”
which he explained as meaning that
“[g]iven a large enough beta-tester and co-developer base,
almost every problem will be characterized quickly and the fix obvious to someone.”
Perhaps this was true in 1997 (perhaps not),
but it’s certainly not true today,
as the Go macOS cache bug shows.
A quarter century later, software is much larger,
and open-source software is used
by far more people who didn’t develop it
and aren’t familiar with how it should and should not behave.
Eyeballs don’t scale.

</p><p>
I believe that open-source software projects need to explore new
telemetry designs that help developers get
the information they need to work efficiently and effectively,
without collecting invasive traces of detailed user activity.
<a href="#design"></a></p><h2 id="design"><a href="#design">Transparent Telemetry</a></h2>


<p>
This series of blog posts
presents one such design, which I call <i>transparent telemetry</i>,
because it collects as little as possible (kilobytes per year from each installation)
and then publishes every bit that it collects, for public inspection and analysis.

</p><p>
I’d like to explore using this system, or one like it, in the Go toolchain,
which I hope will help Go project developers and users alike.
To be clear, I am only suggesting that the instrumentation be added to the Go
command-line tools written and distributed by the Go team,
such as the <code>go</code> command,
the Go compiler, <code>gopls</code>, and <code>govulncheck</code>.
I am <i>not</i> suggesting that instrumentation be added by the Go compiler
to all Go programs in the world: that’s clearly inappropriate.
Also, throughout these posts, “developer” refers to the authors of a given piece
of software, while “user” refers to the users
of that software. From the point of view of the Go toolchain,
“developer” means a Go toolchain developers like me,
while “user” means one of the millions of Go programmers
using that toolchain.

</p><p>
With transparent telemetry,
as programs from the Go toolchain run,
they would increment counters for various events of interest
(for example: cache hit, use of a given feature, measured latency in a given range) in a per-week on-disk file.
These files hold only counter values, not user data nor user identifiers.
Some counter names include a short stack trace (function names and line offsets only, no argument data).

</p><p>
The Go team at Google would run a collection server.
Each week, with 10% probability (averaging ~5 times per year)
the user’s Go installation would download a “collection configuration”
to find out which counter values are of interest to the server and at what sample rate.
The collection configuration would be served in a Go module
validated using the <a href="https://go.dev/design/25530-sumdb">Go checksum database</a>,
for added confidence that all clients are being served
the same configuration.
Based on the sample rates, the Go installation might send a report
containing the counter values of interest.
Typical sample rates would be around 2% (averaging ~1 report per installation per year),
but very rare events could be sampled at a higher rate, up to the 10% limit.
As more systems take part in transparent telemetry,
the overall sample rate on any given system will decrease,
because <a href="https://research.swtch.com/sample">only a fixed number of samples is necessary</a>.

</p><p>
The report would contain no ID of any form – no user login, no machine ID, no MAC address, no IP address,
no IP address prefix, no geolocation information,
no randomly-generated pseudo-ID, no other kind of identifiers.
The report would contain basic information about the toolchain,
such as its version and what operating system and architecture it was built for.
The report could also contain coarse-grained information about the version
of the host operating system (for example, “Windows 8”) and other tools the Go toolchain uses,
such as the local C compiler (“gcc 2.95”).

</p><p>
The server would collect each day’s uploaded reports,
update telemetry graphs served publicly on go.dev,
and post the full set of uploaded reports for public download, inspection, and analysis.

</p><p>
Although the report would not include any identifiers, the TCP connection uploading the report
would expose the system’s public IP address to the server if a proxy is not being used.
This IP address would not be associated with the uploaded reports in any way.
Standard system maintenance, including DoS prevention, might require logs that include the IP address,
but uploaded reports will be kept separate from those logs.
The privacy policy would be similar to the one used by
<a href="https://proxy.golang.org/privacy">the Go module mirror and checksum database</a>.

</p><p>
The <a href="https://go.dev/dl">Go home page</a> and <a href="https://go.dev/dl">download page</a>
already include a notice about the default
use of the Go module mirror and a link to more information.
That notice and link would be updated to disclose on-by-default telemetry.
To opt out, users would set <code>GOTELEMETRY=off</code> in their environment
or run a simple command like <code>go env -w GOTELEMETRY=off</code>;
The first telemetry report is not sent until at least one week after installation,
giving ample time to opt out.
Opting out stops all collection and reporting: no “opt out” event is sent.
It is simply impossible to see systems that install Go and then opt out in the next seven days.
<a href="#summary"></a></p><h2 id="summary"><a href="#summary">Summary</a></h2>


<p>
Transparent telemetry has the following key properties:
</p><ul>
<li>


<p>
The decisions about what metrics to collect are made in an
open, public process.
</p></li><li>


<p>
The collection configuration is automatically generated from
the actively tracked metrics: no data is collected that isn’t needed
for the metrics.
</p></li><li>


<p>
The collection configuration is served using a tamper-evident
transparent log, making it very difficult to serve different
collection configurations to different systems.
</p></li><li>


<p>
The collection configuration is a cacheable, proxied Go module,
so any privacy-enhancing local Go proxy already in use for
ordinary modules will automatically be used for collection configuration.
To further ameliorate concerns about tracking systems
by the downloading of the collection configuration,
each installation only bothers downloading the configuration
each week with probability 10%,
so that each installation only asks for the configuration
about five times per year.
</p></li><li>


<p>
Uploaded reports only include total event counts over a full week,
not any kind of time-ordered event trace.
</p></li><li>


<p>
Uploaded reports do not include user IDs, machine IDs, or any other kind of ID.
</p></li><li>


<p>
Uploaded reports only contain strings that are already known to the collection server:
counter names, program names, and version strings repeated from the collection configuration,
along with the names of functions in specific, unmodified Go toolchain programs
for stack traces.
The only types of non-string data in the reports are event counts, dates, and line numbers.
</p></li><li>


<p>
IP addresses exposed by the HTTP session that uploads the report are not
recorded with the reports.
</p></li><li>


<p>
Thanks to <a href="https://research.swtch.com/sample">sampling</a>, only a constant number of uploaded reports
are needed to achieve a specific accuracy target, no matter how many
installations exist. Specifically, only about 16,000 reports are needed
for 1% accuracy at a 99% confidence level.
This means that <i>as new systems are added to
the system, each system reports less often</i>.
With a conservative estimate of two million Go installations,
about 16,000 reporting each week corresponds to an overall
reporting rate of well under 2% per week,
meaning each installation would upload a report on average less than
once per year.
</p></li><li>


<p>
The aggregate computed metrics are made public in graphical and tabular form.
</p></li><li>


<p>
The full raw data as collected is made public, so that project maintainers
have no proprietary advantage or insights in their role as the direct data collector.
</p></li><li>


<p>
The system is on by default, but opting out is easy, effective, and persistent.</p></li></ul>
<a href="#next_steps"><h2 id="next_steps">Next Steps</h2></a>


<p>
For more detail about the design, see the <a href="https://research.swtch.com/telemetry-design">next post</a>.
For more use cases, see the <a href="https://research.swtch.com/telemetry-uses">post after that</a>.

</p><p>
Although these posts use Go as the example system using
transparent telemetry, I hope that the ideas apply and
can be adopted by other open-source projects too,
in their own, separate collection systems.
For example, even though VS Code collects high-resolution event traces
(sometimes tens or hundreds of events per minute),
a close reading of those traces shows hardly anything is new in each event.
That is, VS Code suffers the reputational hit of collecting lots of data
but appears to gather relatively little actual information.
Perhaps using transparent telemetry in VS Code or a similar editor could offer
the editor’s developers roughly equivalent insights and development velocity
at a much lower privacy cost to users.

</p><p>
I am posting these to start a <a href="https://go.dev/s/telemetry-discussion">discussion about how the Go toolchain can
adopt telemetry</a>
in some form to help the Go toolchain developers make better
decisions about the development and maintenance of Go.
I have written an implementation of local counter collection
to convince myself it could be made cheap enough,
but no other part of the system exists today in any form.
I hope that the system can be built over the course of 2023.
      </p></div>
    </div></div>
  </body>
</html>
