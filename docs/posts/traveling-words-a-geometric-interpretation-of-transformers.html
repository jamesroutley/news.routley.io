<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2309.07315">Original</a>
    <h1>Traveling words: a geometric interpretation of transformers</h1>
    
    <div id="readability-page-1" class="page"><div>
      <div id="content">
<!--
rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:dc="http://purl.org/dc/elements/1.1/"
         xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
    <rdf:Description
        rdf:about="/abs/2309.07315"
        dc:identifier="/abs/2309.07315"
        dc:title="Traveling Words: A Geometric Interpretation of Transformers"
        trackback:ping="/trackback/2309.07315" />
    </rdf:RDF>
--><div id="abs-outer">

  <div>
    

    <p><strong>arXiv:2309.07315</strong> (cs)
    </p>

<div id="content-inner">
  <div id="abs">
    
    
                
    <p><a href="https://arxiv.org/pdf/2309.07315.pdf">Download PDF</a></p><blockquote>
            <span>Abstract:</span>Transformers have significantly advanced the field of natural language processing, but comprehending their internal mechanisms remains a challenge. In this paper, we introduce a novel geometric perspective that elucidates the inner mechanisms of transformer operations. Our primary contribution is illustrating how layer normalization confines the latent features to a hyper-sphere, subsequently enabling attention to mold the semantic representation of words on this surface. This geometric viewpoint seamlessly connects established properties such as iterative refinement and contextual embeddings. We validate our insights by probing a pre-trained 124M parameter GPT-2 model. Our findings reveal clear query-key attention patterns in early layers and build upon prior observations regarding the subject-specific nature of attention heads at deeper layers. Harnessing these geometric insights, we present an intuitive understanding of transformers, depicting them as processes that model the trajectory of word particles along the hyper-sphere.
    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div>
    <div>
      <h2>Submission history</h2><p> From: Raul Molina [<a href="https://arxiv.org/show-email/faa68f4f/2309.07315">view email</a>]      </p></div>
  </div>
  <!--end leftcolumn-->
<div>    
    <!--end full-text-->    <div><p>
    Current browse context: </p><p>cs.CL</p>

  

    </div>

    

<p>

    <span id="bib-cite-trigger">export BibTeX citation</span>
    
</p>

<div>
  <p><h3>Bookmark</h3></p><p><a href="https://arxiv.org/ct?url=http%3A%2F%2Fwww.bibsonomy.org%2FBibtexHandler%3FrequTask%3Dupload%26url%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F2309.07315%26description%3DTraveling+Words%3A+A+Geometric+Interpretation+of+Transformers&amp;v=0a5cb667" title="Bookmark on BibSonomy">
    <img src="https://static.arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png" alt="BibSonomy logo"/>
  </a>
  <a href="https://arxiv.org/ct?url=https%3A%2F%2Freddit.com%2Fsubmit%3Furl%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F2309.07315%26title%3DTraveling+Words%3A+A+Geometric+Interpretation+of+Transformers&amp;v=09d7fbfa" title="Bookmark on Reddit">
    <img src="https://static.arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png" alt="Reddit logo"/>
  </a>
</p></div>  </div>
  <!--end extra-services-->
<!-- LABS AREA -->
<div id="labstabs">
  <div><p>
    <label for="tabone">Bibliographic Tools</label></p><div>
      
      <div>
        <div>
          <p><label>
              
              <span></span>
              <span>Bibliographic Explorer Toggle</span>
            </label>
          </p>
          
        </div>

        
      </div>
        
        
        
    </div>


    <p>
    <label for="tabtwo">Code, Data, Media</label></p>


      <p>
      <label for="labstabs-demos-input" id="labstabs-demos-label">Demos</label></p>
      <p>
      <label for="tabfour">Related Papers</label></p>

      <p>
      <label for="tabfive">
        About arXivLabs
      </label></p><div>
        <div>
          <div>
            
            <p>arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.</p>
            <p>Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.</p>
            <p>Have an idea for a project that will add value for arXiv&#39;s community? <a href="https://info.arxiv.org/labs/index.html"><strong>Learn more about arXivLabs</strong></a>.</p>
          </div>
          
        </div>
      </div>

    </div>
</div>
<!-- END LABS AREA -->
  
  
</div>
      </div>
    </div></div>
  </body>
</html>
