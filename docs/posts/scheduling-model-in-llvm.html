<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://myhsu.xyz/llvm-sched-model-1/">Original</a>
    <h1>Scheduling Model in LLVM</h1>
    
    <div id="readability-page-1" class="page"><div><p>Instruction scheduling is essential to modern compilers. It tries to hide latencies and increases the throughput of a straight line code by reordering the enclosing instructions.
In order to do that, compilers have to know a whole bunch of information, ranging from individal instruction’s latency to microarchitecture details. The system that describes these is called a scheduling model. In LLVM, a scheduling model is used by not just the instruction scheduler, but also target-specific optimizations like MachineCombiner and components like MCA (Machine Code Analyzer)<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>. Which makes it an important factor in performance tuning for low-level code.</p><p>This series is about LLVM’s scheduling model, how it interacts / affects other parts of LLVM and how can we fine tune this model for better performance. I’ll cover how scheduling models are used in other part of LLVM in later posts, but in this one, I’m focusing on the scheduling model itself first, and talk about how to specify scheduling information for individual instructions. Let’s start with a really basic example.</p><h3 id="the-basics">The Basics</h3><p>Scheduling models are part of the target definition. They are associated with one or more target processors. Multiple processors can also share the same scheduling model. Take RISC-V as an example, we can find <a href="https://github.com/llvm/llvm-project/blob/fdb9f96fa2a926425bdf8315048db7623d63547d/llvm/lib/Target/RISCV/RISCVSchedSiFive7.td">one</a> of its scheduling models, <code>SiFive7Model</code>, covers a wide array of processors from <code>sifive-e76</code>, a 32-bit microcontroller, to <code>sifive-x280</code>, which is a high-performance processor designed for AI/ML workloads.</p><p>To describe per-instruction scheduling information like latency, naively, we can express such information with descriptions like <em>“opcode ADD has latency of X”</em>. It’s all fun and games until you realize that there are tens of thousands of opcodes in some of the targets<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup> so it doesn’t scale well. More importantly, many instructions share the same scheduling characteristics.
For example, simple arithmetic operations like add, sub, and shifts usually have the same latency in most modern architectures.</p><p>Instead of spelling out per-opcode information, LLVM chooses a different path that characterizes an instruction’s scheduling properties with <strong>operand reads and writes</strong>:
First, each operand in an instruction is assigned with a “token”.</p><p>Take the <a href="https://github.com/llvm/llvm-project/blob/e70f376b25ea96f3b0db75ff77ae1a58d53f2119/llvm/lib/Target/RISCV/RISCVInstrInfoM.td#L39">DIV instruction</a> in RISC-V shown below as an example, <code>WriteIDIV</code> is the token assigned to the first operand, which is also the destination register, hence a <em>write</em> operand (also called <strong>definition</strong>). The two <code>ReadIDIV</code> follow are tokens for the two source registers, both are <em>read</em> operands (or <strong>use</strong>).</p><div><pre tabindex="0"><code data-lang="c++"><span><span>1</span><span><span>// From llvm/lib/Target/RISCV/RISCVInstrInfoM.td.
</span></span></span><span><span>2</span><span><span></span>def <span>DIV</span>     : ALU_rr<span>&lt;</span><span>0b0000001</span>, <span>0b100</span>, <span>&#34;div&#34;</span><span>&gt;</span>,
</span></span><span><span>3</span><span>              Sched<span>&lt;</span>[WriteIDiv, ReadIDiv, ReadIDiv]<span>&gt;</span>;</span></span></code></pre></div><p>A write token is a <code>SchedWrite</code> TableGen instance, while a read token is a <code>SchedRead</code> instance.</p><p>With these <code>SchedWrite</code> and <code>SchedRead</code> tokens, a scheduling model then assigns processor-specific information to them, like <em>latency</em> and the <em>hardware resources</em> they use.</p><p>Take <code>SiFive7Model</code> we saw earlier as an example, in the file where the model is defined (i.e. <code>llvm/lib/Target/RISCV/RISCVSchedSiFive7.td</code>), we find this piece of snippet:</p><div><pre tabindex="0"><code data-lang="c++"><span><span>1</span><span><span>// Integer division
</span></span></span><span><span>2</span><span><span></span><span>def</span> : WriteRes<span>&lt;</span>WriteIDiv, [SiFive7PipeB, SiFive7IDiv]<span>&gt;</span> {
</span></span><span><span>3</span><span>  let Latency <span>=</span> <span>66</span>;
</span></span><span><span>4</span><span>  let ReleaseAtCycles <span>=</span> [<span>1</span>, <span>65</span>];
</span></span><span><span>5</span><span>}</span></span></code></pre></div><p>The <code>WriteRes</code> TableGen class<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup> takes a <code>SchedWrite</code> – in this case <code>WriteIDIV</code> – and assigns it three different kinds of information:</p><ul><li>Latency of 66 cycles</li><li>The hardware resources it uses: <code>SiFive7PipeB</code> and <code>SiFive7IDiv</code></li><li>The <code>ReleaseAtCycles</code> field specifies the number of cycles this instruction spends on each hardware resource it uses. In this case, <code>WriteIDIV</code> spends 1 cycle on <code>SiFive7PipeB</code> and 65 cycles on <code>SiFive7IDiv</code></li></ul><p>What about <code>ReadIDIV</code>, the <code>SchedRead</code> instances we saw earlier? By default, LLVM’s scheduling model assumes that operand reads finish instantly, so a <code>SchedRead</code> cannot be assigned a latency property nor consuming any cycle the same way as a <code>SchedWrite</code>.</p><p>Which means that effectively, write operands dictate the instruction’s scheduling properties and it’s expected to repersent majority of the changes an instruction makes to the processor states. So, it’s safe to say that in this case, an <code>DIV</code> instruction has a latency of 66 cycles.</p><p>Alright, so far we have covered the most basic part of a scheduling model, specifically on how to specify the scheduling information for an instruction. We can summarize it into three quick steps:</p><ol><li>Assigning <code>SchedRead</code> and <code>SchedWrite</code> to your instruction</li><li>In your scheduling model, find the processor resources <code>SchedWrite</code> use…</li><li>…and map those resources to <code>SchedWrite</code> using <code>WriteRes</code>, along with other info like latency.</li></ol><p>I have intentionally left many details aside, like what exactly is a hardware resource (e.g <code>SiFive7PipeB</code>). These details are strongly related to what we’re going to cover in the next section: the <strong>microarchitecture</strong> of a processor.</p><h3 id="modeling-microarchitecture">Modeling microarchitecture</h3><p>Recall the goals of instruction scheduling are to minimize latency, maximize throughput, and eliminate as many potential hazards in the processor’s pipeline as possible.
These goals have a lot to do with how instructions <em>move</em> within the processor’s pipeline. For instance, extra cycles might be spent on fulfilling RAW (Read after Write) data dependencies, or certain units, like an integer division unit, might be overwhelmed and stall the entire pipeline.</p><p>And this is where the processor’s microarchitecture comes into play. Specifically, we’re focusing on <strong>superscalar</strong> architecture here, which has a typical structure shown below:</p><div><picture><source srcset="/images/llvm-sched-model-basic-superscalar.dark.svg" media="(prefers-color-scheme: dark)"/><img src="https://myhsu.xyz/images/llvm-sched-model-basic-superscalar.light.svg"/></picture></div><p>The above structure has an issue width of 3, meaning it can feed at most 3 instructions to later stages at a time; micro-codes, or <em>uops</em>, are generated at the decode stage; at this moment we’re a little hand-waving on the definition of dispatch and issue, as we’ll see several of their variants later; there are three different <strong>functional units</strong> in the execution stage for integer, floating point, and memory instructions (load / store unit). All of them are fully pipelined.</p><p>Despite the fact that scheduling model has strong connections with microarchitectures, we don’t want our model to cover <em>too</em> many microarchitecture details – it’s simply too expensive! What we want is a model that can raise red flags when we come up with a bad scheduling, yet abstract away all the nitty-gritty details and present just the information we care.</p><p>“What kind of red flags?” you may ask. The most important thing is probably whether we have enough <em>resources</em> to even run an instruction. Remember, the whole idea of superscalar is to run instructions of different types in different function units <strong>in parallel</strong>: integer uops go into integer units and floating point uops go into another floating point unit, where these units can run independently.
Each of these units has one or more <em>pipes</em> that actually run the uops. In many places “functional units” and “pipes” are interchangable.
If there is no enough resource, we won’t be able to <strong>dispatch</strong> an uop into any of the pipes. After all, processors have only a limited number of pipes for a specific type of instructions.</p><p>Let’s look at a concrete example: SiFive’s P670 RISC-V processor.</p><figure><img src="https://myhsu.xyz/images/p670-uarch.png"/><figcaption>Image source: <a href="https://www.cnx-software.com/2022/11/02/sifive-p670-and-p470-risc-v-processors-add-risc-v-vector-extensions/">CNX software</a></figcaption></figure><p>P670 is an out-of-order processor targeting consumer devices like smartphones. It has four integer pipes, two floating point pipes, and two vector units. And this is how integer pipes look like in its <a href="https://github.com/llvm/llvm-project/blob/e70f376b25ea96f3b0db75ff77ae1a58d53f2119/llvm/lib/Target/RISC/RISCVSchedSiFiveP600.td">scheduling model</a>:</p><div><pre tabindex="0"><code data-lang="c++"><span><span>1</span><span>def <span>SiFiveP600IEXQ0</span>       : ProcResource<span>&lt;</span><span>1</span><span>&gt;</span>;
</span></span><span><span>2</span><span>def <span>SiFiveP600IEXQ1</span>       : ProcResource<span>&lt;</span><span>1</span><span>&gt;</span>;
</span></span><span><span>3</span><span>def <span>SiFiveP600IEXQ2</span>       : ProcResource<span>&lt;</span><span>1</span><span>&gt;</span>;
</span></span><span><span>4</span><span>def <span>SiFiveP600IEXQ3</span>       : ProcResource<span>&lt;</span><span>1</span><span>&gt;</span>;
</span></span><span><span>5</span><span>...
</span></span><span><span>6</span><span>def <span>SiFiveP600IntArith</span>    : ProcResGroup<span>&lt;</span>[SiFiveP600IEXQ0, SiFiveP600IEXQ1, SiFiveP600IEXQ2, SiFiveP600IEXQ3]<span>&gt;</span>;</span></span></code></pre></div><p>In LLVM, a <code>ProcResource</code> instance in TableGen represents a unit that can executes uops, hence an equivalent to a single pipe in most cases. Here, each integer pipe in P600 is represented by <code>SiFiveP600IEXQ[0-3]</code></p><p>Nevertheless, it’s not the only instance you can <em>dispatch</em> uops to, because sometimes you can dispatch uops into a collection of pipes in which <em>any</em> of the pipes in that group can execute the dispatched uop. Such groups are represented by <code>ProcResGroup</code> instances in TableGen, like the <code>SiFiveP600IntArith</code> we see above: when an instruction – like <code>ADD</code> who uses <code>WriteIALU</code> – <a href="https://github.com/llvm/llvm-project/blob/fdb9f96fa2a926425bdf8315048db7623d63547d/llvm/lib/Target/RISCV/RISCVSchedSiFiveP600.td#L104">consumes</a> a <code>SiFiveP600IntArith</code> shown in the snippet below, it means that such instruction can be dispatched to any of the <code>SiFiveP600IEXQ[0-3]</code>.</p><div><pre tabindex="0"><code data-lang="c++"><span><span>1</span><span><span>// Integer arithmetic and logic
</span></span></span><span><span>2</span><span><span></span><span>def</span> : WriteRes<span>&lt;</span>WriteIALU, [SiFiveP600IntArith]<span>&gt;</span>;</span></span></code></pre></div><p>When there is no available pipe, a stall or a hazard occurs, in this case it is considered a <em>dispatch</em> hazard since you can’t dispatch more instructions. To avoid such hazards, each pipe usually has a buffer or a queue to store a certain number of candidates. Knowing the <strong>size</strong> of that buffer is crucial to our scheduling model, because then we can make a wiser decision on distributing the instructions <em>evenly</em> across all pipes, rather than jamming them into a single one just like freeways in LA.</p><p>Most<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup> processors with buffers / queues like this have a more well-known functionality: reordering instructions. In an our-of-order processor, each of these buffers is used as some sort of staging area for the scheduler to preemptively <strong>issue</strong> instructions with no dependencies on the others, in the hope of hiding latencies.</p><p>Aside from the size of the buffer, how to <em>organize</em> these buffers (e.g. several pipes might share a buffer) is just as important as their sizes. In the next section, we’re gonna go through processor resource buffers of different sizes and layouts.</p><h4 id="processor-resource-buffers">Processor resource buffers</h4><p>Let’s sort out some terminologies first. The buffer, or <strong>scheduler queue</strong> we’re talking about here, is often known as a <strong>reservation station</strong> too.
Reservation station stores instructions whose <em>operands</em> might not be ready yet, until those operands are available. <strong>Reorder buffer (ROB)</strong>, on the other hand, is a kind of unit with slightly confusing name. ROB reorders the <em>results</em> of an instruction; it’s usually depicted at the end of the execution stage, as shown in the previous diagram. Nevertheless, we need to reserve a slot in the ROB <em>before</em> we can even dispatch an instruction, therefore, to some extent, the size of ROB also controls how many instructions are allowed into the execution stage.</p><p>Back to the buffers, in LLVM’s scheduling model, we use the <code>BufferSize</code> field in a <code>ProcessorResource</code> or a <code>ProcResGroup</code> instance to specify the size of the associated buffer.
Special values are also used to denote unconventional buffer layouts, or even the <em>absent</em> of buffers.</p><p>LLVM currently supports 4 different kinds of buffer layouts: decoupled reservation stations, unified reservation station, in-order core, and latency device. Let’s go through each of them now.</p><h5 id="decoupled-reservation-stations">Decoupled reservation stations</h5><div><picture><source srcset="/images/llvm-sched-model-decoupled-reservation-stations.dark.svg" media="(prefers-color-scheme: dark)"/><img src="https://myhsu.xyz/images/llvm-sched-model-decoupled-reservation-stations.light.svg"/></picture></div><p>When we assign a <strong>positive</strong> <code>BufferSize</code> to individual processor resources or processor resource groups, we’re modeling processor resources with their own scheduler queues. Here is an example from PowerPC POWER9’s <a href="https://github.com/llvm/llvm-project/blob/22c06aa5e94e30fb1333ecaf46ce33c65d148634/llvm/lib/Target/PowerPC/PPCScheduleP9.td#L123">scheduling model</a>:</p><div><pre tabindex="0"><code data-lang="c++"><span><span>1</span><span><span>// Only one Branch unit.
</span></span></span><span><span>2</span><span><span></span>def <span>BR</span> : ProcResource<span>&lt;</span><span>1</span><span>&gt;</span> {
</span></span><span><span>3</span><span>  let BufferSize <span>=</span> <span>16</span>;
</span></span><span><span>4</span><span>}</span></span></code></pre></div><p>Here, the execution unit for branch instructions has its own scheduler and it has 16 entries in its buffer.</p><p>A more common arrangement is assigning <code>BufferSize</code> on a processor resource <em>group</em>.
For instance, in AMD Zen2 CPU, there are total of 4 general integer execution units with their own schedulers; each scheduler has 16 buffer entries.</p><figure><img src="https://myhsu.xyz/images/zen2-uarch.svg"/><figcaption>Image source: <a href="https://en.wikichip.org/wiki/amd/microarchitectures/zen_2">Wiki Chip</a></figcaption></figure><p>In Zen2’s LLVM scheduling model, these schedulers are represented by 4 <code>ProcessorResource</code> instances: <code>Zn2ALU[0-3]</code>.
Despite the fact that there are 4 separate schedulers in real hardware, Zen2’ scheduling model doesn’t assign <code>BufferSize = 16</code> to each <code>Zn2ALU[0-3]</code> instance, but instead group all 4 instances into a single <code>ProcResGroup</code> called <code>Zn2ALU</code> and <a href="https://github.com/llvm/llvm-project/blob/22c06aa5e94e30fb1333ecaf46ce33c65d148634/llvm/lib/Target/X86/X86ScheduleZnver2.td#L75">assign</a> a buffer size of 16 * 4:</p><div><pre tabindex="0"><code data-lang="c++"><span><span>1</span><span><span>// 64 Entry (16x4 entries) Int Scheduler
</span></span></span><span><span>2</span><span><span></span>def <span>Zn2ALU</span> : ProcResGroup<span>&lt;</span>[Zn2ALU0, Zn2ALU1, Zn2ALU2, Zn2ALU3]<span>&gt;</span> {
</span></span><span><span>3</span><span>  let BufferSize<span>=</span><span>64</span>;
</span></span><span><span>4</span><span>}</span></span></code></pre></div><p>The rationale behind this is that most integer instructions map <code>Zn2ALU</code>, rather than a specific <code>Zn2ALU[0-3]</code>, to their <code>SchedWrite</code> token. Because similar to the SiFive P600 example we’ve seen earlier, these instructions simply don’t care which <code>Zn2ALU[0-3]</code> they are going to run on. Therefore, it makes more sense to specify the buffer size on the processor resource group.</p><h5 id="unified-reservation-station">Unified reservation station</h5><p>By default, <code>BufferSize</code> is set to <strong>-1</strong>, representing a layout where all the processor resources effectively share the same buffer as illustrated in the diagram below.</p><div><picture><source srcset="/images/llvm-sched-model-unified-reservation-station.dark.svg" media="(prefers-color-scheme: dark)"/><img src="https://myhsu.xyz/images/llvm-sched-model-unified-reservation-station.light.svg"/></picture></div><p>In this case, the actual size of such buffer is dictated by <code>MicroOpBufferSize</code>, which is a global attribute to the entire scheduling model rather than individual processor resource:</p><div><pre tabindex="0"><code data-lang="c++"><span><span>1</span><span>def <span>SiFiveP600Model</span> : SchedMachineModel {
</span></span><span><span>2</span><span>  let IssueWidth <span>=</span> <span>4</span>;         <span>// 4 micro-ops are dispatched per cycle.
</span></span></span><span><span>3</span><span><span></span>  let MicroOpBufferSize <span>=</span> <span>160</span>; <span>// Max micro-ops that can be buffered.
</span></span></span><span><span>4</span><span><span></span>  ...
</span></span><span><span>5</span><span>}</span></span></code></pre></div><p>This <code>MicroOpBufferSize</code> attribute is primarily used to throttle the number of uops to be <em>dispatched</em>. There are, however, many factors that control this throttle at the same time. Notably, ROB and register renaming. We’ve explained the effect of ROB earlier; for register renaming, if there aren’t enough physical registers for us to rename, a dispatch hazard also occurs. Consequently, the value of <code>MicroOpBufferSize</code> should be the minimal of reorder buffer size, size of register renaming pool, and the actual size of unified reservation station.</p><h5 id="in-order-core">In-order core</h5><p>So far we have been discussing out-of-order cores, which is one of the primary reasons why we need buffers in the first place. But in-order cores are still a thing, because it dramatically simplifies the chip design and reduces the area, which are top on the list for products like embedded devices. It’s also getting more popular in cases where you want save areas for specialized units like BIG vector units or even matrix multiplication units. SiFive’s X280, which we’ve seen its scheduling model previously, and X390 are good examples, where they save area by adopting in-order design and enjoying a whopping 512- and 1024-bit vector, respectively.</p><p>For in-order cores, you simply set <code>BufferSize</code> and <code>MicroOpBufferSize</code> mentioned earlier to <strong>zero</strong>.
When an uop is handed to an in-order unit in LLVM’s scheduling model, <em>dispatch</em> and <em>issue</em> are conisdered happening at the same time.</p><div><picture><source srcset="/images/llvm-sched-model-in-order.dark.svg" media="(prefers-color-scheme: dark)"/><img src="https://myhsu.xyz/images/llvm-sched-model-in-order.light.svg"/></picture></div><p>The uop then holds the processor resource until <code>ReleaseAtCycle</code> before another uop can be dispatched. In other words, the younger uop would encounter a dispatch hazard until <code>ReleaseAtCycle</code> has passed in the older uop.</p><h5 id="latency-device">Latency device</h5><p>When <code>BufferSize</code> equals to 1, we create a unique kind of resource known as latency device. Resources of this kind act just like an in-order pipeline, except two things:</p><ol><li>Since there is still a buffer, albiet being a really small one, a younger uop waits in the buffer until the older uop finishes, rather than encounters a dispatch hazard.</li><li>LLVM’s instruction scheduler always treats two adjacent uops that use this resource as producer and consumer (even if there is no data dependency between them). Meaning, the younger uop always waits <code>Latency</code> cycles after the old uop was issued – as opposed to waiting until <code>ReleaseAtCycle</code> has passed in a normal in-order pipeline – before it can be issued.</li></ol><p>This kind of resource was designed to model in-order units within an out-of-order core. It’s most commonly used for modeling <strong>un-pipelined</strong> units nowadays (which, of course, is in-order). Yes, it’s 2024 and there are still computations that are difficult to be pipelined, most notably integer – sometimes even floating point – <em>divisions</em>. It is possible to make integer divisions pipelined, but many of the times it’s not worth the chip area.</p><div><picture><source srcset="/images/llvm-sched-model-latency-device.dark.svg" media="(prefers-color-scheme: dark)"/><img src="https://myhsu.xyz/images/llvm-sched-model-latency-device.light.svg"/></picture></div><p>Some examples: Samsung Exynos M5’s serialized (i.e. un-pipelined) integer division unit, represented by <code>M5UnitD</code> from <a href="https://github.com/llvm/llvm-project/blob/5262865aac683b72f3e66de7a122e0c455ab6b9b/llvm/lib/Target/AArch64/AArch64SchedExynosM5.td#L41">here</a>, has a buffer size of 1:</p><div><pre tabindex="0"><code data-lang="c++"><span><span>1</span><span>let Super <span>=</span>  M5UnitC, BufferSize <span>=</span> <span>1</span> in
</span></span><span><span>2</span><span>def <span>M5UnitD</span>  : ProcResource<span>&lt;</span><span>1</span><span>&gt;</span>; <span>// Integer division (inside C0, serialized)
</span></span></span></code></pre></div><p>RISC-V’s Rocket chip <a href="https://github.com/llvm/llvm-project/blob/fdb9f96fa2a926425bdf8315048db7623d63547d/llvm/lib/Target/RISCV/RISCVSchedRocket.td#L41">marks</a> their integer and floating point division units, <code>RocketUnitIDiv</code> and <code>RocketUnitFPDivSqrt</code>, as latency devices too:</p><div><pre tabindex="0"><code data-lang="c++"><span><span>1</span><span>let BufferSize <span>=</span> <span>1</span> in {
</span></span><span><span>2</span><span>def <span>RocketUnitIDiv</span>       : ProcResource<span>&lt;</span><span>1</span><span>&gt;</span>; <span>// Int Division
</span></span></span><span><span>3</span><span><span></span>def <span>RocketUnitFPDivSqrt</span>  : ProcResource<span>&lt;</span><span>1</span><span>&gt;</span>; <span>// FP Divide/Sqrt
</span></span></span><span><span>4</span><span><span></span>}</span></span></code></pre></div><h3 id="epilogue">Epilogue</h3><p>In this post, I went through the basics of LLVM’s scheduling model and show how to specify the scheduling information for individual instructions. On top of that, I explained different kinds of processor resource buffers and their use cases.</p><p>In the next post, I will talk about <em>how</em> exactly these resource buffer kinds and latency are used in LLVM’s instruction scheduler and LLVM MCA, as well as their performance impact on the generated code. Which helps us to make a better decision on designing new scheduling models.</p></div></div>
  </body>
</html>
