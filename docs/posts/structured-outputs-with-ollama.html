<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ollama.com/blog/structured-outputs">Original</a>
    <h1>Structured Outputs with Ollama</h1>
    
    <div id="readability-page-1" class="page"><div>
      
  <article>
    
    <h2>December 6, 2024</h2>
    <section>
      <p><img src="https://ollama.com/public/blog/ollama-json.png" alt="Ollama playing with building blocks" width="70%"/></p>

<p>Ollama now supports structured outputs making it possible to constrain a model’s output to a specific format defined by a JSON schema. The Ollama Python and JavaScript libraries have been updated to support structured outputs.</p>

<p>Use cases for structured outputs include:</p>

<ul>
<li>Parsing data from documents</li>
<li>Extracting data from images</li>
<li>Structuring all language model responses</li>
<li>More reliability and consistency than JSON mode</li>
</ul>

<h3>Get started</h3>

<p>Download the latest version of <a href="https://ollama.com/download">Ollama</a></p>

<p>Upgrade to the latest version of the Ollama Python or JavaScript library:</p>

<p><sub>Python</sub></p>

<pre><code>pip install -U ollama
</code></pre>

<p><sub>JavaScript</sub></p>

<pre><code>npm i ollama
</code></pre>

<p>To pass structured outputs to the model, the <code>format</code> parameter can be used in the cURL request or the <code>format</code> parameter in the Python or JavaScript libraries.</p>

<h4>cURL</h4>

<pre><code>curl -X POST http://localhost:11434/api/chat -H &#34;Content-Type: application/json&#34; -d &#39;{
  &#34;model&#34;: &#34;llama3.1&#34;,
  &#34;messages&#34;: [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Tell me about Canada.&#34;}],
  &#34;stream&#34;: false,
  &#34;format&#34;: {
    &#34;type&#34;: &#34;object&#34;,
    &#34;properties&#34;: {
      &#34;name&#34;: {
        &#34;type&#34;: &#34;string&#34;
      },
      &#34;capital&#34;: {
        &#34;type&#34;: &#34;string&#34;
      },
      &#34;languages&#34;: {
        &#34;type&#34;: &#34;array&#34;,
        &#34;items&#34;: {
          &#34;type&#34;: &#34;string&#34;
        }
      }
    },
    &#34;required&#34;: [
      &#34;name&#34;,
      &#34;capital&#34;, 
      &#34;languages&#34;
    ]
  }
}&#39;
</code></pre>

<h5>Output</h5>

<p>The response is returned in the format defined by the JSON schema in the request.</p>

<pre><code>{
  &#34;capital&#34;: &#34;Ottawa&#34;,
  &#34;languages&#34;: [
    &#34;English&#34;,
    &#34;French&#34;
  ],
  &#34;name&#34;: &#34;Canada&#34;
}
</code></pre>

<h4>Python</h4>

<p>Using the <a href="https://github.com/ollama/ollama-python">Ollama Python library</a>, pass in the schema as a JSON object to the <code>format</code> parameter as either <code>dict</code> or use Pydantic (recommended) to serialize the schema using <code>model_json_schema()</code>.</p>

<pre><code>from ollama import chat
from pydantic import BaseModel

class Country(BaseModel):
  name: str
  capital: str
  languages: list[str]

response = chat(
  messages=[
    {
      &#39;role&#39;: &#39;user&#39;,
      &#39;content&#39;: &#39;Tell me about Canada.&#39;,
    }
  ],
  model=&#39;llama3.1&#39;,
  format=Country.model_json_schema(),
)

country = Country.model_validate_json(response.message.content)
print(country)
</code></pre>

<h5>Output</h5>

<pre><code>name=&#39;Canada&#39; capital=&#39;Ottawa&#39; languages=[&#39;English&#39;, &#39;French&#39;]
</code></pre>

<h4>JavaScript</h4>

<p>Using the <a href="https://github.com/ollama/ollama-js">Ollama JavaScript library</a>, pass in the schema as a JSON object to the <code>format</code> parameter as either <code>object</code> or use Zod (recommended) to serialize the schema using <code>zodToJsonSchema()</code>.</p>

<pre><code>import ollama from &#39;ollama&#39;;
import { z } from &#39;zod&#39;;
import { zodToJsonSchema } from &#39;zod-to-json-schema&#39;;

const Country = z.object({
    name: z.string(),
    capital: z.string(), 
    languages: z.array(z.string()),
});

const response = await ollama.chat({
    model: &#39;llama3.1&#39;,
    messages: [{ role: &#39;user&#39;, content: &#39;Tell me about Canada.&#39; }],
    format: zodToJsonSchema(Country),
});

const country = Country.parse(JSON.parse(response.message.content));
console.log(country);
</code></pre>

<h5>Output</h5>

<pre><code>{
  name: &#34;Canada&#34;,
  capital: &#34;Ottawa&#34;,
  languages: [ &#34;English&#34;, &#34;French&#34; ],
}
</code></pre>

<h2>Examples</h2>

<h3>Data extraction</h3>

<p>To extract structured data from text, define a schema to represent information. The model then extracts the information and returns the data in the defined schema as JSON:</p>

<pre><code>from ollama import chat
from pydantic import BaseModel

class Pet(BaseModel):
  name: str
  animal: str
  age: int
  color: str | None
  favorite_toy: str | None

class PetList(BaseModel):
  pets: list[Pet]

response = chat(
  messages=[
    {
      &#39;role&#39;: &#39;user&#39;,
      &#39;content&#39;: &#39;&#39;&#39;
        I have two pets.
        A cat named Luna who is 5 years old and loves playing with yarn. She has grey fur.
        I also have a 2 year old black cat named Loki who loves tennis balls.
      &#39;&#39;&#39;,
    }
  ],
  model=&#39;llama3.1&#39;,
  format=PetList.model_json_schema(),
)

pets = PetList.model_validate_json(response.message.content)
print(pets)

</code></pre>

<h4>Example output</h4>

<pre><code>pets=[
  Pet(name=&#39;Luna&#39;, animal=&#39;cat&#39;, age=5, color=&#39;grey&#39;, favorite_toy=&#39;yarn&#39;), 
  Pet(name=&#39;Loki&#39;, animal=&#39;cat&#39;, age=2, color=&#39;black&#39;, favorite_toy=&#39;tennis balls&#39;)
]
</code></pre>

<h3>Image description</h3>

<p>Structured outputs can also be used with vision models. For example, the following code uses <code>llama3.2-vision</code> to describe the following image and returns a structured output:</p>

<p><img src="https://ollama.com/public/blog/beach.jpg" alt="image"/></p>

<pre><code>from ollama import chat
from pydantic import BaseModel

class Object(BaseModel):
  name: str
  confidence: float
  attributes: str 

class ImageDescription(BaseModel):
  summary: str
  objects: List[Object]
  scene: str
  colors: List[str]
  time_of_day: Literal[&#39;Morning&#39;, &#39;Afternoon&#39;, &#39;Evening&#39;, &#39;Night&#39;]
  setting: Literal[&#39;Indoor&#39;, &#39;Outdoor&#39;, &#39;Unknown&#39;]
  text_content: Optional[str] = None

path = &#39;path/to/image.jpg&#39;

response = chat(
  model=&#39;llama3.2-vision&#39;,
  format=ImageDescription.model_json_schema(),  # Pass in the schema for the response
  messages=[
    {
      &#39;role&#39;: &#39;user&#39;,
      &#39;content&#39;: &#39;Analyze this image and describe what you see, including any objects, the scene, colors and any text you can detect.&#39;,
      &#39;images&#39;: [path],
    },
  ],
  options={&#39;temperature&#39;: 0},  # Set temperature to 0 for more deterministic output
)

image_description = ImageDescription.model_validate_json(response.message.content)
print(image_description)
</code></pre>

<h4>Example output</h4>

<pre><code>summary=&#39;A palm tree on a sandy beach with blue water and sky.&#39; 
objects=[
  Object(name=&#39;tree&#39;, confidence=0.9, attributes=&#39;palm tree&#39;), 
  Object(name=&#39;beach&#39;, confidence=1.0, attributes=&#39;sand&#39;)
], 
scene=&#39;beach&#39;, 
colors=[&#39;blue&#39;, &#39;green&#39;, &#39;white&#39;], 
time_of_day=&#39;Afternoon&#39; 
setting=&#39;Outdoor&#39; 
text_content=None
</code></pre>

<h4>OpenAI compatibility</h4>

<pre><code>from openai import OpenAI
import openai
from pydantic import BaseModel

client = OpenAI(base_url=&#34;http://localhost:11434/v1&#34;, api_key=&#34;ollama&#34;)

class Pet(BaseModel):
    name: str
    animal: str
    age: int
    color: str | None
    favorite_toy: str | None

class PetList(BaseModel):
    pets: list[Pet]

try:
    completion = client.beta.chat.completions.parse(
        temperature=0,
        model=&#34;llama3.1:8b&#34;,
        messages=[
            {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#39;&#39;&#39;
                I have two pets.
                A cat named Luna who is 5 years old and loves playing with yarn. She has grey fur.
                I also have a 2 year old black cat named Loki who loves tennis balls.
            &#39;&#39;&#39;}
        ],
        response_format=PetList,
    )

    pet_response = completion.choices[0].message
    if pet_response.parsed:
        print(pet_response.parsed)
    elif pet_response.refusal:
        print(pet_response.refusal)
except Exception as e:
    if type(e) == openai.LengthFinishReasonError:
        print(&#34;Too many tokens: &#34;, e)
        pass
    else:
        print(e)
        pass
</code></pre>

<h2>Tips</h2>

<p>For reliable use of structured outputs, consider to:</p>

<ul>
<li>Use Pydantic (Python) or Zod (JavaScript) to define the schema for the response</li>
<li>Add “return as JSON” to the prompt to help the model understand the request</li>
<li>Set the temperature to 0 for more deterministic output</li>
</ul>

<h2>What’s next?</h2>

<ul>
<li>Exposing logits for controlled generation</li>
<li>Performance and accuracy improvements for structured outputs</li>
<li>GPU acceleration for sampling</li>
<li>Additional format support beyond JSON schema</li>
</ul>

    </section>
  </article>

    </div></div>
  </body>
</html>
