<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://research.nvidia.com/labs/toronto-ai/VideoLDM/">Original</a>
    <h1>Align Your Latents: High-Resolution Video Synthesis with Latent Diffusion Models</h1>
    
    <div id="readability-page-1" class="page">

    

    <hr/>
    
    <hr/>

    <hr/>
    <center>
    <div> 
        <a href="https://reserialised.routley.io/great-expectations/samples.html" target="_blank" rel="noopener noreferrer"><p>Click for more samples</p></a>
        <hr/>
        <section id="abstract">
            <h2>Abstract</h2>
            <hr/>
            <p>
                    Latent Diffusion Models (LDMs) enable high-quality image synthesis while avoiding excessive compute demands by training a diffusion model in a compressed lower-dimensional latent space. 
                    Here, we apply the LDM paradigm to high-resolution video generation, a particularly resource-intensive task. We first pre-train an LDM on images only; then, we turn the image generator 
                    into a video generator by introducing a temporal dimension to the latent space diffusion model and fine-tuning on encoded image sequences, i.e., videos. Similarly, we temporally align
                    diffusion model upsamplers, turning them into temporally consistent video super resolution models. We focus on two relevant real-world applications: Simulation of in-the-wild driving
                    data and creative content creation with text-to-video modeling. In particular, we validate our Video LDM on real driving videos of resolution 512 x 1024, achieving state-of-the-art 
                    performance. Furthermore, our approach can easily leverage off-the-shelf pre-trained image LDMs, as we only need to train a temporal alignment model in that case. Doing so, we turn 
                    the publicly available, state-of-the-art text-to-image LDM Stable Diffusion into an efficient and expressive text-to-video model with resolution up to 1280 x 2048. We show that the 
                    temporal layers trained in this way generalize to different fine-tuned text-to-image LDMs. Utilizing this property, we show the first results for personalized text-to-video generation,
                    opening exciting directions for future content creation.
                </p>
        </section>
        <hr/>
    </div>
    </center>
    <hr/>


    <section id="pipeline">
            <center>
                <div>
                    <center>
                    <div>
                    <figure>
                        <video width="100%" autoplay="" loop="" muted="" playsinline="" class="video-background">
                        <source src="assets/figures/video_ldm_animation.mp4" type="video/mp4"/>
                        Your browser does not support the video tag.
                        </video>
                        <p>
                        <b>Animation of temporal video fine-tuning in our Video Latent Diffusion Models (Video LDMs).</b> We turn pre-trained image diffusion models into temporally consistent video generators.
                        Initially, different samples of a batch synthesized by the model are independent. After temporal video fine-tuning, the samples are temporally aligned and form coherent videos.
                        The stochastic generation processes before and after fine-tuning are visualised for a diffusion model of a one-dimensional toy distribution. For clarity, the figure corresponds to alignment in pixel space.
                        In practice, we perform alignment in LDM&#39;s latent space and obtain videos after applying LDM&#39;s decoder.</p>
                    </figure>
                    </div>
                    </center>
                </div>
            </center>
    </section>


    <section id="overview">
        <h2>Video Latent Diffusion Models</h2>
        <hr/>
        <div>
            <p> We present Video Latent Diffusion Models (Video LDMs) for computationally efficient high-resolution video generation. To alleviate the intensive compute and memory demands
                of high-resolution video synthesis, we leverage the LDM paradigm and extend it to video generation. Our Video LDMs map videos into a compressed latent space and 
                model sequences of latent variables corresponding to the video frames (see animation above). We initialize the models from image LDMs and insert temporal layers into the 
                LDMs&#39; denoising neural networks to temporally model encoded video frame sequences. The temporal layers are based on temporal attention as well as 
                3D convolutions. We also fine-tune the model&#39;s decoder for video generation (see figure below).
            </p>
            <center>
                <div>
                <figure>
                <img width="65%" src="https://reserialised.routley.io/great-expectations/assets/figures/video_ldm_pipeline.png"/>
                </figure>
                </div>
                <p>
                    <b>Latent diffusion model framework and video fine-tuning of decoder.</b> <i>Top:</i> During temporal decoder fine-tuning, we process video sequences with a frozen per-frame encoder and enforce temporally coherent reconstructions across frames. We additionally employ a video-aware discriminator. <i>Bottom:</i> in LDMs, a diffusion model is trained in latent space. It synthesizes latent features, which are then transformed through the decoder into images. Note that in practice we model entire videos and video fine-tune the latent diffusion model to generate temporally consistent frame sequences.</p>
            </center>
            <p> Our Video LDM initially generates sparse keyframes at low frame rates, which are then temporally upsampled twice by another interpolation latent diffusion model. 
                Moreover, optionally training Video LDMs for video prediction by conditioning on starting frames allows us to generate long videos in an autoregressive manner.  
                To achieve high-resolution generation, we further leverage spatial diffusion model upsamplers and temporally align them for video upsampling. 
                The entire generation stack is shown below.
            </p>
            <center>
                <div>
                <figure>
                <img width="70%" src="https://reserialised.routley.io/great-expectations/assets/figures/video_ldm_stack.png"/>
                </figure>
                </div>
                <p>
                    <b>Video LDM Stack.</b> We first generate sparse key frames. Then we temporally interpolate in two steps with the same interpolation model to achieve high frame rates. These operations use latent diffusion models (LDMs) that share the same image backbone. Finally, the latent video is decoded to pixel space and optionally a video upsampler diffusion model is applied.</p>
            </center>
            <p> 
                <b>Applications.</b> 
                We validate our approach on two relevant but distinct applications: Generation of in-the-wild driving scene videos and creative content creation with text-to-video modeling. For 
                driving video synthesis, our Video LDM enables generation of temporally coherent, multiple minute long videos at resolution 512 x 1024, achieving state-of-the-art performance. For text-to-video, we demonstrate synthesis
                of short videos of several seconds lengths with resolution up to 1280 x 2048, leveraging Stable Diffusion as backbone image LDM as well as the Stable Diffusion upscaler. We also explore the convolutional-in-time application of our models as an alternative approach to extend the length of videos. Our main keyframe models only train the newly inserted temporal layers,
                but do not touch the layers of the backbone image LDM. Because of that the learnt temporal layers can be transferred to other image LDM backbones, for instance to ones that
                have been fine-tuned with DreamBooth. Leveraging this property, we additionally show initial results for personalized text-to-video generation.
            </p>            
        </div>
    </section>


    <section id="text-to-video">
        <h2>Text-to-Video Synthesis</h2>
        <hr/>
        <div>
            <p> Many generated videos can be found at the top of the page as well as <a href="https://reserialised.routley.io/great-expectations/samples.html" target="_blank" rel="noopener noreferrer"><b>here</b></a>. The generated videos have a resolution of 1280 x 2048 pixels, consist of 113 frames and are rendered at 24 fps, resulting in 4.7 second long clips. Our Video LDM for text-to-video generation is based on Stable Diffusion and has a total of 4.1B parameters, including all components except the CLIP text encoder. Only 2.7B of these parameters are trained on videos. 
            This means that our models are significantly smaller than those of several concurrent works. Nevertheless, we can produce high-resolution, temporally consistent and diverse videos. This can be attributed to the efficient
            LDM approach. Below is another text-to-video sample, one of our favorites.
            </p>
            <center>
                <div>
                    <center>
                    <div>
                    <figure>
                        <video width="100%" autoplay="" loop="" muted="" playsinline="" class="video-background">
                        <source src="assets/text_to_video/teddy_bear_guitar.mp4" type="video/mp4"/>
                        Your browser does not support the video tag.
                        </video>
                        <p>
                            Text prompt: &#34;A teddy bear is playing the electric guitar, high definition, 4k.&#34;
                        </p>
                    </figure>
                    </div>
                    </center>
                </div>
            </center>


            <p> <b>Personalized Video Generation.</b> We insert the temporal layers that were trained for our Video LDM for text-to-video synthesis into image LDM backbones that we previously fine-tuned on a set of images
                following <a href="https://dreambooth.github.io/" target="_blank" rel="noopener noreferrer"><b>DreamBooth</b></a>. The temporal layers generalize to the DreamBooth checkpoints, thereby enabling personalized text-to-video generation.
            </p>
            <div>
                <div>
                <center>
                <figure>
                        <img width="95%" src="https://reserialised.routley.io/great-expectations/assets/text_to_video/dreambooth/cat_db.png"/>
                        <p>
                             Training images for DreamBooth.
                        </p>
                </figure>
                </center>
                </div>
                <div>
                <center>
                <figure>
                        <video width="100%" muted="" playsinline="" controls="controls" preload="metadata" onclick="this.play()" class="video-background">
                        <source src="assets/text_to_video/dreambooth/cat_db_1.mp4" type="video/mp4"/>
                        Your browser does not support the video tag.
                        </video>
                        <p>
                            Text prompt: &#34;A <i><b>sks</b></i> cat playing in the grass.&#34;
                        </p>
                </figure>
                </center>
                </div>
                <div>
                <center>
                <figure>
                        <video width="100%" muted="" playsinline="" controls="controls" preload="metadata" onclick="this.play()" class="video-background">
                        <source src="assets/text_to_video/dreambooth/cat_db_2.mp4" type="video/mp4"/>
                        Your browser does not support the video tag.
                        </video>
                        <p>
                            Text prompt: &#34;A <i><b>sks</b></i> cat getting up.&#34;
                        </p>
                </figure>
                </center>
                </div>
            </div>

            <div>
                <div>
                <center>
                <figure>
                        <img width="95%" src="https://reserialised.routley.io/great-expectations/assets/text_to_video/dreambooth/opera_db.png"/>
                        <p>
                             Training images for DreamBooth.
                        </p>
                </figure>
                </center>
                </div>
                <div>
                <center>
                <figure>
                        <video width="100%" controls="controls" preload="metadata" onclick="this.play()" muted="" playsinline="" class="video-background">
                        <source src="assets/text_to_video/dreambooth/opera_db_1.mp4" type="video/mp4"/>
                        Your browser does not support the video tag.
                        </video>
                        <p>
                            Text prompt: &#34;A <i><b>sks</b></i> building next to the Eiffel Tower.&#34;
                        </p>
                </figure>
                </center>
                </div>
                <div>
                <center>
                <figure>
                        <video width="100%" controls="controls" preload="metadata" onclick="this.play()" muted="" playsinline="" class="video-background">
                        <source src="assets/text_to_video/dreambooth/opera_db_2.mp4" type="video/mp4"/>
                        Your browser does not support the video tag.
                        </video>
                        <p>
                            Text prompt: &#34;Waves crashing against a <i><b>sks</b></i> building, ominous lighting.&#34;
                        </p>
                </figure>
                </center>
                </div>
            </div>

            <div>
                <div>
                <center>
                <figure>
                        <img width="95%" src="https://reserialised.routley.io/great-expectations/assets/text_to_video/dreambooth/frog_db.png"/>
                        <p>
                             Training images for DreamBooth.
                        </p>
                </figure>
                </center>
                </div>
                <div>
                <center>
                <figure>
                        <video width="100%" controls="controls" preload="metadata" onclick="this.play()" muted="" playsinline="" class="video-background">
                        <source src="assets/text_to_video/dreambooth/frog_db_1.mp4" type="video/mp4"/>
                        Your browser does not support the video tag.
                        </video>
                        <p>
                            Text prompt: &#34;A <i><b>sks</b></i> frog playing a guitar in a band.&#34;
                        </p>
                </figure>
                </center>
                </div>
                <div>
                <center>
                <figure>
                        <video width="100%" controls="controls" preload="metadata" onclick="this.play()" muted="" playsinline="" class="video-background">
                        <source src="assets/text_to_video/dreambooth/frog_db_2.mp4" type="video/mp4"/>
                        Your browser does not support the video tag.
                        </video>
                        <p>
                            Text prompt: &#34;A <i><b>sks</b></i> frog writing a scientific research paper.&#34;
                        </p>
                </figure>
                </center>
                </div>
            </div>

            <div>
                <div>
                <center>
                <figure>
                        <img width="95%" src="https://reserialised.routley.io/great-expectations/assets/text_to_video/dreambooth/teapot_db.png"/>
                        <p>
                             Training images for DreamBooth.
                        </p>
                </figure>
                </center>
                </div>
                <div>
                <center>
                <figure>
                        <video width="100%" controls="controls" preload="metadata" onclick="this.play()" muted="" playsinline="" class="video-background">
                        <source src="assets/text_to_video/dreambooth/teapot_db_1.mp4" type="video/mp4"/>
                        Your browser does not support the video tag.
                        </video>
                        <p>
                            Text prompt: &#34;A <i><b>sks</b></i> tea pot floating in the ocean.&#34;
                        </p>
                </figure>
                </center>
                </div>
                <div>
                <center>
                <figure>
                        <video width="100%" controls="controls" preload="metadata" onclick="this.play()" muted="" playsinline="" class="video-background">
                        <source src="assets/text_to_video/dreambooth/teapot_db_2.mp4" type="video/mp4"/>
                        Your browser does not support the video tag.
                        </video>
                        <p>
                            Text prompt: &#34;A <i><b>sks</b></i> tea pot on top of a building in New York, drone flight, 4k.&#34;
                        </p>
                </figure>
                </center>
                </div>
            </div>


            <p> <b>Convolutional-in-Time Synthesis.</b> We also explored synthesizing slightly longer videos &#34;for free&#34; by applying our learnt temporal layers convolutionally in time. The below videos consist of 175 frames rendered at 24 fps, resulting in 7.3 second long clips. A minor degradation in quality can be observed.
            </p>
            <div>
                <div>
                <center>
                <figure>
                        <video width="100%" controls="controls" preload="metadata" onclick="this.play()" muted="" playsinline="" class="video-background">
                        <source src="assets/text_to_video/conv_in_time/conv_in_time_video1.mp4" type="video/mp4"/>
                        Your browser does not support the video tag.
                        </video>
                        <p>
                            Text prompt: &#34;Teddy bear walking down 5th Avenue, front view, beautiful sunset, close up, high definition, 4k.&#34;
                        </p>
                </figure>
                </center>
                </div>
                <div>
                <center>
                <figure>
                        <video width="100%" controls="controls" preload="metadata" onclick="this.play()" muted="" playsinline="" class="video-background">
                        <source src="assets/text_to_video/conv_in_time/conv_in_time_video2.mp4" type="video/mp4"/>
                        Your browser does not support the video tag.
                        </video>
                        <p>
                            Text prompt: &#34;Waves crashing against a lone lighthouse, ominous lighting.&#34;
                        </p>
                </figure>
                </center>
                </div>
            </div>
        </div>
    </section>


    <section id="text-to-video">
        <h2>Driving Scene Video Generation</h2>
        <hr/>
        <div>
            <p> We also train a Video LDM on in-the-wild real driving scene videos and generate videos at 512 x 1024 resolution. Here, we are additionally training prediction models to enable long video generation, allowing us to generate temporally coherent videos that are several minutes long. Below we show four short synthesized videos. Furthermore, several 5 minute long generated videos can be found <a href="https://drive.google.com/file/d/1xlE079d4QmVZ-kWLZVsIk8iHWWH5wzKO/view?usp=share_link" target="_blank" rel="noopener noreferrer"><b>here</b></a>.
            </p>
            <div>
                <p>
                <center>
                <figure>
                        <video width="100%" controls="controls" preload="metadata" onclick="this.play()" muted="" playsinline="" class="video-background">
                        <source src="assets/driving/high_res_driving_1.mp4" type="video/mp4"/>
                        Your browser does not support the video tag.
                        </video>
                </figure>
                </center>
                </p>
                <p>
                <center>
                <figure>
                        <video width="100%" controls="controls" preload="metadata" onclick="this.play()" muted="" playsinline="" class="video-background">
                        <source src="assets/driving/high_res_driving_2.mp4" type="video/mp4"/>
                        Your browser does not support the video tag.
                        </video>
                </figure>
                </center>
                </p>
            </div>
            <div>
                <p>
                <center>
                <figure>
                        <video width="100%" controls="controls" preload="metadata" onclick="this.play()" muted="" playsinline="" class="video-background">
                        <source src="assets/driving/high_res_driving_3.mp4" type="video/mp4"/>
                        Your browser does not support the video tag.
                        </video>
                </figure>
                </center>
                </p>
                <p>
                <center>
                <figure>
                        <video width="100%" controls="controls" preload="metadata" onclick="this.play()" muted="" playsinline="" class="video-background">
                        <source src="assets/driving/high_res_driving_4.mp4" type="video/mp4"/>
                        Your browser does not support the video tag.
                        </video>
                </figure>
                </center>
                </p>
            </div>
            </div>
    </section>

    
    <section id="paper">
        <h2>Paper</h2>
        <hr/>
        <div>
            <div>
                <p><a href="https://arxiv.org/abs/2304.08818" target="_blank" rel="noopener noreferrer"><img src="https://reserialised.routley.io/great-expectations/assets/figures/video_ldm_paper_preview.png"/></a>
                </p>
            </div>
            <div>
                <p><b>Align your Latents:</b></p>
                <p>Andreas Blattmann*, Robin Rombach*, Huan Ling*, Tim Dockhorn*, Seung Wook Kim, Sanja Fidler, Karsten Kreis</p>
                <p><i>* Equal contribution.</i></p>
                <p><i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023</i></p>
                
                
            </div>
        </div>
    </section>

    <section id="bibtex">
        <h2>Citation</h2>
        <hr/>
        <pre><code>@inproceedings{blattmann2023videoldm,
    title={Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models},
    author={Blattmann, Andreas and Rombach, Robin and Ling, Huan and Dockhorn, Tim and Kim, Seung Wook and Fidler, Sanja and Kreis, Karsten},
    booktitle={IEEE Conference on Computer Vision and Pattern Recognition ({CVPR})},
    year={2023}
}</code></pre>
    </section>






</div>
  </body>
</html>
