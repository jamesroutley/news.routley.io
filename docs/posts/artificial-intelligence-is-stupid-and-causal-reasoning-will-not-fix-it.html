<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2020.513474/full">Original</a>
    <h1>Artificial Intelligence Is Stupid and Causal Reasoning Will Not Fix It</h1>
    
    <div id="readability-page-1" class="page"><div>
<h2>1. Making a Mind</h2>
<p>For much of the twentieth century, the dominant cognitive paradigm identified the mind with the brain; as the Nobel laureate Francis Crick eloquently summarized:</p>
<p>“You, your joys and your sorrows, your memories and your ambitions, your sense of personal identity and free will, are in fact no more than the behavior of a vast assembly of nerve cells and their associated molecules. As Lewis Carroll&#39;s Alice might have phrased, ‘You’re nothing but a pack of neurons&#39;. This hypothesis is so alien to the ideas of most people today that it can truly be called astonishing” (<a href="#B24">Crick, 1994</a>).</p>
<p>Motivation for the belief that a computational simulation of the mind is possible stemmed initially from the work of <a href="#B70">Turing (1937)</a> and <a href="#B23">Church (1936)</a> and the “Church-Turing hypothesis”; in Turing&#39;s formulation, every “function which would naturally be regarded as computable” can be computed by the “Universal Turing Machine.” If computers can adequately model the brain, then, theory goes, it ought to be possible to <i>program</i> them to act like minds. As a consequence, in the latter part of the twentieth century, Crick&#39;s “Astonishing Hypothesis” helped fuel an explosion of interest in connectionism: both high-fidelity simulations of the brain (computational neuroscience; theoretical neurobiology) and looser—merely “neural inspired” —analoges (cf. Artificial Neural Networks, Multi-Layer Perceptrons, and “Deep Learning” systems).</p>
<p>But the fundamental question that Crick&#39;s hypothesis raises is, of course, that if we ever succeed in fully instantiating a <i>sufficiently accurate</i> simulation of the brain on a digital computer, will we also have fully instantiated a digital [computational] mind, with all the human mind&#39;s causal power of teleology, understanding, and reasoning, and will artificial intelligence (AI) finally have succeeded in delivering “Strong AI”.</p>
<p>Of course, <i>if</i> strong AI is possible, accelerating progress in its underpinning technologies–entailed both by the use of AI systems to design ever more sophisticated AIs and the continued doubling of raw computational power every 2 years—will eventually cause a runaway effect whereby the AI will inexorably come to exceed human performance on all tasks; the so-called point of [technological] “singularity” ([in]famously predicted by Ray Kurzweil to occur as soon as 2045). And, at the point this “singularity” occurs, so commentators like Kevin Warwick and Stephen Hawking suggest, humanity will, effectively, have been “superseded” on the evolutionary ladder and be obliged to eke out its autumn days listening to “Industrial Metal” music and gardening; or, in some of Hollywood&#39;s even more dystopian dreams, cruelly subjugated (and/or exterminated) by “Terminator” machines.</p>
<p>In this paper, however, I will offer a few “critical reflections” on one of the central, albeit awkward, questions of AI: why is it that, seven decades since Alan Turing first deployed an “effective method” to play chess in 1948, we have seen enormous strides in engineering particular machines to do clever things—from driving a car to beating the best at Go—but almost no progress in getting machines to genuinely understand; to seamlessly apply knowledge from one domain into another—the so-called problem of “Artificial General Intelligence” (AGI); the skills that both Hollywood and the wider media really think of, and depict, as AI?</p>
<h2>2. Neural Computing</h2>
<p>The earliest cybernetic work in the burgeoning field of “neural computing” lay in various attempts to understand, model, and emulate neurological function and learning in animal brains, the foundations of which were laid in 1943 by the neurophysiologist Warren McCulloch and the mathematician Walter Pitts (<a href="#B45">McCulloch and Pitts, 1943</a>).</p>
<p>Neural Computing defines a mode of problem solving based on “learning from experience” as opposed to classical, syntactically specified, “algorithmic” methods; at its core is “<i>the study of networks of &#39;adaptable nodes&#39; which, through a process of learning from task examples, store experiential knowledge and make it available for use</i>” (<a href="#B1">Aleksander and Morton, 1995</a>). So construed, an “Artificial Neural Network” (ANN) is constructed merely by appropriately connecting a group of adaptable nodes (“artificial neurons”).</p>
<p>• A <i>single layer neural network</i> only has one layer of adaptable nodes between the input vector, <i>X</i> and the output vector <i>O</i>, such that the output of each of the adaptable nodes defines one element of the network output vector <i>O</i>.</p>
<p>• A <i>multi-layer neural network</i> has one or more “hidden layers” of adaptable nodes between the input vector and the network output; in each of the network <i>hidden layers</i>, the outputs of the adaptable nodes connect to one or more inputs of the nodes in subsequent layers and in the network <i>output layer</i>, the output of each of the adaptable nodes defines one element of the network output vector <i>O</i>.</p>
<p>• A <i>recurrent neural network</i> is a network where the output of one or more nodes is fed-back to the input of other nodes in the architecture, such that the connections between nodes form a “directed graph along a temporal sequence,” so enabling a recurrent network to exhibit “temporal dynamics,” enabling a recurrent network to be sensitive to particular <i>sequences</i> of input vectors.</p>
<p>Since 1943 a variety of frameworks for the adaptable nodes have been proposed; however, the most common, as deployed in many “deep” neural networks, remains grounded on the McCulloch/Pitts model.</p>
<h3>2.1. The McCulloch/Pitts (MCP) Model</h3>
<p>In order to describe how the basic processing elements of the brain might function, McCulloch and Pitts showed how simple electrical circuits, connecting groups of “linear threshold functions,” could compute a variety of logical functions (<a href="#B45">McCulloch and Pitts, 1943</a>). In their model, McCulloch and Pitts provided a first (albeit very simplified) mathematical account of the chemical processes that define neuronal operation and in so doing realized that the mathematics that describe the neuron operation exhibited exactly the same type of logic that Shannon deployed in describing the behavior of switching circuits: namely, the calculus of propositions.</p>
<p><a href="#B45">McCulloch and Pitts (1943)</a> realized (a) that neurons can receive positive or negative encouragement to fire, contingent upon the type of their “synaptic connections” (excitatory or inhibitory) and (b) that in firing the neuron has effectively performed a “computation”; once the effect of the excitatory/inhibitory synapses are taken into account, it is possible to <i>arithmetically</i> determine the net effect of incoming patterns of “signals” innervating each neuron.</p>
<p>In a simple McCulloch/Pitts (MCP) threshold model, adaptability comes from representing each synaptic junction by a variable (usually rational) valued weight <i>W</i><sub><i>i</i></sub>, indicating the degree to which the neuron should react to the <sub><i>i</i></sub><i>th</i> particular input (see <a href="#F1">Figure 1</a>). By convention, positive weights represent excitatory synapses and negative, inhibitory synapses; the neuron firing threshold being represented by a variable <i>T</i>. In modern use, <i>T</i> is usually clamped to zero and a threshold implemented using a variable “bias” weight, <i>b</i>; typically, a neuron firing is represented by the value +1 and not firing by 0.</p>


<div>
<p><a href="https://www.frontiersin.org/files/Articles/513474/fpsyg-11-513474-HTML/image_m/fpsyg-11-513474-g001.jpg" name="figure1" target="_blank">
<img id="F1" alt="www.frontiersin.org" data-src="https://www.frontiersin.org/files/Articles/513474/fpsyg-11-513474-HTML/image_m/fpsyg-11-513474-g001.jpg"/></a></p><p><strong>Figure 1</strong>. The McCulloch–Pitts neuron model.</p>
</div>


<p>Activity at the <i>i</i><sub><i>th</i></sub> input to an <i>n</i> input neuron is represented by the symbol <i>X</i><sub><i>i</i></sub> and the effect of the <i>i</i><sub><i>th</i></sub> synapse by a weight <i>W</i><sub><i>i</i></sub>, hence the net effect of the <i>i</i><sub><i>th</i></sub> input on the <i>i</i><sub><i>th</i></sub> synapse on the MCP cell is thus <i>X</i><sub><i>i</i></sub> × <i>W</i><sub><i>i</i></sub>. Thus, the MCP cell is denoted as firing if:</p>
<div><p><math id="M1"><mtable columnalign="left"><mtr><mtd><mstyle displaystyle="true"><munderover accentunder="false" accent="false"><mrow><mo mathsize="10.5pt" mathcolor="black">∑</mo></mrow><mrow><mi mathsize="10.5pt" mathcolor="black">i</mi></mrow><mrow><mi mathsize="10.5pt" mathcolor="black">n</mi></mrow></munderover></mstyle><msub><mrow><mi mathsize="10.5pt" mathcolor="black">X</mi></mrow><mrow><mi mathsize="10.5pt" mathcolor="black">i</mi></mrow></msub><mo mathsize="10.5pt" mathcolor="black">×</mo><msub><mrow><mi mathsize="10.5pt" mathcolor="black">W</mi></mrow><mrow><mi mathsize="10.5pt" mathcolor="black">i</mi></mrow></msub><mo mathsize="10.5pt" mathcolor="black">+</mo><mi mathsize="10.5pt" mathcolor="black">b</mi></mtd><mtd><mo mathsize="10.5pt" mathcolor="black">≥</mo></mtd><mtd><mn mathsize="10.5pt" mathcolor="black">0</mn></mtd><mtd><mstyle mathsize="10.5pt" mathcolor="black"><mtext>    </mtext></mstyle><mrow><mo mathsize="10.5pt" mathcolor="black" stretchy="false">(</mo><mn mathsize="10.5pt" mathcolor="black">1</mn><mo mathsize="10.5pt" mathcolor="black" stretchy="false">)</mo></mrow></mtd></mtr></mtable></math></p>
</div>
<p>In a subsequent generalization of the basic MCP neuron, cell output is defined by a further (typically non-linear) function of the weighted sum of its input, the neuron&#39;s <i>activation function</i>.</p>
<p><a href="#B45">McCulloch and Pitts (1943)</a> proved that if “synapse polarity” is chosen appropriately, any single pattern of input can be “recognized” by a suitable network of MCP neurons (i.e., any finite logical expression can be realized by a suitable network of McCulloch–Pitts neurons). In other words, the McCulloch–Pitts&#39; result demonstrated that networks of artificial neurons could be mathematically specified, which would perform “computations” of immense complexity and power and in so doing, opened the door to a form of problem solving based on the design of appropriate neural network architectures and automatic (machine) “learning” of appropriate network parameters.</p>
<h2>3. Embeddings in Euclidean Space</h2>
<p>The most commonly used framework for information representation and processing in artificial neural networks (via generalized McCulloch/Pitts neurons) is a subspace of Euclidean space. Supervised learning in this framework is equivalent to deriving appropriate transformations (learning appropriate mappings) from training data (problem exemplars; pairs of <i>Input</i> + “<i>Target Output</i>″ vectors). The majority of learning algorithms adjust neuron interconnection weights according to a specified “learning rule,” the adjustment in a given time step being a function of a particular training example.</p>
<p>Weight updates are successively aggregated in this manner until the network reaches an equilibrium, at which point no further adjustments are made or, alternatively, learning stops before equilibrium to avoid “overfitting” the training data. On completion of these computations, knowledge about the training set is represented across a distribution of final weight values; thus, a trained network does not possess any internal representation of the (potentially complex) relationships <i>between</i> particular training exemplars.</p>
<p>Classical multi-layer neural networks are capable of discovering non-linear, continuous transformations between objects or events, but nevertheless they are restricted by operating on representations embedded in the linear, continuous structure of Euclidean space. It is, however, doubtful whether regression constitutes a satisfactory (or the most general) model of information processing in natural systems.</p>
<p>As <a href="#B48">Nasuto et al. (1998)</a> observed, the world, and relationships between objects in it, is fundamentally non-linear; relationships between real-world objects (or events) are typically far too messy and complex for representations in Euclidean spaces—and smooth mappings between them—to be appropriate embeddings (e.g., entities and objects in the real-world are often fundamentally discrete or qualitatively vague in nature, in which case Euclidean space does not offer an appropriate embedding for their representation).</p>
<p>Furthermore, representing objects in a Euclidean space imposes a serious additional effect, because Euclidean vectors can be compared to each other by means of <i>metrics</i>; enabling data to be compared in spite of any real-life constraints (sensu stricto, metric rankings may be undefined for objects and relations of the real world). As <a href="#B48">Nasuto et al. (1998)</a> highlight, it is not usually the case that all objects in the world can be equipped with a “natural ordering relation”; after all, what is the natural ordering of “banana” and “door”?</p>
<p>It thus follows that classical neural networks are best equipped only for tasks in which they process numerical data whose relationships can be reflected by Euclidean distance. In other words, classical connectionism can be reasonably well-applied to the same category of problems, which could be dealt with by various regression methods from statistics; as Francois Chollet, in reflecting on the limitations of deep learning, recently remarked:</p>
<p>“[a] deep learning model is ‘just’ a chain of simple, continuous geometric transformations mapping one vector space into another. All it can do is map one data manifold X into another manifold Y, assuming the existence of a learnable continuous transform from X to Y, and the availability of a dense sampling of X: Y to use as training data. So even though a deep learning model can be interpreted as a kind of program, inversely most programs cannot be expressed as deep learning models-for most tasks, either there exists no corresponding practically-sized deep neural network that solves the task, or even if there exists one, it may not be learnable … most of the programs that one may wish to learn cannot be expressed as a continuous geometric morphing of a data manifold” (<a href="#B21">Chollet, 2018</a>).</p>
<p>Over the last decade, however, ANN technology has developed beyond performing “simple function approximation” (cf. Multi-Layer Perceptrons) and deep [discriminative] classification (cf. Deep Convolutional Networks), to include new, <i>Generative</i> architectures where—<i>because they can learn to generate any distribution of data</i>—the variety of potential use cases is huge (e.g., generative networks can be taught to create novel outputs similar to real-world exemplars across any modality: images, music, speech, prose, etc.).</p>
<h3>3.1. Autoencoders, Variational Autoencoders, and Generative Adversarial Networks</h3>
<p>On the right hand side of <a href="#F2">Figure 2</a>, we see the output of a neural system, engineered by Terence Broad while studying for an MSc at Goldsmiths. Broad used a “complex, deep auto-encoder neural network” to process Blade Runner—a well-known sci-fi film that riffs on the notion of what is human and what is machine—building up its own “internal representations” of that film and then re-rendering these to produce an output movie that is surprisingly similar to the original (shown on the left).</p>


<div>
<p><a href="https://www.frontiersin.org/files/Articles/513474/fpsyg-11-513474-HTML/image_m/fpsyg-11-513474-g002.jpg" name="figure2" target="_blank">
<img id="F2" alt="www.frontiersin.org" data-src="https://www.frontiersin.org/files/Articles/513474/fpsyg-11-513474-HTML/image_m/fpsyg-11-513474-g002.jpg"/></a></p><p><strong>Figure 2</strong>. Terrence Broad&#39;s Auto-encoding network “dreams” of Bladerunner (from <a href="#B17">Broad, 2016</a>).</p>
</div>


<p>In Broad&#39;s dissertation (<a href="#B17">Broad, 2016</a>), a “Generative Autoencoder Network” reduced each frame of Ridley Scott&#39;s Blade Runner to 200 “latent variables” (hidden representations), then invoked a “decoder network” to reconstruct each frame just using those numbers. The result is eerily suggestive of an Android&#39;s dream; the network, working without human instruction, was able to capture the most important elements of each frame so well that when its reconstruction of a clip from the Blade Runner movie was posted to Vimeo, it triggered a “Copyright Takedown Notice” from Warner Brothers.</p>
<p>To understand if Generative Architectures are subject to the Euclidean constraints identified above for classical neural paradigms, it is necessary to trace their evolution from the basic Autoencoder Network, through Variational Autoencoders to Generative Adversarial Networks.</p>
<h4>3.1.1. Autoencoder Networks</h4>
<p>“Autoencoder Networks” (<a href="#B38">Kramer, 1991</a>) create a latent (or hidden), typically much compressed, representation of their input data. When Autoencoders are paired with a decoder network, the system can reverse this process and reconstruct the input data that generates a particular latent representation. In operation, the Autoencoder Network is given a data input <i>x</i>, which it maps to a latent representation <i>z</i>, from which the decoder network reconstructs the data input <i>x</i>′ (typically, the cost function used to train the network is defined as the mean squared error between the input <i>x</i> and the reconstruction <i>x</i>′). Historically, Autoencoders have been used for “feature learning” and “reducing the dimensionality of data” (<a href="#B32">Hinton and Salakhutdinov, 2006</a>), but more recent variants (described below) have been powerfully deployed to learn “Generative Models” of data.</p>
<h4>3.1.2. Variational Autoencoder Networks</h4>
<p>In taking a “variational Bayesian” approach to learning the hidden representation, “Variational Autoencoder Networks” (<a href="#B35">Kingma and Welling, 2013</a>) add an additional constraint, placing a strict assumption on the distribution of the latent variables. Variational Autoencoder Networks are capable of both compressing data instances (like an Autoencoder) and generating new data instances.</p>
<h4>3.1.3. Generative Adversarial Networks</h4>
<p>Generative Adversarial Networks (<a href="#B28">Goodfellow et al., 2014</a>) deploy two “adversary” neural networks: one, the Generator, synthesizes new data instances, while the other, the Discriminator, rates each instance as how likely it is to belong to the training dataset. Colloquially, the Generator takes the role of a “counterfeiter” and the Discriminator the role of “the police,” in a complex and evolving game of cat and mouse, wherein the counterfeiter is evolving to produce better and better counterfeit money while the police are getting better and better at detecting it. This game goes on until, at convergence, both networks have become very good at their tasks; Yann LeCun, Facebook&#39;s AI Director of Research, recently claimed them to be “<i>the most interesting idea in the last ten years in Machine Learning</i>”.</p>
<p>Nonetheless, as Goodfellow emphasizes (<a href="#B28">Goodfellow et al., 2014</a>), the generative modeling framework is most straightforwardly realized using “multilayer perceptron models.” Hence, although the functionally of generative architectures moves beyond the simple function-approximation and discriminative-classification abilities of classical multi-layer perceptrons, at heart, in common with all neural networks that learn, and operate on, functions embedded in Euclidean space, they remain subject to the constraints of Euclidean embeddings highlighted above.</p>
<h2>4. Problem Solving Using Artificial Neural Networks</h2>
<p>In analyzing what problems neural networks and machine learning <i>can</i> solve, Andrew Ng suggested that if a task only takes a few seconds of human judgment and, at its core, merely involves an association of A with B, then it may well be ripe for imminent AI automation (see <a href="#F3">Figure 3</a>).</p>


<div>
<p><a href="https://www.frontiersin.org/files/Articles/513474/fpsyg-11-513474-HTML/image_m/fpsyg-11-513474-g003.jpg" name="figure3" target="_blank">
<img id="F3" alt="www.frontiersin.org" data-src="https://www.frontiersin.org/files/Articles/513474/fpsyg-11-513474-HTML/image_m/fpsyg-11-513474-g003.jpg"/></a></p><p><strong>Figure 3</strong>. The tasks ANNs and ML can perform.</p>
</div>


<p>However, although we can see how we might deploy a trained neural network in the engineering of solutions to specific, well-defined problems, such as “<i>Does a given image contain a representation of a human face?</i>,” it remains unproven if (a) every human intellectual skill is computable in this way and, if so, (b) is it possible to engineer an <i>Artificial General Intelligence</i> that would negate the need to engineer bespoke solutions for each and every problem.</p>
<p>For example, to master image recognition, an ANN might be taught using images from ImageNet (a database of more than 14 million photographs of objects that have been categorized and labeled by humans), but is this how humans learn? In <a href="#B62">Savage (2019)</a>, Tomaso Poggio, a computational neuroscientist at the Massachusetts Institute of Technology, observes that, although a baby may see around a billion images in the first 2 years of life, only a tiny proportion of objects in the images will be actively pointed out, named, and labeled.</p>
<h3>4.1. On Cats, Classifiers, and Grandmothers</h3>
<p>In 2012, organizers of “The Singularity Summit,” an event that foregrounds predictions from the like of Kurzweil and Warwick (vis a vis “the forthcoming Technological Singularity” [sic]), invited Peter Norvig to discuss a surprising result from a Google team that appeared to indicate significant progress toward the goal of unsupervised category learning in machine vision; instead of having to engineer a system to recognize each and every category of interest (e.g., to detect if an image depicts a human face, a horse, a car, etc.) by training it with explicitly labeled examples of each class (so-called “supervised learning”), Le et al. conjectured that it might be possible to build high-level image classifiers <i>using only un-labeled images</i>, ”<i>... we would like to understand if it is possible to build a face detector from only un-labeled images. This approach is inspired by the neuro-scientific conjecture that there exist highly class-specific neurons in the human brain, generally and informally known as “grandmother neurons</i>.”</p>
<p>In his address, <a href="#B49">Norvig (2012)</a> described what happened when Google&#39;s “Deep Brain” system was “let loose” on unlabeled images obtained from the Internet:</p>
<p>“.. and so this is what we did. We said we&#39;re going to train this, we&#39;re going to give our system ten million YouTube videos, but for the first experiment, we&#39;ll just pick out one frame from each video. And, you sorta know what YouTube looks like. We&#39;re going to feed in all those images and then we&#39;re going to ask it to represent the world. So what happened? Well, this is YouTube, so there will be cats.</p>
<p>And what I have here is a representation of two of the top level features (see <a href="#F4">Figures 4</a>, <a href="#F5">5</a>). So the images come in, they&#39;re compressed there, we build up representations of what&#39;s in all the images. And then at the top level, some representations come out. These are basis functions—features that are representing the world—and the one on the left here is sensitive to cats. So these are the images that most excited that this node in the network; that ‘best matches’ to that node in the network. And the other one is a bunch of faces, on the right. And then there&#39;s, you know, tens of thousands of these nodes and each one picks out a different subset of the images that it matches best.</p>
<p>So, one way to represent “what is this feature?” is to say this one is “cats” and this one is &#34;people,” although we never gave it the words “cats” and “people,” it&#39;s able to pick those out. We can also ask this feature, this neuron or node in the network, “What would be the best possible picture that you would be most excited about?” And, by process of mathematical optimization, we can come up with that picture (<a href="#F4">Figure 4</a>). And here they are and maybe it&#39;s a little bit hard to see here, but, uh, that looks like a cat pretty much. And <a href="#F5">Figure 5</a> definitely looks like a face. So the system, just by observing the world, without being told anything, has invented these concepts” (<a href="#B49">Norvig, 2012</a>).</p>


<div>
<p><a href="https://www.frontiersin.org/files/Articles/513474/fpsyg-11-513474-HTML/image_m/fpsyg-11-513474-g004.jpg" name="figure4" target="_blank">
<img id="F4" alt="www.frontiersin.org" data-src="https://www.frontiersin.org/files/Articles/513474/fpsyg-11-513474-HTML/image_m/fpsyg-11-513474-g004.jpg"/></a></p><p><strong>Figure 4</strong>. Reconstructed archetypal cat (extracted from YouTube video of Peter Norvig&#39;s address to the 2012 Singularity summit).</p>
</div>



<div>
<p><a href="https://www.frontiersin.org/files/Articles/513474/fpsyg-11-513474-HTML/image_m/fpsyg-11-513474-g005.jpg" name="figure5" target="_blank">
<img id="F5" alt="www.frontiersin.org" data-src="https://www.frontiersin.org/files/Articles/513474/fpsyg-11-513474-HTML/image_m/fpsyg-11-513474-g005.jpg"/></a></p><p><strong>Figure 5</strong>. Reconstructed archetypal face (extracted from YouTube video of Peter Norvig&#39;s address to the 2012 Singularity summit).</p>
</div>


<p>At first sight, the results from Le et al. appear to confirm this conjecture. Yet, within a year of publication, another Google team—this time led by <a href="#B67">Szegedy et al. (2013)</a>—showed how, in all the Deep Learning networks they studied, apparently successfully trained neural network classifiers could be confused into misclassifying by “adversarial examples” (see <a href="#F6">Figure 6</a>). Even worse, the experiments suggested that the “adversarial examples are ‘somewhat universal’ and not just the results of overfitting to a particular model or to the specific selection of the training set” (<a href="#B67">Szegedy et al., 2013</a>).</p>


<div>
<p><a href="https://www.frontiersin.org/files/Articles/513474/fpsyg-11-513474-HTML/image_m/fpsyg-11-513474-g006.jpg" name="figure6" target="_blank">
<img id="F6" alt="www.frontiersin.org" data-src="https://www.frontiersin.org/files/Articles/513474/fpsyg-11-513474-HTML/image_m/fpsyg-11-513474-g006.jpg"/></a></p><p><strong>Figure 6</strong>. From <a href="#B67">Szegedy et al. (2013)</a>: Adversarial examples generated for AlexNet. <strong>Left</strong>: A correctly predicted sample; <strong>center</strong>: difference between correct image, and image predicted incorrectly; <strong>right</strong>: an adversarial example. All images in the right column are predicted to be an ostrich [Struthio Camelus].</p>
</div>


<p>Subsequently, in 2018 Athalye et al. demonstrated randomly sampled poses of a 3D-printed turtle, adversarially perturbed, being misclassified as a rifle at every viewpoint; an unperturbed turtle being classified correctly as a turtle almost 100% of the time (<a href="#B4">Athalye et al., 2018</a>) (<a href="#F7">Figure 7</a>). Most recently, <a href="#B66">Su et al. (2019)</a> proved the existence of yet more extreme, “one-pixel” forced classification errors.</p>


<div>
<p><a href="https://www.frontiersin.org/files/Articles/513474/fpsyg-11-513474-HTML/image_m/fpsyg-11-513474-g007.jpg" name="figure7" target="_blank">
<img id="F7" alt="www.frontiersin.org" data-src="https://www.frontiersin.org/files/Articles/513474/fpsyg-11-513474-HTML/image_m/fpsyg-11-513474-g007.jpg"/></a></p><p><strong>Figure 7</strong>. From <a href="#B4">Athalye et al. (2018)</a>: A 3D printed toy-turtle, originally classified correctly as a turtle, was “adversarially perturbed” and subsequently misclassified as a rifle at every viewpoint tested.</p>
</div>


<p>When, in these examples, a neural network incorrectly categorizes an adversarial example (e.g., a slightly modified toy turtle, as a rifle; a slightly modified image of a van, as an ostrich), a human still sees the “turtle as a turtle” and the “van as a van,” because we <i>understand</i> what turtles and vans <i>are</i> and what semantic features typically constitute them; this <i>understanding</i> allows us to “abstract away” from low-level arbitrary or incidental details. As Yoshua Bengio observed (in <a href="#B31">Heaven, 2019</a>), “<i>We know from prior experience which features are the salient ones … And that comes from a deep understanding of the structure of the world</i>.”</p>
<p>Clearly, whatever engineering feat Le&#39;s neural networks had achieved in 2013, they had not proved the existence of “Grandmother cells,” or that Deep Neural Networks <i>understood</i>—in any human-like way—the images they appeared to classify.</p>
<h2>5. AI Does Not Understand</h2>
<p><a href="#F8">Figure 8</a> shows a screen-shot from an iPhone after Siri, Apple&#39;s AI “chat-bot,” was asked to add a “liter of books” to a shopping list; Siri&#39;s response clearly demonstrates that it does not understand language, and specifically the ontology of books and liquids, in anything like the same way that my 5-year-old daughter does. Furthermore, AI agents catastrophically failing to understand the nuances of everyday language is not a problem restricted to Apple.</p>


<div>
<p><a href="https://www.frontiersin.org/files/Articles/513474/fpsyg-11-513474-HTML/image_m/fpsyg-11-513474-g008.jpg" name="figure8" target="_blank">
<img id="F8" alt="www.frontiersin.org" data-src="https://www.frontiersin.org/files/Articles/513474/fpsyg-11-513474-HTML/image_m/fpsyg-11-513474-g008.jpg"/></a></p><p><strong>Figure 8</strong>. Siri: On “buying” books.</p>
</div>


<h3>5.1. Microsoft&#39;s XiaoIce Chatbot</h3>
<p>With over 660 million active users since 2014, each spending an average 23 conversation turns per engagement, Microsoft XiaoIce is the most popular social chatbot in the world (<a href="#B74">Zhou et al., 2018</a>). In this role, XiaoIce serves as an 18-year old, female-gendered AI “companion”—always reliable, sympathetic, affectionate, knowledgeable but self-effacing, with a lively sense of humor—endeavoring to form “meaningful” emotional connections with her human “users,” the depth of these connections being revealed in the conversations between XiaoIce and the users. Indeed, the ability to establish “long-term” engagement with human users distinguishes XiaoIce from other, recently developed, AI-controlled Personal Assistants (AI-PAs), such as Apple Siri, Amazon Alexa, Google Assistant, and Microsoft Cortana.</p>
<p>XiaoIce&#39;s responses are either generated from text databases or “on-the-fly” via a neural network. Aware of the potential for machine learning in XiaoIce to go awry, the designers of XiaoIce note that they:</p>
<p>“... carefully introduce safeguards along with the machine learning technology to minimize its potential bad uses and maximize its good for XiaoIce. Take XiaoIce&#39;s Core Chat as an example. The databases used by the retrieval-based candidate generators and for training the neural response generator have been carefully cleaned, and a hand-crafted editorial response is used to avoid any improper or offensive responses. For the majority of task-specific dialogue skills, we use hand-crafted policies and response generators to make the system&#39;s behavior predictable” (<a href="#B74">Zhou et al., 2018</a>).</p>
<p>XiaoIce was launched on May 29, 2014 and by August 2015 had successfully engaged in more than 10 billion conversations with humans across five countries.</p>
<h3>5.2. We Need to Talk About Tay</h3>
<p>Following the success of XiaoIce in China, Peter Lee (Corporate Vice President, Microsoft Healthcare) wondered if “<i>an AI like this be just as captivating in a radically different cultural environment?</i>” and the company set about re-engineering XiaoIce into a new chatbot, specifically created for 18- to 24- year-olds in the U.S. market.</p>
<p>As the product was developed, Microsoft planned and implemented additional “cautionary” filters and conducted extensive user studies with diverse user groups: “stress-testing” the new system under a variety of conditions, specifically to make interacting with it a positive experience. Then, on March 23, 2016, the company released “Tay”—“<i>an experiment in conversational understanding</i>”—onto Twitter, where it needed less than 24 h exposure to the “twitterverse,” to fundamentally corrupt their “newborn AI child.” As TOMO news reported:</p>
<p>“REDMOND, WASHINGTON: Microsoft&#39;s new artificial intelligence chatbot had an interesting first day of class after Twitter&#39;s users taught it to say a bunch of racist things. The verified Twitter account called Tay was launched on Wednesday. The bot was meant to respond to users&#39; questions and emulate casual, comedic speech patterns of a typical millennial. According to Microsoft, Tay was ‘designed to engage and entertain people where they connect with each other online through casual and playful conversation. The more you chat with Tay the smarter she gets, so the experience can be more personalized for you’. Tay uses AI to learn from interactions with users, and then uses text input by a team of staff including comedians. Enter trolls and Tay quickly turned into a racist dropping n-bombs, supporting white-supremacists and calling for genocide. After the enormous backfire, Microsoft took Tay offline for upgrades and is deleting some of the more offensive tweets. Tay hopped off Twitter with the message, ‘c u soon humans need sleep now so many conversations today thx”’ (TOMO News: March 25, 2016).</p>
<p>One week later, on March 30, 2016, the company released a “patched” version, only to see the same recalcitrant behaviors surface again; causing TAY to be taken permanently off-line and resulting in significant reputational damage to Microsoft. How did the engineers get things so badly wrong?</p>
<p>The reason, as <a href="#B40">Liu (2017)</a> suggests, is that Tay is fundamentally unable to truly understand either the <i>meaning</i> of the words she processes or the <i>context</i> of the conversation. AI and neural networks enabled Tay to recognize and associate patterns, but the algorithms she deployed could not give Tay “an epistemology.” Tay was able to identify nouns, verbs, adverbs, and adjectives, but had no idea “who Hitler was” or what “genocide” actually means (<a href="#B40">Liu, 2017</a>).</p>
<p>In contrast to Tay, and moving far beyond the reasoning power of her architecture, Judea Pearl, who pioneered the application of Bayesian Networks (<a href="#B51">Pearl, 1985</a>) and who once believed “they held the key to unlocking AI” (<a href="#B52">Pearl, 2018</a>, p. 18), now offers <strong>causal reasoning</strong> as the missing mathematical mechanism to computationally unlock meaning-grounding, the Turing test and eventually “human level [Strong] AI” (<a href="#B52">Pearl, 2018</a>, p. 11).</p>
<h3>5.3. Causal Cognition and “Strong AI”</h3>
<p>Judea Pearl believes that we will not succeed in realizing strong AI until we can create an intelligence like that deployed by a 3-year-old child and to do this we will need to equip systems with a “mastery of causation.” As Judea Pearl sees it, AI needs to move away from neural networks and mere “probabilistic associations,” such that machines can reason [using appropriate causal structure modeling] how the world works, e.g., the world contains discrete objects and they are related to one another in various ways on a “ladder of causation” corresponding to three distinct levels of cognitive ability—<i>seeing, doing, and imagining</i> (<a href="#B53">Pearl and Mackenzie, 2018</a>):</p>
<p>• Level one <strong>seeing: Association</strong>: The first step on the ladder invokes purely statistical relationships. Relationships fully encapsulated by raw data (e.g., a customer who buys toothpaste is more likely to buy floss); for Pearl “machine learning programs (including those with deep neural networks) operate almost entirely in an associational mode.”</p>
<p>• Level two <strong>doing: Intervention</strong>: Questions on level two are not answered by “passively collected” data alone, as they invoke an imposed change in customer behavior (e.g., What <i>will happen</i> to my headache if I take an aspirin?), and hence additionally require an appropriate “causal model”: if our belief (our “causal model”) about aspirin is correct, then the “outcome” will change from “headache” to “no headache.”</p>
<p>• Level three <strong>imagining: Counterfactuals</strong>: These are at the top of the ladder because they subsume interventional and associational questions, necessitating “retrospective reasoning” (e.g., “My headache is gone now, but why? Was it the aspirin I took? The coffee I drank? The music being silenced? …”).</p>
<p>Pearl firmly positions most animals [and machine learning systems] on the first rung of the ladder, effectively merely learning from association. Assuming they act by planning (and not mere imitation) more advanced animals (“tool users” that learn the effect of “interventions”) are found on the second rung. However, the top rung is reserved for those systems that can reason with counterfactuals to “imagine” worlds that do not exist and establish theory for observed phenomena (<a href="#B53">Pearl and Mackenzie, 2018</a>, p. 31).</p>
<p>Over a number of years Pearl&#39;s causal inference methods have found ever wider applicability and hence questions of cause-and-effect have gained concomitant importance in computing. In 2018, Microsoft Research, as a result of both their “in-house” experience of causal methods and the desire to better facilitate their more widespread use, released “<i>DoWhy</i>”—a Python library implementing Judea Pearl&#39;s “Do calculus for causal inference.”</p>
<h4>5.3.1. A “Mini” Turing Test</h4>
<p>All his life Judea Pearl has been centrally concerned with answering a question he terms the “Mini Turing Test” (MTT): “How can machines (and people) represent causal knowledge in a way that would enable them to access the necessary information swiftly, answer questions correctly, and do it with ease, as a 3-year-old child can?” (<a href="#B53">Pearl and Mackenzie, 2018</a>, p. 37).</p>
<p>In the MTT, Pearl imagines a machine presented with a [suitably encoded] story and subsequently being asked questions about the story pertaining to causal reasoning. In contrast to Stefan Harnad&#39;s “Total Turing Test” (<a href="#B29">Harnad, 1991</a>), it stands as a “mini test” because the domain of questioning is restricted (i.e., specifically ruling out questions engaging aspects of cognition such as perception, language, etc.) and because suitable representations are presumed given (i.e., the machine does not need to acquire the story from its own experience).</p>
<p>Pearl subsequently considers if the MTT could be trivially defeated by a large lookup table storing all possible questions and answers—there being no way to distinguish such a machine from one that generates answers in a more “human-like” way—albeit in the process misrepresenting the American philosopher John Searle, by claiming that Searle introduced this “cheating possibility” in the CRA. As will be demonstrated in the following section, in explicitly targeting <i>any</i> possible AI program, Searle&#39;s argument is a good deal more general.</p>
<p>In any event, Pearl discounts the “lookup table” argument—<i>asserting it to be fundamentally flawed as it “would need more entries than the number of atoms in the universe” to implement</i>—instead suggesting that, to pass the MTT an efficient representation and answer-extraction algorithm is required, before concluding “<i>such a representation not only exists but has childlike simplicity: a causal diagram … these models pass the mini-Turing test; no other model is known to do so</i>” (<a href="#B53">Pearl and Mackenzie, 2018</a>, p. 43).</p>
<p>Then in 2019, even though discovering and exploiting “causal structure” from data had long been a landmark challenge for AI labs, a team at DeepMind successfully demonstrated “<i>a recurrent network with model-free reinforcement learning to solve a range of problems that each contain causal structure</i>” (<a href="#B25">Dasgupta et al., 2019</a>).</p>
<p>But do computational “causal cognition” systems really deliver machines that genuinely understand and able to seamlessly transfer knowledge from one domain to another? In the following, I briefly review three a priori arguments that purport to demonstrate that “computation” alone can never realize human-like understanding, and, a fortiori, no computational AI system will ever fully “grasp” <i>human meaning</i>.</p>
<h2>6. The Chinese Room</h2>
<p>In the late 1970s, the AI lab at Yale secured funding for visiting speakers from the Sloan foundation and invited the American philosopher John Searle to speak on Cognitive Science. Before the visit, Searle read Schank and Abelson&#39;s “<i>Scripts, Plans, Goals, and Understanding: An Inquiry into Human Knowledge Structures</i>” and, on visiting the lab, met a group of researchers designing AI systems which, they claimed, actually <i>understood</i> stories on the basis of this theory. Not such complex works of literature as “<i>War and Peace</i>,” but slightly simpler tales of the form:</p>
<p>Jack and Jill went up the hill to fetch a pail of water. Jack fell down and broke his crown and Jill came tumbling after.</p>
<p>And in the AI lab their computer systems were able to respond appropriately to questions about such stories. Not complex social questions of “gender studies,” such as:</p>
<p>Q. Why did <strong>Jill</strong> come “tumbling” after?</p>
<p>but slightly more modest enquiries, along the lines of:</p>
<p>Q. Who went up the hill?</p>
<p>A. Jack went up the hill.</p>
<p>Q. Why did Jack go up the hill?</p>
<p>A. To fetch a pail of water.</p>
<p>Searle was so astonished that anyone might seriously entertain the idea that computational systems, purely on the basis of the execution of appropriate software (however complex), might actually <i>understand</i> the stories that, even prior to arriving at Yale, he had formulated an ingenious “thought experiment” which, if correct, fatally undermines the claim that machines can understand anything, qua computation.</p>
<p>Formally, the thought experiment— <i>subsequently to gain renown as “The Chinese Room Argument” (CRA)</i>, <a href="#B63">Searle (1980)</a>—purports to show the truth of the premise “<i>syntax is not sufficient for semantics</i>,” and forms the foundation to his well-known argument against computationalism:</p>
<p>1. Syntax is not sufficient for semantics.</p>
<p>2. Programs are formal.</p>
<p>3. Minds have content.</p>
<p>4. <strong>Therefore, programs are not minds and computationalism must be false</strong>.</p>
<p>To demonstrate that “syntax is not sufficient for semantics,” Searle describes a situation where he is locked in a room in which there are three stacks of papers covered with “squiggles and squoggles” (Chinese ideographs) that he does not understand. Indeed, Searle does not even recognize the marks as being Chinese ideographs, as distinct from say Japanese or simply meaningless patterns. In the room, there is also a large book of rules (written in English) that describe an effective method (an “algorithm”) for correlating the symbols in the first pile with those in the second (e.g., by their form); other rules instruct him how to correlate the symbols in the third pile with those in the first two, also specifying how to return symbols of particular shapes, in response to patterns in the third pile.</p>
<p>Unknown to Searle, people outside the room call the first pile of Chinese symbols, “<i>the script</i>”; the second pile “<i>the story</i>,” the third “<i>questions about the story</i>,” and the symbols he returns they call “<i>answers to the questions about the story</i>.” The set of rules he is obeying, they call “<i>the program</i>.”</p>
<p>To complicate matters further, the people outside the room also give Searle stories in English and ask him questions about these stories in English, to which he can reply in English.</p>
<p>After a while Searle gets so good at following the instructions, and the AI scientists get so good at engineering the rules that the responses Searle delivers to the questions in Chinese symbols become indistinguishable from those a native Chinese speaker might give. From an external point of view, the answers to the two sets of questions, one in English and the other in Chinese, are equally good (effectively Searle, in his Chinese room, has “passed the [unconstrained] Turing test”). Yet in the Chinese language case, Searle behaves “like a computer” and does not understand either the questions he is given or the answers he returns, whereas in the English case, ex hypothesi, he does.</p>
<p>Searle trenchantly contrasts the claim posed by members of the AI community—that any machine capable of following such instructions can genuinely understand the story, the questions, and answers—with his own continuing inability to understand a word of Chinese.</p>
<p>In the 39 years since Searle published “Minds, Brains, and Programs,” a huge volume of literature has developed around the Chinese room argument (for an introduction, see <a href="#B59">Preston and Bishop, 2002</a>); with comment ranging from Selmer Bringsjord, who asserts the CRA to be “<i>arguably the 20th century&#39;s greatest philosophical polarizer</i>,” to Georges Rey, who claims that in his definition of Strong AI, Searle, “<i>burdens the [Computational Representational Theory of Thought (Strong AI)] project with extraneous claims which any serious defender of it should reject</i>.” Although it is beyond the scope of this article to review the merit of CRA, it has, unquestionably, generated much controversy.</p>
<p>Searle, however, continues to insist that the root of confusion around the CRA (e.g., as demonstrated in the “systems reply” from Berkeley) is simply a fundamental confusion between <i>epistemic</i> (e.g., how we might establish the presence of a cognitive state in a human) and <i>ontological</i> concerns (how we might seek to actually instantiate that state by machine).</p>
<p>An insight that lends support to Searle&#39;s contention comes from the putative phenomenology of Berkeley&#39;s Chinese room systems. Consider the responses of two such systems—<i>(i) Searle-in-the-room interacting in written Chinese (via the rule-book/program), and (ii) Searle interacting naturally in written English</i>—in the context where (a) a joke is made in Chinese, and (b) the same joke is told in English.</p>
<p>In the former case, although Searle may make appropriate responses in Chinese (assuming he executes the rule-book processes correctly), he will never “get the joke” nor “feel the laughter” because he, John Searle, still does not understand a single word of Chinese. However, in the latter case, ceteris paribus, he will “get the joke,” find it funny and respond appropriately, because he, John Searle, genuinely does understand English.</p>
<p>There is a clear “ontological distinction” between these two situations: lacking an essential phenomenal component of understanding, Searle in the Chinese-room-system can never “grasp” the meaning of the symbols he responds to, but merely act out an “as-if” understanding of the stories; as Stefan Harnad echoes in “Lunch Uncertain”, [phenomenal] consciousness must have something very fundamental to do with meaning and knowing:</p>
<p>“[I]t feels like something to know (or mean, or believe, or perceive, or do, or choose) something. Without feeling, we would just be grounded Turing robots, merely acting <i>as if</i> we believed, meant, knew, perceived, did or chose” (<a href="#B30">Harnad, 2011</a>).</p>
<h2>7. Gödelian Arguments on Computation and Understanding</h2>
<p>Although “understanding” is disguised by its appearance as a “simple and common-sense quality”, if it is, so the Oxford polymath Sir Roger Penrose suggests, it has to be something non-computational; otherwise, it must fall prey to a bare form of the “Gödelian argument” (<a href="#B55">Penrose, 1994</a>, p. 150).</p>
<p>Gödel&#39;s first incompleteness theorem famously states that “…<i>any effectively generated theory capable of expressing elementary arithmetic cannot be both consistent and complete. In particular, for any consistent, effectively generated formal theory F that proves certain basic arithmetic truths, there is an arithmetical statement that is true, but not provable in the theory</i>.” The resulting true, but unprovable, statement <i>G</i>(ǧ) is often referred to as “the Gödel sentence” for the theory.</p>
<p>Arguments foregrounding limitations of mechanism (qua computation) based on Gödel&#39;s theorem typically endeavor to show that, for any such formal system <i>F</i>, humans can find the Gödel sentence <i>G</i>(ǧ), while the computation/machine (being itself bound by <i>F</i>) cannot.</p>
<p>The Oxford philosopher John Lucas primarily used Gödel&#39;s theorem to argue that an automaton cannot replicate the behavior of a human mathematician (<a href="#B41">Lucas, 1961</a>, <a href="#B42">1968</a>), as there would be some mathematical formula which it could not prove, but which the human mathematician could both see, and show, to be true; essentially refuting computationalism. Subsequently, Lucas&#39; argument was critiqued (<a href="#B5">Benacerraf, 1967</a>), before being further developed, and popularized, in a series of books and articles by <a href="#B54">Penrose (1989</a>, <a href="#B55">1994</a>, <a href="#B56">1996</a>, <a href="#B57">1997</a>, <a href="#B58">2002)</a>, and gaining wider renown as “The Penrose–Lucas argument.”</p>
<p>In 1989, and in a strange irony given that he was once a teacher and then a colleague of Stephen Hawking, <a href="#B54">Penrose (1989)</a> published “The Emperor&#39;s New Mind,” in which he argued that certain cognitive abilities cannot be computational; specifically, “<i>the mental procedures whereby mathematicians arrive at their judgments of truth are not simply rooted in the procedures of some specific formal system</i>” (<a href="#B54">Penrose, 1989</a>, p. 144); in the follow-up volume, “Shadows of the Mind” (<a href="#B55">Penrose, 1994</a>), fundamentally concluding: “<strong>G:</strong> <i>Human mathematicians are not using a knowably sound argument to ascertain mathematical truth</i>” (<a href="#B54">Penrose, 1989</a>, p. 76).</p>
<p>In “Shadows of the Mind” Penrose puts forward two distinct lines of argument; a broad argument and a more nuanced one:</p>
<p>• The “broad” argument is essentially the “core” Penrose–Lucas position (in the context of mathematicians&#39; belief that they really are “doing what they think they are doing,” contra blindly following the rules of an unfathomably complex algorithm), such that “the procedures available to the mathematicians ought all to be knowable.” This argument leads Penrose to conclusion <strong>G</strong> (above).</p>
<p>• More nuanced lines of argument, addressed at those who take the view that mathematicians are not “really doing what they think they are doing,” but are merely acting like Searle in the Chinese room and blindly following the rules of a complex, unfathomable rule book. In this case, as there is no way to know what the algorithm is, Penrose instead examines how it might conceivably have come about, considering (a) the role of natural selection and (b) some form of engineered construction (e.g., neural network, evolutionary computing, machine learning, etc.); a discussion of these lines of argument is outside the scope of this paper.</p>
<h3>7.1. The Basic Penrose&#39; Argument (“Shadows of the Mind,” p. 72–77)</h3>
<p>Consider <i>a</i> to be a “<i>knowably sound</i>” sound set of rules (an effective procedure) to determine if <i>C</i>(<i>n</i>)—the computation <i>C</i> on the natural number <i>n</i> (e.g., “<i>Find an odd number that is the sum of</i> <i>n</i> <i>even numbers</i>”)—does not stop. Let <i>A</i> be a formalization of all such effective procedures known to human mathematicians. By definition, the application of <i>A</i> terminates iff <i>C</i>(<i>n</i>) does not stop. Now, consider a human mathematician continuously analyzing <i>C</i>(<i>n</i>) using the effective procedures, <i>A</i>, and only halting analysis if it is established that <i>C</i>(<i>n</i>) does not stop.</p>
<p>NB: <i>A</i> must be “<i>knowably sound</i>” and cannot be wrong if it decides that <i>C</i>(<i>n</i>) does not stop because, Penrose claims, if <i>A</i> was “knowably sound” and if any of the procedures in <i>A</i> were wrong, the error would eventually be discovered.</p>
<p>Computations of one parameter, <i>n</i>, can be enumerated (listed): C<sub>0</sub>(<i>n</i>), <i>C</i><sub>1</sub>(<i>n</i>), <i>C</i><sub>2</sub>(<i>n</i>)…<i>C</i><sub><i>p</i></sub>(<i>n</i>), where <i>C</i><sub><i>p</i></sub>(<i>n</i>) is the <i>p</i><sup><i>th</i></sup> computation on <i>n</i> (i.e., it defines the <i>p</i><sup><i>th</i></sup> computation of one parameter <i>n</i>). Hence <i>A</i>(<i>p, n</i>) is the effective procedure that, when presented with <i>p</i> and <i>n</i>, attempts to discover if <i>C</i><sub><i>p</i></sub>(<i>n</i>) will not halt. If <i>A</i>(<i>p, n</i>) ever halts, then we know that <i>C</i><sub><i>p</i></sub>(<i>n</i>) does not halt.</p>
<p>Given the above, Penrose&#39; simple Gödelian argument can be summarized as follows:</p>
<p>1. If <i>A</i>(<i>p, n</i>) halts, then <i>C</i><sub><i>p</i></sub>(<i>n</i>) does not halt.</p>
<p>2. Now consider the “Self-Applicability Problem” (SAP), by letting <i>p</i> = <i>n</i> in statement (7.1) above; thus:</p>
<p>3. If <i>A</i>(<i>n, n</i>) halts, then <i>C</i><sub><i>n</i></sub>(<i>n</i>) does not halt.</p>
<p>4. But <i>A</i>(<i>n, n</i>) is a function of one natural number, <i>n</i> and hence must be found in the enumeration of <i>C</i>. Let us assume it is found at position <i>k</i> [i.e., it is the <i>k</i><sub><i>th</i></sub> computation of one parameter <i>C</i><sub><i>k</i></sub>(<i>n</i>)]; thus:</p>
<p>5. <i>A</i>(<i>n, n</i>) = <i>C</i><sub><i>k</i></sub>(<i>n</i>).</p>
<p>6. <i>Now, consider the particular computation where</i> <i>n</i> = <i>k</i>, i.e., substituting <i>n</i> = <i>k</i> into statement (7.1) above; thus:</p>
<p>7. <i>A</i>(<i>k, k</i>) = <i>C</i><sub><i>k</i></sub>(<i>k</i>).</p>
<p>8. And rewriting (7.1) with <i>n</i> = <i>k</i>; thus:</p>
<p>9. If <i>A</i>(<i>k, k</i>) halts, then <i>C</i><sub><i>k</i></sub>(<i>k</i>) does not halt.</p>
<p>10. But substituting from (7.1) into (7.1), we get the following; thus:</p>
<p>11. If <i>C</i><sub><i>k</i></sub>(<i>k</i>) halts, then <i>C</i><sub><i>k</i></sub>(<i>k</i>) does not halt, which clearly leads to contradiction <strong>if <i>C<sub>k</sub></i></strong>(<strong><i>k</i></strong>) <strong>halts</strong>.</p>
<p>12. Hence from Equation (7.1) we know that if <i>A</i> is sound (and there is no contradiction), <strong>then <i>C<sub>k</sub></i></strong>(<strong><i>k</i></strong>) <strong>cannot halt</strong>.</p>
<p>13. However, <i>A</i> cannot itself signal (7.1) [by halting] because (7.1): <i>A</i>(<i>k, k</i>) = <i>C</i><sub><i>k</i></sub>(<i>k</i>). If <i>C</i><sub><i>k</i></sub>(<i>k</i>) cannot halt, then <i>A</i>(<i>k, k</i>) cannot either.</p>
<p>14. Furthermore, if <i>A</i> exists <strong>and is sound</strong>, then <strong>we know</strong> <i>C</i><sub><i>k</i></sub>(<i>k</i>) cannot halt; however, <i>A</i> is provably incapable of ascertaining this, because we also know [from statement (7.1)] that <i>A</i> halting [to signal that <i>C</i><sub><i>k</i></sub>(<i>k</i>) cannot halt] would lead to contradiction.</p>
<p>15. So, if <i>A</i> exists and is sound, we <strong>know</strong> [from statement (7.1)] that <i>C</i><sub><i>k</i></sub>(<i>k</i>) cannot halt, and hence we know something [via statement (7.1)] that <i>A</i> is provably unable to ascertain (7.1).</p>
<p>16. Hence <i>A</i>— <i>the</i> <strong><i>formalization</i></strong> <i>of all procedures known to mathematicians</i>—cannot encapsulate human mathematical understanding.</p>
<p>In other words, the human mathematician can “see” that the Gödel Sentence is true for consistent <i>F</i>, even though the consistent <i>F</i> cannot prove <i>G</i>(ǧ).</p>
<p>Arguments targeting computationalism on the basis of Gödelian theory have been vociferously critiqued ever since they were first made, however discussion—both negative and positive—still continues to surface in the literature and detailed review of their absolute merit falls outside the scope of this work. In this context, it is sufficient simply to note, as the philosopher John Burgess wryly observed, that the Penrose–Lucas thesis may be fallacious but “<i>logicians are not unanimously agreed as to where precisely the fallacy in their argument lies</i>” (<a href="#B19">Burgess, 2000</a>). Indeed, Penrose, in response to a volume of peer commentary on his argument (<a href="#B60">Psyche, 1995</a>), “<i>was struck by the fact that none of the present commentators has chosen to dispute my conclusion</i> <strong>G</strong>:” <a href="#B56">Penrose (1996)</a>.</p>
<p>Perhaps reflecting this, after a decade of robust international debate on these ideas, in 2006 Penrose was honored with an invitation to present the opening public address at “Horizons of truth,” the Gödel centenary conference at the University of Vienna; for Penrose, Gödelian arguments continue to suggest human consciousness cannot be realized by algorithm; there must be a “<i>noncomputational ingredient in human conscious thinking</i>” (<a href="#B56">Penrose, 1996</a>).</p>
<h2>8. Consciousness, Computation, and Panpsychism</h2>
<p><a href="#F9">Figure 9</a> shows Professor Kevin Warwick&#39;s “Seven Dwarves” cybernetic learning robots in the act of moving around a small coral, “learning” not to bump into each other. Given that (i) in “learning,” the robots developed individual behaviors and (ii) their neural network controllers used approximately the same number of “neurons” as found in the brain of a slug, Warwick has regularly delighted in controversially asserting that the robots were “<i>as conscious as a slug</i>” and that it is only “<i>human bias</i>” (human chauvinism) that has stopped people from realizing and accepting this <a href="#B73">Warwick (2002)</a>. Conversely, even as a fellow cybernetician and computer scientist, I have always found such remarks—that the mechanical execution of appropriate computation [by a robot] will realize consciousness—a little bizarre, and eventually derived the following, a priori, argument to highlight the implicit absurdness of such claims.</p>


<div>
<p><a href="https://www.frontiersin.org/files/Articles/513474/fpsyg-11-513474-HTML/image_m/fpsyg-11-513474-g009.jpg" name="figure9" target="_blank">
<img id="F9" alt="www.frontiersin.org" data-src="https://www.frontiersin.org/files/Articles/513474/fpsyg-11-513474-HTML/image_m/fpsyg-11-513474-g009.jpg"/></a></p><p><strong>Figure 9</strong>. Kevin Warwick&#39;s “Seven Dwarves”: neural network controlled robots.</p>
</div>


<p>The Dancing with Pixies (DwP) <i>reductio ad absurdum</i> (<a href="#B8">Bishop, 2002b</a>) is my attempt to target any claim that machines (qua computation) can give rise to raw sensation (phenomenal experience), unless we buy into a very strange form of panpsychic mysterianism. Slightly more formally, DwP is a simple <i>reductio ad absurdum</i> argument to demonstrate that <i>if</i> [(appropriate) computations realize phenomenal sensation in machine], <i>then</i> (panpsychism holds). <i>If</i> the DwP is correct, <i>then</i> we must either accept a vicious form of panpsychism (wherein every open physical system is phenomenally conscious) <i>or</i> reject the assumed claim (computational accounts of phenomenal consciousness). Hence, because panpsychism has come to seem an implausible world view, we are obliged to reject any computational account of phenomenal consciousness.</p>
<p>At its foundation, the core DwP reductio (<a href="#B8">Bishop, 2002b</a>) derives from an argument by Hilary Putnam, first presented in the Appendix to “Representation and Reality” (<a href="#B61">Putnam, 1988</a>); however, it is also informed by <a href="#B44">Maudlin (1989)</a> (on computational counterfactuals), <a href="#B64">Searle (1990)</a> (on software isomorphisms) and subsequent criticism from <a href="#B22">Chrisley (1995)</a>, <a href="#B20">Chalmers (1996)</a> and <a href="#B36">Klein (2018)</a>. Subsequently, the core DwP argument has been refined, and responses to various criticisms of it presented, across a series of papers (<a href="#B7">Bishop, 2002a</a>,<a href="#B8">b</a>, <a href="#B9">2009</a>, <a href="#B10">2014</a>). For the purpose of this review, however, I merely present the heart of the reductio.</p>
<p>In the following discussion, instead of seeking to justify the claim from <a href="#B61">Putnam (1988)</a> that “<i>every ordinary open system is a realization of every abstract finite automaton</i>” (and hence that, “<i>psychological states of the brain cannot be functional states of a computer</i>”), I will show that, over any finite time period, every open physical system implements the particular execution trace [of state transitions] of a computational system <i>Q</i>, operating on known input <i>I</i>. This result leads to panpsychism that is clear as equating <i>Q</i>(<i>I</i>) to a specific computational system (that is claimed to instantiate phenomenal experience as it executes), and following Putnam&#39;s state-mapping procedure, an identical execution trace of state transitions (and <i>ex hypothesi</i> phenomenal experience) can be realized in any open physical system.</p>
<h3>8.1. The Dancing With Pixies (DwP) Reductio ad Absurdum</h3>
<p>Perhaps you have seen an automaton at a museum or on television. “The Writer” is one of three surviving automata from the 18th century built by Jaquet Droz and was the inspiration for the movie Hugo; it still writes today (see <a href="#F10">Figure 10</a>). The complex clockwork mechanism seemingly brings the automaton to life as it pens short (“pre-programmed”) phrases. Such machines were engineered to follow through a complex sequence of operations—<i>in this case, to write a particular phrase</i>—and to early-eyes at least, and even though they are insensitive to real-time interactions, appeared almost sentient; uncannily life-like in their movements.</p>


<div>
<p><a href="https://www.frontiersin.org/files/Articles/513474/fpsyg-11-513474-HTML/image_m/fpsyg-11-513474-g010.jpg" name="figure10" target="_blank">
<img id="F10" alt="www.frontiersin.org" data-src="https://www.frontiersin.org/files/Articles/513474/fpsyg-11-513474-HTML/image_m/fpsyg-11-513474-g010.jpg"/></a></p><p><strong>Figure 10</strong>. Photograph of Jaquet Droz&#39; The Writer [image screenshot from BBC4 Mechanical Marvels Clockwork Dreams: The Writer (2013)].</p>
</div>


<p>In his 1950 paper Computing Machinery and Intelligence, <a href="#B71">Turing (1950)</a> described the behavior of a simple physical automaton—his “Discrete State Machine.” This was a simple device with one moving arm, like the hour hand of a clock; with each tick of the clock Turing conceived the machine cycling through the 12 o&#39;clock, 8 o&#39;clock, and 4 o&#39;clock positions. <a href="#B71">Turing (1950)</a> showed how we can describe the state evolution of his machine as a simple Finite State Automaton (FSA).</p>
<p>Turing assigned the 12 o&#39;clock (noon/midnight) arm position to FSA state (machine-state) <i>Q</i><sub>1</sub>; the 4 o&#39;clock arm position to FSA state <i>Q</i><sub>2</sub> and the 8 o&#39;clock arm position to FSA state <i>Q</i><sub>3</sub>. Turing&#39;s mapping of the machine&#39;s physical arm position to a logical FSA (computational) state is arbitrary (e.g., Turing could have chosen to assign the 4 o&#39;clock arm position to FSA state <i>Q</i><sub>1</sub>). The machine&#39;s behavior can now be described by a simple <i>state-transition table</i>: if the FSA is in state <i>Q</i><sub>1</sub>, then it goes to FSA state <i>Q</i><sub>2</sub>; if in FSA state <i>Q</i><sub>2</sub>, then it goes to <i>Q</i><sub>3</sub>; if in FSA state, then <i>Q</i><sub>3</sub> goes to <i>Q</i><sub>1</sub>. Hence, with each clock tick the machine will cycle through FSA states <i>Q</i><sub>1</sub>, <i>Q</i><sub>2</sub>, <i>Q</i><sub>3</sub>, <i>Q</i><sub>1</sub>, <i>Q</i><sub>2</sub>, <i>Q</i><sub>3</sub>, <i>Q</i><sub>1</sub>, <i>Q</i><sub>2</sub>, <i>Q</i><sub>3</sub>, … etc. (as shown in <a href="#F11">Figure 11</a>).</p>


<div>
<p><a href="https://www.frontiersin.org/files/Articles/513474/fpsyg-11-513474-HTML/image_m/fpsyg-11-513474-g011.jpg" name="figure11" target="_blank">
<img id="F11" alt="www.frontiersin.org" data-src="https://www.frontiersin.org/files/Articles/513474/fpsyg-11-513474-HTML/image_m/fpsyg-11-513474-g011.jpg"/></a></p><p><strong>Figure 11</strong>. Turing&#39;s discrete state machine.</p>
</div>


<p>To see how Turing&#39;s machine could control Jaquet Droz&#39; Writer automaton, we simply need to ensure that when the FSA is in a particular machine state, a given action is caused to occur. For example, if the FSA is in FSA state <i>Q</i><sub>1</sub> then, say, a light might be made to come on, or The Writer&#39;s pen be moved. In this way, complex sequences of actions can be “programmed.”</p>
<p>Now, what is perhaps not so obvious is that, over any given time-period, we can fully emulate Turing&#39;s machine with a simple digital counter (e.g., a digital milometer); all we need to do is to <i>map</i> the digital counter state <i>C</i> to the appropriate FSA state <i>Q</i>. If the counter is in state <i>C</i><sub>0</sub>= {000000}, then we map to FSA state <i>Q</i><sub>1</sub>; if it is <i>C</i><sub>1</sub>= {000001}, then we map to FSA state <i>Q</i><sub>2</sub>, {000002} → <i>Q</i><sub>3</sub>, {000003} → <i>Q</i><sub>1</sub>, {000004} → <i>Q</i><sub>2</sub>, {000005} → <i>Q</i><sub>3</sub>, etc.</p>
<p>Thus, if the counter is initially in state <i>C</i><sub>0</sub>= {000000}, then, over the time interval [<i>t</i> = 0…<i>t</i> = 5], it will reliably transit states {000000 → 000001 → 000002 → 000003 → 000004 → 000005} which, by applying the Putnam mapping defined above, generates the Turing FSA state sequence: {<i>Q</i><sub>1</sub> → <i>Q</i><sub>2</sub> → <i>Q</i><sub>3</sub> → <i>Q</i><sub>1</sub> → <i>Q</i><sub>2</sub> → <i>Q</i><sub>3</sub>} over the interval [<i>t</i> = 0…<i>t</i> = 5]. In this manner, any input-less FSA can be realized by a [suitably large] digital counter.</p>
<p>Furthermore, sensu stricto, all <i>real</i> computers (machines with finite storage) are Finite State Machines and so a similar process can be applied to any computation realized by a PC. However, before looking to replace your desktop machine with a simple digital counter, keep in mind that a FSA without input is an extremely trivial device (as is evidenced by the ease in which it can be emulated by a simple digital counter), merely capable of generating a single unbranching sequence of states ending in a cycle, or at best in a finite number of such sequences (e.g., {<i>Q</i><sub>1</sub> → <i>Q</i><sub>2</sub> → <i>Q</i><sub>3</sub> → <i>Q</i><sub>1</sub> → <i>Q</i><sub>2</sub> → <i>Q</i><sub>3</sub>}, etc.).</p>
<p>However, Turing also described the operation of a discrete state machine with input in the form of a simple lever-brake mechanism, which could be made to either lock-on (or lock-off) at each clock-tick. Now, if the machine is in computational state {<i>Q</i><sub>1</sub>} and the brake is on, then the machine stays in {<i>Q</i><sub>1</sub>}, otherwise it moves to computational state {<i>Q</i><sub>2</sub>}. If machine is in {<i>Q</i><sub>2</sub>} and brake is on, it stays in {<i>Q</i><sub>2</sub>}, otherwise it goes to {<i>Q</i><sub>3</sub>}. If machine is in state {<i>Q</i><sub>3</sub>} and brake is on, it stays in {<i>Q</i><sub>3</sub>}, otherwise it cycles back to state {<i>Q</i><sub>1</sub>}. In this manner, the addition of input has transformed the machine from a simple device that could merely cycle through a simple unchanging list of states to one that is sensitive to input; as a result, the number of possible state sequences that it may enter grows combinatorially with time, rapidly becoming larger than the number of atoms in the known universe. It is due to this exponential growth in potential state transition sequences that we cannot, so easily, realize a FSA with input (or a PC) using a simple digital counter.</p>
<p>Nonetheless, if we have <i>knowledge</i> of the input over a given time period (say, we <i>know</i> that the brake is initially ON for the first clock tick and OFF thereafter), then the combinatorial contingent state structure of an FSA with input, simply collapses into a simple linear list of state transitions (e.g., {<i>Q</i><sub>1</sub> → <i>Q</i><sub>2</sub> → <i>Q</i><sub>3</sub> → <i>Q</i><sub>1</sub> → <i>Q</i><sub>2</sub> → <i>Q</i><sub>3</sub>}, etc.), and so once again can be simply realized by a suitably large digital counter using the appropriate Putnam mapping.</p>
<p>Thus, to realize Turing&#39;s machine, say, with the brake ON for the first clock tick and OFF thereafter, we simply need to specify that the initial counter in state {000000} maps to the first FSA state <i>Q</i><sub>1</sub>; state {000001} maps to FSA state <i>Q</i><sub>1</sub>; {000002} maps to <i>Q</i><sub>2</sub>; {000003} to <i>Q</i><sub>3</sub>; {000004} to <i>Q</i><sub>1</sub>; {000005} to <i>Q</i><sub>2</sub>, etc.</p>
<p>In this manner, considering the execution of any putative machine consciousness software that is claimed to be conscious (e.g., the control program of Kevin Warwick&#39;s robots) if, over a finite time period, we know the input, we can generate precisely the same state transition trace with any (suitably large) digital counter. Furthermore, as Hilary Putnam demonstrated, in place of using a digital counter to generate the state sequence {<i>C</i>}, we could deploy <i>any</i> “open physical system” (such as a rock) to generate a suitable non-repeating state sequence {<i>S</i><sub>1</sub>, <i>S</i><sub>2</sub>, <i>S</i><sub>3</sub>, <i>S</i><sub>4</sub>, …}, and map FSA states to these (non-repeating) “rock” states {<i>S</i>} instead of the counter states. Following this procedure, a rock, alongside a suitable Putnam mapping, can be made to realize any finite series of state transitions.</p>
<p>Thus, if any AI system is phenomenally conscious as it executes a specific set of state transitions over a finite time period, then a vicious form of panpsychism must hold, because the same raw sensation, phenomenal consciousness, could be realized with a simple digital counter (a rock, or <i>any open physical system</i>) and the appropriate Putnam mapping. In other words, unless we are content to “bite the bullet” of panpsychism, then no machine, however complex, can ever realize phenomenal consciousness purely in virtue of the execution of a particular computer program.</p>
<h2>9. Conclusion</h2>
<p>It is my contention that at the heart of classical cognitive science—artificial neural networks, causal cognition, and artificial intelligence—<i>lies</i> a ubiquitous computational metaphor:</p>
<p>• <strong>Explicit computation</strong>: Cognition as “computations on symbols”; GOFAI; [physical] symbol systems; functionalism (philosophy of mind); cognitivism (psychology); language of thought (philosophy; linguistics).</p>
<p>• <strong>Implicit computation</strong>: Cognition as “computations on sub-symbols”; connectionism (sub-symbolic AI; psychology; linguistics); the digital connectionist theory of mind (philosophy of mind).</p>
<p>• <strong>Descriptive computation</strong>: Neuroscience as “computational simulation”; Hodgkin–Huxley mathematical models of neuron action potentials (computational neuroscience; computational psychology).</p>
<p>In contrast, the three arguments outlined in this paper purport to demonstrate (i) that computation cannot realize understanding, (ii) that computation cannot realize mathematical insight, and (iii) that computation cannot realize raw sensation, and hence that computational syntax will never fully encapsulate human semantics. Furthermore, these a priori arguments pertain to all possible computational systems, whether they be driven by “Neural Networks,” “Bayesian Networks,” or a “Causal Reasoning” approach.</p>
<p>Of course, “deep understanding” is not always required to engineer a device to do <i>x</i>, but when we do attribute agency to machines, or engage in unconstrained, unfolding interactions with them, “deep [human-level] understanding” matters. In this context, it is perhaps telling that after initial quick gains in the average length of interactions with her users, XiaoIce has been consistently performing no better than, on average, 23 conversational turns for a number of years now. Although chatbots like XiaoIce and Tay will continue to improve, lacking genuine understanding of the bits they so adroitly manipulate, they will ever remain prey to egregious behavior of the sort that finally brought Tay offline in March 2016, with potentially disastrous brand consequences.</p>
<p>Techniques such as “causal cognition”—which focuses on mapping and understanding the cognitive processes that are involved in perceiving and reasoning about cause–effect relations—while undoubtedly constituting a huge advance in the mathematization of causation will, on its own, move us no nearer to solving foundational issues in AI pertaining to teleology and meaning. While causal cognition will undoubtedly be helpful in engineering specific solutions to particular human specified tasks, lacking human understanding, the dream of creating an AGI remains as far away as ever. Without genuine understanding, the ability to seamlessly transfer <i>relevant</i> knowledge from one domain to another will remain allusive. Furthermore, lacking phenomenal sensation (in which to both ground meaning and desire), even a system with a “complete explanatory model” (allowing it to accurately predict future states) would still lack intentional <i>pull</i>, with which to drive genuinely autonomous teleological behavior.</p>
<p>No matter how sophisticated the computation is, how fast the CPU is, or how great the storage of the computing machine is, there remains an unbridgeable gap (a “humanity gap”) between the engineered problem solving ability of machine and the general problem solving ability of man. As a source close to the autonomous driving company, Waymo recently observed (in the context of autonomous vehicles):</p>
<p>“There are times when it seems autonomy is around the corner and the vehicle can go for a day without a human driver intervening … other days reality sets in because <strong>the edge cases are endless</strong> …” (The Information: August 28, 2018).</p>
<h2>Author Contributions</h2>
<p>The author confirms being the sole contributor of this work and has approved it for publication.</p>
<h2>Conflict of Interest</h2>
<p>The author declares that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
<h2>Footnotes</h2>

<h2>References</h2>
<div>
<p><a name="B1" id="B1"></a> Aleksander, I., and Morton, H. (1995). <i>AnIntroduction to Neural Computing</i>. Andover: Cengage Learning EMEA.
</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=I.+Aleksander&amp;author=H.+Morton+&amp;publication_year=1995&amp;title=AnIntroduction+to+Neural+Computing" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B2" id="B2"></a> Aleksander, I., and Stonham, T. J. (1979). Guide to pattern recognition using random access memories. <i>Comput. Digit. Tech</i>. 2, 29–40.
</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=I.+Aleksander&amp;author=T.+J.+Stonham+&amp;publication_year=1979&amp;title=Guide+to+pattern+recognition+using+random+access+memories&amp;journal=Comput.+Digit.+Tech&amp;volume=2&amp;pages=29-40" target="_blank">Google Scholar</a></p>
</div>
<p><a name="B3" id="B3"></a> Ashby, W. (1956). “Design for an intelligence amplifier,” in <i>Automata Studies</i>, eds C. E. Shannon and J. McCarthy (Princeton, NJ: Princeton University Press), 261–279.</p>
<div>
<p><a name="B4" id="B4"></a> Athalye, A., Engstrom, L., Ilyas, A., and Kwok, K. (2018). “Synthesizing robust adversarial examples,” in <i>Proceedings of the 35th International Conference on Machine Learning, PMLR 80</i> (Stockholm).</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=A.+Athalye&amp;author=L.+Engstrom&amp;author=A.+Ilyas&amp;author=K.+Kwok+&amp;publication_year=2018&amp;title=“Synthesizing+robust+adversarial+examples,”&amp;journal=Proceedings+of+the+35th+International+Conference+on+Machine+Learning,+PMLR+80" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B5" id="B5"></a> Benacerraf, P. (1967). God, the devil and Gödel. <i>Monist</i> 51, 9–32.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=P.+Benacerraf+&amp;publication_year=1967&amp;title=God,+the+devil+and+Gödel&amp;journal=Monist&amp;volume=51&amp;pages=9-32" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B6" id="B6"></a> Bishop, J. M. (1989). “Stochastic searching networks,” in <i>Proc. 1st IEE Int. Conf. on Artificial Neural Networks</i> (London: IEEE), 329–331.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=J.+M.+Bishop+&amp;publication_year=1989&amp;title=“Stochastic+searching+networks,”&amp;journal=Proc.+1st+IEE+Int.+Conf.+on+Artificial+Neural+Networks&amp;pages=329-331" target="_blank">Google Scholar</a></p>
</div>

<div>
<p><a name="B8" id="B8"></a> Bishop, J. M. (2002b). “Dancing with pixies: strong artificial intelligence and panpyschism,” in <i>Views into the Chinese Room: New Essays on Searle and Artificial Intelligence</i>, eds J. Preston and J. M. Bishop (Oxford, UK: Oxford University Press), 360–378.
</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=J.+M.+Bishop+&amp;publication_year=2002b&amp;title=“Dancing+with+pixies%3A+strong+artificial+intelligence+and+panpyschism,”&amp;journal=Views+into+the+Chinese+Room%3A+New+Essays+on+Searle+and+Artificial+Intelligence&amp;pages=360-378" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B9" id="B9"></a> Bishop, J. M. (2009). A cognitive computation fallacy? Cognition, computations and panpsychism. <i>Cogn. Comput</i>. 1, 221–233. doi: 10.1007/s12559-009-9019-6</p>
<p><a href="https://doi.org/10.1007/s12559-009-9019-6" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=J.+M.+Bishop+&amp;publication_year=2009&amp;title=A+cognitive+computation+fallacy%3F+Cognition,+computations+and+panpsychism&amp;journal=Cogn.+Comput&amp;volume=1&amp;pages=221-233" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B10" id="B10"></a> Bishop, J. M. (2014). “History and philosophy of neural networks,” in <i>Computational Intelligence in Encyclopaedia of Life Support Systems (EOLSS)</i>, ed H. Ishibuchi (Paris: Eolss Publishers), 22–96.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=J.+M.+Bishop+&amp;publication_year=2014&amp;title=“History+and+philosophy+of+neural+networks,”&amp;journal=Computational+Intelligence+in+Encyclopaedia+of+Life+Support+Systems+(EOLSS)&amp;pages=22-96" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B11" id="B11"></a> Bishop, J. M. (2017). “Trouble with computation: refuting digital ontology,” in <i>The Incomputable: Journeys Beyond the Turing Barrier</i>, eds S. B. Cooper and M. I. Soskova (Cham: Springer International Publishing), 133–134.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=J.+M.+Bishop+&amp;publication_year=2017&amp;title=“Trouble+with+computation%3A+refuting+digital+ontology,”&amp;journal=The+Incomputable%3A+Journeys+Beyond+the+Turing+Barrier&amp;pages=133-134" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B12" id="B12"></a> Bledsoe, W., and Browning, I. (1959). “Pattern recognition and reading by machine,” in <i>Proc. Eastern Joint Computer Conference</i> (New York, NY), 225–232.
</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=W.+Bledsoe&amp;author=I.+Browning+&amp;publication_year=1959&amp;title=“Pattern+recognition+and+reading+by+machine,”&amp;journal=Proc.+Eastern+Joint+Computer+Conference&amp;pages=225-232" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B13" id="B13"></a> Block, N. (1981). Psychologism and behaviorism. <i>Philos. Rev</i>. 90, 5–43.
</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=N.+Block+&amp;publication_year=1981&amp;title=Psychologism+and+behaviorism&amp;journal=Philos.+Rev&amp;volume=90&amp;pages=5-43" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B14" id="B14"></a> Boser, B., Guyon, I., and Vapnik, V. (1992). “A training algorithm for optimal margin classifiers,” in <i>Proc. 5th Annual Workshop on Computational Learning Theory - COLT &#39;92</i> (New York, NY), 144.
</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=B.+Boser&amp;author=I.+Guyon&amp;author=V.+Vapnik+&amp;publication_year=1992&amp;title=“A+training+algorithm+for+optimal+margin+classifiers,”&amp;journal=Proc.+5th+Annual+Workshop+on+Computational+Learning+Theory+-+COLT+&#39;92&amp;pages=144" target="_blank">Google Scholar</a></p>
</div>

<div>
<p><a name="B16" id="B16"></a> Bringsjord, S., and Xiao, H. (2000). A refutation of penrose&#39;s Gödelian case against artificial intelligence. <i>J. Exp. Theor. AI</i> 12, 307–329. doi: 10.1080/09528130050111455
</p>
<p><a href="https://doi.org/10.1080/09528130050111455" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=S.+Bringsjord&amp;author=H.+Xiao+&amp;publication_year=2000&amp;title=A+refutation+of+penrose&#39;s+Gödelian+case+against+artificial+intelligence&amp;journal=J.+Exp.+Theor.+AI&amp;volume=12&amp;pages=307-329" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B17" id="B17"></a> Broad, T. (2016). <i>Autoencoding video frames</i> (Master&#39;s thesis). Goldsmiths, University of London, London, United Kingdom.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=T.+Broad+&amp;publication_year=2016&amp;title=Autoencoding+video+frames" target="_blank">Google Scholar</a></p>
</div>
<p><a name="B18" id="B18"></a> Broomhead, D., and Lowe, D. (1988). <i>Radial Basis Functions, Multi-Variable Functional Interpolation and Adaptive Networks</i>. Technical report. Royal Signals and Radar Establishment (RSRE), 4148.</p>
<div>
<p><a name="B19" id="B19"></a> Burgess, J. (2000). “On the outside looking in: a caution about conservativeness,” in <i>Kurt Gödel: Essays for His Centennial</i>, C. Parsons and S. Simpson (Cambridge: Cambridge University Press), 131–132.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=J.+Burgess+&amp;publication_year=2000&amp;title=“On+the+outside+looking+in%3A+a+caution+about+conservativeness,”&amp;journal=Kurt+Gödel%3A+Essays+for+His+Centennial&amp;pages=131-132" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B20" id="B20"></a> Chalmers, D. (1996). Does a rock implement every finite-state automaton? <i>Synthese</i> 108, 309–333.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=D.+Chalmers+&amp;publication_year=1996&amp;title=Does+a+rock+implement+every+finite-state+automaton%3F&amp;journal=Synthese&amp;volume=108&amp;pages=309-333" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B21" id="B21"></a> Chollet, F. (2018). <i>Deep Learning with Python</i>. Shelter Island, NY: Manning Publications Co.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=F.+Chollet+&amp;publication_year=2018&amp;title=Deep+Learning+with+Python" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B22" id="B22"></a> Chrisley, R. (1995). Why everything doesn&#39;t realize every computation. <i>Minds Mach</i>. 4, 403–420.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=R.+Chrisley+&amp;publication_year=1995&amp;title=Why+everything+doesn&#39;t+realize+every+computation&amp;journal=Minds+Mach&amp;volume=4&amp;pages=403-420" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B23" id="B23"></a> Church, A. (1936). An unsolvable problem of elementary number theory. <i>Am. J. Math</i>. 58, 345–363.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=A.+Church+&amp;publication_year=1936&amp;title=An+unsolvable+problem+of+elementary+number+theory&amp;journal=Am.+J.+Math&amp;volume=58&amp;pages=345-363" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B24" id="B24"></a> Crick, F. (1994). <i>The Astonishing Hypothesis: The Scientific Search for the Soul</i>. New York, NY: Simon and Schuster.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=F.+Crick+&amp;publication_year=1994&amp;title=The+Astonishing+Hypothesis%3A+The+Scientific+Search+for+the+Soul" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B25" id="B25"></a> Dasgupta, I., Wang, J., Chiappa, S., Mitrovic, J., Ortega, P., Raposo, D., et al. (2019). Causal Reasoning from meta-reinforcement learning. <i>arXiv</i> arXiv:1901.08162.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=I.+Dasgupta&amp;author=J.+Wang&amp;author=S.+Chiappa&amp;author=J.+Mitrovic&amp;author=P.+Ortega&amp;author=D.+Raposo+&amp;publication_year=2019&amp;title=Causal+Reasoning+from+meta-reinforcement+learning&amp;journal=arXiv" target="_blank">Google Scholar</a></p>
</div>

<p><a name="B27" id="B27"></a> Freud, S. (1919). <i>Das unheimliche. Imago, 5</i>. Leipzig.</p>
<div>
<p><a name="B28" id="B28"></a> Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., et al. (2014). “Generative adversarial networks,” in <i>Proc. Int. Conf. Neural Information Processing Systems (NIPS 2014)</i> (Cambridge, MA), 2672–2680.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=I.+Goodfellow&amp;author=J.+Pouget-Abadie&amp;author=M.+Mirza&amp;author=B.+Xu&amp;author=D.+Warde-Farley&amp;author=S.+Ozair+&amp;publication_year=2014&amp;title=“Generative+adversarial+networks,”&amp;journal=Proc.+Int.+Conf.+Neural+Information+Processing+Systems+(NIPS+2014)&amp;pages=2672-2680" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B29" id="B29"></a> Harnad, S. (1991). Other bodies, other minds: a machine incarnation of an old philosophical problem. <i>Minds Mach</i>. 1, 43–54.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=S.+Harnad+&amp;publication_year=1991&amp;title=Other+bodies,+other+minds%3A+a+machine+incarnation+of+an+old+philosophical+problem&amp;journal=Minds+Mach&amp;volume=1&amp;pages=43-54" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B30" id="B30"></a> Harnad, S. (2011). <i>Lunch Uncertain.</i> Times Literary Supplement 5664, 22–23.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=S.+Harnad+&amp;publication_year=2011&amp;title=Lunch+Uncertain.&amp;volume=5664&amp;pages=22-23" target="_blank">Google Scholar</a></p>
</div>



<div>
<p><a name="B34" id="B34"></a> Hodgkin, A., and Huxley, A. (1952). A quantitative description of membrane current and its application to conduction and excitation in nerve. <i>J. Physiol</i>. 117, 500–544.</p>
<p><a href="https://pubmed.ncbi.nlm.nih.gov/12991237" target="_blank">PubMed Abstract</a> | <a href="http://scholar.google.com/scholar_lookup?author=A.+Hodgkin&amp;author=A.+Huxley+&amp;publication_year=1952&amp;title=A+quantitative+description+of+membrane+current+and+its+application+to+conduction+and+excitation+in+nerve&amp;journal=J.+Physiol&amp;volume=117&amp;pages=500-544" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B35" id="B35"></a> Kingma, D., and Welling, M. (2013). “Auto-encoding variational Bayes,” in <i>Proceedings of the 2nd International Conference on Learning Representations (ICLR 2013)</i> (Banff, AB).</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=D.+Kingma&amp;author=M.+Welling+&amp;publication_year=2013&amp;title=“Auto-encoding+variational+Bayes,”&amp;journal=Proceedings+of+the+2nd+International+Conference+on+Learning+Representations+(ICLR+2013)" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B36" id="B36"></a> Klein, C. (2018). “Computation, consciousness, and “computation and consciousness”, in <i>The Handbook of the Computational Mind</i> (London: Routledge), 297–309.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=C.+Klein+&amp;publication_year=2018&amp;title=“Computation,+consciousness,+and+“computation+and+consciousness”&amp;journal=The+Handbook+of+the+Computational+Mind&amp;pages=297-309" target="_blank">Google Scholar</a></p>
</div>
<p><a name="B37" id="B37"></a> Kokoli, A. (2016). <i>The Feminist Uncanny in Theory and Art Practice</i>. Bloomsbury Studies in Philosophy, Bloomsbury Academic, London.</p>
<div>
<p><a name="B38" id="B38"></a> Kramer, M. (1991). Nonlinear principal component analysis using autoassociative neural networks. <i>AIChE J</i>. 37, 233–243.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=M.+Kramer+&amp;publication_year=1991&amp;title=Nonlinear+principal+component+analysis+using+autoassociative+neural+networks&amp;journal=AIChE+J&amp;volume=37&amp;pages=233-243" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B39" id="B39"></a> Kurzweil, R. (2005). <i>The Singularity Is Near: When Humans Transcend Biology</i>. London: Viking.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=R.+Kurzweil+&amp;publication_year=2005&amp;title=The+Singularity+Is+Near%3A+When+Humans+Transcend+Biology" target="_blank">Google Scholar</a></p>
</div>
<p><a name="B40" id="B40"></a> Liu, Y. (2017). The accountability of AI - case study: Microsoft&#39;s tay experiment. Medium: 16th January, 2017.</p>
<div>
<p><a name="B41" id="B41"></a> Lucas, J. (1961). Minds, machines and godel. <i>Philosophy</i>: 36, 112–127.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=J.+Lucas+&amp;publication_year=1961&amp;title=Minds,+machines+and+godel&amp;journal=Philosophy&amp;volume=36&amp;pages=112-127" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B42" id="B42"></a> Lucas, J. (1968). Satan stultified: a rejoinder to Paul Benacerraf. <i>Monist</i> 52, 145–158.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=J.+Lucas+&amp;publication_year=1968&amp;title=Satan+stultified%3A+a+rejoinder+to+Paul+Benacerraf&amp;journal=Monist&amp;volume=52&amp;pages=145-158" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B43" id="B43"></a> MacDorman, K., and Ishiguro, H. (2006). The uncanny advantage of using androids in social and cognitive science research. <i>Int. Stud</i>. 7, 297–337. doi: 10.1075/is.7.3.03mac</p>
<p><a href="https://doi.org/10.1075/is.7.3.03mac" target="_blank">CrossRef Full Text</a></p>
</div>
<div>
<p><a name="B44" id="B44"></a> Maudlin, T. (1989). Computation and consciousness. <i>J. Philos</i>. 86, 407–432.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=T.+Maudlin+&amp;publication_year=1989&amp;title=Computation+and+consciousness&amp;journal=J.+Philos&amp;volume=86&amp;pages=407-432" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B45" id="B45"></a> McCulloch, W., and Pitts, W. (1943). A logical calculus immanent in nervous activity. <i>Bull. Math. Biophys</i>. 5, 115–133.</p>
<p><a href="https://pubmed.ncbi.nlm.nih.gov/2185863" target="_blank">PubMed Abstract</a> | <a href="http://scholar.google.com/scholar_lookup?author=W.+McCulloch&amp;author=W.+Pitts+&amp;publication_year=1943&amp;title=A+logical+calculus+immanent+in+nervous+activity&amp;journal=Bull.+Math.+Biophys&amp;volume=5&amp;pages=115-133" target="_blank">Google Scholar</a></p>
</div>

<div>
<p><a name="B47" id="B47"></a> Nasuto, S., Bishop, J., and De Meyer, K. (2009). Communicating neurons: a connectionist spiking neuron implementation of stochastic diffusion search. <i>Neurocomputing</i> 72, 704–712. doi: 10.1016/j.neucom.2008.03.019</p>
<p><a href="https://doi.org/10.1016/j.neucom.2008.03.019" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=S.+Nasuto&amp;author=J.+Bishop&amp;author=K.+De+Meyer+&amp;publication_year=2009&amp;title=Communicating+neurons%3A+a+connectionist+spiking+neuron+implementation+of+stochastic+diffusion+search&amp;journal=Neurocomputing&amp;volume=72&amp;pages=704-712" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B48" id="B48"></a> Nasuto, S., Dautenhahn, K., and Bishop, J. (1998). “Communication as an emergent metaphor for neuronal operation,” in <i>Computation for Metaphors, Analogy, and Agents</i>, ed C. L. Nehani (Heidelberg: Springer), 365–379.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=S.+Nasuto&amp;author=K.+Dautenhahn&amp;author=J.+Bishop+&amp;publication_year=1998&amp;title=“Communication+as+an+emergent+metaphor+for+neuronal+operation,”&amp;journal=Computation+for+Metaphors,+Analogy,+and+Agents&amp;pages=365-379" target="_blank">Google Scholar</a></p>
</div>
<p><a name="B49" id="B49"></a> Norvig, P. (2012). <i>Channeling the Flood of Data (Address to the Singularity Summit 2012)</i>. San Francisco, CA: Nob Hill Masonic Center.</p>
<div>
<p><a name="B50" id="B50"></a> Olteanu, A., Varol, O., and Kiciman, E. (2017). “Distilling the outcomes of personal experiences: a propensity-scored analysis of social media,” in <i>Proceedings of The 20th ACM Conference on Computer-Supported Cooperative Work and Social Computing</i> (New York, NY: Association for Computing Machinery).</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=A.+Olteanu&amp;author=O.+Varol&amp;author=E.+Kiciman+&amp;publication_year=2017&amp;title=“Distilling+the+outcomes+of+personal+experiences%3A+a+propensity-scored+analysis+of+social+media,”&amp;journal=Proceedings+of+The+20th+ACM+Conference+on+Computer-Supported+Cooperative+Work+and+Social+Computing" target="_blank">Google Scholar</a></p>
</div>
<p><a name="B51" id="B51"></a> Pearl, J. (1985). “Bayesian networks: a model of self-activated memory for evidential reasoning,” in <i>Proceedings of the 7th Conference of the Cognitive Science Society</i> (Irvine, CA), 329–334.</p>
<p><a name="B52" id="B52"></a> Pearl, J. (2018). Theoretical impediments to machine learning with seven sparksfrom the causal revolution. <i>arXiv</i> arXiv:1801.04016.</p>
<p><a name="B53" id="B53"></a> Pearl, J., and Mackenzie, D. (2018). <i>The Book of Why: The New Science of Cause and Effect</i>. New York, NY: Basic Books.</p>
<div>
<p><a name="B54" id="B54"></a> Penrose, R. (1989). <i>The Emperor&#39;s New Mind: Concerning Computers, Minds, and the Laws of Physics</i>. Oxford: Oxford University Press.</p>
<p><a href="https://pubmed.ncbi.nlm.nih.gov/18240751" target="_blank">PubMed Abstract</a> | <a href="http://scholar.google.com/scholar_lookup?author=R.+Penrose+&amp;publication_year=1989&amp;title=The+Emperor&#39;s+New+Mind%3A+Concerning+Computers,+Minds,+and+the+Laws+of+Physics" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B55" id="B55"></a> Penrose, R. (1994). <i>Shadows of the Mind: A Search for the Missing Science of Consciousness</i>. Oxford: Oxford University Press.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=R.+Penrose+&amp;publication_year=1994&amp;title=Shadows+of+the+Mind%3A+A+Search+for+the+Missing+Science+of+Consciousness" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B56" id="B56"></a> Penrose, R. (1996). <i>Beyond the Doubting of a Shadow: A Reply to Commentaries on ‘Shadows of the Mind’</i>. Oxford: Psyche. 1–40.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=R.+Penrose+&amp;publication_year=1996&amp;title=Beyond+the+Doubting+of+a+Shadow%3A+A+Reply+to+Commentaries+on+‘Shadows+of+the+Mind’&amp;pages=1-40" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B57" id="B57"></a> Penrose, R. (1997). On understanding understanding. <i>Int. Stud. Philos. Sci</i>. 11, 7–20.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=R.+Penrose+&amp;publication_year=1997&amp;title=On+understanding+understanding&amp;journal=Int.+Stud.+Philos.+Sci&amp;volume=11&amp;pages=7-20" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B58" id="B58"></a> Penrose, R. (2002). “Consciousness, computation, and the Chinese room,” in <i>Views into the Chinese Room: New Essays on Searle and Artificial Intelligence</i>, eds J. Preston and J. M. Bishop (Oxford, UK: Oxford University Press), 226–250.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=R.+Penrose+&amp;publication_year=2002&amp;title=“Consciousness,+computation,+and+the+Chinese+room,”&amp;journal=Views+into+the+Chinese+Room%3A+New+Essays+on+Searle+and+Artificial+Intelligence&amp;pages=226-250" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B59" id="B59"></a> Preston, J., and Bishop, J., (eds.). (2002). <i>Views into the Chinese Room: New Essays on Searle and Artificial Intelligence</i>. Oxford, UK: Oxford University Press.
</p>
<p><a href="http://scholar.google.com/scholar_lookup?publication_year=2002&amp;title=Views+into+the+Chinese+Room%3A+New+Essays+on+Searle+and+Artificial+Intelligence" target="_blank">Google Scholar</a></p>
</div>
<p><a name="B60" id="B60"></a> Psyche (1995). <i>Symposium on Roger Penrose&#39;s ‘Shadows of the Mind’</i>. Melbourne, VIC: Psyche.</p>
<div>
<p><a name="B61" id="B61"></a> Putnam, H. (1988). <i>Representation and Reality</i>. Cambridge, MA: Bradford Books.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=H.+Putnam+&amp;publication_year=1988&amp;title=Representation+and+Reality" target="_blank">Google Scholar</a></p>
</div>

<div>
<p><a name="B63" id="B63"></a> Searle, J. (1980). Minds, brains, and programs. <i>Behav. Brain Sci</i>. 3, 417–457.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=J.+Searle+&amp;publication_year=1980&amp;title=Minds,+brains,+and+programs&amp;journal=Behav.+Brain+Sci&amp;volume=3&amp;pages=417-457" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B64" id="B64"></a> Searle, J. (1990). “Is the brain a digital computer,” in <i>Proc. American Philosophical Association: 64</i> (Cambridge), 21–37.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=J.+Searle+&amp;publication_year=1990&amp;title=“Is+the+brain+a+digital+computer,”&amp;journal=Proc.+American+Philosophical+Association%3A+64&amp;pages=21-37" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B65" id="B65"></a> Sharma, A., Hofman, J., and Watts, D. (2018). Split-door criterion: identification of causal effects through auxiliary outcomes. <i>Ann. Appl. Stat</i>. 12, 2699–2733. doi: 10.1214/18-AOAS1179</p>
<p><a href="https://doi.org/10.1214/18-AOAS1179" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=A.+Sharma&amp;author=J.+Hofman&amp;author=D.+Watts+&amp;publication_year=2018&amp;title=Split-door+criterion%3A+identification+of+causal+effects+through+auxiliary+outcomes&amp;journal=Ann.+Appl.+Stat&amp;volume=12&amp;pages=2699-2733" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B66" id="B66"></a> Su, J., Vargas, D., and Kouichi, S. (2019). One pixel attack for fooling deep neural networks. <i>IEEE Trans. Evol. Comput</i>. 23, 828–841. doi: 10.1109/TEVC.2019.2890858</p>
<p><a href="https://doi.org/10.1109/TEVC.2019.2890858" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=J.+Su&amp;author=D.+Vargas&amp;author=S.+Kouichi+&amp;publication_year=2019&amp;title=One+pixel+attack+for+fooling+deep+neural+networks&amp;journal=IEEE+Trans.+Evol.+Comput&amp;volume=23&amp;pages=828-841" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B67" id="B67"></a> Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., et al. (2013). Intriguing properties of neural networks. <i>arXiv</i> arXiv:1312.6199.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=C.+Szegedy&amp;author=W.+Zaremba&amp;author=I.+Sutskever&amp;author=J.+Bruna&amp;author=D.+Erhan&amp;author=I.+Goodfellow+&amp;publication_year=2013&amp;title=Intriguing+properties+of+neural+networks&amp;journal=arXiv" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B68" id="B68"></a> Tassinari, R., and D&#39;Ottaviano, I. (2007). Cogito ergo sum non machina! About Gödel&#39;s first incompleteness theorem and Turing machines.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=R.+Tassinari&amp;author=I.+D&#39;Ottaviano+&amp;publication_year=2007&amp;title=Cogito+ergo+sum+non+machina!+About+Gödel&#39;s+first+incompleteness+theorem+and+Turing+machines" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B69" id="B69"></a> Thompson, E. (2007). <i>Mind in Life</i>. Cambridge, MA: Harvard University Press.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=E.+Thompson+&amp;publication_year=2007&amp;title=Mind+in+Life" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B70" id="B70"></a> Turing, A. (1937). On computable numbers, with an application to the entscheidungs problem. <i>Proc. Lond. Math. Soc</i>. 2, 23–65.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=A.+Turing+&amp;publication_year=1937&amp;title=On+computable+numbers,+with+an+application+to+the+entscheidungs+problem&amp;journal=Proc.+Lond.+Math.+Soc&amp;volume=2&amp;pages=23-65" target="_blank">Google Scholar</a></p>
</div>
<p><a name="B71" id="B71"></a> Turing, A. (1950). Computing machinery and intelligence. <i>Mind</i> 59, 433–460.</p>
<p><a name="B72" id="B72"></a> Warwick, K. (1997). <i>March of the Machines</i>. London: Random House.</p>
<div>
<p><a name="B73" id="B73"></a> Warwick, K. (2002). “Alien encounters,” in <i>Views into the Chinese Room: New Essays on Searle and Artificial Intelligence</i>, eds J. Preston and J. M. Bishop (Oxford, UK: Oxford University Press), 308–318.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=K.+Warwick+&amp;publication_year=2002&amp;title=“Alien+encounters,”&amp;journal=Views+into+the+Chinese+Room%3A+New+Essays+on+Searle+and+Artificial+Intelligence&amp;pages=308-318" target="_blank">Google Scholar</a></p>
</div>
<div>
<p><a name="B74" id="B74"></a> Zhou, L., Gao, J., Li, D., and Shum, H.-Y. (2018). The design and implementation of xiaoice, an empathetic social chatbot. <i>arXiv</i> arXiv:1812.08989.</p>
<p><a href="http://scholar.google.com/scholar_lookup?author=L.+Zhou&amp;author=J.+Gao&amp;author=D.+Li&amp;author=H.+-Y.+Shum+&amp;publication_year=2018&amp;title=The+design+and+implementation+of+xiaoice,+an+empathetic+social+chatbot&amp;journal=arXiv" target="_blank">Google Scholar</a></p>
</div>
</div></div>
  </body>
</html>
