<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://dennybritz.com/posts/poe-crafting/">Original</a>
    <h1>Solving Path of Exile Item Crafting with Reinforcement Learning</h1>
    
    <div id="readability-page-1" class="page"><div id="main-content"><article><header></header><section><div><h2 id="background">Background <span><a href="#background" aria-label="Anchor">#</a></span></h2><p><a href="https://www.pathofexile.com/" target="_blank" rel="noreferrer">PoE (Path of Exile)</a> is an ARPG similar to Diablo 2/3/4 or Last Epoch. You kill monsters, collect loot, and progress your character with increasingly better gear, which in turn allows you to complete more challenging content. PoE is somewhat infamous for its complexity. It was first released in 2013 and has received a large number of expansions, each adding new layers of mechanics to the game. Players with a thousand hours of playtime sometimes still call themselves beginners. One of PoE’s more complex systems is item crafting. Due to its high barrier of entry and risk of failure, many players choose to buy items from the trade market instead of crafting them. In this post, we will explore the question:</p><p><strong>Given a target item, can we use an algorithm to learn the optimal sequence of actions to craft it?</strong></p><p>We will look at a few approaches and eventually solve the problem with a model-based Reinforcement Learning algorithm.</p><h2 id="crafting-in-path-of-exile">Crafting in Path of Exile <span><a href="#crafting-in-path-of-exile" aria-label="Anchor">#</a></span></h2><p>In PoE, many of the best items are not dropped by enemies. They are the result of a crafting process where players modify item attributes using various <em>crafting currencies</em>. But first let’s understand what items in PoE look like.</p><figure><img src="https://dennybritz.com/posts/poe-crafting/images/rf-sceptre-step3.png" alt="A sceptre with 2 prefixes and 3 suffixes" width="40%"/><figcaption><p>A sceptre with 2 prefixes and 3 suffixes</p></figcaption></figure><p>The above image is a Sceptre weapon in PoE. There are a lot of things going on, but we are mostly concerned with the <a href="https://www.poewiki.net/wiki/Modifier" target="_blank" rel="noreferrer">modifiers</a> such as <code>18% to Fire Damage Over Time Multiplier</code>. The above item has 5 modifiers, listed at the bottom. There are 2 <em>prefixes</em>, and 3 <em>suffixes</em>. There is no fundamental difference between prefixes and suffixes except that they are different pools of possible modifiers. Some modifiers are always prefixes. Some are always suffixes.</p><p>A rare (yellow) item in PoE can have up to 6 modifiers with a maximum of 3 prefixes and 3 suffixes. A magic (blue) item can only have one prefix and one suffix. When PoE generates a rare item, e.g. dropped by an enemy, it does something like the following pseudocode:</p><div><pre tabindex="0"><code data-lang="python"><span><span>item <span>=</span> Item(<span>&#34;Void Sceptre&#34;</span>, level<span>=</span><span>84</span>)
</span></span><span><span>
</span></span><span><span><span># Spawn 4, 5, or 6 modifiers with 2/3, 1/4 or 1/12 probability</span>
</span></span><span><span>num_modifiers <span>=</span> np<span>.</span>random<span>.</span>choice([<span>4</span>, <span>5</span>, <span>6</span>], p<span>=</span>[<span>2</span><span>/</span><span>3</span>, <span>1</span><span>/</span><span>4</span>, <span>1</span><span>/</span><span>12</span>])
</span></span><span><span>
</span></span><span><span><span>for</span> _ <span>in</span> range(num_modifiers):
</span></span><span><span>    <span># Get a list of spawnable modifiers and their probabilities</span>
</span></span><span><span>    <span># These depend on the item base type, level, and existing modifiers</span>
</span></span><span><span>    spawnable_modifiers, probs <span>=</span> get_spawnable_modifiers(item)
</span></span><span><span>    <span># Sample a modifier</span>
</span></span><span><span>    modifier <span>=</span> np<span>.</span>random<span>.</span>choice(spawnable_modifiers, p<span>=</span>probs)
</span></span><span><span>    <span># Add it to the item</span>
</span></span><span><span>    spawn_modifier(modifier)
</span></span></code></pre></div><p>In reality there are many additional checks and restrictions, but the basic idea is that modifiers are sampled from a distribution that depends on the current state of the item, including its base type, level, and existing modifiers. Luckily, people have extracted information to derive this distribution from game data and made it publicly available on websites such as <a href="https://www.craftofexile.com/" target="_blank" rel="noreferrer">craftofexile.com</a> and <a href="https://poedb.tw/us/" target="_blank" rel="noreferrer">poedb.tw</a>. Each modifier has a weight associated with it, and its spawn probability is the normalized weight. <a href="https://www.craftofexile.com/?b=17&amp;ob=both&amp;v=d&amp;a=x&amp;l=a&amp;lg=16&amp;bp=y&amp;as=1&amp;hb=0&amp;bld=%7b%7d&amp;im=%7b%7d&amp;ggt=%7c&amp;ccp=%7b%7d&amp;gvc=%7b%22limit%22:88%7d" target="_blank" rel="noreferrer">For a Sceptre weapon the modifier data looks like this</a>.</p><p>Crafting in PoE, at least to the extent that we are concerned with in this post, is the process of changing the modifiers on an item through various actions to achieve a desired outcome. These actions typically consist of applying <em>currency</em> to the item, or using other mechanics such as a crafting bench. Because the outcome of most actions is <em>stochastic</em>, crafts in PoE can consist of hundreds of steps, including looping certain actions until some condition is met before moving on to the next step. Let’s look at a basic example:</p><figure><img src="https://dennybritz.com/posts/poe-crafting/images/rf-sceptre-craft.png"/></figure><p>To craft the Void Sceptre we looked at above, we would follow these steps:</p><ol><li>Start with a Magic Sceptre with a <a href="https://www.poewiki.net/wiki/Fractured_item" target="_blank" rel="noreferrer">fractured</a> “Damage over time multiplier” modifier. A fractured modifier is fixed on the item and cannot be changed by subsequent actions.</li><li>Use a <a href="https://www.poewiki.net/wiki/Regal_Orb" target="_blank" rel="noreferrer">Regal Orb</a> to upgrade the item from a magic to a rare item. This will add a random modifier which we don’t care about since we will remove it later. We need to upgrade the item to a rare item so that we can use an Essence on it in the next step.</li><li>Use a <a href="https://www.poewiki.net/wiki/Essence_of_Fear" target="_blank" rel="noreferrer">Essence of Fear</a> to add a guaranteed “Increased Minion Damage” modifier while randomizing all other modifiers on the item, except for the fractured one. If the randomized modifiers include a desired modifier such as “Burning Damage”, continue to the next step. Otherwise, repeat this step.</li><li>Use the crafting bench to add the “Fire Damage over Time multiplier” modifier to the item.</li></ol><p>I picked this simple example for demonstration purposes, but many crafts in PoE are significantly more complex. There exist hundreds of different actions, many of which can be composed, to modify the probability distribution of outcomes in various ways. For example, an <a href="https://www.poewiki.net/wiki/Orb_of_Annulment" target="_blank" rel="noreferrer">Orb of Annulment</a> removes a random modifier from an item. But it can be combined with a <a href="https://www.poewiki.net/wiki/Metamod" target="_blank" rel="noreferrer">Cannot Roll Attack Modifiers metamod</a> to increase the chances of removing a specific modifier that does not have an “Attack” tag.</p><p>If you are interested in learning more about crafting in PoE, I recommend watching <a href="https://www.youtube.com/watch?v=FFTuowD1w4Q" target="_blank" rel="noreferrer">part 1</a> and <a href="https://www.youtube.com/watch?v=Ax1ULfE95yw" target="_blank" rel="noreferrer">part 2</a> of subtractem’s basic crafting guide on YouTube.</p><p><strong>The important takeaway is this: Given a desired item, the optimal way to craft it is not obvious.</strong> By <em>optimal</em> we typically mean either the fastest (fewest steps) or most cost-efficient way to make the item. Often they are the same, but sometimes they are not. Each action uses up currency items, which can be bought from the trade market or collected yourself. Even experienced players need to research modifier distributions on external websites and try a few things before coming up with a good crafting strategy. That’s the problem we are trying to solve algorithmically: <strong>Given a target item, can we learn the optimal sequence of actions to craft it?</strong></p><h2 id="why-game-tree-search-doesnt-work">Why game tree search doesn’t work <span><a href="#why-game-tree-search-doesnt-work" aria-label="Anchor">#</a></span></h2><p>A popular algorithm for finding optimal actions in games is <a href="https://en.wikipedia.org/wiki/Minimax" target="_blank" rel="noreferrer">Minimax</a> search. In our case, we have one player instead of two, and our environment is stochastic. We can still use this approach by replacing the <code>min</code> operation with an expectation, also called <em>expectimax</em>. This approach has been applied to games such as 2048 to find optimal plays. Can we build a game tree to find the optimal crafting strategy? It would look something like this:</p><figure><img src="https://dennybritz.com/posts/poe-crafting/images/expectimax.png"/></figure><p>It turns out, the answer is no. At least not easily without a lot of additional heuristics and tricks. Our problem has two properties that makes this approach difficult to apply:</p><ul><li><strong>Cyclic Graph</strong> - Many crafting actions result in identical states, as we’ve seen in the example above. And certain randomizing currencies can transport you into a large number of states. Not only do we not have a tree, but our graph is both dense and highly cyclic. If we had a DAG (Directed Acyclic Graph) we could reduce redundant computation by aliasing identical states using <a href="https://en.wikipedia.org/wiki/Transposition_table" target="_blank" rel="noreferrer">transposition tables</a>. This is true for many board games. In some cases it’s possible to break cycles using heuristics, but in our case this would result in incorrect estimates since cycles are a result of the environment stochasticity that we must take an expectation over.</li><li><strong>Unprunable large branching factor</strong> - The space of possible items is huge. With ~100 different spawnable modifiers on an item base we have on the order of \(10^{12}\) possible outcomes for randomizing actions. The number of valid states is much less than this due to restrictions on which modifiers can appear together, but it is still extremely large. Dealing with large branching factors in tree searches can be done using techniques like <a href="https://en.wikipedia.org/wiki/Alpha%E2%80%93beta_pruning" target="_blank" rel="noreferrer">alpha-beta pruning</a> to avoid exploring unpromising branches. But in our case the branching factor is due to stochasticity in the environment, which we must take an expectation over, meaning that we cannot easily prune away paths like we could when choosing an action.</li></ul><h3 id="what-about-mcts">What about MCTS? <span><a href="#what-about-mcts" aria-label="Anchor">#</a></span></h3><p><a href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search" target="_blank" rel="noreferrer">Monte Carlo Tree Search (MCTS)</a>, popular for its role in AlphaGo, is an alternative that allows us to explore the game tree using sampling techniques and confidence bounds for exploration. Instead of rolling out the full tree, we would run Monte Carlo simulations to estimate the value of intermediate nodes. This solves the branching factor issue by building an “asymmetric” tree. And while MCTS can be <a href="https://github.com/lightvector/KataGo/blob/master/docs/GraphSearch.md" target="_blank" rel="noreferrer">applied to graphs instead of trees with some modifications</a>, it still requires a DAG for its backpropagation algorithm to be sound if we want to share computation across states.</p><h2 id="formalizing-the-problem-as-a-markov-decision-process">Formalizing the problem as a Markov Decision Process <span><a href="#formalizing-the-problem-as-a-markov-decision-process" aria-label="Anchor">#</a></span></h2><p>Fortunately, our problem fits neatly into the framework of <a href="https://en.wikipedia.org/wiki/Markov_decision_process" target="_blank" rel="noreferrer">Markov Decision Processes (MDPs)</a> and Reinforcement Learning. An MDP is defined by a tuple \((S, A, P, R)\) where:</p><ul><li>\(S\) is the set of possible states. Each \(s \in S\) represents the current state of the item being crafted. We’ll get to how we represent the state using a feature vector later.</li><li>\(A_s\) is the set of actions available in state \(s\). An action \(a \in A_s\) corresponds to modifying the item using a crafting currency.</li><li>\(P(s_{t+1} | s_t, a)\) is the transition function. Given a state \(s_t\) and an action \(a\), it defines the probability distribution over the next state \(s_{t+1}\), i.e. the possible outcomes of a crafting step.</li><li>\(R(s_t, a)\) is the reward function. It defines the reward received from taking action \(a\) in state \(s_t\). We’ll discuss it more below.</li></ul><p>Solving an MDP means finding a policy \(\pi(s)\) that maps each state to an optimal action. Optimal in what sense? This policy typically maximizes the expected sum of rewards over time, i.e. the average total reward over possible trajectories. One way to find such a policy is by learning a state-action value function, also called q-function, \(Q(s, a)\). This function assigns a value to each state-action pair, representing the expected sum of rewards when taking action \(a\) in state \(s\) and following the optimal policy thereafter. When you have the optimal q function, the optimal policy is easily derived by picking the best action in each state: \(\pi(s) = \text{argmax}_a Q(s, a)\).</p><p>Depending on what we want to optimize, we can define the reward function in different ways. One useful reward function for our crafting problem is a constant \(R(s_t, a) = -1\) for all states and actions. This would encourage the agent to craft the item in as few steps as possible. The total reward for 50 steps (-50) is larger than the total reward for 100 steps (-100), making it preferable. In this case, the Q function would represent the expected number of steps needed to reach the target item given the current state and next action.</p><p>Another option is to define the reward as the cost of the action being taken, \(R(s_t, a) = -\text{cost}(a)\). This would encourage the agent to craft the item in the cheapest way possible. We can obtain an estimate of the cost for various actions from the <a href="https://poe.ninja/economy/necropolis/currency" target="_blank" rel="noreferrer">economy data available on poeninja</a>. In this case, the Q function represents the expected cost of crafting the target item from a given state. Low cost may come at the expense of a long crafting process. In PoE, this is not uncommon. Because certain transition probabilities are very low, it may take thousands of steps to craft an item in the most economical way. Some players prefer to spend more, but be done quickly.</p><p>The transition function \(P(s_{t+1} | s_t, a)\) is is what makes our problem difficult. Knowing \(P\) means you know the <em>dynamics</em> of the environment. In combination with knowing the reward function (which we always do), this is also called a <em>model</em> of the environment and we can use it to simulate transitions without needing to touch the real world. In deterministic fully observable board games such as Chess or Go we know the rules of the game and have a perfect model. In other problem domains, such as robotics, we may not know the dynamics of the environment, or we may be unable to express them in closed form. The environment could also be stochastic, e.g. wind could push your robot into random directions.</p><p>In our case, we know the dynamics of the environment in theory from datamined information available on sites such as <a href="https://www.craftofexile.com/" target="_blank" rel="noreferrer">craftofexile.com</a> and <a href="https://poedb.tw/us/" target="_blank" rel="noreferrer">poedb.tw</a>. However, for many actions we cannot express \(P(s_{t+1} | s_t, a)\) directly because the distribution of possible outcomes is too large. We also cannot easily factor this categorical distribution. However, we can quite easily sample transitions \(s_{t+1} \sim P(s_{t+1} | s_t, a)\) by writing code that simulates each currency action. That is, we have <em>sample model</em> but not a <em>distribution model</em> of our environment. For example, the implementation of an <a href="https://www.poewiki.net/wiki/Orb_of_Annulment" target="_blank" rel="noreferrer">Orb of Annulment</a>, which removes a random prefix or suffix from an item, looks something like this in my Rust implementation:</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>impl</span> Currency <span>for</span> Annulment {
</span></span><span><span>    <span>fn</span> <span>apply</span>(self, item: <span>&amp;</span><span>mut</span> Item) -&gt; Result<span>&lt;</span>()<span>&gt;</span> {
</span></span><span><span>        <span>// Handle metamods
</span></span></span><span><span><span></span>        <span>let</span> mmf <span>=</span> MetamodFilter::from_existing_mods(item.all_mods());
</span></span><span><span>
</span></span><span><span>        <span>// Check for non-fractured candidate mods to remove
</span></span></span><span><span><span></span>        <span>// Filter out mods that cannot be removed
</span></span></span><span><span><span></span>        <span>let</span> existing_prefixes <span>=</span> item.mods.for_generation_type(ModGenerationType::Prefix);
</span></span><span><span>        <span>let</span> existing_suffixes <span>=</span> item.mods.for_generation_type(ModGenerationType::Suffix);
</span></span><span><span>        <span>let</span> candidate_mods <span>=</span> existing_prefixes
</span></span><span><span>            .chain(existing_suffixes)
</span></span><span><span>            .filter(<span>|</span>m<span>|</span> mmf.filter_existing(m))
</span></span><span><span>            .filter(<span>|</span>m<span>|</span> mmf.filter_new(m))
</span></span><span><span>            .collect::<span>&lt;</span>Vec<span>&lt;</span>_<span>&gt;&gt;</span>();
</span></span><span><span>
</span></span><span><span>        <span>if</span> candidate_mods.is_empty() {
</span></span><span><span>            <span>return</span> Err(CurrencyError::NoModsToRemove.into());
</span></span><span><span>        }
</span></span><span><span>
</span></span><span><span>        <span>// Remove a random mod
</span></span></span><span><span><span></span>        <span>let</span> <span>mut</span> rng <span>=</span> thread_rng();
</span></span><span><span>        <span>let</span> mod_to_remove <span>=</span> candidate_mods.choose(<span>&amp;</span><span>mut</span> rng).unwrap();
</span></span><span><span>        item.mods.mods_mut().remove(mod_to_remove);
</span></span><span><span>
</span></span><span><span>        Ok(())
</span></span><span><span>    }
</span></span><span><span>}
</span></span></code></pre></div><p>(I am sorry for mixing Python pseudocode with Rust code in this post. My real implementation is in Rust and those snippets are copied directly from my codebase. But I prefer Python pseudocode for getting high-level ideas across)</p><h2 id="item-representation">Item representation <span><a href="#item-representation" aria-label="Anchor">#</a></span></h2><p>We need to represent the state space \(S\) in a way that’s appropriate for learning a good policy. Naively, each item could be represented as a bitstring where each bit corresponds to the presence of a specific modifier. We often have 100+ spawnable modifiers for an item base. Let’s say we cap this at 512. A set of item modifiers could be represented as 512 bits <code>0001....0</code>, with a total of \(2^{512}\) possible combinations. Because an item can have at most 3 prefixes and 3 suffixes, only a small portion of this space is valid and the representation is quite sparse.</p><p>One problem with such a representation is that it does not easily allow for <em>generalization</em>. Imagine the Q-function \(Q(s, a)\) as a table that assigns a value to each state-action pair. With the naive representation we would learn a different value for each possible combination of modifiers, even though many states are similar. For example, if we have an item with two modifiers we want and an additional modifier we don’t care about, it typically (there are exceptions) does not matter what exactly this additional modifier is. It would be better if our representation could capture this. In other words, we want a more compact <em>feature vector</em> suitable for Machine Learning.</p><p>We have a few choices here. We can either design a feature vector by hand, or we can use a Neural Network (or similar) to learn a good feature representation. While I believe the Neural Network approach would work and has the benefit of being more generic, interleaving Neural Network training and Reinforcement Learning opens a whole new can of worms. It would probably increase our iteration time by 10-100x. So let’s see if we can get away with hand-crafted features first and come back to the idea of learning features later.</p><p>The intuition behind our feature representation is that it should capture the difference between our <em>target item</em> and the current state of the item. Let’s say our target item has 5 modifiers \(m_1, \dots, m_5\) and our current state contains 3 of these modifiers \(m_1, m_3, m_5\). We can represent the state as a bitstring of length 5, where each a bit is 1 if the modifier is present and 0 otherwise: <code>10101</code>. More formally, if \(T = \{m_1, \dots, m_n\}\) is the set of modifiers on the target item and \(X \subseteq T\) is the set of modifiers on the current item our feature vector can be written as \(b_n \dots b_1\) where:</p><p>$$
\begin{aligned}
b_i =
\begin{cases}
1 &amp; \text{if } m_i \in X, \\
0 &amp; \text{if } m_i \notin X
\end{cases}
\end{aligned}
$$</p><p>(An nice side effect of a bitstring representation is that we can efficiently compare two states using bitwise operations)</p><p>Knowing which target modifiers are present should allow our agent to make an informed decision about what to do next. But this representation is missing a crucial bit of information: How full is our item? If we have 3 ouf of 5 matching modifiers but our item is at the maximum of 6 modifiers, 3 of which are irrelevant to us, we can’t add new modifiers. In such a case we may need to use an <a href="https://www.poewiki.net/wiki/Orb_of_Annulment" target="_blank" rel="noreferrer">Orb of Annulment</a> and hope that we remove one of the irrelevant modifiers to continue crafting. This is quite a different state from hitting 3 out of 5 modifiers and still having 3 open spaces on the item. Thus, we also add the total number of prefixes and suffixes on the item to our feature vector. Let’s look at an example:</p><pre tabindex="0"><code>Target modifier IDs:    3 (prefix), 9 (prefix), 5 (suffix), 1 (suffix), 8 (suffix)
Current modifier IDs:   3 (prefix), 5 (suffix), 1 (suffix), 4 (suffix)
</code></pre><pre tabindex="0"><code>Feature vector:         01101|1|3
</code></pre><p>In this example, we are missing the modifier at index 2 (id = 9) and index 5 (id = 8), and we have one irrelevant modifier with id = 4. The item has 1 prefix, and 3 suffixes.</p><p>This is a pretty compact feature representation, but it has some important limitations:</p><ul><li><a href="https://www.poewiki.net/wiki/Modifier#Group/family" target="_blank" rel="noreferrer">Mod families</a> - In PoE, each modifier may belong to one or more mod families. Only one modifier from any given family may appear on an item. This restriction changes the distribution of possible outcomes of each action. Our feature representation does not capture <em>which</em> irrelevant mods are on the item, and hence does not capture which mod families they belong to. Note that we are still capturing the mod families of target mods because we specify those explicitly. We are only missing those of non-target mods. In many crafts this is not a big problem, but in some cases this information can be important.</li><li><a href="https://www.poewiki.net/wiki/Metamod" target="_blank" rel="noreferrer">Mod tags</a> - Similiar to mod families, mods can have associated tags. Certain <a href="https://www.poewiki.net/wiki/Metamod" target="_blank" rel="noreferrer">metamods</a> interact with these tags and change the distribution of possible outcomes depending on the tags present. Again, we are capturing the tags of target mods. But we are missing those of non-target mods.</li><li><a href="https://www.poewiki.net/wiki/Fractured_item" target="_blank" rel="noreferrer">Fractures</a> - Modifiers on items can be fractured, meaning that they are fixed on the item and cannot be modified or removed through actions. Because our feature representation does not capture which mods are fractured, we are not allowed to change the fractures as part of the crafting process. While it would be nice to have fracturing as part of the optimization, this is not a big problem in practice beccause fractures are typically fixed at the start of the crafting process. We can simply treat each possible fracture as a different crafting problem.</li></ul><p>Despite these limitations, the above representation will be sufficient to solve a good portion of crafting problems in PoE.</p><p><small>(Update: I extended the feature representation with both mod families and mod tags after writing this post. While this significantly increased the computation required to solve the MDP, the approach presented here continued to work just fine)</small></p><h3 id="side-note-sim-to-real-gap-and-modifier-tags">Side note: Sim-to-Real Gap and modifier tags <span><a href="#side-note-sim-to-real-gap-and-modifier-tags" aria-label="Anchor">#</a></span></h3><p>In Reinforcement Learning, The <em>Sim-to-Real gap</em> (Simulation to Reality gap) is a term used to descripe the discrepancy between how an agent trained in a simulation behaves in the real world. There are many factors that can lead to discrepeancies. A simplified model of the world using a featurized representation of the state that does not capture all aspects of the real world is one of them. In my early experiments, I ran into this exact issue because our featurized representation did not include mod tags. The <a href="https://www.poewiki.net/wiki/Metamod" target="_blank" rel="noreferrer">metamod</a> “Cannot roll Attack Modifiers” changes which mods may be removed by an <a href="https://www.poewiki.net/wiki/Orb_of_Annulment" target="_blank" rel="noreferrer">Orb of Annulment</a>. Training an agent on a model using the featurized representation, it learned to use an Annuls in a certain state. However, if this state included a mod with the “Attack” tag, the agent would deterministically remove an unwanted modifier. In the real world, this could result in an infinite loop of the agent deterministically removing a target modifier and crafting it again. In this case, the problem was easily avoided by adding a simple heuristic (see below) as opposed to expanding the feature representation.</p><h2 id="learning-a-model">Learning a Model <span><a href="#learning-a-model" aria-label="Anchor">#</a></span></h2><p>In featurized state space we can represent the distribution of outcomes for each action as a table in memory. We can also represent \(Q(s, a)\), where each entry is the expected return of taking action \(a\) in state \(s\), as a table and don’t need a function approximator such as a Neural Network. How do we learn such a table? We could use <em>model-free</em> control algorithms such as <a href="https://en.wikipedia.org/wiki/Q-learning" target="_blank" rel="noreferrer">Q-Learning</a> which do not require the full distribution \(P(s_{t+1} | s_t, a)\), only the ability to sample transitions and simulate the crafting process.</p><p>That would work, but we’ll take a slightly different approach here. Instead of learning \(Q(s, a)\) directly, we learn \(P(s_{t+1} | s_t, a)\) where \(s\) is now our feature vector instead of the original item representation. In other words, we learn a model of the dynamics of the crafting process in feature space. With such a model we can use it to solve the MDP using <a href="https://en.wikipedia.org/wiki/Markov_decision_process#Value_iteration" target="_blank" rel="noreferrer">q-value iteration</a> (dynamic programming). Because our space of \(|S| |A|\) is rather small, we can learn a model by iterating over all possible states and actions, sampling each transition N times, and counting the outcomes. We only need enough samples to get a good enough estimate of the outcome distribution. In my experiments, I found that \(10^5\) samples per action were sufficient. With a larger feature representation, we would need more samples.</p><p>Why learn a model of the environment and solve the MDP instead of directly learning a policy with model-free RL algorithms? There are some benefits to having a model. For example, you can freely change the reward function without needing to re-learn the model. But the short answer is that it seems to work better for our problem domain. In my experiments, Q-Learning and its variations took 10+ minutes to converge to a good policy. They were also quite sensitive to hyperparameters such as the learning rate, exploration rate and algorithm, and decay factors. In contrast, the model-based approach converged in seconds and required no hyperparameter tuning. The tradeoff is that this approach may not work for much larger state spaces. We are naively exploring the full space of possible outcomes for each action, and we can only get away with this because our featurized space is small.</p><p>In pseudoce, a naive implementation of learning a model is as simple as this:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span># Count all transitions</span>
</span></span><span><span><span>for</span> s <span>in</span> all_states:
</span></span><span><span>    <span>for</span> a <span>in</span> valid_actions(state):
</span></span><span><span>        <span>for</span> _ <span>in</span> range(NUM_SAMPLES):
</span></span><span><span>            s_next <span>=</span> apply_action(s, a)
</span></span><span><span>            model[(s, a)][s_next] <span>+=</span> <span>1</span>
</span></span><span><span>            counts[(s, a)] <span>+=</span> <span>1</span>
</span></span><span><span>
</span></span><span><span><span># Normalize the probabilities</span>
</span></span><span><span><span>for</span> (s, a) <span>in</span> model:
</span></span><span><span>    model[(s, a)] <span>/=</span> counts[(s, a)]
</span></span></code></pre></div><p>In practice, it’s a little bit more tricky. Before looking at more realistic code, let’s talk about a few optimizations we can make.</p><h3 id="afterstates">Afterstates <span><a href="#afterstates" aria-label="Anchor">#</a></span></h3><p>Let’s think about what happens when we apply an <a href="https://www.poewiki.net/wiki/Essence" target="_blank" rel="noreferrer">Essence</a> to an item. An Essence adds a fixed modifier (depending on the Essence type) and randomizes the rest of the item. In this case, the outcome distribution is independent of the current item state. We only need to learn the transition probabilities for \((s, a)\) pairs where \(a\) is some Essence once, and can re-use the result for all \(s\). Such cases are also called <em>afterstates</em>. They are common in board games like Tic-Tac-Toe where the value of an action is determined by the state of the board <em>after</em> the action is taken. It doesn’t what the board state was before, and you can get to the same board state in multiple ways. In our code, we can replace the \((s,a)\) tuple in \(P(s,a)\) with an enum:</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>pub</span> <span>enum</span> <span>StateActionAlias</span><span>&lt;</span>S<span>&gt;</span> {
</span></span><span><span>    Pair((S, Action)),
</span></span><span><span>    AfterEssence(Action),
</span></span><span><span>    <span>// more possible afterstates ...
</span></span></span><span><span><span></span>}
</span></span></code></pre></div><p>In practice, we need to be a bit careful with this. Requirements often change. For example, applying an Essence only leads to the same distribution if we don’t change fractured modifiers as part of the crafting process. Currently we don’t, but in the future we might, and then our afterstates will lead to incorrect results.</p><h3 id="pruning-the-action-space-with-heuristics">Pruning the action space with heuristics <span><a href="#pruning-the-action-space-with-heuristics" aria-label="Anchor">#</a></span></h3><p>We can use heuristics to filter out obviously bad actions based on the current state, reducing the size of the action space. For example, using the <a href="https://www.poewiki.net/wiki/Crafting_Bench" target="_blank" rel="noreferrer">crafting bench</a> to deterministically add a modifier to an item is almost always the last step in a craft. It’s only a good idea when the rest of the item is already done.</p><h3 id="use-full-distributions-when-possible">Use full distributions when possible <span><a href="#use-full-distributions-when-possible" aria-label="Anchor">#</a></span></h3><p>We’ve already discussed that we cannot express the full distribution of outcomes in general. But for some actions we can. In such cases we don’t need to waste computation trying to approximate the distribution using sampling techniques. We can just update the model with the distribution directly. Only for state-action pairs where the distribution is too large do we need to sample transitions. I did not implement this optimization in my code because performance was good enough for my use case already, but it’s something that will come in handy when scaling up to larger state spaces.</p><h3 id="implementation">Implementation <span><a href="#implementation" aria-label="Anchor">#</a></span></h3><p>With these optimizations, the implementation of learning a model looks roughly as follows. Unlike the pseudocode above, we are no longer iterating over all states and actions. Instead, we “naturally” explore the state space in depth-first manner by sampling the transitions and adding unseen states to a queue. The output of the model learning process is a table representing \(P(s_{t+1} | s_t, a)\).</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>/// Learn a table model by sampling transition probabilities
</span></span></span><span><span><span></span><span>pub</span> <span>fn</span> <span>learn_model_v2</span><span>&lt;</span>S, F<span>&gt;</span>(
</span></span><span><span>    <span>// description of the crafting task
</span></span></span><span><span><span></span>    task: <span>rl</span>::CraftingTask,
</span></span><span><span>    <span>// number of samples per state-action pair
</span></span></span><span><span><span></span>    samples_per_state: <span>usize</span>,
</span></span><span><span>    <span>// alias function mapping a state-action pair to an alias
</span></span></span><span><span><span></span>    alias_fn: <span>F</span>,
</span></span><span><span>    <span>// state featurizer function
</span></span></span><span><span><span></span>    state_featurizer: <span>impl</span> Fn(<span>&amp;</span>Item) -&gt; <span>S</span>,
</span></span><span><span>) -&gt; Result<span>&lt;</span>TableModelV2<span>&lt;</span>S, F<span>&gt;&gt;</span>
</span></span><span><span><span>where</span>
</span></span><span><span>    S: <span>rl</span>::StateRepr,
</span></span><span><span>    F: Fn(<span>&amp;</span>S, Action) -&gt; <span>StateActionAlias</span><span>&lt;</span>S<span>&gt;</span>,
</span></span><span><span>{
</span></span><span><span>    <span>let</span> <span>mut</span> model <span>=</span> TableModelV2Builder::new(alias_fn);
</span></span><span><span>
</span></span><span><span>    <span>// Keeps track of states we have already learned
</span></span></span><span><span><span></span>    <span>let</span> <span>mut</span> done_states <span>=</span> HashSet::new();
</span></span><span><span>
</span></span><span><span>    <span>// Keeps track of the states we need to learn, starting with the initial item state
</span></span></span><span><span><span></span>    <span>let</span> <span>mut</span> queue <span>=</span> Vec::new();
</span></span><span><span>    queue.push(task.base.clone());
</span></span><span><span>
</span></span><span><span>    <span>loop</span> {
</span></span><span><span>        <span>let</span> item <span>=</span> <span>match</span> queue.pop() {
</span></span><span><span>            Some(item) <span>=&gt;</span> item,
</span></span><span><span>            <span>// We are done
</span></span></span><span><span><span></span>            None <span>=&gt;</span> <span>break</span>,
</span></span><span><span>        };
</span></span><span><span>
</span></span><span><span>        <span>// If the state is terminal we don&#39;t need to sample further transitions from it
</span></span></span><span><span><span></span>        <span>if</span> task.target.is_item_done(<span>&amp;</span>item) {
</span></span><span><span>            <span>continue</span>;
</span></span><span><span>        }
</span></span><span><span>
</span></span><span><span>        <span>// Compute the feature representation of the ite,
</span></span></span><span><span><span></span>        <span>let</span> features <span>=</span> state_featurizer(<span>&amp;</span>item);
</span></span><span><span>
</span></span><span><span>        <span>// Mark the state as done
</span></span></span><span><span><span></span>        done_states.insert(features);
</span></span><span><span>
</span></span><span><span>        <span>// Get the valid actions for the item
</span></span></span><span><span><span></span>        <span>let</span> actions <span>=</span> task.action_space.all_for_item(<span>&amp;</span>item);
</span></span><span><span>
</span></span><span><span>        <span>// For each action, sample the transition N times
</span></span></span><span><span><span></span>        <span>for</span> action <span>in</span> actions {
</span></span><span><span>
</span></span><span><span>            <span>// If we have already learned a model for this state-action pair, skip it
</span></span></span><span><span><span></span>            <span>if</span> model.check_alias(features, action) {
</span></span><span><span>                <span>continue</span>;
</span></span><span><span>            }
</span></span><span><span>
</span></span><span><span>            <span>// Sample the transition N times
</span></span></span><span><span><span></span>            <span>for</span> _ <span>in</span> <span>0</span><span>..</span>samples_per_state {
</span></span><span><span>                <span>let</span> <span>mut</span> item <span>=</span> item.clone();
</span></span><span><span>                action.apply(<span>&amp;</span><span>mut</span> item)<span>?</span>;
</span></span><span><span>
</span></span><span><span>                <span>let</span> next_features <span>=</span> state_featurizer(<span>&amp;</span>item);
</span></span><span><span>
</span></span><span><span>                <span>// Update the model with the transition
</span></span></span><span><span><span></span>                model.update(features, action, next_features);
</span></span><span><span>
</span></span><span><span>                <span>// If we have not learned a model for the resulting state yet, add it to the queue
</span></span></span><span><span><span></span>                <span>if</span> <span>!</span>done_states.contains(<span>&amp;</span>next_features) {
</span></span><span><span>                    done_states.insert(next_features); <span>// avoid duplicates
</span></span></span><span><span><span></span>                    queue.push(item.clone());
</span></span><span><span>                }
</span></span><span><span>            }
</span></span><span><span>        }
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    <span>// Normalizes transition probabilities
</span></span></span><span><span><span></span>    Ok(model.build())
</span></span><span><span>}
</span></span></code></pre></div><h2 id="solving-the-mdp">Solving the MDP <span><a href="#solving-the-mdp" aria-label="Anchor">#</a></span></h2><p>With a learned distribution model of the environment (in featurized space) it is now trivial to solve the resulting MDP using <a href="https://en.wikipedia.org/wiki/Markov_decision_process#Value_iteration" target="_blank" rel="noreferrer">q-value iteration</a>, which is the continuous application of the <a href="https://en.wikipedia.org/wiki/Bellman_equation" target="_blank" rel="noreferrer">Bellman Equation</a> that expresses the optimal value of a state-action pair in terms of the expected reward of taking the action and the value of the next state:</p><p>$$
\begin{aligned}
Q(s, a) &amp;= R(s, a) + \gamma \sum_{s’} P(s’ | s, a) \max_{a’} Q(s’, a’) \
\end{aligned}
$$</p><p>Here, \(\gamma\) is the discount factor, which is mostly important for convergence properties in non-episodic tasks. We always set it to 1 because we don’t need or want to discount future rewards. The algorithm converges once the change in the value function falls below a threshold \(\theta\). In our code, we also return the state-value function \(V(s) = \max_a Q(s, a)\), which is the expected return when following optimal policy from that state. We do this only because it’s “free” to compute during the algorithm and we might as well have it for debugging purposes, but we don’t use it for anything important.</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>/// Compute the optimal state-action value function using the Bellman equation
</span></span></span><span><span><span></span><span>pub</span> <span>fn</span> <span>value_iteration</span>(<span>&amp;</span>self, gamma: <span>f64</span>, theta: <span>f64</span>) -&gt; (ValueFn<span>&lt;</span>S<span>&gt;</span>, StateActionValueFn<span>&lt;</span>S<span>&gt;</span>) {
</span></span><span><span>    <span>// State-value function
</span></span></span><span><span><span></span>    <span>let</span> <span>mut</span> v <span>=</span> HashMap::new();
</span></span><span><span>    <span>// State-action value function
</span></span></span><span><span><span></span>    <span>let</span> <span>mut</span> q <span>=</span> HashMap::new();
</span></span><span><span>
</span></span><span><span>    <span>let</span> <span>mut</span> i <span>=</span> <span>0</span>;
</span></span><span><span>    <span>loop</span> {
</span></span><span><span>        <span>let</span> <span>mut</span> delta: <span>f64</span> <span>=</span> <span>0.0</span>;
</span></span><span><span>
</span></span><span><span>        <span>// For each state, update the value function
</span></span></span><span><span><span></span>        <span>for</span> state <span>in</span> self.model.state_space() {
</span></span><span><span>            <span>let</span> <span>mut</span> max_q <span>=</span> <span>f64</span>::<span>NEG_INFINITY</span>;
</span></span><span><span>            <span>let</span> state <span>=</span> <span>*</span>state;
</span></span><span><span>
</span></span><span><span>            <span>// For each action in the state
</span></span></span><span><span><span></span>            <span>for</span> action <span>in</span> self.model.valid_actions(<span>&amp;</span>state).unwrap().iter() {
</span></span><span><span>
</span></span><span><span>                <span>// Compute the expected value of the action by taking the expectation over the next states
</span></span></span><span><span><span></span>                <span>let</span> action <span>=</span> <span>*</span>action;
</span></span><span><span>                <span>let</span> transitions <span>=</span> self.model.distribution(state, action);
</span></span><span><span>                <span>let</span> reward <span>=</span> (self.reward_fn)(<span>&amp;</span>state, action);
</span></span><span><span>                <span>let</span> <span>mut</span> new_q_value <span>=</span> reward;
</span></span><span><span>                <span>for</span> (next_state, prob) <span>in</span> transitions {
</span></span><span><span>                    <span>let</span> next_value <span>=</span> v.get(<span>&amp;</span>next_state).unwrap_or(<span>&amp;</span><span>0.0</span>);
</span></span><span><span>                    new_q_value <span>+=</span> gamma <span>*</span> prob <span>*</span> next_value;
</span></span><span><span>                }
</span></span><span><span>
</span></span><span><span>                <span>// Update and track the maximum delta 
</span></span></span><span><span><span></span>                <span>let</span> q_value <span>=</span> q.entry((state, action)).or_insert(<span>0.0</span>);
</span></span><span><span>                delta <span>=</span> delta.max((new_q_value <span>-</span> <span>*</span>q_value).abs());
</span></span><span><span>                <span>*</span>q_value <span>=</span> new_q_value;
</span></span><span><span>
</span></span><span><span>                max_q <span>=</span> max_q.max(new_q_value);
</span></span><span><span>            }
</span></span><span><span>
</span></span><span><span>            <span>// Update the state-value function with the maximum action value
</span></span></span><span><span><span></span>            v.insert(state, max_q);
</span></span><span><span>        }
</span></span><span><span>
</span></span><span><span>        <span>// check for convergence
</span></span></span><span><span><span></span>        <span>if</span> delta <span>&lt;</span> theta {
</span></span><span><span>            <span>break</span>;
</span></span><span><span>        }
</span></span><span><span>        i <span>+=</span> <span>1</span>;
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    tracing::info!(<span>%</span>i, <span>%</span>theta, <span>&#34;q-value iteration done&#34;</span>);
</span></span><span><span>    (v, q)
</span></span><span><span>}
</span></span></code></pre></div><p>From the Q-function we can now derive our policy \(\pi(s) = \text{argmax}_a Q(s, a)\) which maps each state to the optimal action to take.</p><h2 id="example-crafts">Example Crafts <span><a href="#example-crafts" aria-label="Anchor">#</a></span></h2><p>A good test case for our algorithm is <a href="https://maxroll.gg/poe/crafting/boneshatter-juggernaut-axe" target="_blank" rel="noreferrer">this leaguestart Boneshatter Axe</a>. The crafting process is rather simple, but in the second step the optimal action is to use a “Cannot Roll Attack Mods” <a href="https://www.poewiki.net/wiki/Metamod" target="_blank" rel="noreferrer">metamod</a> in combination with an Orb of Annulment, which wouldn’t be immediately obvious to many players. Let’s see if our algorithm discovers this optimal policy.</p><h3 id="optimizing-for-path-length">Optimizing for Path Length <span><a href="#optimizing-for-path-length" aria-label="Anchor">#</a></span></h3><p>Remember that setting the reward function to a constant <code>-1</code> encourages the agent to craft the item in as few steps as possible. In this case, the values of \(Q(s, a)\) represent the expected remaining number of steps to reach the target item from a given state. Let’s look at some of these q-values. The full table would be too large to show, but here is a slice, mapping our featurized representation and action to the expected number of remaining steps:</p><pre tabindex="0"><code>[Magic |00000100|1|0] -&gt; Regal      | -131.12
[Magic |00000100|1|1] -&gt; Regal      | -131.12
[Magic |00000101|1|1] -&gt; Regal      | -132.11
[Magic |00000110|1|1] -&gt; Regal      | -131.13
[Rare  |00000100|1|0] -&gt; Contempt(7) | -130.13
[Rare  |00000100|1|1] -&gt; Contempt(7) | -130.12
[Rare  |00000100|1|2] -&gt; Contempt(7) | -130.13
[Rare  |00000100|1|3] -&gt; Contempt(7) | -130.13
[Rare  |00000100|2|0] -&gt; Contempt(7) | -130.12
[Rare  |00000100|2|1] -&gt; Contempt(7) | -130.13
[Rare  |00000100|2|2] -&gt; Contempt(7) | -130.13
[Rare  |00000100|2|3] -&gt; Contempt(7) | -130.13
[Rare  |00000110|1|3] -&gt; Contempt(7) | -130.13
[Rare  |00000110|2|1] -&gt; Contempt(7) | -130.12
[Rare  |00001110|3|3] -&gt; Contempt(7) | -130.13
[Rare  |00001111|2|2] -&gt; RemoveBench | -131.12
[Rare  |00010110|2|1] -&gt; Contempt(7) | -130.12
[Rare  |00010110|2|2] -&gt; Contempt(7) | -130.12
[Rare  |00010110|2|3] -&gt; Contempt(7) | -130.13
[Rare  |00010111|2|2] -&gt; RemoveBench | -131.12
[Rare  |00010111|2|3] -&gt; RemoveBench | -131.12
[Rare  |00011100|3|0] -&gt; BenchCraft(StrIntMasterItemGenerationCanHaveMultipleCraftedMods) | -3.00
[Rare  |00011100|3|1] -&gt; BenchCraft(IntMasterItemGenerationCannotRollAttackAffixes) | -8.01
[Rare  |00011100|3|2] -&gt; BenchCraft(IntMasterItemGenerationCannotRollAttackAffixes) | -11.03
[Rare  |00011100|3|3] -&gt; Annulment  | -59.56
[Rare  |00011101|3|1] -&gt; RemoveBench | -4.00
[Rare  |00011101|3|2] -&gt; Annulment  | -7.01
[Rare  |00011101|3|3] -&gt; Annulment  | -10.03
[Rare  |00011110|3|1] -&gt; BenchCraft(EinharMasterLocalIncreasedAttackSpeed3) | -2.00
[Rare  |00011110|3|2] -&gt; BenchCraft(IntMasterItemGenerationCannotRollAttackAffixes) | -8.51
[Rare  |00011110|3|3] -&gt; RemoveBench | -12.03
[Rare  |00011111|3|2] -&gt; RemoveBench | -4.00
[Rare  |00011111|3|3] -&gt; Annulment  | -7.51
[Rare  |00100100|1|1] -&gt; Contempt(7) | -130.13
[Rare  |00111110|3|2] -&gt; BenchCraft(EinharMasterLocalIncreasedAttackSpeed3) | -1.00
[Rare  |00111110|3|3] -&gt; RemoveBench | -9.01
[Rare  |00111111|3|3] -&gt; Annulment  | -4.00
[Rare  |01000100|1|1] -&gt; Contempt(7) | -130.12
[Rare  |01000100|1|2] -&gt; Contempt(7) | -130.13
[Rare  |01000100|1|3] -&gt; Contempt(7) | -130.13
[Rare  |01000101|1|2] -&gt; RemoveBench | -131.12
[Rare  |01010100|2|1] -&gt; Contempt(7) | -130.12
[Rare  |01010111|2|3] -&gt; RemoveBench | -131.12
[Rare  |01011110|3|2] -&gt; BenchCraft(JunMaster2DoubleDamageChance2h2) | -1.00
[Rare  |01011110|3|3] -&gt; RemoveBench | -9.01
[Rare  |01011111|3|3] -&gt; Annulment  | -3.51
</code></pre><p>The perhaps most interesting entry is the value of the optimal action in the initial state, which is <code>[Magic |00000100|1|0]</code> in our case. This tells us the that expected total reward, i.e. the expected number of steps to craft the item from scratch, is <code>-131.12</code>. Most of these steps consist of spamming <a href="https://www.poewiki.net/wiki/Essence_of_Contempt" target="_blank" rel="noreferrer">Essences of Contempt</a> until we are lucky to hit the modifiers we want. Once we hit these modifiers, we can finish up the item with a few deterministic benchcrafts. The following picture shows an example rollout of our learned optimal policy with pruning of repeated steps for visualization. Since the expected number of steps is 131, the full graph for an average craft would be too large to show here. Each node shows the current item mods and each edge shows the action taken and reward received. Our learned optimal policy matches the expert policy from the guide above, including the use of the metamod in the second stage.</p></div></section></article></div></div>
  </body>
</html>
