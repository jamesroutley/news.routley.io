<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.synacktiv.com/en/publications/exploring-grapheneos-secure-allocator-hardened-malloc">Original</a>
    <h1>Exploring GrapheneOS secure allocator: Hardened Malloc</h1>
    
    <div id="readability-page-1" class="page"><p>GrapheneOS is a mobile operating system based on Android and focusing on privacy and security. To enhance further the security of their product, GrapheneOS developers introduced a new libc allocator : <strong>hardened malloc.</strong> This allocator has a security-focused design in mind to protect processes against common memory corruption vulnerabilities. This article will explain in details its internal architecture and how security mitigation are implemented from a security researcher point of view.</p><div property="schema:text"><h2>Introduction</h2>
<p>GrapheneOS is a security and privacy-focused mobile operating system based on a modified version of Android (AOSP). To enhance its protection, it integrates advanced security features, including its own memory allocator for libc: <strong>hardened malloc</strong>. Designed to be as robust as the operating system itself, this allocator specifically seeks to protect against memory corruption.</p>
<p>This technical article details the internal workings of hardened malloc and the protection mechanisms it implements <span>to prevent common memory corruption vulnerabilities</span>. It is intended for a technical audience, particularly security researchers or exploit developers, who wish to gain an in-depth understanding of this <span>allocator&#39;s internals</span>.</p>
<p>The analyses and tests in this article were performed on two devices running GrapheneOS:</p>
<ul>
<li><strong>Pixel 4a 5G</strong>: <code>google/bramble/bramble:14/UP1A.231105.001.B2/2025021000:user/release-keys</code></li>
<li><strong>Pixel 9a</strong>: <code>google/tegu/tegu:16/BP2A.250705.008/2025071900:user/release-keys</code></li>
</ul>
<p>The devices were rooted with Magisk 29 in order to use Frida to observe the internal state of <strong>hardened malloc</strong> within system processes. The study was based on the source code from the official <a href="https://github.com/GrapheneOS/hardened_malloc">GrapheneOS GitHub repository</a> (commit <code>7481c8857faf5c6ed8666548d9e92837693de91b</code>).</p>
<h2>GrapheneOS</h2>
<p><strong>GrapheneOS</strong> is a hardened operating system based on Android. As an actively maintained open-source project, it benefits from frequent updates and the swift application of security patches. All information is available on <a href="https://grapheneos.org/">GrapheneOS website</a>.</p>
<p>To effectively protect the processes running on the device, GrapheneOS implements several security mechanisms. The following sections briefly describe the specific mechanisms that contribute to the hardening of its memory allocator.</p>
<h3>Extended Address Space</h3>
<p>On standard Android systems, the address space for userland processes is limited to 39 bits, ranging from <code>0</code> to <code>0x8000000000</code>. On GrapheneOS, this space is extended to 48 bits, and to take advantage of this extension, ASLR entropy has also been increased from 18 to 33 bits. This detail is important as <strong>hardened malloc</strong> relies heavily on <code>mmap</code> for its internal structures and its allocations.</p>
<pre><code>tegu:/ # cat /proc/self/maps
c727739a2000-c727739a9000 rw-p 00000000 00:00 0                          [anon:.bss]
c727739a9000-c727739ad000 r--p 00000000 00:00 0                          [anon:.bss]
c727739ad000-c727739b1000 rw-p 00000000 00:00 0                          [anon:.bss]
c727739b1000-c727739b5000 r--p 00000000 00:00 0                          [anon:.bss]
c727739b5000-c727739c1000 rw-p 00000000 00:00 0                          [anon:.bss]
e5af7fa30000-e5af7fa52000 rw-p 00000000 00:00 0                          [stack]
tegu:/ # cat /proc/self/maps
d112736be000-d112736c5000 rw-p 00000000 00:00 0                          [anon:.bss]
d112736c5000-d112736c9000 r--p 00000000 00:00 0                          [anon:.bss]
d112736c9000-d112736cd000 rw-p 00000000 00:00 0                          [anon:.bss]
d112736cd000-d112736d1000 r--p 00000000 00:00 0                          [anon:.bss]
d112736d1000-d112736dd000 rw-p 00000000 00:00 0                          [anon:.bss]
ea0de59be000-ea0de59e1000 rw-p 00000000 00:00 0                          [stack]
tegu:/ # cat /proc/self/maps
d71f87043000-d71f8704a000 rw-p 00000000 00:00 0                          [anon:.bss]
d71f8704a000-d71f8704e000 r--p 00000000 00:00 0                          [anon:.bss]
d71f8704e000-d71f87052000 rw-p 00000000 00:00 0                          [anon:.bss]
d71f87052000-d71f87056000 r--p 00000000 00:00 0                          [anon:.bss]
d71f87056000-d71f87062000 rw-p 00000000 00:00 0                          [anon:.bss]
f69f7c952000-f69f7c974000 rw-p 00000000 00:00 0                          [stack]
</code></pre>
<h3>Secure app spawning</h3>
<p>On standard Android, each application is launched via a <code>fork</code> of the <em>zygote</em> process. This mechanism, designed to speed up startup, has a major security consequence: all applications inherit the same address space as <em>zygote</em>. In practice, this means that pre-loaded libraries end up at identical addresses from one application to another. For an attacker, this predictability makes it easy to bypass ASLR protection without needing a prior information leak.</p>
<p>T<span>o overcome this limitation</span>, GrapheneOS fundamentally changes this process. Instead of just a <code>fork</code>, new applications are launched with <code>exec</code>. This method creates an entirely new and randomized address space for each process, thereby restoring the full effectiveness of ASLR. It is no longer possible to predict the location of remote memory regions. This enhanced security does, however, come at a cost: a slight impact on launch performance and an increased memory footprint for each application.</p>
<pre><code>tegu:/ # cat /proc/$(pidof zygote64)/maps | grep libc\.so
d6160aac0000-d6160ab19000 r--p 00000000 07:f0 24                         /apex/com.android.runtime/lib64/bionic/libc.so
d6160ab1c000-d6160abbe000 r-xp 0005c000 07:f0 24                         /apex/com.android.runtime/lib64/bionic/libc.so
d6160abc0000-d6160abc5000 r--p 00100000 07:f0 24                         /apex/com.android.runtime/lib64/bionic/libc.so
d6160abc8000-d6160abc9000 rw-p 00108000 07:f0 24                         /apex/com.android.runtime/lib64/bionic/libc.so
tegu:/ # cat /proc/$(pidof com.android.messaging)/maps | grep libc\.so
d5e4a9c68000-d5e4a9cc1000 r--p 00000000 07:f0 24                         /apex/com.android.runtime/lib64/bionic/libc.so
d5e4a9cc4000-d5e4a9d66000 r-xp 0005c000 07:f0 24                         /apex/com.android.runtime/lib64/bionic/libc.so
d5e4a9d68000-d5e4a9d6d000 r--p 00100000 07:f0 24                         /apex/com.android.runtime/lib64/bionic/libc.so
d5e4a9d70000-d5e4a9d71000 rw-p 00108000 07:f0 24                         /apex/com.android.runtime/lib64/bionic/libc.so
tegu:/ # cat /proc/$(pidof com.topjohnwu.magisk)/maps | grep libc\.so
dabc42ac5000-dabc42b1e000 r--p 00000000 07:f0 24                         /apex/com.android.runtime/lib64/bionic/libc.so
dabc42b21000-dabc42bc3000 r-xp 0005c000 07:f0 24                         /apex/com.android.runtime/lib64/bionic/libc.so
dabc42bc5000-dabc42bca000 r--p 00100000 07:f0 24                         /apex/com.android.runtime/lib64/bionic/libc.so
dabc42bcd000-dabc42bce000 rw-p 00108000 07:f0 24                         /apex/com.android.runtime/lib64/bionic/libc.so
</code></pre>
<h3>Memory Tagging Extension (MTE)</h3>
<p><strong>Memory Tagging Extension</strong>, or <strong>MTE</strong>, is an extension of the ARM architecture introduced with Armv8.5. MTE aims to prevent memory corruption vulnerabilities from being exploited by an attacker. This protection relies on a mechanism of tagging memory regions.</p>
<p>During an allocation, a 4-bit tag is associated with the allocated region and stored in the top bits of the pointer. To access the data, both the address and the tag must be correct. If the tag is wrong, an exception is raised. This mechanism allows for the detection and blocking of vulnerabilities such as <em>out-of-bound reads/writes</em> and <em>use-after-free</em>.</p>
<p>For example, the out-of-bounds write in the following C code could be detected, depending on the allocator&#39;s implementation with MTE:</p>
<pre><code>char* ptr = malloc(8); 
ptr[16] = 12; // oob write, this tag is not valid for the area</code></pre>
<p>Since MTE is a feature offered by the CPU, it is necessary for the hardware to be compatible. This is the case for all Google Pixel smartphones since the Pixel 8. For more information, refer to the <a href="https://developer.arm.com/documentation/108035/0100/Introduction-to-the-Memory-Tagging-Extension?lang=en">ARM documentation</a>.</p>
<p><strong>Hardened malloc</strong> therefore uses MTE on compatible smartphones to prevent this type of memory corruption.</p>
<p><span>In order to benefit from MTE, a binary must be compiled with the </span>appropriate flags. For the purposes of this article, the flags below were added to the <code>Application.mk</code> file of our test binaries to enable MTE.</p>
<pre><code>APP_CFLAGS := -fsanitize=memtag -fno-omit-frame-pointer -march=armv8-a+memtag
APP_LDFLAGS := -fsanitize=memtag -march=armv8-a+memtag</code></pre>
<p>The <a href="https://developer.android.com/ndk/guides/arm-mte">Android documentation</a> provides all the necessary information to create an MTE-compatible application.</p>
<p><strong>Hardened malloc</strong> relies heavily on MTE by adding tags to its allocations. Please note that only small allocation (less than <code>0x20000</code> bytes) are tagged.</p>
<h2>Hardened malloc architecture</h2>
<p>To enhance security, <strong>hardened malloc</strong> isolates metadata from user data in separate memory regions, holding it primarily within two main structures :</p>
<ul>
<li><code>ro</code>: the main structure in the <code>.bss</code> section of libc.</li>
<li><code>allocator_state</code>: a large structure grouping all metadata for the different allocation types. Its memory region is reserved only once at initialization.</li>
</ul>
<figure role="group">
<img alt="Architecture globale" data-entity-type="file" data-entity-uuid="3334307a-2616-4a1c-9f58-ba09f42538aa" height="778" src="https://www.synacktiv.com/sites/default/files/inline-images/global_architecture_0.webp" width="1061"/>
<figcaption>Main structures</figcaption>
</figure>
<p>Similar to <em>jemalloc</em>, <strong>hardened malloc</strong> partitions threads into <em>arenas</em>, with each arena managing its own allocations. This implies that memory allocated in one arena cannot be managed or freed by another arena. However, there is no explicit data structure to define these arenas; their existence is implicit and primarily affects the size of certain internal arrays.</p>
<p>Although the arena concept is present in the source code, analysis of the libc binaries from the test devices revealed that <strong>hardened malloc</strong> was compiled to use only a single arena. As a result, all threads share the same pool of allocation metadata.</p>
<h3>ro structure</h3>
<p>The <code>ro</code> structure is the allocator&#39;s main metadata structure. It is contained within the <code>.bss</code> section of libc and consists of the following attributes:</p>
<pre><code>static union {
    struct {
        void *slab_region_start;
        void *_Atomic slab_region_end;
        struct size_class *size_class_metadata[N_ARENA];
        struct region_allocator *region_allocator;
        struct region_metadata *regions[2];
#ifdef USE_PKEY
        int metadata_pkey;
#endif
#ifdef MEMTAG
        bool is_memtag_disabled;
#endif
    };
    char padding[PAGE_SIZE];
} ro __attribute__((aligned(PAGE_SIZE)));</code></pre>
<ul>
<li>
<p><code>slab_region_start</code>: The start of the memory area containing the regions for small allocations.</p>
</li>
<li>
<p><code>slab_region_end</code>: The end of the memory area containing the regions for small allocations.</p>
</li>
<li>
<p><code>size_class_metadata[N_ARENA]</code>: An array of pointers to the metadata for small allocations, per arena.</p>
</li>
<li>
<p><code>region_allocator</code>: A pointer to the management structure for large allocations.</p>
</li>
<li>
<p><code>regions[2]</code>: A pointer to the hash tables that reference the large allocations.</p>
</li>
</ul>
<h3>allocator_state</h3>
<p>This structure contains all the metadata used for both <strong>small</strong> and <strong>large allocations</strong>. It is mapped only once when the allocator initializes and is isolated by <strong>guard pages</strong>. Its size is fixed and computed based on the maximum number of allocations the allocator can handle.</p>
<pre><code>struct __attribute__((aligned(PAGE_SIZE))) allocator_state {
    struct size_class size_class_metadata[N_ARENA][N_SIZE_CLASSES];
    struct region_allocator region_allocator;
    // padding until next page boundary for mprotect
    struct region_metadata regions_a[MAX_REGION_TABLE_SIZE] __attribute__((aligned(PAGE_SIZE)));
    // padding until next page boundary for mprotect
    struct region_metadata regions_b[MAX_REGION_TABLE_SIZE] __attribute__((aligned(PAGE_SIZE)));
    // padding until next page boundary for mprotect
    struct slab_info_mapping slab_info_mapping[N_ARENA][N_SIZE_CLASSES];
    // padding until next page boundary for mprotect
};</code></pre>
<ul>
<li>
<p><code>size_class_metadata[N_ARENA][N_SIZE_CLASSES]</code>: An array of <code>size_class</code> structures containing the metadata for small allocations for each class.</p>
</li>
<li>
<p><code>region_allocator</code>: The metadata for the <em>large</em> allocations regions.</p>
</li>
<li>
<p><code>regions_a/b[MAX_REGION_TABLE_SIZE]</code>: A hash table that groups information about the mappings of large allocations.</p>
</li>
<li>
<p><code>slab_info_mapping</code>: The metadata for the slabs of small allocations.</p>
</li>
</ul>
<h3>User data</h3>
<p><strong>Hardened malloc</strong> stores user data in two types of regions, separate from its metadata:</p>
<ul>
<li>
<p><em>Slabs region</em>: a very large area reserved only once at initialization, which contains the slabs for small allocations. It is initialized in the <code>init_slow_path</code> function and its starting address is stored in <code>ro.slab_region_start</code>.</p>
</li>
<li>
<p><em>Large regions</em>: dynamically reserved areas that hold the data for large allocations. Each such region contains only a single large allocation.</p>
</li>
</ul>
<h2>Allocations</h2>
<p>There are two types of allocations in <strong>hardened malloc</strong>: <em>small</em> allocations and <em>large</em> allocations.</p>
<h3>Small allocations</h3>
<h4>Size classes/bins</h4>
<p>Small allocations are categorized by size into <strong>size classes</strong>, also known as <strong>bins</strong>. <strong>hardened malloc</strong> utilizes 49 such classes, which are indexed by increasing size and represented by the <code>size_class</code> structure:</p>
<table>
<caption>Small allocations size classes</caption>
<thead>
<tr>
<th>Size Class</th>
<th>Total Bin Size</th>
<th>Available Size</th>
<th>Slots</th>
<th>Slab size</th>
<th>Max slabs</th>
<th>Quarantines Size (random / FIFO)</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0x10</td>
<td>0x10</td>
<td>256</td>
<td>0x1000</td>
<td>8388608</td>
<td>8192 / 8192</td>
</tr>
<tr>
<td>1</td>
<td>0x10</td>
<td>0x8</td>
<td>256</td>
<td>0x1000</td>
<td>8388608</td>
<td>8192 / 8192</td>
</tr>
<tr>
<td>2</td>
<td>0x20</td>
<td>0x18</td>
<td>128</td>
<td>0x1000</td>
<td>8388608</td>
<td>4096 / 4096</td>
</tr>
<tr>
<td>3</td>
<td>0x30</td>
<td>0x28</td>
<td>85</td>
<td>0x1000</td>
<td>8388608</td>
<td>4096 / 4096</td>
</tr>
<tr>
<td>4</td>
<td>0x40</td>
<td>0x38</td>
<td>64</td>
<td>0x1000</td>
<td>8388608</td>
<td>2048 / 2048</td>
</tr>
<tr>
<td>5</td>
<td>0x50</td>
<td>0x48</td>
<td>51</td>
<td>0x1000</td>
<td>8388608</td>
<td>2048 / 2048</td>
</tr>
<tr>
<td>6</td>
<td>0x60</td>
<td>0x58</td>
<td>42</td>
<td>0x1000</td>
<td>8388608</td>
<td>2048 / 2048</td>
</tr>
<tr>
<td>7</td>
<td>0x70</td>
<td>0x68</td>
<td>36</td>
<td>0x1000</td>
<td>8388608</td>
<td>2048 / 2048</td>
</tr>
<tr>
<td>8</td>
<td>0x80</td>
<td>0x78</td>
<td>64</td>
<td>0x2000</td>
<td>4194304</td>
<td>1024 / 1024</td>
</tr>
<tr>
<td>9</td>
<td>0xa0</td>
<td>0x98</td>
<td>51</td>
<td>0x2000</td>
<td>4194304</td>
<td>1024 / 1024</td>
</tr>
<tr>
<td>10</td>
<td>0xc0</td>
<td>0xb8</td>
<td>64</td>
<td>0x3000</td>
<td>2796202</td>
<td>1024 / 1024</td>
</tr>
<tr>
<td>11</td>
<td>0xe0</td>
<td>0xd8</td>
<td>54</td>
<td>0x3000</td>
<td>2796202</td>
<td>1024 / 1024</td>
</tr>
<tr>
<td>12</td>
<td>0x100</td>
<td>0xf8</td>
<td>64</td>
<td>0x4000</td>
<td>2097152</td>
<td>512 / 512</td>
</tr>
<tr>
<td>13</td>
<td>0x140</td>
<td>0x138</td>
<td>64</td>
<td>0x5000</td>
<td>1677721</td>
<td>512 / 512</td>
</tr>
<tr>
<td>14</td>
<td>0x180</td>
<td>0x178</td>
<td>64</td>
<td>0x6000</td>
<td>1398101</td>
<td>512 / 512</td>
</tr>
<tr>
<td>15</td>
<td>0x1c0</td>
<td>0x1b8</td>
<td>64</td>
<td>0x7000</td>
<td>1198372</td>
<td>512 / 512</td>
</tr>
<tr>
<td>16</td>
<td>0x200</td>
<td>0x1f8</td>
<td>64</td>
<td>0x8000</td>
<td>1048576</td>
<td>256 / 256</td>
</tr>
<tr>
<td>17</td>
<td>0x280</td>
<td>0x278</td>
<td>64</td>
<td>0xa000</td>
<td>838860</td>
<td>256 / 256</td>
</tr>
<tr>
<td>18</td>
<td>0x300</td>
<td>0x2f8</td>
<td>64</td>
<td>0xc000</td>
<td>699050</td>
<td>256 / 256</td>
</tr>
<tr>
<td>19</td>
<td>0x380</td>
<td>0x378</td>
<td>64</td>
<td>0xe000</td>
<td>599186</td>
<td>256 / 256</td>
</tr>
<tr>
<td>20</td>
<td>0x400</td>
<td>0x3f8</td>
<td>64</td>
<td>0x10000</td>
<td>524288</td>
<td>128 / 128</td>
</tr>
<tr>
<td>21</td>
<td>0x500</td>
<td>0x4f8</td>
<td>16</td>
<td>0x5000</td>
<td>1677721</td>
<td>128 / 128</td>
</tr>
<tr>
<td>22</td>
<td>0x600</td>
<td>0x5f8</td>
<td>16</td>
<td>0x6000</td>
<td>1398101</td>
<td>128 / 128</td>
</tr>
<tr>
<td>23</td>
<td>0x700</td>
<td>0x6f8</td>
<td>16</td>
<td>0x7000</td>
<td>1198372</td>
<td>128 / 128</td>
</tr>
<tr>
<td>24</td>
<td>0x800</td>
<td>0x7f8</td>
<td>16</td>
<td>0x8000</td>
<td>1048576</td>
<td>64 / 64</td>
</tr>
<tr>
<td>25</td>
<td>0xa00</td>
<td>0x9f8</td>
<td>8</td>
<td>0x5000</td>
<td>1677721</td>
<td>64 / 64</td>
</tr>
<tr>
<td>26</td>
<td>0xc00</td>
<td>0xbf8</td>
<td>8</td>
<td>0x6000</td>
<td>1398101</td>
<td>64 / 64</td>
</tr>
<tr>
<td>27</td>
<td>0xe00</td>
<td>0xdf8</td>
<td>8</td>
<td>0x7000</td>
<td>1198372</td>
<td>64 / 64</td>
</tr>
<tr>
<td>28</td>
<td>0x1000</td>
<td>0xff8</td>
<td>8</td>
<td>0x8000</td>
<td>1048576</td>
<td>32 / 32</td>
</tr>
<tr>
<td>29</td>
<td>0x1400</td>
<td>0x13f8</td>
<td>8</td>
<td>0xa000</td>
<td>838860</td>
<td>32 / 32</td>
</tr>
<tr>
<td>30</td>
<td>0x1800</td>
<td>0x17f8</td>
<td>8</td>
<td>0xc000</td>
<td>699050</td>
<td>32 / 32</td>
</tr>
<tr>
<td>31</td>
<td>0x1c00</td>
<td>0x1bf8</td>
<td>8</td>
<td>0xe000</td>
<td>599186</td>
<td>32 / 32</td>
</tr>
<tr>
<td>32</td>
<td>0x2000</td>
<td>0x1ff8</td>
<td>8</td>
<td>0x10000</td>
<td>524288</td>
<td>16 / 16</td>
</tr>
<tr>
<td>33</td>
<td>0x2800</td>
<td>0x27f8</td>
<td>6</td>
<td>0xf000</td>
<td>559240</td>
<td>16 / 16</td>
</tr>
<tr>
<td>34</td>
<td>0x3000</td>
<td>0x2ff8</td>
<td>5</td>
<td>0xf000</td>
<td>559240</td>
<td>16 / 16</td>
</tr>
<tr>
<td>35</td>
<td>0x3800</td>
<td>0x37f8</td>
<td>4</td>
<td>0xe000</td>
<td>599186</td>
<td>16 / 16</td>
</tr>
<tr>
<td>36</td>
<td>0x4000</td>
<td>0x3ff8</td>
<td>4</td>
<td>0x10000</td>
<td>524288</td>
<td>8 / 8</td>
</tr>
<tr>
<td>37</td>
<td>0x5000</td>
<td>0x4ff8</td>
<td>1</td>
<td>0x5000</td>
<td>1677721</td>
<td>8 / 8</td>
</tr>
<tr>
<td>38</td>
<td>0x6000</td>
<td>0x5ff8</td>
<td>1</td>
<td>0x6000</td>
<td>1398101</td>
<td>8 / 8</td>
</tr>
<tr>
<td>39</td>
<td>0x7000</td>
<td>0x6ff8</td>
<td>1</td>
<td>0x7000</td>
<td>1198372</td>
<td>8 / 8</td>
</tr>
<tr>
<td>40</td>
<td>0x8000</td>
<td>0x7ff8</td>
<td>1</td>
<td>0x8000</td>
<td>1048576</td>
<td>4 / 4</td>
</tr>
<tr>
<td>41</td>
<td>0xa000</td>
<td>0x9ff8</td>
<td>1</td>
<td>0xa000</td>
<td>838860</td>
<td>4 / 4</td>
</tr>
<tr>
<td>42</td>
<td>0xc000</td>
<td>0xbff8</td>
<td>1</td>
<td>0xc000</td>
<td>699050</td>
<td>4 / 4</td>
</tr>
<tr>
<td>43</td>
<td>0xe000</td>
<td>0xdff8</td>
<td>1</td>
<td>0xe000</td>
<td>599186</td>
<td>4 / 4</td>
</tr>
<tr>
<td>44</td>
<td>0x10000</td>
<td>0xfff8</td>
<td>1</td>
<td>0x10000</td>
<td>524288</td>
<td>2 / 2</td>
</tr>
<tr>
<td>45</td>
<td>0x14000</td>
<td>0x13ff8</td>
<td>1</td>
<td>0x14000</td>
<td>419430</td>
<td>2 / 2</td>
</tr>
<tr>
<td>46</td>
<td>0x18000</td>
<td>0x17ff8</td>
<td>1</td>
<td>0x18000</td>
<td>349525</td>
<td>2 / 2</td>
</tr>
<tr>
<td>47</td>
<td>0x1c000</td>
<td>0x1bff8</td>
<td>1</td>
<td>0x1c000</td>
<td>299593</td>
<td>2 / 2</td>
</tr>
<tr>
<td>48</td>
<td>0x20000</td>
<td>0x1fff8</td>
<td>1</td>
<td>0x20000</td>
<td>262144</td>
<td>1 / 1</td>
</tr>
</tbody>
</table>
<p>Within each arena, an array of 49 <code>size_class</code> entries maintains the metadata for every size class. For each class, the allocator reserves a dedicated memory region to hold its corresponding allocations. This region is segmented into <strong>slabs</strong>, which are in turn subdivided into <strong>slots</strong>. Each slot corresponds to a single memory chunk returned to the user.</p>
<figure role="group">
<img alt="Structures en charge des petites allocations" data-entity-type="file" data-entity-uuid="d42b1272-88ae-4e4c-a5e0-e03881d19c70" height="726" src="https://www.synacktiv.com/sites/default/files/inline-images/small_allocs_0.webp" width="1111"/>
<figcaption>Small allocations structures</figcaption>
</figure>
<p>The regions for all classes are reserved contiguously in memory when the allocator is initialized. Each region occupies 32 GiB of memory at a random offset within a 64 GiB area. The empty areas before and after the region act as page-aligned guards of a random size.</p>
<p>To summarize:</p>
<ul>
<li>A 32 GiB region is allocated per size class.</li>
<li>It is encapsulated at a random offset within a zone twice its size (64 GiB).</li>
<li>The 64 GiB zones are contiguous and ordered by increasing size class.</li>
</ul>
<figure role="group">
<img alt="Segmentation des régions pour les petites allocations" data-entity-type="file" data-entity-uuid="90c48a04-861b-4252-8f72-7c92bfbca166" height="673" src="https://www.synacktiv.com/sites/default/files/inline-images/small_regions_1.webp" width="730"/>
<figcaption>Small allocations regions</figcaption>
</figure>
<p>The size of the contiguous memory area reserved during initialization is <code>N_ARENA * 49 * 64 GiB</code>. On the test devices, which use a single arena, this amounts to <code>0x31000000000</code> bytes (~3 TB). By default, these pages are protected with <code>PROT_NONE</code>, meaning they are not backed by physical memory. This protection is changed to Read/Write (RW) on demand for specific pages as allocations are needed.</p>
<pre><code>// CONFIG_EXTENDED_SIZE_CLASSES := true
// CONFIG_LARGE_SIZE_CLASSES := true
// CONFIG_CLASS_REGION_SIZE := 34359738368 # 32GiB
// CONFIG_N_ARENA := 1

#define CLASS_REGION_SIZE (size_t)CONFIG_CLASS_REGION_SIZE
#define REAL_CLASS_REGION_SIZE (CLASS_REGION_SIZE * 2)
#define ARENA_SIZE (REAL_CLASS_REGION_SIZE * N_SIZE_CLASSES)
static const size_t slab_region_size = ARENA_SIZE * N_ARENA; // 0x31000000000 on Pixel 4a 5G and Pixel 9a

// ...

COLD static void init_slow_path(void) {
    // ...

    // Create a big mapping with MTE enabled
    ro.slab_region_start = memory_map_tagged(slab_region_size);
    if (unlikely(ro.slab_region_start == NULL)) {
        fatal_error(&#34;failed to allocate slab region&#34;);
    }
    void *slab_region_end = (char *)ro.slab_region_start + slab_region_size;
    memory_set_name(ro.slab_region_start, slab_region_size, &#34;malloc slab region gap&#34;);
    // ...
}
</code></pre>
<p>Each size class (or bin) is represented by the <code>size_class</code> structure, a relatively large structure that holds all the relevant information for that class.</p>
<pre><code>struct __attribute__((aligned(CACHELINE_SIZE))) size_class {
    struct mutex lock;

    void *class_region_start;
    struct slab_metadata *slab_info;
    struct libdivide_u32_t size_divisor;
    struct libdivide_u64_t slab_size_divisor;

#if SLAB_QUARANTINE_RANDOM_LENGTH &gt; 0
    void *quarantine_random[SLAB_QUARANTINE_RANDOM_LENGTH &lt;&lt; (MAX_SLAB_SIZE_CLASS_SHIFT - MIN_SLAB_SIZE_CLASS_SHIFT)];
#endif

#if SLAB_QUARANTINE_QUEUE_LENGTH &gt; 0
    void *quarantine_queue[SLAB_QUARANTINE_QUEUE_LENGTH &lt;&lt; (MAX_SLAB_SIZE_CLASS_SHIFT - MIN_SLAB_SIZE_CLASS_SHIFT)];
    size_t quarantine_queue_index;
#endif

    // slabs with at least one allocated slot and at least one free slot
    //
    // LIFO doubly-linked list
    struct slab_metadata *partial_slabs;

    // slabs without allocated slots that are cached for near-term usage
    //
    // LIFO singly-linked list
    struct slab_metadata *empty_slabs;
    size_t empty_slabs_total; // length * slab_size

    // slabs without allocated slots that are purged and memory protected
    //
    // FIFO singly-linked list
    struct slab_metadata *free_slabs_head;
    struct slab_metadata *free_slabs_tail;
    struct slab_metadata *free_slabs_quarantine[FREE_SLABS_QUARANTINE_RANDOM_LENGTH];

#if CONFIG_STATS
    u64 nmalloc; // may wrap (per jemalloc API)
    u64 ndalloc; // may wrap (per jemalloc API)
    size_t allocated;
    size_t slab_allocated;
#endif

    struct random_state rng;
    size_t metadata_allocated;
    size_t metadata_count;
    size_t metadata_count_unguarded;
};
</code></pre>
<p>Its main members are:</p>
<ul>
<li><code>class_region_start</code>: start address of the memory region for this class&#39;s slabs.</li>
<li><code>slab_info</code>: pointer to the beginning of the slab metadata array.</li>
<li><code>quarantine_random</code>, <code>quarantine_queue</code>: arrays of pointers to allocations currently in quarantine (see the section on quarantines).</li>
<li><code>partial_slabs</code>: a stack of metadata for partially filled slabs.</li>
<li><code>free_slabs_{head, tail}</code>: a queue of metadata for empty slabs.</li>
</ul>
<p>Slab metadata is held in the <code>slab_metadata</code> structure. For any given size class, these structures form a contiguous array, accessible via the <code>size_class-&gt;slab_info</code> pointer. The layout of this metadata array directly mirrors the layout of the slabs in their memory region. This design allows for direct lookup: a slab&#39;s metadata can be found simply by using the slab&#39;s index to access the array.</p>
<pre><code>struct slab_metadata {
    u64 bitmap[4];
    struct slab_metadata *next;
    struct slab_metadata *prev;
#if SLAB_CANARY
    u64 canary_value;
#endif
#ifdef SLAB_METADATA_COUNT
    u16 count;
#endif
#if SLAB_QUARANTINE
    u64 quarantine_bitmap[4];
#endif
#ifdef HAS_ARM_MTE
    // arm_mte_tags is used as a u4 array (MTE tags are 4-bit wide)
    //
    // Its size is calculated by the following formula:
    // (MAX_SLAB_SLOT_COUNT + 2) / 2
    // MAX_SLAB_SLOT_COUNT is currently 256, 2 extra slots are needed for branchless handling of
    // edge slots in tag_and_clear_slab_slot()
    //
    // It&#39;s intentionally placed at the end of struct to improve locality: for most size classes,
    // slot count is far lower than MAX_SLAB_SLOT_COUNT.
    u8 arm_mte_tags[129];
#endif
};
</code></pre>
<ul>
<li><code>bitmap[4]</code> : bitmap tracking which slots in the slab are in use.</li>
<li><code>next, prev </code>: pointers to the next/previous elements when the structure belongs to a linked list (for example, in the stack of partially used slabs <code>size_class-&gt;partial_slabs</code>)</li>
<li><code>canary_value</code> : canary value appended to the end of each slot within the slab (on non-MTE devices only). This value is verified upon <code>free</code> to detect buffer overflows.</li>
<li><code>arm_mte_tags[129] </code>: MTE tags currently in use per slot</li>
</ul>
<h4>Alloc</h4>
<p>First, the actual size to be allocated is calculated by adding 8 bytes to the user&#39;s requested size. These extra bytes are filled with a canary and placed immediately after the data. An allocation is considered &#34;small&#34; only if this new size is less than <code>0x20000</code> bytes (131,072 bytes) . Next, a free slot must be retrieved from a slab by following these steps:</p>
<ol start="1">
<li>Retrieve the arena: the current arena is fetched from the thread&#39;s local storage.</li>
<li>Get size class metadata: the metadata for the corresponding size class (the <code>size_class</code> structure) is retrieved using <code>ro.size_class_metadata[arena][size_class]</code>, where <code>arena</code> is the arena number and <code>size_class</code> is the index calculated from the allocation size.</li>
<li>Find a slab with a free slot:
	<ul>
<li>if a partially filled slab exists (<code>size_class-&gt;partial_slabs != NULL</code>), this slab is used.</li>
<li>otherwise, if at least one empty slab is available (<code>size_class-&gt;empty_slabs != NULL</code>), the first slab from this list is used.</li>
<li>if no slab is available, a new one is allocated (by allocating a <code>slab_metadata</code> structure using the <code>alloc_metadata()</code> function). A &#34;guard&#34; slab is reserved between each real slab.</li>
</ul>
</li>
<li>Select a random free slot: A free slot is chosen randomly from within the selected slab. Occupied slots are marked by <code>1</code>s in the <code>slab_metadata-&gt;bitmap</code>.</li>
<li>Select a MTE tag: A new MTE tag is chosen for the slot, ensuring it is different from adjacent tags to prevent simple linear overflows. The following tags are excluded:
	<ul>
<li>the previous slot&#39;s tag.</li>
<li>the next slot&#39;s tag.</li>
<li>the old tag of the currently selected slot.</li>
<li>the <code>RESERVED_TAG</code> (0), which is used for freed allocations.</li>
</ul>
</li>
<li>Set protections:
	<ul>
<li>on devices without MTE, the canary (which is common to all slots in the slab) is written into the last 8 bytes of the slot.</li>
<li>on MTE-enabled devices, these 8 bytes are set to 0.</li>
</ul>
</li>
<li>Return the address of the slot, now tagged with the MTE tag.</li>
</ol>
<p>For a small allocation, the address returned by <code>malloc</code> is a pointer to a slot with a 4-bit MTE tag encoded in its most significant bits. The pointers below, retrieved from successive calls to <code>malloc(8)</code>, are located in the same slab but at random offsets and have different MTE tags.</p>
<pre><code>ptr[0] = 0xa00cd70ad02a930
ptr[1] = 0xf00cd70ad02ac50
ptr[2] = 0x300cd70ad02a2f0
ptr[3] = 0x900cd70ad02a020
ptr[4] = 0x300cd70ad02ac90
ptr[5] = 0x700cd70ad02a410
ptr[6] = 0xc00cd70ad02a3c0
ptr[7] = 0x500cd70ad02a3d0
ptr[8] = 0xf00cd70ad02a860
ptr[9] = 0x600cd70ad02ad20</code></pre>
<p>If an overflow occurs, a <code>SIGSEGV/SEGV_MTESERR</code> exception is raised, indicating that an MTE-protected area was accessed with an incorrect tag. On GrapheneOS, this causes the application to terminate and sends a crash log to logcat.</p>
<pre><code>07-23 11:32:19.948  4169  4169 F DEBUG   : Cmdline: /data/local/tmp/bin
07-23 11:32:19.948  4169  4169 F DEBUG   : pid: 4165, tid: 4165, name: bin  &gt;&gt;&gt; /data/local/tmp/bin &lt;&lt;&lt;
07-23 11:32:19.948  4169  4169 F DEBUG   : uid: 2000
07-23 11:32:19.949  4169  4169 F DEBUG   : tagged_addr_ctrl: 000000000007fff3 (PR_TAGGED_ADDR_ENABLE, PR_MTE_TCF_SYNC, mask 0xfffe)
07-23 11:32:19.949  4169  4169 F DEBUG   : pac_enabled_keys: 000000000000000f (PR_PAC_APIAKEY, PR_PAC_APIBKEY, PR_PAC_APDAKEY, PR_PAC_APDBKEY)
07-23 11:32:19.949  4169  4169 F DEBUG   : signal 11 (SIGSEGV), code 9 (SEGV_MTESERR), fault addr 0x0500d541414042c0
07-23 11:32:19.949  4169  4169 F DEBUG   :     x0  0800d541414042c0  x1  0000d84c01173140  x2  0000000000000015  x3  0000000000000014
07-23 11:32:19.949  4169  4169 F DEBUG   :     x4  0000b1492c0f16b5  x5  0300d6f2d01ea99b  x6  0000000000000029  x7  203d207972742029
07-23 11:32:19.949  4169  4169 F DEBUG   :     x8  5dde6df273e81100  x9  5dde6df273e81100  x10 0000000000001045  x11 0000000000001045
07-23 11:32:19.949  4169  4169 F DEBUG   :     x12 0000f2dbd10c1ca4  x13 0000000000000000  x14 0000000000000001  x15 0000000000000020
07-23 11:32:19.949  4169  4169 F DEBUG   :     x16 0000d84c0116e228  x17 0000d84c010faf50  x18 0000d84c1eb38000  x19 0500d541414042c0
07-23 11:32:19.949  4169  4169 F DEBUG   :     x20 0000000000001e03  x21 0000b1492c0f16e8  x22 0800d541414042c0  x23 0000000000000001
07-23 11:32:19.949  4169  4169 F DEBUG   :     x24 0000d541414042c0  x25 0000000000000000  x26 0000000000000000  x27 0000000000000000
07-23 11:32:19.949  4169  4169 F DEBUG   :     x28 0000000000000000  x29 0000f2dbd10c1f10
07-23 11:32:19.949  4169  4169 F DEBUG   :     lr  002bb1492c0f2ba0  sp  0000f2dbd10c1f10  pc  0000b1492c0f2ba4  pst 0000000060001000
</code></pre>

<h4>Free</h4>
<p>To free a small allocation, the allocator first determines its size class index from the pointer. This index allows it to locate the relevant metadata and the memory region where the data resides. The <code>slab_size_class</code> function performs this initial calculation.</p>
<pre><code>static struct slab_size_class_info slab_size_class(const void *p) {
    size_t offset = (const char *)p - (const char *)ro.slab_region_start;
    unsigned arena = 0;
    if (N_ARENA &gt; 1) {
        arena = offset / ARENA_SIZE;
        offset -= arena * ARENA_SIZE;
    }
    return (struct slab_size_class_info){arena, offset / REAL_CLASS_REGION_SIZE};
}
</code></pre>
<p>With this index, now referred to as <code>class_id</code>, it is possible to gather various details about the slab containing the allocation:</p>
<ul>
<li><code>size_class</code> structure: <code>size_class *c = &amp;ro.size_class_metadata[size_class_info.arena][class_id]</code></li>
<li>Allocation size: the size for this class is found using the <code>size_classes</code> lookup table: <code>size_t size = size_classes[class_id]</code></li>
<li>Slots per slab: the number of slots is found using the <code>size_class_slots</code> lookup table: <code>slots = size_class_slots[class_id]</code></li>
<li>Slab size: <code>slab_size = page_align(slots * size)</code></li>
<li>Current slab metadata: <code>offset = (const char *)p - (const char *)c-&gt;class_region_start</code> <code>index = offset / slab_size</code> <code>slab_metadata = c-&gt;slab_info + index</code></li>
</ul>
<p>With this information, the allocator can pinpoint the slab&#39;s base address and determine the specific slot&#39;s index and offset within that slab using the <code>get_slab()</code> function.</p>
<pre><code>static void *get_slab(const struct size_class *c, size_t slab_size, const struct slab_metadata *metadata) {
    size_t index = metadata - c-&gt;slab_info;
    return (char *)c-&gt;class_region_start + (index * slab_size);
}
</code></pre>
<p>The slot&#39;s address is then deduced with the formula <code>slot = (const char*)slab - p</code>, as is its index: <code>slot_index = ((const char*)slab - slot) / slots</code>.</p>
<p>Once the slot is identified, a series of crucial security and integrity checks are performed to validate the <code>free</code> operation:</p>
<ol start="1">
<li>Pointer alignment: the allocator verifies that the pointer is perfectly aligned with the start of a slot. Any misalignment indicates some kind of corruption, and the operation is immediately aborted.</li>
<li>Slot state: it then checks the slab&#39;s metadata to confirm the slot is currently marked as &#34;in use.&#34;</li>
<li>Canary verification: the 8-byte canary at the end of the slot is checked for integrity. A key difference from scudo is that this canary is shared across the entire slab. This means a memory leak from one slot could theoretically allow an attacker to forge a valid canary for another slot and prevent a crash in case of a <code>free</code>.</li>
<li>MTE Tag Invalidation: the slot&#39;s MTE tag is reset to the reserved value (0), effectively invalidating the original pointer and preventing dangling pointer access.</li>
<li>Zero out: The slot&#39;s memory is completely wiped by zeroing it out.</li>
</ol>
<p>If an invalid canary is detected, an <code>abort</code> is called with the following message:</p>
<pre><code>07-23 02:14:09.559  7610  7610 F libc    : hardened_malloc: fatal allocator error: canary corrupted
07-23 02:14:09.559  7610  7610 F libc    : Fatal signal 6 (SIGABRT), code -1 (SI_QUEUE) in tid 7610 (bin), pid 7610 (bin)
07-23 02:14:09.775  7614  7614 F DEBUG   : *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
07-23 02:14:09.775  7614  7614 F DEBUG   : Build fingerprint: &#39;google/bramble/bramble:14/UP1A.231105.001.B2/2025021000:user/release-keys&#39;
07-23 02:14:09.776  7614  7614 F DEBUG   : Revision: &#39;MP1.0&#39;
07-23 02:14:09.776  7614  7614 F DEBUG   : ABI: &#39;arm64&#39;
07-23 02:14:09.776  7614  7614 F DEBUG   : Timestamp: 2025-07-23 02:14:09.603643955+0200
07-23 02:14:09.776  7614  7614 F DEBUG   : Process uptime: 1s
07-23 02:14:09.776  7614  7614 F DEBUG   : Cmdline: /data/local/tmp/bin
07-23 02:14:09.776  7614  7614 F DEBUG   : pid: 7610, tid: 7610, name: bin  &gt;&gt;&gt; /data/local/tmp/bin &lt;&lt;&lt;
07-23 02:14:09.776  7614  7614 F DEBUG   : uid: 2000
07-23 02:14:09.776  7614  7614 F DEBUG   : signal 6 (SIGABRT), code -1 (SI_QUEUE), fault addr --------
07-23 02:14:09.776  7614  7614 F DEBUG   : Abort message: &#39;hardened_malloc: fatal allocator error: canary corrupted&#39;
07-23 02:14:09.776  7614  7614 F DEBUG   :     x0  0000000000000000  x1  0000000000001dba  x2  0000000000000006  x3  0000ea4a84242960
07-23 02:14:09.776  7614  7614 F DEBUG   :     x4  716e7360626e6b6b  x5  716e7360626e6b6b  x6  716e7360626e6b6b  x7  7f7f7f7f7f7f7f7f
07-23 02:14:09.777  7614  7614 F DEBUG   :     x8  00000000000000f0  x9  0000cf1d482da2a0  x10 0000000000000001  x11 0000cf1d48331980
07-23 02:14:09.777  7614  7614 F DEBUG   :     x12 0000000000000004  x13 0000000000000033  x14 0000cf1d482da118  x15 0000cf1d482da050
07-23 02:14:09.777  7614  7614 F DEBUG   :     x16 0000cf1d483971e0  x17 0000cf1d48383650  x18 0000cf1d6fe40000  x19 0000000000001dba
07-23 02:14:09.777  7614  7614 F DEBUG   :     x20 0000000000001dba  x21 00000000ffffffff  x22 0000cc110ff0d150  x23 0000000000000000
07-23 02:14:09.777  7614  7614 F DEBUG   :     x24 0000000000000001  x25 0000cf0f4a421300  x26 0000000000000000  x27 0000cf0f4a421328
07-23 02:14:09.777  7614  7614 F DEBUG   :     x28 0000cf0f7ba30000  x29 0000ea4a842429e0
07-23 02:14:09.777  7614  7614 F DEBUG   :     lr  0000cf1d4831a9f8  sp  0000ea4a84242940  pc  0000cf1d4831aa24  pst 0000000000001000
</code></pre>
<p>Finally, the slot is not immediately made available. Instead, it is placed into quarantine to delay its reuse, a key defense against <em>use-after-free</em> vulnerabilities.</p>
<h4>Quarantines</h4>
<p>Each allocation class uses a two-stage quarantine system for its freed slots. When an allocation is freed, it isn&#39;t immediately available for reuse but is passed instead through two distinct holding areas:</p>
<ol start="1">
<li>A random quarantine: a fixed-size array where incoming slots replace a randomly chosen existing slot.</li>
<li>A queue quarantine: a First-In, First-Out queue that receives slots ejected from the random quarantine.</li>
</ol>
<p>When a slot enters the random quarantine, it overwrites a randomly selected entry. That ejected entry is then pushed into the queue quarantine. The queue then ejects its oldest element, which is finally made available for new allocations. This entire process is managed within each class&#39;s <code>size_class</code> structure :</p>
<pre><code>struct __attribute__((aligned(CACHELINE_SIZE))) size_class {
  // ...
  #if SLAB_QUARANTINE_RANDOM_LENGTH &gt; 0
    void *quarantine_random[SLAB_QUARANTINE_RANDOM_LENGTH &lt;&lt; (MAX_SLAB_SIZE_CLASS_SHIFT - MIN_SLAB_SIZE_CLASS_SHIFT)];
#endif

#if SLAB_QUARANTINE_QUEUE_LENGTH &gt; 0
    void *quarantine_queue[SLAB_QUARANTINE_QUEUE_LENGTH &lt;&lt; (MAX_SLAB_SIZE_CLASS_SHIFT - MIN_SLAB_SIZE_CLASS_SHIFT)];
    size_t quarantine_queue_index;
#endif
  // ...
}
</code></pre>

<figure role="group">
<img alt="Quarantaine avant insertion" data-entity-type="file" data-entity-uuid="826c7bfe-4f53-4203-8785-02ece445f261" height="481" src="https://www.synacktiv.com/sites/default/files/inline-images/quarantine_before.webp" width="561"/>
<figcaption>Quarantines before insertion</figcaption>
</figure>
<figure role="group">
<img alt="Quarantaine après insertion" data-entity-type="file" data-entity-uuid="6ae1741f-c4a5-491a-a8ac-22e71a5fde77" height="451" src="https://www.synacktiv.com/sites/default/files/inline-images/quarantine_after_0.webp" width="561"/>
<figcaption>Quarantines after insertion</figcaption>
</figure>
<p>This design is a significant shift from traditional allocators, which use a simple LIFO (Last-In, First-Out) freelist. In <strong>hardened malloc</strong>, the last item freed is never the first to be reallocated. To reclaim a specific slot, an attacker must trigger enough <code>free</code> operations to successfully cycle their target slot through both the random quarantine and the queue quarantine. This adds a substantial layer of non-determinism and complexity to <em>use-after-free</em> exploits, providing a robust defense even on devices that lack MTE.</p>
<p>Since the allocator has no a freelist, the most straightforward way to force a reuse is to chain calls to <code>malloc</code> and <code>free</code>. The number of <code>free</code> operations required depends on the quarantine sizes for that specific size class.</p>
<pre><code>void reuse(void* target_ptr, size_t size) {
  free(target_ptr);
  for (int i = 0; ; i++) {
    void* new_ptr = malloc(size);
    if (untag(target_ptr) == untag(new_ptr)) {
      printf(&#34;REUSED [size = 0x%x] target_ptr @ %p (new_ptr == %p) try = %d\n&#34;, size, target_ptr, new_ptr, i);
      break;
    }
    free(new_ptr);
  }
}
</code></pre>
<p>For an 8-byte allocation, both quarantines hold 8,192 elements. While this implies at least 8,192 frees are needed, the random nature of the first stage means the actual number is far greater. In testing, it required an average of ~19,000 <code>free</code> operations to reliably reclaim a slot. The double quarantine turns predictable memory reuse into a costly and unreliable lottery, severely hindering a common exploit vector.</p>
<h3>Large allocations</h3>
<figure role="group">
<img alt="Large allocations structures" data-entity-type="file" data-entity-uuid="a9047ba9-c263-401e-9567-f78081261411" height="591" src="https://www.synacktiv.com/sites/default/files/inline-images/large_allocs.webp" width="959"/>
<figcaption>Large allocations structures</figcaption>
</figure>
<h4>Alloc</h4>
<p>Unlike small allocations, large allocations are not sorted by size into pre-reserved regions. Instead, the allocator maps them on demand. This mechanism is the only one in <strong>hardened malloc</strong> that dynamically creates memory mappings. The total size of the mapping depends on several factors:</p>
<ul>
<li><strong>Aligned Size</strong>: computed in <code>get_large_size_class</code> by aligning the requested size to predefined classes, continuing from the small allocation sizes.</li>
<li><strong>Guard Page Size</strong>: a random number of pages preceding and following the actual allocation.</li>
</ul>
<pre><code>static size_t get_large_size_class(size_t size) {
    if (CONFIG_LARGE_SIZE_CLASSES) {
        // Continue small size class growth pattern of power of 2 spacing classes:
        //
        // 4 KiB [20 KiB, 24 KiB, 28 KiB, 32 KiB]
        // 8 KiB [40 KiB, 48 KiB, 54 KiB, 64 KiB]
        // 16 KiB [80 KiB, 96 KiB, 112 KiB, 128 KiB]
        // 32 KiB [160 KiB, 192 KiB, 224 KiB, 256 KiB]
        // 512 KiB [2560 KiB, 3 MiB, 3584 KiB, 4 MiB]
        // 1 MiB [5 MiB, 6 MiB, 7 MiB, 8 MiB]
        // etc.
        return get_size_info(max(size, (size_t)PAGE_SIZE)).size;
    }
    return page_align(size);
}
</code></pre>
<p>Once these sizes are determined, the allocator creates a mapping via <code>mmap()</code> for their combined total. The guard areas before and after the data are mapped with <code>PROT_NONE</code>, while the data region itself is mapped <code>PROT_READ|PROT_WRITE</code>. This use of randomly sized guard pages means that two large allocations of the same requested size will occupy mapped areas of different total sizes, adding a layer of non-determinism.</p>
<pre><code>void *allocate_pages_aligned(size_t usable_size, size_t alignment, size_t guard_size, const char *name) {
    //...

    // Compute real mapped size = alloc_size + 2 * guard_size
    size_t real_alloc_size;
    if (unlikely(add_guards(alloc_size, guard_size, &amp;real_alloc_size))) {
        errno = ENOMEM;
        return NULL;
    }
    // Mapping whole region with PROT_NONE
    void *real = memory_map(real_alloc_size);
    if (unlikely(real == NULL)) {
        return NULL;
    }
    memory_set_name(real, real_alloc_size, name);

    void *usable = (char *)real + guard_size;

    size_t lead_size = align((uintptr_t)usable, alignment) - (uintptr_t)usable;
    size_t trail_size = alloc_size - lead_size - usable_size;
    void *base = (char *)usable + lead_size;

    // Change protection to usable data with PROT_RAD|PROT_WRITE
    if (unlikely(memory_protect_rw(base, usable_size))) {
        memory_unmap(real, real_alloc_size);
        return NULL;
    }

    //...
    return base;
}
</code></pre>
<p>If the mapping is successful, a structure containing the address, usable size, and guard size is inserted into a hash table of regions. This hash table is implemented using two arrays of <code>region_metadata</code> structs: <code>allocator_state.regions_a</code> and <code>allocator_state.regions_b</code> (referenced as <code>ro.regions[0]</code> and <code>ro.regions[1]</code>). These arrays have a static size and are reserved at initialization.</p>
<p>Initially, only a portion of these arrays is accessible (marked Read/Write); the rest is protected with <code>PROT_NONE</code>. As the number of active large allocations grows and exceeds the available metadata slots, the accessible portion of the arrays is doubled. This expansion uses a two-table system: the current hash table is copied to the previously unused table, which then becomes the active one. The old table is re-mapped back to <code>PROT_NONE</code> to render it inaccessible.</p>
<pre><code>static int regions_grow(void) {
    struct region_allocator *ra = ro.region_allocator;

    if (ra-&gt;total &gt; SIZE_MAX / sizeof(struct region_metadata) / 2) {
        return 1;
    }

    // Compute new grown size
    size_t newtotal = ra-&gt;total * 2;
    size_t newsize = newtotal * sizeof(struct region_metadata);
    size_t mask = newtotal - 1;

    if (newtotal &gt; MAX_REGION_TABLE_SIZE) {
        return 1;
    }

    // Select new metadata array
    struct region_metadata *p = ra-&gt;regions == ro.regions[0] ?
        ro.regions[1] : ro.regions[0];

    // Enlarge new metadata elements
    if (memory_protect_rw_metadata(p, newsize)) {
        return 1;
    }

    // Copy elements to the new array
    for (size_t i = 0; i &lt; ra-&gt;total; i++) {
        const void *q = ra-&gt;regions[i].p;
        if (q != NULL) {
            size_t index = hash_page(q) &amp; mask;
            while (p[index].p != NULL) {
                index = (index - 1) &amp; mask;
            }
            p[index] = ra-&gt;regions[i];
        }
    }

    memory_map_fixed(ra-&gt;regions, ra-&gt;total * sizeof(struct region_metadata));
    memory_set_name(ra-&gt;regions, ra-&gt;total * sizeof(struct region_metadata), &#34;malloc allocator_state&#34;);
    ra-&gt;free = ra-&gt;free + ra-&gt;total;
    ra-&gt;total = newtotal;

    // Switch current metadata array/hash table
    ra-&gt;regions = p;
    return 0;
}
</code></pre>
<p>Eventually, allocation metadata, <code>address + size + guard size</code>, is inserted in the current hash table <code>ro.region_allocator-&gt;regions</code>.</p>
<p>For large allocations, which are not protected by MTE, the randomly sized guard pages are the primary defense against overflows. If an attacker can bypass this randomization and has an out-of-bounds read/write vulnerability with a precise offset, corrupting adjacent data remains a possible, though complex, scenario.</p>
<p>For example, a call to <code>malloc(0x28001)</code> creates the following metadata. A random guard size of <code>0x18000</code> bytes was chosen by the allocator.</p>
<pre><code>large alloc @ 0xc184d36f4ac8
  ptr       : 0xbe6cadf4c000
  size      : 0x30000
  guard size: 0x18000</code></pre>
<p>By inspecting the process&#39;s memory maps, we can see that the large allocation (which aligns to a size of <code>0x30000</code>) is securely sandwiched between two <code>PROT_NONE</code> guard regions, each <code>0x18000</code> bytes in size.</p>
<pre><code>be6cadf34000-be6cadf4c000 ---p 00000000 00:00 0
be6cadf4c000-be6cadf7c000 rw-p 00000000 00:00 0
be6cadf7c000-be6cadf94000 ---p 00000000 00:00 0</code></pre>
<h4>Free</h4>
<p>Freeing a large allocation is a relatively simple process that uses the same quarantine mechanism as small allocations.</p>
<ol start="1">
<li>Calculate Pointer Hash: the hash of the pointer is calculated to locate its metadata.</li>
<li>Retrieve Metadata: the allocation&#39;s metadata structure is retrieved from the current hash table (<code>ro-&gt;region_allocator.regions</code>).</li>
<li>Quarantine or Unmap: the next step depends on the allocation&#39;s size.
	<ul>
<li><strong>If the size is less than <code>0x2000000</code> (32 MiB)</strong>, the allocation is placed into a two-stage quarantine system identical to the one for small allocations (a random-replacement cache followed by a FIFO queue). This quarantine is global for all large allocations and is managed in <code>ro.region_allocator</code>.</li>
<li><strong>If the size is <code>0x2000000</code> or greater</strong>, or when an allocation is ejected from the quarantine, it is immediately unmapped from memory. The entire memory region, including the data area and its surrounding guard pages, is unmapped using <code>munmap()</code>
<ul>
<li><code>munmap((char *)usable - guard_size, usable_size + guard_size * 2);</code></li>
</ul>
</li>
</ul>
</li>
</ol>
<h2>Conclusion</h2>
<p><strong>Hardened Malloc</strong> is a security-hardened memory allocator that implements several advanced protection mechanisms, most notably leveraging the ARM Memory Tagging Extension (MTE) to detect and prevent memory corruption. While it offers an improvement over the standard scudo allocator, particularly against <em>use-after-free</em> vulnerabilities, its true strength lies in its integration with GrapheneOS. This combination achieves a higher level of security than a typical Android device that uses scudo.</p>
<p>Furthermore, the use of canaries and numerous guard pages complements its arsenal, especially on older devices without MTE, by quickly triggering exceptions in case of unwanted memory access.</p>
<p>From an attacker&#39;s perspective, <strong>hardened malloc</strong> significantly reduces opportunities to exploit memory corruption vulnerabilities:</p>
<ul>
<li>
<p><em>Heap overflow</em>: <strong>hardened malloc</strong> is relatively similar to scudo, yet it adds guard pages between slabs, which prevents an overflow from spreading from one slab to another. However, with MTE enabled, the protection becomes much more granular: even an overflow within the same slab (from one slot to another) is detected and blocked without the need to check canaries, making the exploitation of this type of vulnerability nearly impossible.</p>
</li>
<li>
<p><em>Use-after-free</em>: the double quarantine mechanism complicates the reuse of a freed memory region but does not make it entirely impossible. However, MTE radically changes the deal. The pointer and its associated memory region are &#34;tagged.&#34; Upon being freed, this tag is modified. Any subsequent attempt to use the old pointer (with its now-invalid tag) will very likely raise an exception, neutralizing the attack. For large allocations, which are not covered by MTE, the strategy is different: each allocation is isolated by guard pages and its location in memory is randomized. This combination of isolation and randomization makes any attempt to reuse these memory regions difficult and unreliable for an attacker.</p>
</li>
</ul>
<p>Additionally, its implementation has proven to be particularly clear and concise, facilitating its audit and maintenance.</p>
</div></div>
  </body>
</html>
