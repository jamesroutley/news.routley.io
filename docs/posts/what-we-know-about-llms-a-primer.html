<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://willthompson.name/what-we-know-about-llms-primer">Original</a>
    <h1>What We Know About LLMs (A Primer)</h1>
    
    <div id="readability-page-1" class="page"><div><div><article id="block-what-we-know-about-llms-primer"><p><span><span>Will Thompson</span></span></p><p><span><span><span><span><span>@</span>July 23, 2023</span></span></span><span> </span></span></p><div id="block-e6aa957fa7064aad95329c7de0339c27"><p><span><span></span><img alt="Crypto VCs &amp; ‚Äùbuilders‚Äù making a hard left into AI (Borrowed from ML Twitter)" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive"/></span></p><figcaption><span><span><em>Crypto VCs &amp; ‚Äùbuilders‚Äù making a hard left into AI (Borrowed from ML Twitter)</em></span></span></figcaption></div><p><span><span>We are in the midst of yet another AI Summer where the possible futures enumerated in the Press seem both equally amazing and terrifying. LLMs are predicted to both create immeasurable wealth for society as well as potentially compete with (or deprecate?) knowledge workers. While bond markets are trying to read the tea leaves on future Fed rate hikes, equity markets are bullish on all things AI. Many companies are rapidly adopting some form of AI play in order to appease shareholder FOMO. A large percentage of YC cohort members are, unsurprisingly, generative AI startups now. All the ‚ÄúMAANG‚Äùs (whatever they are called these days) seem to have some form of giant LLM they are building now. It‚Äôs as though crypto was forgotten overnight; the public imagination appears singularly captivated with what possibilities AI may usher forth. </span></span></p><p><span><span>The madness of crowds aside, it is worth reflecting on what we concretely know about LLMs at this point in time and how these insights sparked the latest AI fervor. This will help put into perspective the relevance of current research efforts and the possibilities that abound.</span></span></p><p><span><span>When people say ‚ÄúLarge Language Models‚Äù, they typically are referring to a type of deep learning architecture called a Transformer. Transformers are models that work with sequence data (e.g. text, images, time series, etc) and are part of a larger family of models called  </span><span><a href="https://d2l.ai/chapter_recurrent-neural-networks/sequence.html" target="_blank" rel="noopener noreferrer">Sequence Models</a></span><span>. Many Sequence Models can also be thought of as Language Models, or models that learn a probability distribution of the next word/pixel/value in a sequence: </span><span><!--$--><span id=""><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>t</mi></msub><mi mathvariant="normal">‚à£</mi><msub><mi>w</mi><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>‚àí</mo><mn>2</mn></mrow></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(w_t|w_{t-1},w_{t-2},...)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>‚Äã</span></span><span><span><span></span></span></span></span></span></span><span>‚à£</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>t</span><span>‚àí</span><span>1</span></span></span></span></span><span>‚Äã</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>t</span><span>‚àí</span><span>2</span></span></span></span></span><span>‚Äã</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>...</span><span>)</span></span></span></span></span><!--/$--></span><span>. </span></span></p><div id="block-8ea137637f6c495898ef4406cd64c76b"><p><span><span></span><img alt="Figure 9.7.1. in this illustrated guide¬†" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive"/></span></p><figcaption><span><span>Figure 9.7.1. in this illustrated guide¬†</span><span><a href="https://d2l.ai/chapter_recurrent-modern/seq2seq.html" target="_blank" rel="noopener noreferrer">here</a></span><span>. This is a type of encoder-decoder RNN. Notice the left-to-right causal ordering. Each token is processed sequentially.</span></span></figcaption></div><p><span><span>What differentiates the Transformer from its predecessors is it‚Äôs ability to learn the contextual relationship of values within a sequence through a mechanism called (self-) </span><span><a href="https://lilianweng.github.io/posts/2018-06-24-attention/" target="_blank" rel="noopener noreferrer">Attention</a></span><span>. Unlike the Recurrent Neural Network (</span><span><a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks" target="_blank" rel="noopener noreferrer">RNN</a></span><span>), where the arrow of time is preserved by processing each time step serially within a sequence, Transformers can read the entire sequence at once and learn to ‚Äúpay attention to‚Äù only the values that came earlier in time (via ‚Äúmasking‚Äù). This allows for faster training times (i.e. the whole sequence in parallel) and larger model parameter sizes. Transformers were once considered ‚Äúlarge‚Äù when they were ~ 100MM+ parameters; today, published models are </span><span><strong>~500B-1T parameters</strong></span><span> in size. Anecdotally, several papers have reported a major inflection point in Transformer behavior around ~</span><span><strong>100B+</strong></span><span> parameters. (Note: these models are generally too large to fit into a single GPU and require the model to be broken apart and distributed across multiple nodes).</span></span></p><div id="block-fd7c5af78fd946f7b25873b190a09312"><p><span><span><img alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27418%27%20height=%27408%27/%3e"/></span><img alt="From Jay Alammar‚Äôs popular " src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic"/></span></p><figcaption><span><span>From Jay Alammar‚Äôs popular </span><span><a href="http://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener noreferrer">‚ÄúThe Illustrated Transformer‚Äù</a></span><span> blog post.  A context window is used to read in K tokens simultaneously in order to understand the relative meaning of words in a sentence. This is opposed to the RNN (see above), which technically can be thought as having an infinite context window (an observation by Karpathy), although each word must be processed serially. RNNs have a history of requiring more clever architecture tweaks to overcome their innate inability to connect long-term dependencies in a sentence, which is actually where the original (hierarchical) attention mechanism was born out of necessity.</span></span></figcaption></div><p><span><span>Transformers can be generally categorized into one of three categories: ‚Äú</span><span><strong>encoder only</strong></span><span>‚Äù (a la </span><span><a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener noreferrer">BERT</a></span><span>); ‚Äú</span><span><strong>decoder only</strong></span><span>‚Äù (a la </span><span><a href="https://openai.com/gpt-4" target="_blank" rel="noopener noreferrer">GPT</a></span><span>); and having an ‚Äú</span><span><strong>encoder-decoder</strong></span><span>‚Äù architecture (a la </span><span><a href="https://arxiv.org/abs/1910.10683" target="_blank" rel="noopener noreferrer">T5</a></span><span>). Although all of these architectures can be rigged for a broad range of tasks (e.g. classification, translation, etc),  </span><span><a href="https://huggingface.co/learn/nlp-course/chapter1/5?fw=pt" target="_blank" rel="noopener noreferrer">encoders</a></span><span> are thought to be useful for tasks where the entire sequence needs to be understood (such as sentiment classification), whereas </span><span><a href="https://huggingface.co/learn/nlp-course/chapter1/6" target="_blank" rel="noopener noreferrer">decoders</a></span><span> are thought to be useful for tasks where text needs to be completed (such as completing a sentence). Encoder-decoder architectures can be applied to a variety of problems, but are most famously associated with language translation.</span></span></p><p><span><span><em>Decoder-only Transformers such as ChatGPT &amp; GPT-4 are the class of LLM that are ubiquitously referring to as ‚Äúgenerative AI‚Äù.</em></span></span></p><p><span><span>Since the debut of the </span><span><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer">OG Transformer paper</a></span><span> ~6 years ago, we‚Äôve gleamed a couple interesting properties about this class of models.</span></span></p><h2 id="block-89b981fe82344f2e9ca11e2c8b5b3958"><span id="89b981fe82344f2e9ca11e2c8b5b3958"></span><span><span>Generalization üß†</span></span></h2><p><span><span>We learned that the the same trained LLM could figure out how to complete many different tasks with only being shown a few examples for each task; that is, </span><span><a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener noreferrer">LLMs are few-shot learners</a></span><span>. This meant that whatever the LLM had learned about language in it‚Äôs (pre-)training task (which is usually predicting the next word in a sequence), it could translate to new tasks without needing to be trained from scratch to do said task (and with only a handful of examples). </span></span></p><p><span><span>That is, we discovered LLMs‚Äô capacity to </span><span><em>generalize</em></span><span>.</span></span></p><div id="block-15f5462f917748768b78dc4b30517aac"><p><span><span></span><img alt="Figure 1.2 in " src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive"/></span></p><figcaption><span><span>Figure 1.2 in </span><span><a href="https://arxiv.org/pdf/2005.14165.pdf" target="_blank" rel="noopener noreferrer">the GPT-3 paper</a></span><span>. Shows that larger models are better at generalizing to new tasks (especially via ‚Äúin-context learning‚Äù).</span></span></figcaption></div><h2 id="block-85b3ec4df96e4c878c1a6db6357269a2"><span id="85b3ec4df96e4c878c1a6db6357269a2"></span><span><span>Power Laws in Performance üö®</span></span></h2><p><span><span>We also learned that LLMs had </span><span><a href="https://arxiv.org/pdf/2001.08361.pdf" target="_blank" rel="noopener noreferrer">predictable (power law) scaling behavior</a></span><span>. With larger training datasets, models could scale up in parameter size and become more data efficient, ultimately leading to better performance on benchmarks. </span></span></p><p><span><span>Given a dataset size and chosen model size, we can (seemingly magically) predict the performance of the model prior to (pre-)training it.</span></span></p><div id="block-1a811957d73043889a65d8015e2a44a5"><p><span><span></span><img alt="Figure 1 in " src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive"/></span></p><figcaption><span><span>Figure 1 in </span><span><a href="https://arxiv.org/pdf/2001.08361.pdf" target="_blank" rel="noopener noreferrer">the Scaling Laws Paper</a></span><span> üôèüèΩ. </span></span></figcaption></div><h2 id="block-eae32bda8e6f423da00cbd16d0ed325f"><span id="eae32bda8e6f423da00cbd16d0ed325f"></span><span><span>Research Trends üìà</span></span></h2><p><span><span>Given these observations, a large research trend in LLMs was</span><span><strong> training progressively larger and larger LLMs</strong></span><span> and measuring their performance on benchmarks (although, some papers such as the</span><span><a href="https://arxiv.org/abs/2103.00020" target="_blank" rel="noopener noreferrer"> CLIP paper</a></span><span> call into question whether benchmark performance actually reflected generalizability, part of a nuanced observation called ‚ÄúThe Cheating Hypothesis‚Äù). This required splitting these models across many GPUs/TPUs (i.e. </span><span><strong>model parallelism</strong></span><span>) due in large part to a model tweak provided from </span><span><a href="https://arxiv.org/abs/1909.08053" target="_blank" rel="noopener noreferrer">the Megatron paper</a></span><span>, </span><span><a href="https://lilianweng.github.io/posts/2021-09-25-train-large/" target="_blank" rel="noopener noreferrer">innovations in model/pipeline sharding</a></span><span>, and packages such as </span><span><a href="https://github.com/microsoft/DeepSpeed" target="_blank" rel="noopener noreferrer">DeepSpeed</a></span><span>. </span><span><a href="https://arxiv.org/abs/2209.05433" target="_blank" rel="noopener noreferrer">Quantization</a></span><span> also reduced the memory and computational footprint of these models. And since the traditional self-attention mechanism at the core of the Transformer is </span><span><!--$--><span id=""><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N^2)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span><span>N</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span>)</span></span></span></span></span><!--/$--></span><span> in space and time complexity,  naturally research into faster mechanisms such as </span><span><a href="https://arxiv.org/abs/2205.14135" target="_blank" rel="noopener noreferrer">Flash Attention</a></span><span> was of considerable interest. Further, innovations like </span><span><a href="https://arxiv.org/pdf/2108.12409.pdf" target="_blank" rel="noopener noreferrer">Alibi</a></span><span> allowed for variable context windows. This opened the door to </span><span><strong>larger context windows </strong></span><span>and is the reason why today‚Äôs LLMs have as large as </span><span><a href="https://www.anthropic.com/index/100k-context-windows" target="_blank" rel="noopener noreferrer">100k token context windows</a></span><span>. </span></span></p><p><span><span>And given the size of these (very large) LLMs, there was interest in how to </span><span><strong>fine-tune them to problems more efficiently</strong></span><span>. Innovation in Parameter-Efficient Fine-Tuning (</span><span><strong>PEFT</strong></span><span>) such as </span><span><a href="https://arxiv.org/pdf/1902.00751.pdf" target="_blank" rel="noopener noreferrer">Adapters</a></span><span> and </span><span><a href="https://arxiv.org/pdf/2106.09685.pdf" target="_blank" rel="noopener noreferrer">LoRA</a></span><span> allowed for faster fine-tuning since there are fewer parameters to adjust in these paradigms. Combined with the advent of 4- and 8-bit </span><span><strong>quantization</strong></span><span>, it‚Äôs now even possible to fine-tune a model on CPU! (note: most models are trained using 16 or 32 bit floats)</span></span></p><p><span><span>[</span><span><strong>Note</strong></span><span>: This is not a comprehensive overview of research trends. For instance, there was considerable research into other areas such as LLMs‚Äô ability to </span><span><a href="https://arxiv.org/abs/2107.06499" target="_blank" rel="noopener noreferrer">regurgitate information</a></span><span>, </span><span><a href="https://arxiv.org/pdf/2104.13733.pdf" target="_blank" rel="noopener noreferrer">adversarial attacks</a></span><span>, and domain specific LLMs such as </span><span><a href="https://arxiv.org/pdf/2107.03374.pdf" target="_blank" rel="noopener noreferrer">Codex</a></span><span> (for writing code) as well as early-stage multimodal LLMs (i.e. Transformers that understand images, text, etc). Further, </span><span><a href="https://arxiv.org/pdf/2112.04426.pdf" target="_blank" rel="noopener noreferrer">RETRO</a></span><span> and </span><span><a href="https://arxiv.org/pdf/2112.09332.pdf" target="_blank" rel="noopener noreferrer">webGPT</a></span><span> showed that smaller LLMs could perform the same as larger models with efficient querying/ information retrieval].</span></span></p><p><span><span>[And some of these papers like Flash Attention and LoRA came chronologically after the papers discussed in the next few sections].</span></span></p><p><span><span>Yet, a major breakthrough in our understanding of LLM behavior was made with the release of </span><span><a href="https://arxiv.org/abs/2203.02155" target="_blank" rel="noopener noreferrer">the instructGPT paper</a></span><span>. </span></span></p><p><span><span>GPT-3 (particularly the large parameter kind) already demonstrated the ability to follow natural language instructions (i.e. ‚Äúprompts‚Äù), although these instructions typically needed to be </span><span><em>carefully worded </em></span><span>to get a desired output. </span></span></p><p><span><span>Yet, the output tended to be regurgitated language found deep in the dark corners of the Internet: </span><span><em>unfiltered</em></span><span>, likely </span><span><em>offensive</em></span><span>, </span><span><em>untruthful</em></span><span> and most probably </span><span><em>not</em></span><span> the response the user wanted. If the response wasn‚Äôt regurgitated, it could even be entirely made up (what are typically referred to as ‚Äú</span><span><strong>hallucinations</strong></span><span>‚Äù). </span></span></p><p><span><span>That is to say, the LLMs‚Äô capacity to follow natural language instructions could be quite </span><span><em>underwhelming</em></span><span>. </span></span></p><div id="block-b630ee614965424e992bdf837e20f0b4"><p><span><span></span><img alt="GPT-3 v. InstructGPT in response to a prompt. From OpenAI‚Äôs " src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive"/></span></p><figcaption><span><span>GPT-3 v. InstructGPT in response to a prompt. From OpenAI‚Äôs </span><span><a href="https://openai.com/research/instruction-following" target="_blank" rel="noopener noreferrer">blog post</a></span><span>.</span></span></figcaption></div><h2 id="block-c32b3388754146de815fc07ed8fbc2e1"><span id="c32b3388754146de815fc07ed8fbc2e1"></span><span><span>Steerability ‚õ∑Ô∏è &amp; Alignment üßòüèΩ‚Äç‚ôÄÔ∏è</span></span></h2><p><span><span>Training LLMs to predict the next token in a sentence has been surprisingly effective in teaching them to learn a generalizable representation of language. Train for this task over a large corpus (like the entire Internet), maybe add some other prediction tasks into the mix (translation, classification, etc - i.e. ‚Äú</span><span><em>mixture of objectives</em></span><span>‚Äù), and voila, you have yourself an LLM that, if large enough, can be easily taught to do other specific things with only a handful of examples (i.e. few-shot learners).</span></span></p><p><span><span>Yet, this training objective does not seem to translate into LLMs following ‚Äú</span><span><strong>user intent</strong></span><span>‚Äù.</span></span></p><p><span><span>LLMs might be able to answer multiple choice questions well or be fine-tuned to do some specific task; but left to their own devices, they aren‚Äôt shown to be particularly good at following user instructions </span><span><em>without significant guardrails </em></span><span>(particularly in a zero-shot setting). </span></span></p><p><span><span>In fact, many a time they tend to regurgitate an answer that they‚Äôve seen before, or ignore the instructions entirely and ramble, or give a confident sounding answer that was non-sense (i.e. hallucinations). This is the problem that the ML research community calls </span><span><strong>steerability</strong></span><span> - the ability to prompt an LLM to provide a desired result.</span></span></p><div id="block-c9c12af17d3149b58733da9f171b8844"><p><span><span>Compound this with the desire that an LLM output </span><span><strong>embody a set of values</strong></span><span> (e.g. lack racism or homophobia, or say</span><span><strong> </strong></span><span><strong><a href="https://arxiv.org/pdf/2204.05862.pdf" target="_blank" rel="noopener noreferrer">Anthropic‚Äôs HHH</a></strong></span><span>) or quality of output expected by the user and we begin to appreciate the surface of </span><span><a href="https://ai-alignment.com/?gi=903be471f45d" target="_blank" rel="noopener noreferrer"><strong>the alignment problem</strong></a></span><span>. What </span><span><a href="https://openai.com/blog/democratic-inputs-to-ai" target="_blank" rel="noopener noreferrer">values an LLM is aligned to</a></span><span>, how to evaluate for alignment, and whether </span><span><a href="https://openai.com/blog/our-approach-to-alignment-research" target="_blank" rel="noopener noreferrer">we can align systems more intelligent than ourselves</a></span><span> are all interesting open research threads.</span></span></p></div><p><span><span>Within the instructGPT paper, the authors developed a 2 part solution to trying to tackle this problem: </span></span></p><ol type="1"><li id="block-17622ca1f78c452fb85c8e3f4261c19a"><span><span>Supervised (Instruction) Fine-tuning  (SFT)</span></span></li><li id="block-310b159822f34ce59119be1db49ca2b5"><span><span>Reinforcement Learning via Human Feedback (RLHF)</span></span></li></ol><p><span><span>Using these 2 sequential training tasks, the authors are able to convert a GPT-3 into what they call </span><span><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>InstructGPT</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></span><span>. </span></span></p><p><span><span>The InstructGPT paper‚Äôs results showed that simply creating larger models was an </span><span><em>insufficient condition</em></span><span> for developing a steerable and aligned model:</span><span><strong> in fact, the 175B GPT-3 ‚Äúprompted‚Äù model performed worse on average than the 1.3B parameter InstructGPT</strong></span><span>.</span></span></p><div id="block-fe0d3856caf04645b909099586af9e38"><p><span><span></span><img alt="Comparison of human assessments of the outputs of GPT-3, GPT-3-prompted, ‚ÄúSFT‚Äù, and InstructGPT (‚ÄùSFT‚Äù+‚ÄùRLHF‚Äù). Again from the OpenAI " src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive"/></span></p><figcaption><span><span>Comparison of human assessments of the outputs of GPT-3, GPT-3-prompted, ‚ÄúSFT‚Äù, and InstructGPT (‚ÄùSFT‚Äù+‚ÄùRLHF‚Äù). Again from the OpenAI </span><span><a href="https://openai.com/research/instruction-following" target="_blank" rel="noopener noreferrer">blog post</a></span><span>. Using humans to ascertain the quality of a generative model‚Äôs output makes more sense than using a traditional set of benchmarks that need quantitative answers. Both the training and test labelers (completely different set of people - OpenAI did an incredible job designing an experimental design that minimizes label leakage and overfitting to one‚Äôs preferences) were selected using a rigorous evaluation criteria to make sure they were ‚Äúaligned‚Äù with OpenAI‚Äôs values they want to align their LLM to. These Likert scale measurements capture the steerability and alignment quality of these model outputs.</span></span></figcaption></div><p><span><span>That is to say,</span><span><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong> h</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></span><span><em><strong>ow</strong></em></span><span> you trained your LLM can be </span><span><strong>equally as important</strong></span><span> a knob as </span><span><strong>model size</strong></span><span>.</span></span></p><p><span><span>And while you might not have heard of InstructGPT, this paper‚Äôs recipe was the basis for  </span><span><strong><a href="https://chat.openai.com/auth/login" target="_blank" rel="noopener noreferrer">chatGPT</a></strong></span><span><strong> </strong></span><span>as well as a plethora of open source models that one can find on HuggingFace (check out the </span><span><a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" target="_blank" rel="noopener noreferrer">ever changing ü§ó¬†LLM Leaderboard</a></span><span>).</span></span></p><div id="block-92988009e1b140a3a04e3273bb41fcac"><p><span><span><img alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27480%27%20height=%27636.3256784968685%27/%3e"/></span><img alt="What is a blog post without a meme?" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic"/></span></p><figcaption><span><span>What is a blog post without a meme?</span></span></figcaption></div><h2 id="block-63e48dd8cd8a4d46be4029ff8a031e47"><span id="63e48dd8cd8a4d46be4029ff8a031e47"></span><span><span>1. Instruction Fine-Tuning  üéõÔ∏è</span></span></h2><p><span><span>The first step to improving the output of a generative model logically seems to be teach it to follow instructions better. </span></span></p><p><span><span>To that end, the authors culminated a series of prompts via submissions to their playground API from real users (i.e. tackle the cold-start problem) as well as a mixture of prompting tasks of their own devise. Using a rigorous selection process,  labelers were hired to produce high-quality outputs that were aligned with </span><span><u>Anthropic‚Äôs 3 Laws</u></span><span>: helpful, honest and harmless</span><span><strong> (HHH</strong></span><span>, or maybe </span><span><a href="https://en.wikipedia.org/wiki/Triple_H" target="_blank" rel="noopener noreferrer">Triple H</a></span><span>?</span><span><strong>). </strong></span></span></p><p><span><span>Using these gold-standard labels, they fine-tuned a GPT model to learn these outputs. This is what they call ‚Äú</span><span><strong>Supervised Fine-tuning</strong></span><span>‚Äù (SFT).</span></span></p><h2 id="block-44942fd81c604e8fa5b14272b008ba0b"><span id="44942fd81c604e8fa5b14272b008ba0b"></span><span><span>2. RLHF üíé</span></span></h2><p><span><span>Now what about teaching an LLM to produce outputs that embody a set of values?</span></span></p><p><span><span>While the public perception of OpenAI is predominantly tied to GPT* models, a non-trivial percentage of their efforts has been focused on attacking </span><span><a href="https://openai.com/blog/our-approach-to-alignment-research" target="_blank" rel="noopener noreferrer">the alignment problem</a></span><span>. Their solution to this problem, thus far, has been what they call </span><span><strong>Reinforcement Learning via Human Feedback  (RLHF) </strong></span><span>(note: this is expected to not work in the case of </span><span><a href="https://openai.com/blog/introducing-superalignment" target="_blank" rel="noopener noreferrer">superalignment</a></span><span>, or teaching systems smarter than ourselves our values). </span></span></p><p><span><span>Teaching an LLM a set of human values is a challenging modeling task. Traditional loss functions usually measure the model‚Äôs internal belief about the probability the data belongs to the correct label (i.e. cross entropy). But human values are more complex; they are hard to encapsulate within a single label . Rather than </span><span><em>explicitly</em></span><span> labeling data, it might be easier for a human to read two or more LLM outputs and encode their preferences through </span><span><em>comparison</em></span><span>. An AI system can then </span><span><strong>implicitly</strong></span><span> learn these preferences by learning to correctly rank a set of options. </span></span></p><p><span><span>Along these lines, RLHF seeks to teach an AI system a set of preferences by learning a </span><span><strong>reward function/model </strong></span><span>through interacting with a human and gaining their input. </span></span></p><p><span><span>This reward model is then used to </span><span><strong>guide the LLM</strong></span><span> to produce higher-valued outputs through </span><span><strong>reinforcement learning. </strong></span><span> At each step, the reward model takes an input and output and returns a ‚Äúpreferability‚Äù number; the LLM seeks to narrow the difference between the ‚Äúoptimal‚Äù preference and it‚Äôs current output over many interactions. This is used to train the LLM to produce a set of outputs that are ‚Äúaligned‚Äù with the values of the labelers/modelers/etc (for a decent primer on RLHF, check out HuggingFace‚Äôs blog </span><span><a href="https://huggingface.co/blog/rlhf" target="_blank" rel="noopener noreferrer">post</a></span><span>).</span></span></p><div id="block-fa3ffdbc02a343628c2a55f38013926e"><p><span><span></span><img alt="Labeler output rankings. Figure 12 in the appendix of the " src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive"/></span></p><figcaption><span><span>Labeler output rankings. Figure 12 in the appendix of the </span><span><a href="https://arxiv.org/pdf/2203.02155.pdf" target="_blank" rel="noopener noreferrer">paper</a></span><span>. The labeler selection process and metrics are worth diving into.</span></span></figcaption></div><p><span><span>Among the theoretical benefits of using reinforcement learning to teach an LLM human preferences is </span><span><strong>the potential for reducing an LLM‚Äôs tendency towards regurgitation</strong></span><span> (as John Schulman and others have emphasized recently in lectures). Rather than overfitting/blindly learning a specific label distribution, RLHF is using rewards/penalties to guide the LLM outputs towards a more optimal answer through incentives without explicitly seeing a label (as in next token prediction).</span></span></p><p><span><span>InstructGPT was shown to produce outputs that were less toxic, more truthful, and more steerable.  In fact, this paper was a major catalyst¬†in the </span><span><em>outpouring</em></span><span> of new innovations using LLMs. </span></span></p><p><span><span>An emergent property of LLMs that we have gleamed in the wake of the InstructGPT paper is their capacity as </span><span><strong>reasoning agents</strong></span><span>. </span></span></p><div id="block-796a00d399c9419d83d6779999b13cc4"><p><span><span><img alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27528%27%20height=%27311.062818336163%27/%3e"/></span><img alt="An observation by Jan Leike about instructGPT." src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic"/></span></p><figcaption><span><span>An observation by Jan Leike about instructGPT.</span></span></figcaption></div><p><span><span>Given a set of instructions, an instruction fine-tuned/aligned LLM is able (conditional on size and training quality) to</span><span><strong> reason through a set of steps</strong></span><span> to produce a desired output. This has lead to a flurry of research into different prompt writing techniques: </span><span><strong>self-ask</strong></span><span>, </span><span><strong>chain-of-thought,</strong></span><span> etc. are all different parlor tricks designed to guide an LLM towards deducing the right answer. This set of techniques is what the ML community calls </span><span><a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/" target="_blank" rel="noopener noreferrer"><strong>prompt engineering</strong></a></span><span>, which seems more an art form than a science these days. </span></span></p><div id="block-da7f0c9a17914c1b8b80cbe50aaafda6"><p><span><span><img alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27528%27%20height=%27183.49574105621807%27/%3e"/></span><img alt="Great " src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic"/></span></p><figcaption><span><span>Great </span><span><a href="https://twitter.com/karpathy/status/1617979122625712128?s=20" target="_blank" rel="noopener noreferrer">tweet</a></span><span> from Karpathy on prompting. Read the sub-tweets citing some interesting papers. </span></span></figcaption></div><p><span><span>As LLMs improve over time, it is possible that researchers/practitioners will not need to lean on these techniques as much, despite sensational claims in the media that you can make </span><span><a href="https://www.forbes.com/sites/jodiecook/2023/07/12/ai-prompt-engineers-earn-300k-salaries-heres-how-to-learn-the-skill-for-free/?sh=78287b69d4a1" target="_blank" rel="noopener noreferrer">$300k being a  prompt engineer</a></span><span>.</span></span></p><p><span><span>Along with research into prompt engineering, LLM-centric tech stacks (what many refer to as </span><span><strong><a href="https://lilianweng.github.io/posts/2023-06-23-agent/" target="_blank" rel="noopener noreferrer">autonomous agents</a></strong></span><span>) is an area of enthusiasm. These </span><span><strong>agents</strong></span><span> utilize LLMs as problem solvers; provision them with a set of tools at their disposal and these models can solve a surprising number of tasks. </span></span></p><p><span><span>Much of the open source efforts are focused on developing these tools- it‚Äôs an exciting time to see what possibilities LLMs will further unlock üòé.</span></span></p><!--$--><div id="block-3bebe81e0adf4cdc9d013c38c04a4447"><pre><code>@article{
  title   = &#34;What We Know About LLMs (Primer)&#34;,
  author  = &#34;Thompson, Will&#34;,
  journal = &#34;https://willthompson.name&#34;,
  year    = &#34;2023&#34;,
  month   = &#34;July&#34;,
  day   = &#34;23&#34;,
  url     = &#34;https://willthompson.name/what-we-know-about-llms-primer&#34;
}</code></pre><figcaption><span></span></figcaption></div><!--/$--></article></div></div></div>
  </body>
</html>
