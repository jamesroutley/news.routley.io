<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://dubroy.com/blog/self">Original</a>
    <h1>The influence of Self</h1>
    
    <div id="readability-page-1" class="page"><div id="content">
		<div>
			
			<p><small>November 1, 2022
            </small></p><blockquote>
<p>Self is a prototype-based dynamic object-oriented programming language, environment, and virtual machine centered around the principles of simplicity, uniformity, concreteness, and liveness.</p>
</blockquote>
<p>— <em><a href="https://selflanguage.org/">selflanguage.org</a></em></p>
<p>I’ve been reading a bunch of old papers from the <a href="https://selflanguage.org/">Self project</a>. It’s pretty incredible how many things they invented that are absolutely essential in modern, high-performance managed language runtimes. It might be the most influential programming language that most programmers have never heard of. If you work in JavaScript, Java, or any JVM or .NET language — you’re benefitting from ideas pioneered in Self.</p>
<p>I’ve found nearly all of the Self publications to be well-written and easy to understand. If you’re interested in programming language implementation (VMs, JITs, etc.), the ones listed here are definitely worth your time.</p>
<p>But the performance techniques are just a small part of what makes Self so interesting. For a good overview, I’d recommend Dave Ungar’s 2009 talk <a href="https://www.youtube.com/watch?v=3ka4KY7TMTU">Self and Self: Whys and Wherefores</a>.</p>


<h2><a href="https://dl.acm.org/doi/pdf/10.1145/38765.38828">Self: The Power of Simplicity</a> (1987)</h2>
<p>This paper is a great starting point, covering the core of the language and the philosophy behind it. It’s also probably one of the best PL systems papers ever published.</p>
<figure>
    <img src="https://dubroy.com/blog/images/self-vs-class-based-systems@2x.png"/>
    <figcaption>Class-based systems vs. Self</figcaption>
</figure>

<h2><a href="https://dl.acm.org/doi/pdf/10.1145/74878.74884">An Efficient Implementation of Self, a Dynamically-Typed Object-Oriented Language Based on Prototypes</a> (1989)</h2>
<p>It’s hard to imagine a PL paper that starts stronger than this:</p>
<blockquote>
<p>We have developed and implemented techniques that double the performance of dynamically-typed object oriented languages. Our Self implementation runs
twice as fast as the fastest Smalltalk implementation, despite Self’s lack of classes and explicit variables.</p>
</blockquote>
<p>At the time, the benchmark for dynamic OO languages was ParcPlace Smalltalk — the so-called “Deutsch-Schiffman system”<sup id="fnref:deutsch-schiffman"><a href="#fn:deutsch-schiffman" rel="footnote">1</a></sup>. They had pioneered the use of <em>dynamic translation</em> (aka <em>just-in-time compilation</em>), as well as <em>inline caching</em> to reduce method dispatch cost.</p>
<p>The Self paper introduces two new ideas:</p>
<ol>
<li><em>Maps</em>, a concept that’s better known today as <a href="https://richardartoul.github.io/jekyll/update/2015/04/26/hidden-classes.html">hidden classes</a>. This is an extremely important idea that made nearly all the other optimizations in Self possible. For example, an inline cache relies on a “shape check” to determine a hit or miss. In Smalltalk, you check the receiver’s class — in Self, you use the receiver’s <em>map</em>.</li>
<li><em>Customized compilation</em>, or “type specialization”:<blockquote>
<p>The Deutsch-Schiffman Smalltalk system compiles a single machine code method for a given source codemethod. Our SELF compiler […] compiles a different machine code method for each type of receiver that runs a given source method. The advantage of this approach is that the compiler can know the type of the receiver of the message at compile-time, and can generate much better code for each of the specific versions of a method than it could for a single general-purpose compiled method.</p>
</blockquote>
</li>
</ol>
<h2><a href="https://bibliography.selflanguage.org/_static/pics.pdf">Optimizing Dynamically-Typed Object-Oriented Languages With Polymorphic Inline Caches</a> (1991)</h2>
<p>This paper introduced the polymorphic inline cache, an extension of the (monomorphic) inline cache used by Deutsch &amp; Schiffman in Smalltalk:</p>
<blockquote>
<p>Polymorphic inline caches (PICs) provide a new way to reduce the overhead of polymorphic message sends by extending inline caches to include more than one cached lookup result per call site.</p>
</blockquote>
<p><img src="https://dubroy.com/blog/images/polymorphic-inline-cache@2x.png"/></p>
<p>It also covers the use of PICs as a source of type information for an optimizing compiler, and outlines some basic strategies for adaptive recompilation:</p>
<blockquote>
<p>As an important side effect, PICs collect type information by recording all of the receiver types actually used at a given call site. The compiler can exploit this type information to generate better code when recompiling a method.</p>
</blockquote>
<h2><a href="https://dl.acm.org/doi/pdf/10.1145/233561.233562">A Third-Generation Self Implementation: Reconciling Responsiveness with Performance</a> (1996)</h2>
<p>This paper is a basically shorter version of Urs Hölzle’s 1994 PhD thesis, which introduced <em>type feedback</em> and <em>adaptive optimization</em>:</p>
<blockquote>
<p><strong>Type feedback</strong> allows any dynamically dispatched call to be inlined and substantially mitigates the performance penalty of dynamic dispatch. In our example implementation for the dynamically typed object-oriented language Self, type feedback reduces the call frequency by a factor of four and improves performance by 70% compared to a system without type feedback.</p>
<p><strong>Adaptive optimization</strong> discovers and optimizes the “hot spots” of a program while it is running. A method is compiled on-demand by a fast and dumb compiler, and the result is instrumented and cached. Only if a method is executed often is it recompiled with an optimizing compiler.</p>
</blockquote>


<p>Most of these ideas would later make their way into Java, though not by the most direct route. By 1991, Dave Ungar and Randy Smith — the founders of the Self project — were working on Self at Sun Microsystems. In 1994, Urs Hölzle and Lars Bak, two of the key Self engineers, co-founded a company called Animorphic Systems to work on <a href="http://strongtalk.org/">Strongtalk</a>. They later pivoted to Java, and in 1997 Animorphic was acquired by Sun, where their technology became the basis of the HotSpot engine.<sup id="fnref:strongtalk-history"><a href="#fn:strongtalk-history" rel="footnote">2</a></sup></p>
<p>Surprisingly, even though Self was a direct inspiration for JavaScript<sup id="fnref:inspiration"><a href="#fn:inspiration" rel="footnote">3</a></sup>, it took much longer for JS engines to adopt these techniques. Lars Bak was of course hired by Google to build V8, which was released in 2008. Within a few years, most of the techniques pioneered in Self had made their way into the major JS engines.</p>

		</div>	</div></div>
  </body>
</html>
