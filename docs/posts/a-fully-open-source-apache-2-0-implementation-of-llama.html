<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/Lightning-AI/lit-llama">Original</a>
    <h1>Show HN: A fully open-source (Apache 2.0)implementation of llama</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">

<p dir="auto">Independent implementation of <a href="https://github.com/facebookresearch/llama">LLaMA</a> that is fully open source under the <strong>Apache 2.0 license.</strong></p>
<p dir="auto">This implementation builds on <a href="https://github.com/karpathy/nanoGPT">nanoGPT</a>.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-why" aria-hidden="true" href="#why"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Why?</h2>
<p dir="auto">We believe that AI should be fully open source and part of the collective knowledge.</p>
<p dir="auto">The original <a href="https://github.com/facebookresearch/llama">LLaMA code</a> is <a href="https://github.com/facebookresearch/llama/blob/main/LICENSE">GPL licensed</a> which means any project using it must also be released under GPL.</p>
<p dir="auto">This &#34;taints&#34; any other code and prevents meaningful academic and commercial use.</p>
<p dir="auto"><strong>Lit-LLaMA solves that for good.</strong></p>

<h2 tabindex="-1" dir="auto"><a id="user-content-design-principles" aria-hidden="true" href="#design-principles"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Design principles</h2>
<p dir="auto"><strong>Lit-LLaMA</strong> is:</p>
<ul dir="auto">
<li><strong>Simple:</strong> Single-file implementation without boilerplate.</li>
<li><strong>Correct:</strong> Numerically equivalent to the original model.</li>
<li><strong>Optimized:</strong> Runs on consumer hardware or at scale.</li>
<li><strong>Open-source:</strong> No strings attached.</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-get-involved" aria-hidden="true" href="#get-involved"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Get involved!</h2>
<p dir="auto"><a href="https://discord.gg/VptPCZkGNa" rel="nofollow">Join our Discord</a> to build high-performance, truly open-source models for the common benefit of the community.</p>

<h2 tabindex="-1" dir="auto"><a id="user-content-setup" aria-hidden="true" href="#setup"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Setup</h2>
<p dir="auto">Clone the repo</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/Lightning-AI/lit-llama
cd lit-llama"><pre>git clone https://github.com/Lightning-AI/lit-llama
<span>cd</span> lit-llama</pre></div>
<p dir="auto">install dependencies</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre>pip install -r requirements.txt</pre></div>
<p dir="auto">You are all set! <g-emoji alias="tada" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png">ðŸŽ‰</g-emoji></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-use-the-model" aria-hidden="true" href="#use-the-model"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Use the model</h2>
<p dir="auto">To generate text predictions, download the model weights following the instructions on the official <a href="https://github.com/facebookresearch/llama">LLaMA repository</a>. Now you should have a folder like this:</p>
<div data-snippet-clipboard-copy-content="checkpoints/llama
â”œâ”€â”€ 7B
â”‚   â”œâ”€â”€ checklist.chk
â”‚   â”œâ”€â”€ consolidated.00.pth
â”‚   â””â”€â”€ params.json
â”œâ”€â”€ 13B
â”‚   ...
â”œâ”€â”€ tokenizer_checklist.chk
â””â”€â”€ tokenizer.model"><pre lang="text"><code>checkpoints/llama
â”œâ”€â”€ 7B
â”‚   â”œâ”€â”€ checklist.chk
â”‚   â”œâ”€â”€ consolidated.00.pth
â”‚   â””â”€â”€ params.json
â”œâ”€â”€ 13B
â”‚   ...
â”œâ”€â”€ tokenizer_checklist.chk
â””â”€â”€ tokenizer.model
</code></pre></div>
<p dir="auto">Convert the weights to the Lit-LLaMA format:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python scripts/convert_checkpoint.py \
    --output_dir checkpoints/lit-llama \
    --ckpt_dir checkpoints/llama \
    --tokenizer_path checkpoints/llama/tokenizer.model \
    --model_size 7B"><pre>python scripts/convert_checkpoint.py \
    --output_dir checkpoints/lit-llama \
    --ckpt_dir checkpoints/llama \
    --tokenizer_path checkpoints/llama/tokenizer.model \
    --model_size 7B</pre></div>
<p dir="auto">Run inference:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python generate.py --prompt &#34;Hello, my name is&#34;"><pre>python generate.py --prompt <span><span>&#34;</span>Hello, my name is<span>&#34;</span></span></pre></div>
<p dir="auto">This will run the 7B model and require ~26 GB of GPU memory (A100 GPU).</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-run-lit-llama-on-consumer-devices" aria-hidden="true" href="#run-lit-llama-on-consumer-devices"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Run Lit-LLaMA on consumer devices</h3>
<p dir="auto">For GPUs with less memory, enable quantization (<code>--quantize true</code>). This will take longer to load but require ~8GB of memory.
This can run on any consumer GPU.</p>
<div dir="auto" data-snippet-clipboard-copy-content="python generate.py --quantize true --prompt &#34;Hello, my name is&#34;"><pre>python generate.py --quantize <span>true</span> --prompt <span><span>&#34;</span>Hello, my name is<span>&#34;</span></span></pre></div>
<p dir="auto">See <code>python generate.py --help</code> for more options.</p>

<h2 tabindex="-1" dir="auto"><a id="user-content-get-involved-1" aria-hidden="true" href="#get-involved-1"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Get involved!</h2>
<p dir="auto">We&#39;re in a quest towards fully open source AI.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/b1a52a9f33eedc5ae3701ca5fad90e3bb92587fbaa77f7aea988a0df148083fb/68747470733a2f2f706c2d7075626c69632d646174612e73332e616d617a6f6e6177732e636f6d2f6173736574735f6c696768746e696e672f4c69745f4c4c614d415f496c6c757374726174696f6e33782e706e67"><img src="https://camo.githubusercontent.com/b1a52a9f33eedc5ae3701ca5fad90e3bb92587fbaa77f7aea988a0df148083fb/68747470733a2f2f706c2d7075626c69632d646174612e73332e616d617a6f6e6177732e636f6d2f6173736574735f6c696768746e696e672f4c69745f4c4c614d415f496c6c757374726174696f6e33782e706e67" alt="Lit-LLaMA" width="128" data-canonical-src="https://pl-public-data.s3.amazonaws.com/assets_lightning/Lit_LLaMA_Illustration3x.png"/></a></p>
<p dir="auto">Join us and start contributing, especially on the following areas:</p>
<ul>
<li> <a href="https://github.com/Lightning-AI/lit-llama/labels/pre-training">Pre-training</a></li>
<li> <a href="https://github.com/Lightning-AI/lit-llama/labels/fine-tuning">Fine-tuning (full and LoRA)</a></li>
<li> <a href="https://github.com/Lightning-AI/lit-llama/labels/quantization">Quantization</a></li>
<li> <a href="https://github.com/Lightning-AI/lit-llama/labels/sparsification">Sparsification</a></li>
</ul>
<p dir="auto">Look at <code>train.py</code> for a starting point towards pre-training / fine-tuning using <a href="https://lightning.ai/docs/fabric/stable/" rel="nofollow">Lightning Fabric</a>.</p>
<p dir="auto">Don&#39;t forget to <a href="https://discord.gg/VptPCZkGNa" rel="nofollow">join our Discord</a>!</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-acknowledgements" aria-hidden="true" href="#acknowledgements"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Acknowledgements</h2>
<ul dir="auto">
<li><a href="https://github.com/karpathy">@karpathy</a> for <a href="https://github.com/karpathy/nanoGPT">nanoGPT</a></li>
<li><a href="https://github.com/facebookresearch">@FacebookResearch</a> for the original <a href="https://github.com/facebookresearch/llama">LLaMA implementation</a></li>
<li><a href="https://github.com/TimDettmers">@TimDettmers</a> for <a href="https://github.com/TimDettmers/bitsandbytes">bitsandbytes</a></li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-license" aria-hidden="true" href="#license"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>License</h2>
<p dir="auto">Lit-LLaMA is released under the <a href="https://github.com/Lightning-AI/lightning-llama/blob/main/LICENSE">Apache 2.0</a> license.</p>
</article>
          </div></div>
  </body>
</html>
