<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.mentat.ai/benchmarking-gpt-4-turbo-a-cautionary-tale">Original</a>
    <h1>Benchmarking GPT-4 Turbo – A Cautionary Tale</h1>
    
    <div id="readability-page-1" class="page"><div data-framer-name="Content" data-framer-component-type="RichTextContainer"><p>Since we first introduced <!-- --><a href="https://github.com/AbanteAI/mentat" target="_blank" rel="noopener">Mentat</a>, GPT-4 has been the default model. While Mentat can run with GPT-3.5 or even local models, GPT-4’s higher quality code edits are well worth the extra cost. We are excited to unlock GPT-4 Turbo&#39;s full potential — being 2-3x cheaper and having a significantly larger context — but does it match the quality of GPT-4, especially for editing code?<!-- --></p><h2>Benchmarking GPT-4 Turbo</h2><p>To answer this question, we ran both GPT-4 and GPT-4 Turbo on our <!-- --><a href="https://github.com/AbanteAI/mentat/blob/main/tests/benchmarks/exercism_practice.py" target="_blank" rel="noopener">Exercism benchmarks</a>, compiled from a set of 122 <!-- --><a href="https://exercism.org/" target="_blank" rel="noopener">Exercism</a> programming exercises. We got the idea to use exercism exercises from <!-- --><a href="https://aider.chat/docs/benchmarks.html" target="_blank" rel="noopener">Aider</a> and ended up using the same prompts as them to allow us to directly compare our performances.<!-- --></p><p>We ran GPT-4 on each task and gave it two tries to succeed (on the second attempt, it is shown why it failed). GPT-4 ended up solving 86/122, or 70%, of the JavaScript exercises.</p><p>Running that same benchmark with GPT-4 Turbo we got 84/122, or 68.8% of the exercises. Slightly worse, but close enough that it could just be statistical noise. But wait! Looking closer at the results there was a significant difference: GPT-4 solved 76 on the first try and only an additional 10 on the second attempt, while GPT-4 Turbo only solved 56 on the first try and solved an additional 28 on the second attempt.</p><h2>Interpreting Results</h2><p>Why would GPT-4 Turbo solve fewer tasks on the first attempt? We examined individual exercises and the code it wrote for them and found that it often wrote reasonable solutions but failed on the first attempt due to unclear or ambiguous instructions. But then how does GPT-4 solve them? Our theory was that GPT-4 had substantially memorized the Exercism training tasks, but that when GPT-4 was downsized (most likely through distillation) to GPT-4 Turbo, it lost some of this raw memorization capability.</p><p>We designed a test for this theory: we reran the benchmarks without showing the models the instructions to each exercise. Instead, we just told them that they were Exercism exercises, and gave them the exercise names and function stubs. This is not enough to solve the problem unless the model has it memorized. Due to rate limits on GPT-4 Turbo, we only ran the first 40 exercises. GPT-4 solved 23/40, or 57.5%, on the first try and an additional 5 on the second try. GPT-4 Turbo, on the other hand, only solved 12/40, or 30%, on the first try and an additional 11 on the second try. We interpret this as confirming that GPT-4 has more of the exercises memorized than GPT-4-Turbo.</p><p><img alt="" data-framer-asset="data:framer/asset-reference,aoa9JDugtTsDNSAVzXKgW8vq1o.png" data-framer-height="878" data-framer-width="1330" height="439" src="https://framerusercontent.com/images/aoa9JDugtTsDNSAVzXKgW8vq1o.png" width="665"/></p><p>Our results seem similar to <!-- --><a href="https://twitter.com/wangzjeff/status/1721934560919994823" target="_blank" rel="noopener">this finding</a>:<!-- --></p><p><img alt="" data-framer-asset="data:framer/asset-reference,xOoj8r4n0N1TQ4Px32I4PlhECz8.png" data-framer-height="2584" data-framer-width="1262" height="1292" src="https://framerusercontent.com/images/xOoj8r4n0N1TQ4Px32I4PlhECz8.png" width="631"/></p><p>Although the author OCR’ed the SAT questions and believes that they weren’t in the training data, we believe it’s fairly likely that these questions – and definitely some incredibly similar questions – ended up in the training data at some point, making the drop in GPT-4 Turbo&#39;s measured performance hard to interpret.</p><h2>Future Benchmarks</h2><p>Benchmarks derived from content in GPT’s training data are still valuable for determining how good GPT is at responding in the correct edit format, as well as comparing fine-tuned models to each other. But these results make it pretty clear that they aren’t an accurate test for comparing models trained on separate datasets or distilled models – which is what we suspect GPT-4 Turbo is.</p><p>These results emphasize the need for better benchmarks, something we&#39;ve already begun building for Mentat: real world, realistic coding tasks based on recent commits to open source repositories that were made after the training cutoff. Although no benchmark will ever be perfect, we are confident that these improvements will help us gauge the relative accuracy of different models in the future.</p></div></div>
  </body>
</html>
