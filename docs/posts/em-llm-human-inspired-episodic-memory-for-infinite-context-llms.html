<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/em-llm/EM-LLM-model">Original</a>
    <h1>EM-LLM: Human-Inspired Episodic Memory for Infinite Context LLMs</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">This repository contains a version of the code for EM-LLM, published in <b>ICLR 2025</b>: <a href="https://openreview.net/forum?id=BI2int5SAC" rel="nofollow">[openreview link]</a>.</p>

<ul dir="auto">
<li><a href="#overview">Overview</a></li>
<li><a href="#usage">Usage</a></li>
<li><a href="#citation">Citation</a></li>
</ul>

<p dir="auto">While typical LLMs struggle with processing extensive contexts, the human brain excels at organising and retrieving experiences spanning a lifetime. In this work, we introduce EM-LLM, an architecture that integrates key aspects of human episodic memory and event cognition into LLMs with <strong>no fine-tuning</strong>, enabling them to handle practically infinite context lengths while maintaining computational efficiency. EM-LLM organises sequences of tokens into coherent episodic events using a combination of Bayesian surprise and graph-theoretic boundary refinement in an online fashion. When needed, these events are retrieved through a two-stage memory process, combining similarity-based and temporally contiguous retrieval for efficient and human-like access to relevant information. Experiments on the LongBench and <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="b98542d14186e3d5e106405eacc40899">$\infty$</math-renderer>-Bench benchmarks demonstrate EM-LLM&#39;s superior performance, consistently outperforming the SOTA retrieval model InfLLM across various baseline LLMs. In addition, EM-LLM <strong>outperforms RAG</strong> in a wide range of tasks, while requiring similar resources. Notably, EM-LLM&#39;s performance even <strong>surpasses full-context models</strong> in most tasks, while successfully performing retrieval across <strong>10M tokens</strong> - a scale computationally infeasible for such models. Our analysis reveals strong correlations between EM-LLM&#39;s event segmentation and human-perceived events, suggesting a bridge between this artificial system and its biological counterpart, thereby offering a novel computational framework for exploring human memory mechanisms.</p>

<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/em-llm/EM-LLM-model/blob/main/images/architecture.png"><img src="https://github.com/em-llm/EM-LLM-model/raw/main/images/architecture.png" alt="architecture" width="80%"/></a>
</p>
<p dir="auto"><strong>Figure 1:</strong>  Architecture of memory formation and retrieval in each LLM layer. <em>Formation:</em> Input sequence is initially segmented via surprise (purple dashed lines in ①), then segmentation is refined based on group theoretic metrics (green dashed lines in ②). Initial tokens and local context are preserved. <em>Retrieval:</em> via both k-NN search ③ and selecting contiguous events from episodic memory ④.</p>

<p dir="auto">Click <a href="https://github.com/em-llm/EM-LLM-model/blob/main/benchmark/further_results.md">here</a> more complete result tables.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/em-llm/EM-LLM-model/blob/main/images/emllm_rag_fc.png"><img src="https://github.com/em-llm/EM-LLM-model/raw/main/images/emllm_rag_fc.png" alt="emllm_rag_fc" width="70%"/></a>
</p>
<p dir="auto"><strong>Figure 2:</strong> <strong>(Left)</strong> EM-LLM$_S$ vs. RAG (NV-Embed-v2 retriever) vs. full-context, with LLaMA-3.1-8B as the base LLM, evaluated on LongBench. <strong>(Right)</strong> Comparison of various long-sequence methods (sorted based on their context window length) on an extended version of <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="b98542d14186e3d5e106405eacc40899">$\infty$</math-renderer>-Bench&#39;s <em>Retrieve.PassKey</em>.</p>


<p dir="auto">Install requirements:</p>
<div data-snippet-clipboard-copy-content="python3 -m pip install --upgrade pip
pip install -r &#34;${base_dir}/requirements.txt&#34;
pip install -e &#34;${base_dir}/.&#34;"><pre><code>python3 -m pip install --upgrade pip
pip install -r &#34;${base_dir}/requirements.txt&#34;
pip install -e &#34;${base_dir}/.&#34;
</code></pre></div>
<p dir="auto">The YAML files used for configuration can be found in the <code>config/</code> directory.</p>
<p dir="auto">Here is a breakdown of each parameter included in these files:</p>
<div dir="auto" data-snippet-clipboard-copy-content="verbose: false  # print the question/prediction/answer after an example has been processed 
compute_ppl: true  # print and log perplexity for each example/chunk
return_block_size: true  # print and log block size for each example/chunk
logging: true  # save logs to output directory and label individual worker logs during multiprocessing
em_splitter: surprisal  # method by which to split chunks into memory blocks (surprisal, random, sentence)

max_len: 2147483647  # maximum sequence length before truncation is used
chunk_size: 512  # size of chunked input during decoding
conv_type: mistral-inst  # conversation template type

extended_passkey: 1024  # length to extend infinite-bench&#39;s passkey task to in terms of thousands of tokens (k)

model:
  type: em-llm  # Which model to use for inference (only em-llm is made available in this version)
  path: mistralai/Mistral-7B-Instruct-v0.2  # HuggingFace model path
  min_block_size: 8  # the smallest possible block size - blocks smaller than this will be expanded to this size
  max_block_size: 128  # the biggest possible block size - blocks bigger than this will be split to this size
  n_init: 128  # number of initial tokens to include in context window
  n_local: 4096  # number of local tokens to include in context window
  n_mem: 2048  # number of retrieved tokens to include in context window (includes both the similarity and contiguity buffers)
  repr_topk: 4  # number of top-scoring tokens per memory unit considered as representative elements
  max_cached_block: 512  # number of memory blocks to keep in GPU memory - must be greater than n_mem/min_block_size
  exc_block_size: 512  # number of tokens queried at a time as an execution block - each execution block performs retrieval of n_mem tokens once
  base: 1000000  # RoPE base
  distance_scale: 1.0  # RoPE distance scale
  surprisal_threshold_gamma: 1.0  # the standard-deviation scaling factor in the surprisal calculation (see paper)

  min_free_cpu_memory: 100  # minimum amount CPU RAM (GB) to keep free when allocating memory blocks
  disk_offload_threshold: 300000  # number of tokens in a sequence past which disk offloading should be used
  vector_offload_threshold: 50000  # number of tokens in a sequence past which representative tokens should be offloaded to CPU memory

  similarity_refinement_kwargs:  # parameters relating directly to the boundary refinement step of our paper
    similarity_refinement: false  # whether to use boundary refinement or not
    refine_with_buffer: true  # if True, the adjacency matrix will include part of the neighbouring chunks in its calculation of the adjacency matrix - designed to make segmentations more compatible with neighbouring chunks, but also increases computation time
    refine_from_layer: 20  # which layers to use when calculating the adjacency 
    similarity_metric: modularity  # the metric to use as the objective during refinement: modularity or conductance (or intra_inter_sim but this doesn&#39;t work well so far)

  contiguity_buffer_kwargs:  # parameters relating directly to the contiguity buffer
    use_contiguity_buffer: true  # whether to use a contiguity buffer
    contiguity_buffer_size: 0.3  # proportion of n_mem tokens to dedicate to the contiguity buffer

  uniform_blocks: false  # ignore em_splitter (above) and segment chunks into fixed-sized blocks of size max_block_size (above)
  random_topk_blocks: false  # retrieve random blocks rather than the topk most similar blocks"><pre><span>verbose</span>: <span>false  </span><span><span>#</span> print the question/prediction/answer after an example has been processed </span>
<span>compute_ppl</span>: <span>true  </span><span><span>#</span> print and log perplexity for each example/chunk</span>
<span>return_block_size</span>: <span>true  </span><span><span>#</span> print and log block size for each example/chunk</span>
<span>logging</span>: <span>true  </span><span><span>#</span> save logs to output directory and label individual worker logs during multiprocessing</span>
<span>em_splitter</span>: <span>surprisal  </span><span><span>#</span> method by which to split chunks into memory blocks (surprisal, random, sentence)</span>

<span>max_len</span>: <span>2147483647</span>  <span><span>#</span> maximum sequence length before truncation is used</span>
<span>chunk_size</span>: <span>512</span>  <span><span>#</span> size of chunked input during decoding</span>
<span>conv_type</span>: <span>mistral-inst  </span><span><span>#</span> conversation template type</span>

<span>extended_passkey</span>: <span>1024</span>  <span><span>#</span> length to extend infinite-bench&#39;s passkey task to in terms of thousands of tokens (k)</span>

<span>model</span>:
  <span>type</span>: <span>em-llm  </span><span><span>#</span> Which model to use for inference (only em-llm is made available in this version)</span>
  <span>path</span>: <span>mistralai/Mistral-7B-Instruct-v0.2  </span><span><span>#</span> HuggingFace model path</span>
  <span>min_block_size</span>: <span>8</span>  <span><span>#</span> the smallest possible block size - blocks smaller than this will be expanded to this size</span>
  <span>max_block_size</span>: <span>128</span>  <span><span>#</span> the biggest possible block size - blocks bigger than this will be split to this size</span>
  <span>n_init</span>: <span>128</span>  <span><span>#</span> number of initial tokens to include in context window</span>
  <span>n_local</span>: <span>4096</span>  <span><span>#</span> number of local tokens to include in context window</span>
  <span>n_mem</span>: <span>2048</span>  <span><span>#</span> number of retrieved tokens to include in context window (includes both the similarity and contiguity buffers)</span>
  <span>repr_topk</span>: <span>4</span>  <span><span>#</span> number of top-scoring tokens per memory unit considered as representative elements</span>
  <span>max_cached_block</span>: <span>512</span>  <span><span>#</span> number of memory blocks to keep in GPU memory - must be greater than n_mem/min_block_size</span>
  <span>exc_block_size</span>: <span>512</span>  <span><span>#</span> number of tokens queried at a time as an execution block - each execution block performs retrieval of n_mem tokens once</span>
  <span>base</span>: <span>1000000</span>  <span><span>#</span> RoPE base</span>
  <span>distance_scale</span>: <span>1.0</span>  <span><span>#</span> RoPE distance scale</span>
  <span>surprisal_threshold_gamma</span>: <span>1.0</span>  <span><span>#</span> the standard-deviation scaling factor in the surprisal calculation (see paper)</span>

  <span>min_free_cpu_memory</span>: <span>100</span>  <span><span>#</span> minimum amount CPU RAM (GB) to keep free when allocating memory blocks</span>
  <span>disk_offload_threshold</span>: <span>300000</span>  <span><span>#</span> number of tokens in a sequence past which disk offloading should be used</span>
  <span>vector_offload_threshold</span>: <span>50000</span>  <span><span>#</span> number of tokens in a sequence past which representative tokens should be offloaded to CPU memory</span>

  <span>similarity_refinement_kwargs</span>:  <span><span>#</span> parameters relating directly to the boundary refinement step of our paper</span>
    <span>similarity_refinement</span>: <span>false  </span><span><span>#</span> whether to use boundary refinement or not</span>
    <span>refine_with_buffer</span>: <span>true  </span><span><span>#</span> if True, the adjacency matrix will include part of the neighbouring chunks in its calculation of the adjacency matrix - designed to make segmentations more compatible with neighbouring chunks, but also increases computation time</span>
    <span>refine_from_layer</span>: <span>20</span>  <span><span>#</span> which layers to use when calculating the adjacency </span>
    <span>similarity_metric</span>: <span>modularity  </span><span><span>#</span> the metric to use as the objective during refinement: modularity or conductance (or intra_inter_sim but this doesn&#39;t work well so far)</span>

  <span>contiguity_buffer_kwargs</span>:  <span><span>#</span> parameters relating directly to the contiguity buffer</span>
    <span>use_contiguity_buffer</span>: <span>true  </span><span><span>#</span> whether to use a contiguity buffer</span>
    <span>contiguity_buffer_size</span>: <span>0.3</span>  <span><span>#</span> proportion of n_mem tokens to dedicate to the contiguity buffer</span>

  <span>uniform_blocks</span>: <span>false  </span><span><span>#</span> ignore em_splitter (above) and segment chunks into fixed-sized blocks of size max_block_size (above)</span>
  <span>random_topk_blocks</span>: <span>false  </span><span><span>#</span> retrieve random blocks rather than the topk most similar blocks</span></pre></div>

<p dir="auto"><strong>Data Preparation</strong>
We adopt <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="b98542d14186e3d5e106405eacc40899">$\infty$</math-renderer>-Bench and LongBench for model evaluation. You can download the datasets by running the following command.</p>

<p dir="auto"><strong>Response Generation</strong>
You can evaluate EM-LLM by running the following command. You can also optionally pass in the following arguments to accomodate your hardware resources</p>
<div data-snippet-clipboard-copy-content="bash scripts/run.sh

    -m|--model  # DEFAULT: mistral; OPTIONS: mistral,llama3,llama31,phi3_mini,phi35_mini - Which base LLM to use during evaluation.
    -b|--benchmark  # DEFAULT: long-bench; OPTIONS: long-bench,infinite-bench,passkey - Which benchmark to evaluate. Passkey evaluates an extended version of InfiniteBench&#39;s passkey retrieval task (see yaml for context length parameter). 
    -w|--world-size  # DEFAULT: number of visible GPUs - Total number of GPUs to be used during evaluation. 
    -n|--num_gpus_per_job  # DEFAULT: 1 - How many GPUs to attribute to each job. If &gt;1, model layers will be evenly spread over multiple GPUs. 
    -r|--rank_offset  # DEFAULT: 0 - Ignores the first n GPUs visible to the script. Useful when running multiple experiments on a single node.
    -o|--allow_disk_offload  # DEFAULT: False - Whether to allow dynamic disk offloading of memory blocks or not (see the our paper&#39;s Appendix for more details). In single-GPU instances this will offload the representative tokens to CPU memory as well.
"><pre><code>bash scripts/run.sh

    -m|--model  # DEFAULT: mistral; OPTIONS: mistral,llama3,llama31,phi3_mini,phi35_mini - Which base LLM to use during evaluation.
    -b|--benchmark  # DEFAULT: long-bench; OPTIONS: long-bench,infinite-bench,passkey - Which benchmark to evaluate. Passkey evaluates an extended version of InfiniteBench&#39;s passkey retrieval task (see yaml for context length parameter). 
    -w|--world-size  # DEFAULT: number of visible GPUs - Total number of GPUs to be used during evaluation. 
    -n|--num_gpus_per_job  # DEFAULT: 1 - How many GPUs to attribute to each job. If &gt;1, model layers will be evenly spread over multiple GPUs. 
    -r|--rank_offset  # DEFAULT: 0 - Ignores the first n GPUs visible to the script. Useful when running multiple experiments on a single node.
    -o|--allow_disk_offload  # DEFAULT: False - Whether to allow dynamic disk offloading of memory blocks or not (see the our paper&#39;s Appendix for more details). In single-GPU instances this will offload the representative tokens to CPU memory as well.

</code></pre></div>

<p dir="auto">If you find EM-LLM useful, please cite the following paper:</p>
<div data-snippet-clipboard-copy-content="@inproceedings{fountas2025humaninspired,
    title={Human-inspired Episodic Memory for Infinite Context {LLM}s},
    author={Zafeirios Fountas and Martin Benfeghoul and Adnan Oomerjee and Fenia Christopoulou and Gerasimos Lampouras and Haitham Bou Ammar and Jun Wang},
    booktitle={The Thirteenth International Conference on Learning Representations},
    year={2025},
    url={https://openreview.net/forum?id=BI2int5SAC}
}"><pre><code>@inproceedings{fountas2025humaninspired,
    title={Human-inspired Episodic Memory for Infinite Context {LLM}s},
    author={Zafeirios Fountas and Martin Benfeghoul and Adnan Oomerjee and Fenia Christopoulou and Gerasimos Lampouras and Haitham Bou Ammar and Jun Wang},
    booktitle={The Thirteenth International Conference on Learning Representations},
    year={2025},
    url={https://openreview.net/forum?id=BI2int5SAC}
}
</code></pre></div>
</article></div></div>
  </body>
</html>
