<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.maginative.com/article/ai21-labs-unveils-jamba-the-first-production-grade-mamba-based-ai-model/">Original</a>
    <h1>Jamba: Production-grade Mamba-based AI model</h1>
    
    <div id="readability-page-1" class="page"><div>
        <p>AI21 Labs, has just released <a href="https://huggingface.co/ai21labs/Jamba-v0.1?ref=maginative.com" rel="noreferrer">Jamba</a>, the world&#39;s first production-grade AI model based on the innovative <a href="https://arxiv.org/pdf/2312.00752.pdf?ref=maginative.com" rel="noreferrer">Mamba</a> architecture. Most models today (like GPT, Gemini and Llama) are based on the Transformer architecture. Jamba combines the strengths of both the Mamba Structured State Space model (SSM) and the traditional Transformer architecture, delivering impressive performance and efficiency gains.</p><p>Jamba boasts an extensive context window of 256K tokens, equivalent to around 210 pages of text, while fitting up to 140K tokens on a single 80GB GPU. This remarkable feat is achieved through its hybrid SSM-Transformer architecture, which leverages mixture-of-experts (MoE) layers to draw on just 12B of its available 52B parameters during inference. The result is a model that can handle significantly longer contexts than most of its counterparts, such as Meta&#39;s Llama 2 with its 32,000-token context window, while maintaining high throughput and efficiency.</p><figure><img src="https://www.maginative.com/content/images/2024/03/660559ab523a2f62206cebec_Throughput-1.png" alt="" loading="lazy" width="2000" height="1068" srcset="https://www.maginative.com/content/images/size/w600/2024/03/660559ab523a2f62206cebec_Throughput-1.png 600w, https://www.maginative.com/content/images/size/w1000/2024/03/660559ab523a2f62206cebec_Throughput-1.png 1000w, https://www.maginative.com/content/images/size/w1600/2024/03/660559ab523a2f62206cebec_Throughput-1.png 1600w, https://www.maginative.com/content/images/2024/03/660559ab523a2f62206cebec_Throughput-1.png 2280w" sizes="(min-width: 1200px) 1200px"/><figcaption><span>Jamba delivers 3x throughput on long contexts, making it a more efficient model than Transformer-based models of comparable size like Mixtral 8x7B.</span></figcaption></figure><p>One of the key advantages of Jamba is its ability to deliver 3x throughput on long contexts compared to Transformer-based models of similar size, like Mixtral 8x7B. This is made possible by the model&#39;s unique hybrid architecture, which is composed of Transformer, Mamba, and mixture-of-experts (MoE) layers, optimizing for memory, throughput, and performance simultaneously. </p><figure><img src="https://www.maginative.com/content/images/2024/03/66053d21ac0ce4328ed20029_Frame-1410082065.jpg" alt="" loading="lazy" width="1140" height="682" srcset="https://www.maginative.com/content/images/size/w600/2024/03/66053d21ac0ce4328ed20029_Frame-1410082065.jpg 600w, https://www.maginative.com/content/images/size/w1000/2024/03/66053d21ac0ce4328ed20029_Frame-1410082065.jpg 1000w, https://www.maginative.com/content/images/2024/03/66053d21ac0ce4328ed20029_Frame-1410082065.jpg 1140w" sizes="(min-width: 720px) 720px"/></figure><p>It features a blocks-and-layers approach, with each Jamba block containing either an attention or a Mamba layer, followed by a multi-layer perceptron (MLP). This results in an overall ratio of one Transformer layer out of every eight total layers. AI21 Labs says this approach allows the model to maximize quality and throughput on a single GPU, leaving ample memory for common inference workloads.</p><p>Jamba&#39;s impressive performance extends beyond efficiency and cost-effectiveness. The model has already demonstrated remarkable results on various benchmarks, matching or outperforming state-of-the-art models in its size class across a wide range of tasks. </p><figure><img src="https://www.maginative.com/content/images/2024/03/6605773d17b02bc8e6854f00_Benchmarks-New.png" alt="" loading="lazy" width="2000" height="993" srcset="https://www.maginative.com/content/images/size/w600/2024/03/6605773d17b02bc8e6854f00_Benchmarks-New.png 600w, https://www.maginative.com/content/images/size/w1000/2024/03/6605773d17b02bc8e6854f00_Benchmarks-New.png 1000w, https://www.maginative.com/content/images/size/w1600/2024/03/6605773d17b02bc8e6854f00_Benchmarks-New.png 1600w, https://www.maginative.com/content/images/2024/03/6605773d17b02bc8e6854f00_Benchmarks-New.png 2280w" sizes="(min-width: 1200px) 1200px"/><figcaption><span>Jamba outperforms or matches other state-of-the-art models in its size class on a wide range of benchmarks.</span></figcaption></figure><p>Jamba is being released with open weights under Apache 2.0 license. It is available on <a href="https://huggingface.co/ai21labs/Jamba-v0.1?ref=maginative.com" rel="noreferrer">Hugging Face</a>, and will also be accessible from the <a href="http://ai.nvidia.com/?ref=maginative.com" rel="noopener noreferrer">NVIDIA API catalog</a> as <a href="https://developer.nvidia.com/blog/nvidia-nim-offers-optimized-inference-microservices-for-deploying-ai-models-at-scale/?ref=maginative.com" rel="noopener noreferrer">NVIDIA NIM</a> inference microservice, which enterprise applications developers can deploy with the <a href="https://nvidianews.nvidia.com/news/generative-ai-microservices-for-developers?ref=maginative.com" rel="noopener noreferrer">NVIDIA AI Enterprise software</a> platform.</p><p>For now, Jamba is currently released as a research model without the necessary safeguards for commercial use. However, AI21 Labs plans to release a fine-tuned, safer version in the coming weeks. As the AI community continues to explore and refine new architectures, we can expect to see even more impressive gains in performance, efficiency, and accessibility, paving the way for a new generation of more capable AI models.</p>
    </div></div>
  </body>
</html>
