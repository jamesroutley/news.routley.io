<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="http://muratbuffalo.blogspot.com/2024/01/scalable-oltp-in-cloud-whats-big-deal.html">Original</a>
    <h1>Scalable OLTP in the Cloud: What&#39;s the Big Deal?</h1>
    
    <div id="readability-page-1" class="page"><div id="post-body-7021699056598219221">
<p><a href="https://www.cidrdb.org/cidr2024/papers/p63-helland.pdf">This paper</a> is from Pat Helland, the apostate philosopher of database systems, overall a superb person, and a good friend of mine. The paper appeared this week at CIDR&#39;24. (Check out <a href="https://www.cidrdb.org/cidr2024/program.html">the program</a> for other interesting papers). The motivating question behind this work is: &#34;<b>What are the asymptotic limits to scale for cloud OLTP (OnLine Transaction Processing) systems?</b>&#34; Pat says that the CIDR 2023 paper <a href="https://muratbuffalo.blogspot.com/2023/01/is-scalable-oltp-in-cloud-solved.html">&#34;Is Scalable OLTP in the Cloud a Solved Problem?&#34;</a> prompted this question. </p><p>The answer to the question? Pat says that the answer lies in the joint responsibility of database and the application. If you know of Pat&#39;s work, which I have summarized <a href="https://muratbuffalo.blogspot.com/search?q=Helland">several in this blog</a>, you would know that Pat has been advocating along these lines before. But this paper provides a very crisp, specific, concrete answer. Read on for my summary of the paper.</p><p>Disclaimer: This is a wisdom and technical information/detail packed 13-page paper, so I will try my best to summarize the salient points. I will be using text from the paper to explain/summarize it. (Don&#39;t taze me bro!) </p><p>The database and the application have a BIG DEAL: their isolation semantics! In particular, snapshot isolation (SI) is the sweet spot. At this point, I got a nice database history lesson on how the isolation semantics evolved. I would have guessed the semantics had become more strict over time. No, on the contrary, they evolved to be more relaxed to meet performance and scalability expectations. And SI does hit a sweet point in that it still provides the user good isolation guarantees without jeopardizing the scaling behavior of the database by requiring it to serialize everything. </p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEixuW5RhqakvtCihi8JM91NYzNa7Ud6z3G2SM7D2fS4z1B60hF8w8pZOtnL7TW6n5HJHu9Uap4RNJO9lBCCy5LlXR2FaLterr1EOyZo7cNy7G2hvI0Z82BNQFdPywPLidWFFa76aPyaiVGg5G7ZdpneCtiDASM2uDZZZcLYmMhOW-tlJJVi4FdTMs7tMBI"><img alt="" data-original-height="722" data-original-width="676" height="400" src="https://blogger.googleusercontent.com/img/a/AVvXsEixuW5RhqakvtCihi8JM91NYzNa7Ud6z3G2SM7D2fS4z1B60hF8w8pZOtnL7TW6n5HJHu9Uap4RNJO9lBCCy5LlXR2FaLterr1EOyZo7cNy7G2hvI0Z82BNQFdPywPLidWFFa76aPyaiVGg5G7ZdpneCtiDASM2uDZZZcLYmMhOW-tlJJVi4FdTMs7tMBI=w375-h400" width="375"/></a></p><p>In the rest of the paper, keep in mind that, an OLTP system is defined as a domain-specific application using a <b>RCSI (READ COMMITTED SNAPSHOT ISOLATION) SQL database</b> to provide transactions across many concurrent users.</p><p>The BIG DEAL splits the scaling responsibilities between the database and the application.</p><ul><li><b><u>Scalable DBs don’t coordinate across disjoint TXs updating different keys.</u></b></li><li><b><u>Scalable apps don’t concurrently update the same key.</u></b></li></ul><p>The big deal provides guarantees from the DB to the App. A scalable application can read all it wants. Updates to disjoint records don’t coordinate across TXs. Row-locks on disjoint records don’t coordinate across TXs.</p><p>Applications must tolerate these big deal disclaimers. Reads return snapshots: Records have no &#34;current&#34; value. There is no NOW in a BIG DEAL database! Transactions may abort any time but not too often. SELECT with SKIP LOCKED may subset the set of qualifying records as it returns results.</p><p>This means applications should change business behavior in order to scale. They can only provide a fuzzy/blurry view of the &#34;current&#34; state/changes. So, apps introduce ambiguity in biz domain specific ways: online retail makes ambiguous promises such as &#34;Usually ships in 24 hours&#34;. And apps provide delayed truth: finances of a large company may take days to summarize. Many OLTP apps aggregate values synchronously as they interact with humans. Public TPC benchmarks (e.g., TPC-A, TPC-B, and TPC-C) mandated synchronous aggregations. But, as applications scale they should rethink concentrating the aggregated values of business state in dedicated records. By slowly  and asynchronously aggregating these business state, the application can scale in a domain-specific manner.</p><p>Before suggesting a hypothetical scalable database that satisfies the database side of the big deal, Pat shows us why today’s databases don’t scale!</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEha3LKPdGbzAgf0z8Gr8UwOfZIE5gviSpIYJl7gan52cd7k-tmgXKuzJbXt_je7-vnRpsi_TNc_61xbOZuP5hlRFPPyYdawO4ldh4tXQdCAZH_qH7ehfd-C9Y_t2WJbnrRgLIaNO11wTXITw-yvVn03mjkqR92P3qgcfQ56BRi-5yEAG9EvEs-8oSjyRKs"><img alt="" data-original-height="306" data-original-width="1528" src="https://blogger.googleusercontent.com/img/a/AVvXsEha3LKPdGbzAgf0z8Gr8UwOfZIE5gviSpIYJl7gan52cd7k-tmgXKuzJbXt_je7-vnRpsi_TNc_61xbOZuP5hlRFPPyYdawO4ldh4tXQdCAZH_qH7ehfd-C9Y_t2WJbnrRgLIaNO11wTXITw-yvVn03mjkqR92P3qgcfQ56BRi-5yEAG9EvEs-8oSjyRKs=s16000"/></a></p><p>In today&#39;s MVCC databases, reads &amp; writes fight to access the &#34;current&#34; value of a record. The current version has a <b>home</b> location (a partition, server, or a B+ tree) holding the most recently committed version of the record or perhaps an uncommitted version. To update a record, exclusive access to the record&#39;s home is required. This causes infighting, contention, and coordination between the updating TX and any concurrent reading TXs.</p><p>Even reads contend with each other, since these implementations force MVCC readers to start out looking at the latest version of a key first. Coordination may also be needed to access neighboring records. Accessing key-ranges in B+Trees or similar data structures that may be changing needs cross-transaction coordination.</p><p>Readers coordinate with writers. Writers coordinate with readers. Readers coordinate with other readers!</p><p>Having a home for a record also makes online repartitioning/sharding (which is required for scalability) very difficult. Moving record keys from one partition to another is complex and impacts application availability.</p><p>To address these challenges, Pat proposes a prototype design. The database is structured so that there is no pre-assigned home for a record per key. Unlike partitioned DBs, this allows the database to seamlessly adapt to workload changes.</p><p>I liken this to the <a href="https://en.wikipedia.org/wiki/Everything_Is_Miscellaneous">miscellaneous manifesto</a> or how instead of neatly organizing/allocating everything a place (which inevitably fails, requiring incessant re-orgs), embracing the messiness and using a search engine to get to information quickly.</p><p>The architecture is based on <a href="https://muratbuffalo.blogspot.com/2022/01/decoupled-transactions-low-tail-latency.html">a design Pat explored in a previous work</a>. The work is very technical, and I missed the nuance and contributions of it because I didn&#39;t read through the appendix about details.</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEgBmeGMqsLQWCyBjr7ikas0mIkFdZ-6nVculH07Da5GPn2-TZ7B2XNXNvM_7Ux11JAAAWOR_nbOV98pZr72ILraXKqnhoIGV9n8Kr_rXX8F4Hge4Qj5Z5-GZowCyH6Fg25yoejZlQNgUTfQcDVGL-ELNaeE4eATUrT-3Go1hAPgzlnFsEBXkQE3ZBiRngo"><img alt="" data-original-height="502" data-original-width="832" src="https://blogger.googleusercontent.com/img/a/AVvXsEgBmeGMqsLQWCyBjr7ikas0mIkFdZ-6nVculH07Da5GPn2-TZ7B2XNXNvM_7Ux11JAAAWOR_nbOV98pZr72ILraXKqnhoIGV9n8Kr_rXX8F4Hge4Qj5Z5-GZowCyH6Fg25yoejZlQNgUTfQcDVGL-ELNaeE4eATUrT-3Go1hAPgzlnFsEBXkQE3ZBiRngo=s16000"/></a></p><p><b>Owner servers</b> verify that concurrent transactions have not created any conflicting updates for each key row-locked or updated by the TX that optimistically hopes to commit. Owner servers partitioned by both key-range and time-range. Repartitioning happens dynamically to accommodate scale. </p><p><b>Worker servers</b> are also horizontally scalable, and each have their own transaction log. As TX load increases, workers are added. Each TX happens at a single worker server. The worker servers accept connections from app servers, perform transactions &amp; their queries, commit transactions to their per-worker log, and periodically flush committed new record-versions to the <a href="https://en.wikipedia.org/wiki/Log-structured_merge-tree">LSM (log structured merge tree)</a>.</p><p><b>LSM servers</b> accept flushes from workers and incorporate them into the orderly past stored in the LSM. Record-versions are organized first by time, second by key. Each LSM layer contains record-versions for a band of time. With an LSM, the past scales without coordinating across disjoint transactions reading and updating!</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEhyhoxQSRPZsc1QWeYCex7Gm209SlW0kQMH53tGgHyt5YwnwXwOdDyPbOgYCk8dtiVP-pSxdK_xCI1tS4JCdDukqtg_-TvgwRCHoRFIcHWBs7mITLcdpkmqsx5fwMuOAU9xdweo7NHRS6V11VNBnLVlU_fH6iD1DBLVCSr6bk6oI9E23xoqwLe_FtNzAos"><img alt="" data-original-height="354" data-original-width="832" height="170" src="https://blogger.googleusercontent.com/img/a/AVvXsEhyhoxQSRPZsc1QWeYCex7Gm209SlW0kQMH53tGgHyt5YwnwXwOdDyPbOgYCk8dtiVP-pSxdK_xCI1tS4JCdDukqtg_-TvgwRCHoRFIcHWBs7mITLcdpkmqsx5fwMuOAU9xdweo7NHRS6V11VNBnLVlU_fH6iD1DBLVCSr6bk6oI9E23xoqwLe_FtNzAos=w400-h170" width="400"/></a></p><p>We now deep dive to workers and owners as they are the most significant components in this OLTP architecture. The owners do the concurrency control (the adjudication of transactions with respect to other concurrent transactions), but the workers do the actual work of the transaction. The transaction is centralized in the worker&#39;s log. The workers&#39; logs are ingested by LSM servers for later consumption and durability.</p><p>The worker will accept incoming connections from application servers, and plan/execute SQL statements: Reading with snapshots by key or key-range, acquiring row-locks using their unique record key, and updating records by their unique key. The worker will guess a future time to hopefully commit after being verified that updates and row-locks don’t conflict with concurrent TXs. The worker will then log the transaction’s updates &amp; commit record in its local transaction log, which will then be fed into LSM servers.</p><p>As a commit-time for a transaction is guessed by the worker, every update and row-lock must be verified for no conflicting updates from snapshot to commit by the owner servers. As incoming proposed-updates and verify-locks arrive, they include a proposed-commit-time. Owner-servers align commit-time for records &amp; workers. An incoming request from a worker hopefully arrives at the owner-server before its local clock has reached the proposed-commit-time. If it arrives after commit-time, owner-server returns an error and the TX aborts. If it arrives before commit-time, the owner-server waits until its local clock reaches commit-time.</p><p>What are row-locks you ask?</p><p>Row-locks allow the application to ask the database for help with concurrency across transactions. Traffic cops provide pessimistic concurrency control. They will stall later transactions if they acquire a row-lock held by an earlier transaction. This pessimistic ordering of transactions may be violated when failures happen. Competing transactions usually wait to allow the lock holder to go first but that may be flawed. So correctness will be enforced by OCC prior to commit. Of course, row-locks are moot when scalable apps avoid concurrent updates to the same records. But if the app experience concurrent updates to the same records, row-locks can help with the liveness of transactions when the DB uses them to function as a traffic cop.</p><p>Ok, let&#39;s wrap up the transaction execution discussion by talking about how owners can be horizontally scaled. Owners can close for new business and direct new proposed-updates elsewhere. An owner closed for new business only accept worker requests for snapshot reads in their rectangle of key &amp; time ranges, proposed updates, and notifications of transaction outcome. In contrast an owner open for new business also allow new proposed-updates, and new verify-locks.</p><p>As we have seen, the DB leverages time to provide snapshots, commits, and external consistency. External Consistency ensures new incoming requests see all previously exposed data, even by other database connections. That means, snapshot reads from new incoming work must be after all committed work previously visible outside the database.</p><p>By using current time, T-now, as the snapshot time, this is easy. But this would get trickier and more complex as the geographic scope of a DB grows past a single datacenter.</p><p>Overall, this prototype database architecture is a big vindication for using time in systems. (Some of these ideas have been explored in Pat&#39;s earlier paper, under seniority and retirement.) Everything in the database is versioned by the record-version commit time. The database organizes data by its creation time to achieve scaling. Reads are old record-versions as of a past snapshot. Row-locks ensure locked records unchanged until commit time. And updates materialize as new record-versions for later snapshots.</p>
</div></div>
  </body>
</html>
