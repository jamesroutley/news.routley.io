<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://sakana.ai/transformer-squared/">Original</a>
    <h1>Transformer^2: Self-Adaptive LLMs</h1>
    
    <div id="readability-page-1" class="page"><article>
  <header>
  <time datetime="2025-01-15T00:00:00+09:00">January 15, 2025</time>
</header>

  <center>
<img src="https://sakana.ai/assets/transformer-squared/octopus.jpeg" width="100%"/><br/>
<!--<img class="b-lazy" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-src="/assets/tranformer-squared/octopus.jpeg" style="width: 100%;"/>-->
</center>


<!--more-->



<h2 id="summary">Summary</h2>

<p>Adaptation is one of the most remarkable phenomena in nature. From the way an <a href="https://x.com/Planetary_Sec/status/1858531731613118913">octopus can change their skin color</a> to blend into its surroundings, to how the <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC5649456/">human brain rewires itself after an injury</a>, allowing individuals to recover lost functions and adapt to new ways of thinking or moving. Living organisms exhibit adaptability that allows life to flourish in diverse and ever-changing environments.</p>

<p>In the field of AI, the concept of adaptation holds a similar allure. Imagine a machine learning system that could adjust its own weights dynamically to thrive in unfamiliar settings, essentially illustrating a system that evolves as it learns. Self-adaptiveness in AI promises greater efficiency and the potential for lifelong models ever aligned with the dynamic nature of the real world.</p>

<p>This vision of self-adaptive AI is at the heart of our latest <strong><a href="https://arxiv.org/abs/2501.06252">research paper</a></strong>, <strong>Transformer²</strong> <em>(‘Transformer-squared’)</em>, where we propose a machine learning system that dynamically adjusts its weights for various tasks. The name <em>Transformer²</em> reflects its two-step process: first, the model analyzes the incoming task to understand its requirements, and then it applies task-specific adaptations to generate optimal results. By selectively adjusting critical components of the model weights, our framework allows LLMs to dynamically adapt to new tasks in real time. Transformer² demonstrates significant advancements across various tasks (e.g., math, coding, reasoning, and visual understanding), outperforming traditional, static approaches like LoRA in efficiency and task-specific performance while requiring far fewer parameters.</p>

<p>Our research offers a glimpse into a future where AI models are no longer static. These systems will scale their compute dynamically at test-time to adapt to the complexity of tasks they encounter, embodying <em>living intelligence</em> capable of continuous change and lifelong learning. We believe self-adaptivity will not only transform AI research but also redefine how we interact with intelligent systems, creating a world where adaptability and intelligence go hand in hand.</p>

<center>
<a href="https://arxiv.org/abs/2501.06252"><video autoplay="" muted="" playsinline="" loop=""><source src="/assets/transformer-squared/cover_v3_1.cropped.mp4" type="video/mp4"/></video></a>
</center>
<p><i><small>
<a href="https://arxiv.org/abs/2501.06252">Transformer²</a> is a machine learning system that dynamically adjusts its weights for various tasks. Adaptation is a remarkable natural phenomenon, like how the octopus can blend its color in with its environment, or how the brain rewires itself after injury. We believe our new system paves the way for a new generation of adaptive AI models, modifying their own weights and architecture to adapt to the nature of the tasks they encounter, embodying living intelligence capable of continuous change and lifelong learning.
</small></i></p>


<h2 id="dissecting-the-brain-of-llms">Dissecting the Brain of LLMs</h2>

<p>Just as the human brain stores knowledge and processes information through interconnected neural pathways, LLMs store knowledge within their weight matrices. These matrices are the “brain” of an LLM, holding the essence of what it has learned from its training data.</p>

<p>Understanding this “brain” and ensuring that it can adapt effectively to new tasks requires a closer look at its inner structure. This is where Singular Value Decomposition (<a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">SVD</a>) provides invaluable insights. Think of SVD as a surgeon performing a detailed operation on the brain of an LLM. This surgeon breaks down the vast, complex knowledge stored in the LLM into smaller, meaningful, and independent pieces (e.g., the different pathways or components for math, language understanding, etc).</p>

<p>SVD achieves this purpose by identifying the principal components of the LLM’s weight matrices. In our research, we found that enhancing the signal from a subset of these components while suppressing the others could improve an LLM’s performance on downstream tasks. By building on this foundation, Transformer² takes the next step toward dynamic, task-specific adaptation, enabling LLMs to excel in diverse and complex scenarios.</p>



<h2 id="introducing-transformer">Introducing Transformer²</h2>

<p>Transformer² is a novel approach pioneering the concept of self-adaptive LLMs with a two-step process that redefines how these powerful models tackle diverse tasks. At its core is the ability to dynamically adjust critical components of its weight matrices. At training time, we introduce Singular Value Finetuning (SVF), a method that employs reinforcement learning (RL) to enhance/suppress the signals from different “brain” components for various types of downstream tasks. At inference time, we employ three distinct strategies to detect the identity of the task and adapt the model’s weights accordingly. The figure below gives an overview of our method.</p>


<p><img src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-src="/assets/transformer-squared/introducing.png"/><small>
<i><b>Illustration of our method.</b>
</i></small>
</p>



<h2 id="training-with-svf-and-rl">Training with SVF and RL</h2>

<p>At training time, SVF learns a set of <em>z-vectors</em>, one for each downstream task. Each z-vector, which can be regarded as an expert on a task, is a compact representation that specifies the desired strength of each component in the weight matrix, acting as a set of “amplifiers” or “dampeners” to modulate the influence of different components on the model’s behavior.</p>

<p>For example, suppose SVD decomposes a weight matrix into five components [A, B, C, D, E]. For a math task, the learned z-vector might be [1, 0.8, 0, 0.3, 0.5], meaning that component A is critical for math while component C hardly affects its performance. For a language understanding task, the z-vector could be [0.1, 0.3, 1, 0.7, 0.5], highlighting that component C is essential for this task despite being less useful for math.</p>

<p>SVF employs RL to learn these z-vectors on a pre-defined set of downstream tasks. The learned z-vectors enable Transformer² to adapt to various new downstream tasks while introducing only a minimal number of additional parameters (i.e., the z-vectors).</p>



<h2 id="self-adaptation">Self-Adaptation</h2>

<p>At inference time, we devise a two-pass adaptation strategy for our framework that effectively combines the set of task-specific z-vectors. In the first inference pass, given a task or an individual input prompt, Transformer² analyzes its test-time conditions using one of the three adaptation methods below. In the second pass, Transformer² then modulates the weights accordingly by combining the z-vectors, producing a final response most relevant for its new settings.</p>

<p>We summarize the three methods for task detection/adaptation in the following:</p>

<ol>
  <li>
    <p><strong>Prompt-based adaptation.</strong> A specifically designed adaptation prompt classifies the task (e.g., math, coding) and selects a pre-trained z-vector.</p>
  </li>
  <li>
    <p><strong>Classifier-based adaptation.</strong> A task classifier trained with SVF identifies the task during inference and selects the appropriate z-vector.</p>
  </li>
  <li>
    <p><strong>Few-shot adaptation.</strong> Combines multiple pre-trained z-vectors through weighted interpolation. A simple optimization algorithm tunes these weights based on performance on a few-shot evaluation set.</p>
  </li>
</ol>

<p>These three methods collectively ensure that Transformer² achieves robust and efficient task adaptation, paving the way for remarkable performance across diverse scenarios. Please refer to our <a href="https://arxiv.org/abs/2501.06252">paper</a> for details.</p>



<h2 id="main-results">Main Results</h2>

<p>We apply our methods to both the Llama and Mistral LLMs across a broad range of tasks, including math (GSM8K, MATH), code (MBPP-Pro, HumanEval), reasoning (ARC-Easy, ARC-Challenge), and visual question answering (TextVQA, OKVQA).</p>

<p>We first set out to obtain the z-vectors by SVF on these tasks, and compare it with LoRA. Our results in the table below show that SVF outperforms LoRA on text-based tasks, with particularly strong gains on GSM8K. This can be attributed to our RL training objective, which does not require “perfect solutions” for each question, unlike LoRA’s fine-tuning approach. The histogram on the right also illustrates SVF’s amazing capacity in the vision domain.</p>

<p><small>
<i><b>Evaluation of SVF on broad tasks.</b></i></small>
<img src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-src="/assets/transformer-squared/result1.png"/></p>

<p>We then evaluate our adaptation framework against LoRA on unseen tasks, specifically MATH, HumanEval, and ARC-Challenge. The left table below demonstrates that our strategies achieve increasing performance gains as method complexity increases across all the tasks.</p>

<p>A particularly intriguing finding comes from analyzing how few-shot learning combines different z-vectors to tackle tasks, as shown in the right figure. When solving MATH problems, contrary to expectations, the model does not rely exclusively on its GSM8K (math) specialized z-vectors. This suggests that complex mathematical reasoning benefits from combining mathematical, programmatic, and logical reasoning capabilities. We observe similar unexpected combinations across other tasks and models, highlighting the framework’s ability to synthesize diverse types of expertise for optimal performance.</p>

<p><small>
<i><b>Evaluation of Transformer².</b></i></small>
<img src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-src="/assets/transformer-squared/result2.png"/></p>

<p>Finally, we explored an intriguing question that challenges conventional wisdom in AI development: Can we transfer the knowledge from one model to another? To our excitement, when taking the learned z-vectors from Llama to Mistral, we observe positive effects with the latter showing improved performance on most tasks. See table below for detailed results.</p>

<p>While these findings are promising, we should note that both models share similar architectures, which might explain their compatibility. Whether this knowledge-sharing works between more diverse AI models remains an open question. Still, these results suggest exciting possibilities for opening the doors to disentangling and recycling task-specific skills for newer/larger models.</p>

<p><small>
<i><b>Cross-model z-vector transfer.</b></i></small>
<img src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-src="/assets/transformer-squared/result3.png"/></p>



<h2 id="the-future-from-static-models-to-living-intelligence">The Future: From Static Models to Living Intelligence</h2>

<p>Transformer² represents a significant milestone in the evolution of AI systems. Its ability to dynamically adapt to unseen tasks in real-time with enhanced compositionality demonstrates the potential of self-adaptive LLMs to revolutionize AI research and applications alike.</p>

<p>But this is just the beginning. Transformer² offers a glimpse into a future where AI systems are no longer static entities trained for fixed tasks. Instead, they will embody “living intelligence”, models that continually learn, evolve and adapt over time. Imagine an AI capable of seamlessly integrating new knowledge or adapting its behavior in real-world environments without retraining, much like how humans adjust to new challenges.</p>

<p>The path forward lies in building models that dynamically adapt and collaborate with other systems, combining specialized capabilities to solve complex, multi-domain problems. Self-adaptive systems like Transformer² bridge the gap between static AI and living intelligence, paving the way for efficient, personalized, and fully integrated AI tools that drive progress across industries and our daily lives.</p>


<center>
<img src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-src="/assets/transformer-squared/octopus_v2.jpeg"/>
</center>


<h2 id="sakana-ai">Sakana AI</h2>

<p>Interested in joining us? Please see our <a href="https://sakana.ai/careers/">career opportunities</a> for more information.</p>


  
</article></div>
  </body>
</html>
