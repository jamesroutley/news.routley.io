<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.uninformativ.de/blog/postings/2024-08-03/0/POSTING-en.html">Original</a>
    <h1>Revisiting Linux CPU Scheduling</h1>
    
    <div id="readability-page-1" class="page"><div id="blogpostcontent">

<p><a href="https://www.uninformativ.de/blog/">blog</a> 路 <a href="https://www.uninformativ.de/git/">git</a> 路 <a href="https://www.uninformativ.de/desktop/">desktop</a> 路 <a href="https://www.uninformativ.de/pics/stream.html">images</a> 路 <a href="https://www.uninformativ.de/contact.html">contact</a></p>
<hr/>

<p>2024-08-03</p>

<p><em>Disclaimer: This is to the best of my knowledge. It&#39;s a complicated
topic, there are tons of options, and this only covers a tiny fraction
of this topic anyway. If you spot mistakes, please tell me.</em></p>
<h2 id="the-effects-of-autogroup">The effects of <code>autogroup</code></h2>
<p>A surprisingly long time ago, even though it feels like yesterday,
<a href="https://kernelnewbies.org/Linux_2_6_38#Automatic_process_grouping_.28a.k.a._.22the_patch_that_does_wonders.22.29">automatic process grouping was added to Linux 2.6.38</a>.
It was/is basically a hook into <code>setsid()</code>: That function is usually
called in situations like opening a new terminal window. On a call to
<code>setsid()</code>, the <code>autogroup</code> feature puts the calling process into a new
scheduling group.</p>
<p>In other words, each terminal window gets its own scheduling group.</p>
<p>Running 4 processes in one terminal and 1 process in another results in
50% of the CPU time for the 4 guys (i.e., <em>one</em> of those gets <code>1/2 * 1/4
= 1/8</code>, 12.5%) and 50% for the other single process.</p>
<p>When you run this, <code>load4.sh</code>, in one terminal:</p>
<pre><code>#!/bin/bash

children=()

for i in $(seq 4)
do
    taskset -c 0 sh -c &#39;while true; do true; done&#39; &amp;
    children+=($!)
done

if [[ -t 0 ]]
then
    read -p &#39;Press ENTER to quit&#39;
    kill &#34;${children[@]}&#34;
else
    sleep 2m
fi
</code></pre>
<p>(The <code>sleep</code> call will become relevant later. Also, all of these
examples use <code>taskset -c 0</code> to pin everything to one CPU to make it
easier to understand -- we don&#39;t want your 256 core super mega modern CPU
to get in the way.)</p>
<p>And this, <code>load1.sh</code>, in another one:</p>
<pre><code>#!/bin/bash

taskset -c 0 sh -c &#39;while true; do true; done&#39;
</code></pre>
<p>Then both get roughly the same amount of CPU time if <code>autogroup</code> is in
effect. The single loop of <code>load1.sh</code> gets 50% of CPU time and the other
four loops <em>together</em> also get 50%, because the task groups look like
this:</p>
<pre><code>[ load4[0] load4[1] load4[2] load4[3]  |  load1 ]
  \--------------- 50% -------------/     \50%/
</code></pre>
<p>If you want to see the old behaviour, add the kernel parameter
<code>noautogroup</code>.</p>
<p>To reproduce all this, use a Linux distribution without systemd, like
Void Linux. We&#39;ll soon see why.</p>
<h2 id="nice-was-neutralized"><code>nice</code> was &#34;neutralized&#34;</h2>
<p>One effect of this change was that using <code>nice</code> had surprising results.
The manpage <code>sched(7)</code> tells us why:</p>
<pre><code>Under group scheduling, a thread&#39;s nice value has an effect  for
scheduling  decisions only relative to other threads in the same
task group.
</code></pre>
<p>So, in the example above, running <code>nice -n 19 ./load4.sh</code> in one
terminal and just <code>./load1.sh</code> in another had no effect whatsoever. The
<code>nice</code> values were only relevant <em>inside</em> the task group of <code>load4.sh</code>.
So, if <em>one</em> of those four loops in <code>load4.sh</code> had a lower <code>nice</code> value,
it would only win <em>over the other <strong>three</strong> loops in the same group</em>.
But it wouldn&#39;t affect the loop of <code>load1.sh</code>.</p>
<p>The Internet is full of questions why <code>nice</code> &#34;doesn&#39;t work anymore&#34;.
This is usually the reason.</p>
<p>To be honest, I had <code>autogroup</code> disabled for a long time, because it
didn&#39;t really fit my usage pattern very well. Sometimes it&#39;s helpful,
often times it&#39;s just confusing -- because <code>nice</code> didn&#39;t do anymore what
I expected.</p>
<h2 id="cgroups-win-over-autogroup"><code>cgroups</code> win over <code>autogroup</code></h2>
<p>I recently had to revisit this topic again and, even though <code>autogroup</code>
was <em>enabled</em> now, it didn&#39;t have any effect anymore. What&#39;s going on,
did I disable it and forgot about it? It wasn&#39;t the kernel parameter
<code>noautogroup</code> and <code>/proc/sys/kernel/sched_autogroup_enabled</code> did contain
a <code>1</code>. So why wasn&#39;t it working anymore?</p>
<p>Again, <code>sched(7)</code> has the answer:</p>
<pre><code>The  use  of the cgroups(7) CPU controller to place processes in
cgroups other than the root CPU cgroup overrides the  effect  of
autogrouping.
</code></pre>
<p>I don&#39;t remember assigning any cgroups explicitly, but apparently
systemd does that for me now. In every terminal, I get this answer:</p>
<pre><code>$ cat /proc/self/cgroup
0::/user.slice/user-1000.slice/session-2.scope
</code></pre>
<p>A more comprehensive overview can be found in
<code>/sys/kernel/debug/sched/debug</code>: It lists all runnable tasks and their
scheduling group at the end. You might need to mount this first using
<code>mount -t debugfs none /sys/kernel/debug</code>. (This used to be
<code>/proc/sched_debug</code> but has been
<a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=d27e9ae2f244805bbdc730d85fba28685d2471e5">moved in 2021</a>,
which landed in Linux 5.12, I think.)</p>
<p>The answer is different when I log in to another TTY. Arch&#39;s systemd
uses a
<a href="https://gitlab.archlinux.org/archlinux/packaging/packages/systemd/-/blob/262a14b8e53e577936ef1063d7fac20632046059/PKGBUILD#L150">unified cgroup hierarchy</a>
by default these days and each process&#39;s &#34;cgroup&#34; matches the process&#39;s
position in the output of <code>systemd-cgls</code>. I put &#34;cgroup&#34; in quotes,
because there used to be several cgroup hierarchies and the one used for
CPU scheduling didn&#39;t necessarily match the one used in <code>systemd-cgls</code>.
So, <em>if I understand correctly</em>, the system no longer distinguishes
between &#34;category (A) and (B)&#34; as described in
<a href="https://0pointer.net/blog/projects/cgroups-vs-cgroups.html">this old blog post of Lennart Poettering</a>.</p>
<p>This means, when I log in on one TTY and do nothing special, all
processes spawned from here are put into the same cgroup and thus into
the same task group for scheduling. As a result, <code>nice</code> <em>mostly</em> works
as expected again.</p>
<p>(I tried to find out when this change hit Arch Linux, but I couldn&#39;t
find it. I vaguely remember some big discussion about this ... Did I
dream that? Who knows.)</p>
<p>-- edit: A reader suggested these two links regarding the general history
of cgroups v1 vs. v2:</p>
<ul>
<li><a href="https://lwn.net/Articles/555922/">https://lwn.net/Articles/555922/</a></li>
<li><a href="https://lwn.net/Articles/679786/">https://lwn.net/Articles/679786/</a></li>
</ul>
<p>Back to the example above: These days, <code>nice -n 19 ./load4.sh</code> in one
terminal and <code>./load1.sh</code> in another one results in <code>load1.sh</code> winning
again if you use systemd. <code>load4.sh</code> hardly gets any CPU time (it still
gets <em>some</em>).</p>
<h2 id="scheduling-is-hierarchical">Scheduling is hierarchical</h2>
<p>Suppose you run <code>load4.sh</code> as a systemd unit, for example put this in
<code>~/.local/share/systemd/user/load4.service</code>:</p>
<pre><code>[Service]
ExecStart=/bin/sh -c &#39;nice -n 19 /foo/load4.sh&#39;
</code></pre>
<p>And then run:</p>
<pre><code>$ systemctl --user start load4.service
</code></pre>
<p>And then <code>load1.sh</code> in a regular terminal again.</p>
<p>First of all, <code>nice</code> is irrelevant again: These scripts run in different
cgroups.</p>
<p>What do I have to do to make this a &#34;background&#34; unit? I want <code>load4.sh</code>
to run with the lowest scheduling policy, all other processes shall win
over it (at least those in my regular user session).</p>
<p>This doesn&#39;t work:</p>
<pre><code>[Service]
ExecStart=/foo/load4.sh
Nice=19
</code></pre>
<p>Neither does this:</p>
<pre><code>[Service]
ExecStart=/foo/load4.sh
CPUWeight=idle
</code></pre>
<p>In both cases, running <code>load1.sh</code> in a terminal in parallel gets
<code>load1.sh</code> only 50% of CPU time. I want it to get ~100%.</p>
<p>(<code>CPUQuota=1%</code> would work, but that&#39;s an absolute quota: Even when no
other processes are running, <code>load4.sh</code> gets very little CPU time.)</p>
<p>The manpage <code>systemd.resource-control(5)</code> explains why it doesn&#39;t work:</p>
<pre><code>Controllers in the cgroup hierarchy are hierarchical, and resource
control is realized by distributing resource assignments between
siblings in branches of the cgroup hierarchy.
</code></pre>
<p>In my case, the two scripts are not siblings in the hierarchy:</p>
<pre><code>CGroup /:
-.slice
|-user.slice
| `-user-1000.slice
|   |-user@1000.service ...
|   | |-app.slice
|   | | |-load4.service
|   | | | |-92656 /bin/bash /home/void/tmp/2024-08-02/load4.sh
|   | | | |-92658 sh -c while true; do true; done
|   | | | |-92659 sh -c while true; do true; done
|   | | | |-92660 sh -c while true; do true; done
|   | | | |-92661 sh -c while true; do true; done
|   | | | `-92662 sleep 2m
...
|   |-session-2.scope
|   | |-92675 /bin/bash ./load1.sh
|   | |-92676 sh -c while true; do true; done
...
</code></pre>
<p>If I understand this correctly, then the entirety of the
<code>user@1000.service</code> cgroup is scheduled against <code>session-2.scope</code>. The
scheduling decision should go something like this (simplified):</p>
<ul>
<li>Processes <code>load1.sh</code> and <code>load4.sh</code> want CPU time. Compare their
    hierarchies.</li>
<li><code>-.slice</code> vs. <code>-.slice</code>: Identical.<ul>
<li><code>user.slice</code> vs. <code>user.slice</code>: Identical.<ul>
<li><code>user-1000.slice</code> vs. <code>user-1000.slice</code>: Identical.<ul>
<li><code>user@1000.service</code> vs. <code>session-2.scope</code>: This is where
    the trees diverge. Neither has something like
    <code>CPUWeight</code> set, so both get 50%.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Now ... if I read
<a href="https://systemd.io/DESKTOP_ENVIRONMENTS/">this systemd documentation about desktop environments</a>
correctly, then my regular processes should actually all be below
<code>app.slice</code> (probably in one or more services) instead of
<code>session-2.scope</code>. The fact that they&#39;re not is probably caused by my
setup not caring about systemd very much: I log in on an
<a href="https://github.com/util-linux/util-linux/blob/master/term-utils/agetty.c">agetty</a>,
it launches a shell and in that shell I launch a simple
<a href="https://github.com/Earnestly/sx">startx-like launcher for X11</a>. None of
these programs (except for maybe <code>login</code>) do anything systemd-specific
at all.</p>
<p>Anyway, this isn&#39;t about my setup. The point is: The location of your
process/cgroup in the cgroup hierarchy matters. You can&#39;t just put
<code>CPUWeight=idle</code> in some <code>.service</code> file and expect it to work.</p>
<p>For <em>system-wide</em> units, you can relatively easily put them into a group
that is a sibling of <code>user.slice</code>. That way, you can do <code>systemctl start
foo.service</code> and now <code>CPUWeight=idle</code> is in effect as expected. To do
so, first set a property on a new slice:</p>
<pre><code>sudo systemctl set-property background.slice CPUWeight=idle
</code></pre>
<p>This creates a new file below <code>/etc/systemd/system.control</code>, so this is
a persistent setting.</p>
<p>Then you can set the slice on your service, for example
<code>/etc/systemd/system/load4.service</code>:</p>
<pre><code>[Service]
ExecStart=/foo/load4.sh
Slice=background.slice
</code></pre>
<p>And now <code>systemd-cgls</code> looks like this when you run <code>sudo systemctl
start load4</code> and <code>load1.sh</code> in a terminal:</p>
<pre><code>CGroup /:
-.slice
|-background.slice
| `-test.service
|   |-98581 /bin/bash /home/void/tmp/2024-08-02/load4.sh
|   |-98586 sh -c while true; do true; done
|   |-98587 sh -c while true; do true; done
|   |-98588 sh -c while true; do true; done
|   |-98589 sh -c while true; do true; done
|   `-98590 sleep 2m
|-user.slice
| `-user-1000.slice
|   `-session-2.scope
|     |-98599 /bin/bash ./load1.sh
|     |-98600 sh -c while true; do true; done
...
</code></pre>
<p>Decision tree:</p>
<ul>
<li>Processes <code>load1.sh</code> and <code>load4.sh</code> want CPU time. Compare their
    hierarchies.</li>
<li><code>-.slice</code> vs. <code>-.slice</code>: Identical.<ul>
<li><code>background.slice</code> vs. <code>user.slice</code>: <code>background.slice</code> has
    <code>CPUWeight=idle</code>, so <code>user.slice</code> wins.</li>
</ul>
</li>
</ul>
<p>If <em>only</em> <code>load4.sh</code> is running, each loop gets 25% CPU time:</p>
<p><a href="https://www.uninformativ.de/blog/postings/2024-08-03/0/load4-only.png"><img alt="load4-only.png" src="https://www.uninformativ.de/blog/postings/2024-08-03/0/t/load4-only.png.jpg"/></a></p>
<p>If <code>load1.sh</code> is running in parallel, it gets all the time instead:</p>
<p><a href="https://www.uninformativ.de/blog/postings/2024-08-03/0/load4-and-1.png"><img alt="load4-and-1.png" src="https://www.uninformativ.de/blog/postings/2024-08-03/0/t/load4-and-1.png.jpg"/></a></p>
<p><code>background.slice</code> is a sibling of <code>system.slice</code> as well, so it really
should lose against all other processes running on the system.</p>
<p>Without <code>background.slice</code>, you would end up scheduling <code>system.slice</code>
against <code>user.slice</code> and our service would get up to 50% of CPU time in
this scenario, which is not quite what we want.</p>
<h2 id="can-you-get-global-nice-back">Can you get global <code>nice</code> back?</h2>
<p>Can you disable all this cgroup stuff and go back to using global <code>nice</code>
values? I <em>think</em> the answer to this is: Only if you don&#39;t use systemd
and put <code>noautogroup</code> in your kernel parameters. On my Void Linux box,
this results in all tasks showing a <code>/</code> in
<code>/sys/kernel/debug/sched/debug</code>, so we&#39;re back to flat scheduling.</p>
<p>If you&#39;re using systemd, you&#39;re probably out of luck. cgroups are a core
feature of it and now that it uses a unified hierarchy (i.e., no more
&#34;some cgroups just for labels/organization, other cgroups for
resources&#34;), every process always gets put into a cgroup. (Maybe somehow
disabling the CPU cgroup controller could be an option, but it&#39;s highly
doubtful that it would be a stable, supported setup.)</p>
<h2 id="conclusion">Conclusion</h2>
<p>This is powerful, but also complicated. The sibling thing can be an
unexpected obstacle if you just want to say: &#34;That service over there
shall be very low-priority.&#34;</p>
<p>I could probably make more use of all of this, but then again: A simple
traditional <code>nice</code> does the trick for 99% of my use cases and so I&#39;m
glad that <code>nice</code> is (mostly) back to its old behavior on my Arch box.
Most of what I do happens in my interactive session and this is the area
where I can just use <code>nice</code>, so it&#39;s (mostly) just fine.</p>


        </div></div>
  </body>
</html>
