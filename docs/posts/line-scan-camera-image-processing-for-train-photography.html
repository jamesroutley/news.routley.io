<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://daniel.lawrence.lu/blog/y2025m09d21/">Original</a>
    <h1>Line scan camera image processing for train photography</h1>
    
    <div id="readability-page-1" class="page"><div><header></header><p>I use my line scan camera to take cool pictures of trains and other stuff.</p><p>But there’s a lot that goes into properly processing the images.</p><div><figure id="fig2"><a href="https://i.dllu.net/rgb_0_prod_no_denoise_0d9fee240b0c6e5f.jpg"><img src="https://i.dllu.net/rgb_0_prod_no_denoise_5df7185ac60d16b4.jpg" alt=""/></a><figcaption><a href="#fig2">FIGURE 2</a> A cool train, the Renfe AVE Class 102, nicknamed <em>Pato</em> because of its duck bill-like appearance.</figcaption></figure></div><div><figure id="fig4"><a href="https://i.dllu.net/rgb_0_prod_no_denoise_dd93f40ada264e00.jpg"><img src="https://i.dllu.net/rgb_0_prod_no_denoise_7e6bd0d3e527a2d0.jpg" alt=""/></a><figcaption><a href="#fig4">FIGURE 4</a> Nice CRH6A intercity electric multiple unit.</figcaption></figure></div><p>The way it works is that the camera has a single column of pixels (or in this case, two columns), that scans at a super high speed.
The camaera is stationary, but as a train moves past it, it gets scanned.</p><p>This is essentially also how a <a href="https://en.wikipedia.org/wiki/Photo_finish">photo finish camera</a> works.</p><p>Since the background is static, it gets repeated for every column of the image, giving it its distinctive striped look.</p><p>Line scan cameras are very suitable for capturing trains, since I can capture the full length of the train with minimal perspective distortion.
This is super nice for train nerds who want to make models of the trains.
Also, as you keep the camera running, you can get incredibly high resolution photos that span over 100,000 pixels wide.</p><p>By the way, film photo finish cameras and strip cameras behave almost the same as line scan cameras but with one subtle distinction, which is that you have to pull the film across a strip that’s somewhat wider than a single column of pixels.
This is because film is less sensitive than modern digital image sensors.
However, you’ll need to know the approximate speed of the subject and pull the film across at roughly the right speed.</p><p>I’m using an <a href="https://www.alkeria.com/products/necta-series">Alkeria Necta N4K2-7C</a>.
It has a 4096×2 <a href="https://en.wikipedia.org/wiki/Bayer_filter">Bayer array</a> image sensor.
I’m saving its raw data in 16 bit binary arrays.</p><iframe width="320" height="400" src="https://www.youtube.com/embed/r-GHYwkQD1o" title="line scan photography of Shanghai Transrapid" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe><div><figure id="fig9"><a href="https://i.dllu.net/IMG_6801_654e71a21ee44795.jpg"><img src="https://i.dllu.net/IMG_6801_9c68db912482ad71.jpg" alt=""/></a><figcaption><a href="#fig9">FIGURE 9</a> Waiting for a subway train to roll by in Brooklyn, New York.</figcaption></figure></div><p>Sometimes, I keep the line scan camera running for a while, and it generates tons of boring data of the background.
To detect moving things, I compute an “energy function” that’s defined as</p><p>where <img src="https://daniel.lawrence.lu/texcache/4ba7d3cff9565fbc8341f413e7711422cb61cc27i.svg" alt="\text{max}_\mathbf{I}"/> is the maximum pixel value of the image, and the partial derivative are the <a href="https://en.wikipedia.org/wiki/Image_gradient">image gradient</a>.</p><p>This is because, for a static background, it will be full of horizontal stripes.
By weighing the <img src="https://daniel.lawrence.lu/texcache/11f6ad8ec52a2984abaafd7c3b516503785c2072i.svg" alt="x"/>-direction (time direction) gradient against the total gradient norm, we can find areas where it’s a more vertical-ish structure rather than a horizontal structure.
However, doing this by itself risks noisy gradients in empty (but noisy) areas where the gradient direction is completely random.
The maximum pixel value term ensures that whatever gradient we see is salient.</p><p>The image is divided into chunks and the score of a chunk is the 99th percentile energy.</p><p>Finally, chunks containing moving objects are defined to be ones where the score is at least 1.5× that of the minimum score.</p><p>This heuristic took me longer than I would like to admit to figure out.
Previously, I came up with heuristics that worked well on one capture but couldn’t generalize well to other captures.
Sometimes, the background will contain slowly moving foliage waving in the wind, that would screw up other methods of detection.
That resulted in a lot of wasted time because time spent processing empty regions seriously slows down iteration speed when developing the later steps.</p><p>The most common question I get is, how do I estimate the speed of the subject?
If I don’t do it properly, it will appear stretched out, squished, or flipped.</p><p>Typically, I just set the camera to scan as fast as possible while maintaining a decent exposure, so the scan rate is independent of the subject.
Faster subjects will appear squished, and slower subjects will appear stretched out.</p><p>For most of my earlier works, I just eyeballed it. A good rule of thumb is to look for round things such as the wheels and “no smoking” signs.
But now I have a fully automated technique that works fairly robustly.</p><p>The key idea is to exploit the fact that the line scan camera actually has two lines in a <a href="https://en.wikipedia.org/wiki/Bayer_filter">Bayer array</a>, where one line is red, green, red, green, and the second line is green, blue, green blue.
By comparing the two green channels, we are able to figure out how fast stuff is moving.</p><p>The problem is that the data is very noisy, and salient features are sparse.
Here’s the general approach:</p><ul><li>Divide image into chunks.</li><li>Compute the absolute difference between the 2 green channels of each chunk for various small shifts (from -7 to +7). This gives us a cost array for each chunk.</li><li>Perform subpixel peak interpolation in the cost array using an iteratively reweighted Gaussian, <a href="https://en.wikipedia.org/wiki/Mean_shift">mean shift</a> style. This gives us a shift estimate per chunk.</li><li>Fit a robust spline to the shift estimates.</li></ul><div><figure id="fig11"><a href="https://i.dllu.net/mean_shift_0080_e3d219d77b34d22e.png"><img src="https://i.dllu.net/mean_shift_0080_c375801a598f46a4.png" alt=""/></a><figcaption><a href="#fig11">FIGURE 11</a> Interpolating to find the peak using mean shift.</figcaption></figure></div><p>As you can see, the data is noisy, but we have surprisingly decent granularity for this very subpixel case where we were scanning slower than needed so the spacing is like 0.5.</p><p>The value of the spline is actually the <em>sample spacing</em>. It tells us how close together or far apart the sample points in the original time series we should be using.
This leads us to the next section.</p><div><figure id="fig13"><a href="https://i.dllu.net/rgb_0_prod_no_denoise_b83579bd26d10381.jpg"><img src="https://i.dllu.net/rgb_0_prod_no_denoise_2e790d956d6045d7.jpg" alt=""/></a><figcaption><a href="#fig13">FIGURE 13</a> Uncorrected left end. It’s squished!!!</figcaption></figure></div><div><figure id="fig14"><a href="https://i.dllu.net/rgb_2_prod_no_denoise_32cd95888d846ffa.jpg"><img src="https://i.dllu.net/rgb_2_prod_no_denoise_aeefb00dbe18bf6f.jpg" alt=""/></a><figcaption><a href="#fig14">FIGURE 14</a> Uncorrected right end. It’s slightly squished but not nearly as much.</figcaption></figure></div><div><figure id="fig16"><a href="https://i.dllu.net/rgb_3_prod_no_denoise_fa356ca82a22b333.jpg"><img src="https://i.dllu.net/rgb_3_prod_no_denoise_6c643c2fa36cc924.jpg" alt=""/></a><figcaption><a href="#fig16">FIGURE 16</a> Right end of New York subway train.</figcaption></figure></div><p>Hmm, I think my speed estimation still isn’t perfect. It could be off by about 10%.
For future work, I think I might be able to extract features correspondences such as SIFT or LightGlue. Trains are full of repeating elements that are supposed to be evenly spaced. I can detect those, and add a cost function to evenly space them, and optimize.
Another idea is to use a circle Hough transform to find circles.</p><p>From the spline that gives us the sample spacing, we can basically generate the samples as such:</p><div><pre><span></span><span>samples</span> <span>=</span> <span>[]</span>
<span>sample_position</span> <span>=</span> <span>0.0</span>
<span>while</span> <span>sample_position</span> <span>&lt;</span> <span>raw_width</span><span>:</span>
    <span>samples</span><span>.</span><span>append</span><span>(</span><span>sample_position</span><span>)</span>
    <span>sample_position</span> <span>+=</span> <span>spline</span><span>(</span><span>sample_position</span><span>)</span>
</pre></div>
<p>However, there are a few gotchas:</p><ul><li>If the spline is negative-valued, it means the subject is going the other way, i.e. the image is flipped. In this case, I start with <code>sample_position</code> set to <code>raw_width</code> and go from right to left.</li><li>If the spline goes to zero, we are doomed because the while loop will never terminate. I clamp the steps to at least 0.1 and throw an error if the spline has both positive and negative values.</li><li>This is sort of a naive integration compared to the trapezoidal rule or something. However, given that the spline moves very slowly, it is fine.</li></ul><p>Now, for each sample position, we also store the sample width, which is the value of the spline.
If we were to simply extract a single column from the raw data, we would be throwing away a lot of data, and the result wouldn’t be antialiased.
Instead, it is better to pick a window of width proportional to the sample spacing.
I chose a <a href="https://en.wikipedia.org/wiki/Hann_function">Hann window</a>.</p><div><figure id="fig17"><a href="https://i.dllu.net/2025-08-19-12-10-58_b5c58484522b4579.png"><img src="https://i.dllu.net/2025-08-19-12-10-58_26c04e2f3b5d16e1.png" alt=""/></a><figcaption><a href="#fig17">FIGURE 17</a> Naively selecting columns instead of using a windowing function.</figcaption></figure></div><p>Not only is the first image very grainy, but the rapidly blinking LED display showing the characters for 筲箕灣 is completely illegible without proper sampling.</p><div><figure id="fig19"><a href="https://i.dllu.net/2025-08-21-23-01-25_b836f51e7b72da06.png"><img src="https://i.dllu.net/2025-08-21-23-01-25_f6d4fd8579a3ee52.png" alt=""/></a><figcaption><a href="#fig19">FIGURE 19</a> Upsampling using a rectangular window.</figcaption></figure></div><p>As you can see, the rectangular window performs very poorly when upsampling and introduces horrible jagged artifacts. The Hann window does better. Some other windows like the Sinc are even better supposedly.</p><p>Recall that the camera has two lines forming a Bayer array.</p><p>If we simply create an image of half resolution (i.e. 2048 pixels tall instead of 4096), by grouping each RGGB group into one pixel, we would have some nasty fringing problems since the red and blue pixels are offset.</p><p>Instead we should write out the image with careful attention to offsets, interpolating as necessary.
Note that the horizontal offsets must be done <em>after</em> speed estimation, because, before speed estimation, the <img src="https://daniel.lawrence.lu/texcache/11f6ad8ec52a2984abaafd7c3b516503785c2072i.svg" alt="x"/>-axis is time, and after speed estimation, the <img src="https://daniel.lawrence.lu/texcache/11f6ad8ec52a2984abaafd7c3b516503785c2072i.svg" alt="x"/>-axis is space.
But the 2-pixel wide Bayer array is physically a <em>spatial</em> offset.</p><p>I implemented a basic interpolation scheme that uses bilinear interpolation.
This fixes most of the fringing, although we can do even better. That will be left for future work.</p><p>Unlike a traditional Bayer array, here we have the possibility that the green channels cover 100% of the pixels, so we can potentially do better than traditional demosaicing algorithms.
But there’s currently an annoying problem, which is that the two green channels on my line scan camera don’t match.</p><p>Vertical stripes in the image are common and are due to two main reasons:</p><ul><li>Clock jitter. The exposure time of each column may be randomly slightly off for some reason.</li><li>I’ve noticed that when a dark object shows up, like the coupling between train cars, the whole slice of the image there is brighter.</li></ul><p>To fix this, I use linear regression to fit a basic model of the form:</p><p>where <img src="https://daniel.lawrence.lu/texcache/86f7e437faa5a7fce15d1ddcb9eaeaea377667b8i.svg" alt="a"/>, <img src="https://daniel.lawrence.lu/texcache/e9d71f5ee7c92d6dc9e92ffdad17b8bd49418f98i.svg" alt="b"/>, and <img src="https://daniel.lawrence.lu/texcache/84a516841ba77a5b4648de2cd0dfcb30ea46dbb4i.svg" alt="c"/> are scalar parameters of the model, <img src="https://daniel.lawrence.lu/texcache/5fafb73566c2a261cb4f5b35b6642b16d8284d3di.svg" alt="\mathbf{x}"/> is a 2048-element vector containing the luminance value of the column (mean over the 4 channels), and <img src="https://daniel.lawrence.lu/texcache/ba360d8ea75d540ca5e1b1ce009ddbe6cd0691e4i.svg" alt="\mathbf{k}"/> is the row index (aka the 2048-element vector of <img src="https://daniel.lawrence.lu/texcache/6b8d068c8ae69b58d2484e0ad9b0f6576d9bd529i.svg" alt="\begin{bmatrix}0, 1, \cdots, 2047\end{bmatrix}"/>.</p><p>You can compose models as such:</p><p>This gives us a new model <img src="https://daniel.lawrence.lu/texcache/c1086bb06dd2795dc0b7c7de2b953422fa2bc22di.svg" alt="\text{model}_{12}"/> with parameters:</p><p>The associative property of the composition operator is left as an exercise for the reader.</p><p>There is also the identity model, i.e. one that does nothing, which is</p><p>and there’s also the inverse:</p><p>so the set of these models forms a mathematical group.</p><p>I fit a model to each consecutive pair of columns using weighted least squares, where we assign each row element a weight based on a Gaussian.
The weight would be:</p><p>In other words, the residual would be</p><p>After fitting this model, we redo the steps again several times, where the <img src="https://daniel.lawrence.lu/texcache/5d3a9194bf53d4e2741ee6cf67ae186b5d22770ai.svg" alt="\mathbf{w}"/> vector is recalculated each time.
This is known as <em>iteratively-reweighted least squares</em> and is pretty good at rejecting outliers.</p><div><figure id="fig23"><a href="https://i.dllu.net/jitter_debug_082121e08533fc28.png"><img src="https://i.dllu.net/jitter_debug_c28f675b057b422f.png" alt=""/></a><figcaption><a href="#fig23">FIGURE 23</a> The first plot shows the current column’s luminance and previous column’s luminance, as well as the previous column corrected by the model. The second plot shows the weight. The third plot shows the weighted initial and final error.</figcaption></figure></div><p>This all gives us <em>relative</em> models between the previous column and the current column, but we want <em>global</em>models that tell us how to correct each column overall.
We could set the global models by just composing them forever, but they would soon start to drift arbitrarily far away from the identity model.</p><p>You could prevent them from drifting away by solving a band-diagonal linear system where you have residuals of two types:</p><ul><li>prior residual, penalizing the difference between each model from the identity</li><li>relative model residual, penalizing the difference between the delta between adjacent models and the relative model we computed</li></ul><p>This can be solved in <img src="https://daniel.lawrence.lu/texcache/ebc75cd71fe8ecc45d16e8fbe4ca608d05d1efe0i.svg" alt="O(n)"/>. However, it is a bit of work to implement. In practice, you can mitigate most high frequency stripes by just doing <a href="https://en.wikipedia.org/wiki/Exponential_smoothing">exponential smoothing</a>, which basically acts as a high-pass filter</p><p>for some small <img src="https://daniel.lawrence.lu/texcache/b3931f1ce298c536432fd324b3a1ab4337120689i.svg" alt="\lambda"/>, in this case hardcoded to be 0.02.</p><div><figure id="fig24"><a href="https://i.dllu.net/2025-08-21-23-06-05_7f2b75d765c999be.png"><img src="https://i.dllu.net/2025-08-21-23-06-05_c5de9c2fbf369030.png" alt=""/></a><figcaption><a href="#fig24">FIGURE 24</a> Before. You can see rather subtle stripes in the dark area.</figcaption></figure></div><p>Previously, I also had some success by directly fitting the model to line up each column with the first column. However, it doesn’t work for captures where the background isn’t static (e.g. rotating line scan panoramas, and pointing the line scan camera out of a moving train).</p><p>By the way, I should point out that vertical stripes getting rid of should be done <em>before</em> speed estimation, since it happens in the time domain at capture time.
If a train were speeding up, it would appear stretched out at first, and squished at the end, and the striping would affect the end a lot more than the start.</p><p>I implemented a patch-based denoiser, also known as <a href="https://en.wikipedia.org/wiki/Block-matching_and_3D_filtering">block matching</a>.
It works by making the observation that you often have repeated textures in a line scan photo of a train.
Technically, you also have lots of self-similarity in general photos, so patch-based denoising is a common method for denoising in general.
However, one important distinction is that most denoisers only look in a small neighborhood around the current patch, but mine looks along the entire row.</p><p>What I do is, for each row, we process it independently.
From each 3×3 pixel patch, we can construct a <em>feature vector</em> of size 27 (9 times 3 channels, RGB).
Then, we collect all these features and sort them by mean value.
Now, for each position along the row, we search in the window of size 128 in the sorted vector.
The sorted vector will have similar-looking patches nearby, but we further weigh them by Gaussian similarity to the current patch.
Then, we compute the weighted average of the center pixel of each of those patches.</p><p>Another trick is to realize that the noise is Poisson-distributed which has a standard deviation that scales with the square root of the signal.
But if I just square root the input data first, then we just need to compare it to a constant.</p><p>This works decently, but is incredibly slow.
Let me know if you think of any faster ways to do it. A KD tree in feature space would die from the curse of dimensionality. Perhaps a hash table? To keep things lightweight, we can limit the population in each cell.</p><p>The good thing about the patch-based denoiser is that unique features like this passenger remain virtually unchanged.</p><p>Previously, I also tried using a <a href="https://en.wikipedia.org/wiki/Total_variation_denoising">total variation denoiser</a>, processing each row and column independently.
It worked decently but would often destroy fine detail in textures.</p><p>If the camera isn’t perfectly upright, the resulting image may be slightly skewed.
I’m planning on implementing automatic skew correction.
But here are two caveats:</p><ul><li>skew detection must be done after speed estimation</li><li>proper sampling should happen after skew detection, since the skew transformation introduces generation loss and we can sample directly from the raw data instead.</li></ul><p>So basically we’d need to generate a quick, poorly sampled version, run skew detection on it, and then sample it properly afterwards.
We can implement skew detection using a <a href="https://daniel.lawrence.lu/blog/y2025m09d21/Hough%20transform">Hough transform</a>.
Generally, I do a decent job of keeping the camera upright, so we just need to correct for very small skews, so a Hough transform is suitable (since the complexity scales with the number of bins of the histogram).
We can also use the energy function from the region of interest detector to primarily care about vertical structures.</p><p>I kinda just eyeballed this color calibration matrix.</p><p>But to be honest it looks fairly decent.</p><div><figure id="fig30"><a href="https://i.dllu.net/rgb_7_prod_no_denoise_2474ac2d89335c15.jpg"><img src="https://i.dllu.net/rgb_7_prod_no_denoise_e7ce15b1872fe528.jpg" alt=""/></a><figcaption><a href="#fig30">FIGURE 30</a> People’s skin tones look fine to me.</figcaption></figure></div><p>The code is implemented in Python using numpy.</p><p><a href="https://github.com/dllu/nectar/blob/master/python/preview.py">the code</a></p><p>Due to the large size of the data (4096 rows and hundreds of thousands of columns), it is sometimes impossible to fit all of it in memory, so the code takes several passes and outputs in chunks.
Actually, it is probably okay to fit it in a few gigabytes of RAM, but you’d have to chunk up the storage (there’s no way a contiguous numpy array of 4096 by 100,000 could be allocated).</p><h2 id="s11.1"><a href="#s11.1">11.1</a> <span>Vibe coding experience</span></h2><p>I tried using AI to help with a lot of the implementation. However, the results were mixed.</p><p>AI would often accidentally make things quadratic for no reason when a linear time algorithm would suffice. For example, when trying to implement spline-based resampling, ChatGPT 5 came up with horribly slow (but vectorized) code that constructed a giant tensor with a mask across the entire width of the image for <em>every single sample</em>. Since there are 100,000 samples, and each mask was 100,000 columns wide, you can imagine it would take millennia to run. I ended up reimplementing it from scratch by hand. Then Grok 4 implemented weighted least squares regression by materializing the entire weight vector with <code>np.diag</code> instead of simply pre-multiplying each row of <code>A</code> and <code>x</code> with the square root of the weight before doing <code>np.linalg.solve(A, x)</code>. Again, with 100,000 elements, making the square matrix with <code>np.diag</code> would have instantly run out of memory.</p><p>Both Grok 4 Expert and ChatGPT 5 Thinking also completely failed to implement the band-diagonal least squares to my vertical stripes strategy, but as mentioned, the exponential smoothing trick works okay for now.</p><p>However, for some other stuff, AI was quite helpful.
It created a class that dynamically loads chunks from disk but provides the API to index and slice it.
That was neat.
AI was also incredibly good at helping with Matplotlib’s arcane syntax.</p><h2 id="s12.1"><a href="#s12.1">12.1</a> <span>Adam Magyar</span></h2><p><a href="https://www.magyaradam.com/wp/">Adam Magyar</a> uses a black and white digital line scan camera for his “Stainless” project, and another derived from a scanner for his “Urban Flow” project.</p><p>His camera must have much better sensitivity than mine since he managed to capture fairly clean images even for underground trains (whereas I generally require sunlight for mine).
Apparently, he had to scout out many subway stations to find ones where the lights don’t flicker at 60 Hz.</p><h2 id="s12.2"><a href="#s12.2">12.2</a> <span>KR64’s blog</span></h2><p>At <a href="https://web.archive.org/web/20250715102540/https://kr64.seesaa.net/">kr64.seesaa.net</a> you can find a mind boggling collection of high quality line scan photos of trains from all across Japan.</p><p>They probably do this full time as the variety of trains is far greater than I can ever hope to achieve.
I believe they use a film slit scan camera.</p><p>Unfortunately, their website has a bunch of technical issues and often goes down. I would be happy to help them out but my Japanese is very poor and I don’t see any way to contact them.</p></div></div>
  </body>
</html>
