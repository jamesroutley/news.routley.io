<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://fly.io/phoenix-files/recognize-digits-using-ml-in-elixir/">Original</a>
    <h1>Recognize Digits Using ML in Elixir</h1>
    
    <div id="readability-page-1" class="page"><article> <dl> <dt>Author</dt> <dd> <img src="https://avatars.githubusercontent.com/u/1579059" alt="Philip Brown" srcset=""/> <dl> <dt>Name</dt> <dd> Philip Brown </dd> <dt>Twitter</dt> <dd> <a href="https://twitter.com/philipbrown" target="_blank"> @philipbrown </a> </dd> </dl> </dd> </dl> <section> <img src="https://fly.io/phoenix-files/2022-05-05/training.jpg" alt=""/> <h2 id="introduction"><a href="#introduction" aria-label="Anchor"></a>Introduction</h2><p>Machine learning allows you to solve problems that were once totally unimaginable. The ability for a computer to take an image and tell you what it sees was once only possible in science fiction.</p> <p>Now, it&#39;s possible to build machine learning models that can do amazing things. However, part of the challenge of machine learning is that there are a lot of moving parts to learn. This means that solving a problem with machine learning can be a difficult task for an individual engineer.</p> <p>One of the big advantages Elixir has over similar programming languages is the integrated nature of what you have available to you. You can do a lot in Elixir without ever leaving the comfort of the language you love.</p> <h2 id="what-are-we-going-to-build"><a href="#what-are-we-going-to-build" aria-label="Anchor"></a>What Are We Going to Build?</h2><p>In this tutorial we&#39;re going to look at building out an end-to-end machine learning project using only Elixir. Boom! As if that wasn&#39;t enough, we&#39;re going to build a machine learning model that can recognize a handwritten digit. We&#39;ll train the model so that it will predict the digit from an image. We&#39;ll also build an application that can accept new handwritten digits from the user, and then display the prediction.</p> <p>Here&#39;s a preview of what it looks like:</p>  <p>Let&#39;s get started!</p> <h2 id="setting-up-the-project"><a href="#setting-up-the-project" aria-label="Anchor"></a>Setting Up the Project</h2><p>We&#39;re going to build this project using <a href="https://www.phoenixframework.org">Phoenix</a>, so the first thing we need is to create a new Phoenix project.</p> <p>If you don&#39;t already have Elixir installed on your computer, you can find instructions for your operating system on the <a href="https://elixir-lang.org/install.html">Elixir Website</a>.</p> <p>Once you have Phoenix installed, you can run the following command in terminal:</p> <div><pre><code><span>$ </span>mix archive.install hex phx_new
</code></pre></div><p>With Elixir and Phoenix installed, we can create a new Phoenix project:</p> <div><pre><code>$ mix phx.new digits --no-ecto
</code></pre></div><p>I&#39;m including the <code>--no-ecto</code> flag because we don&#39;t need a database for this project. This command should prompt you to install the project&#39;s dependencies. Hit <code>Y</code> on that prompt and wait for the dependencies to be installed.</p> <p>Once the dependencies are installed, follow the onscreen instructions to run your new Phoenix application and verify that everything was set up correctly.</p> <p>I&#39;m also going to add the <a href="https://github.com/phoenixframework/tailwind">Tailwind</a> package for styling the application. If you want to add Tailwind to your project add the following to the list of dependencies in your <code>mix.exs</code> file:</p> <div><pre><code><span>{</span><span>:tailwind</span><span>,</span> <span>&#34;~&gt; 0.1&#34;</span><span>,</span> <span>runtime:</span> <span>Mix</span><span>.</span><span>env</span><span>()</span> <span>==</span> <span>:dev</span><span>}</span>
</code></pre></div><p>Then follow the configuration instructions listed <a href="https://github.com/phoenixframework/tailwind#installation">here</a>.</p> <h2 id="where-will-get-our-training-data"><a href="#where-will-get-our-training-data" aria-label="Anchor"></a>Where Will Get Our Training Data?</h2><p>One of the most important aspects of machine learning is having good, quality data to train on. When working on real life machine learning projects, expect to spend the majority of your time on the data.</p> <p>Fortunately for us, there is already a ready made dataset we can use. The <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST Database</a> is a large dataset of handwritten digits that have already been prepared and labeled. This dataset is commonly used for training image recognition machine learning models. The dataset consists of images of handwritten digits from 0 - 9 that are already labeled.</p> <h2 id="prepare-the-project-for-machine-learning"><a href="#prepare-the-project-for-machine-learning" aria-label="Anchor"></a>Prepare the Project for Machine Learning</h2><p>Next, we need to set up the machine learning model. The Elixir ecosystem has a number of exciting packages that can be used for training machine learning models.</p> <p>The <a href="https://github.com/elixir-nx/nx/tree/main/nx#readme">Nx</a> package is the foundation of machine learning in Elixir. Nx allows us to manipulate our data using tensors, which are essentially efficient multi-dimensional arrays. When we say &#34;tensor&#34; below, just think &#34;multi-dimensional array&#34;.</p> <p>Next, we have <a href="https://github.com/elixir-nx/nx/tree/main/exla#readme">EXLA</a>, which provides hardware acceleration for training our models. Crunching the numbers of machine learning is a very intensive process, but EXLA makes that much faster.</p> <p><a href="https://github.com/elixir-nx/axon">Axon</a> builds on top of Nx and makes it possible for us to create neural networks in Elixir.</p> <p>Finally we have the <a href="https://github.com/elixir-nx/scidata">Scidata</a> package, which provides conveniences for working with machine learning datasets, including MNIST.</p> <p>So, the first thing we need to do is to add those dependencies to our <code>mix.exs</code> file:</p> <div><pre><code><span>{</span><span>:axon</span><span>,</span> <span>&#34;~&gt; 0.1.0-dev&#34;</span><span>,</span> <span>github:</span> <span>&#34;elixir-nx/axon&#34;</span><span>},</span>
<span>{</span><span>:exla</span><span>,</span> <span>&#34;~&gt; 0.1.0-dev&#34;</span><span>,</span> <span>github:</span> <span>&#34;elixir-nx/nx&#34;</span><span>,</span> <span>sparse:</span> <span>&#34;exla&#34;</span><span>},</span>
<span>{</span><span>:nx</span><span>,</span> <span>&#34;~&gt; 0.1.0-dev&#34;</span><span>,</span> <span>github:</span> <span>&#34;elixir-nx/nx&#34;</span><span>,</span> <span>sparse:</span> <span>&#34;nx&#34;</span><span>,</span> <span>override:</span> <span>true</span><span>},</span>
<span>{</span><span>:scidata</span><span>,</span> <span>&#34;~&gt; 0.1.5&#34;</span><span>}</span>
</code></pre></div><p>Then we can install our new dependencies from a terminal:</p> <h2 id="working-with-our-training-data"><a href="#working-with-our-training-data" aria-label="Anchor"></a>Working With Our Training Data</h2><p>There&#39;s a couple of steps required for getting and transforming the training data, so we&#39;ll start building out a module that can encapsulate everything that we&#39;re building:</p> <div><pre><code><span>defmodule</span> <span>Digits</span><span>.</span><span>Model</span> <span>do</span>
  <span>@moduledoc</span> <span>&#34;&#34;&#34;
  The Digits Machine Learning model
  &#34;&#34;&#34;</span>
<span>end</span>
</code></pre></div><p>First up we&#39;ll add a <code>download/0</code> function that downloads the training data for us. We&#39;re just delegating to the <code>Scidata</code> package for that.</p> <div><pre><code><span>def</span> <span>download</span> <span>do</span>
  <span>Scidata</span><span>.</span><span>MNIST</span><span>.</span><span>download</span><span>()</span>
<span>end</span>
</code></pre></div><p>This function returns a tuple of <code>{images, labels}</code>. However, we want to transform the images and labels so we can use them in our model.</p> <p>First, we&#39;ll use the following function to transform the images:</p> <div><pre><code><span>def</span> <span>transform_images</span><span>({</span><span>binary</span><span>,</span> <span>type</span><span>,</span> <span>shape</span><span>})</span> <span>do</span>
  <span>binary</span>
  <span>|&gt;</span> <span>Nx</span><span>.</span><span>from_binary</span><span>(</span><span>type</span><span>)</span>
  <span>|&gt;</span> <span>Nx</span><span>.</span><span>reshape</span><span>(</span><span>shape</span><span>)</span>
  <span>|&gt;</span> <span>Nx</span><span>.</span><span>divide</span><span>(</span><span>255</span><span>)</span>
<span>end</span>
</code></pre></div><p>The image data from the download includes the following:</p> <ul> <li>Binary data - This is the image data as a binary. </li><li>The type of the data - In this example the type is <code>{:u, 8}</code> unsigned integer. </li><li>The shape of the data - In this example the shape is <code>{60000, 1, 28, 28}</code>. This means there are 60000 images, which all have 1 channel (ie they&#39;re black and white) and have a dimension of 28x28. </li></ul> <p>We can convert the binary into a tensor using Nx.</p> <p>If we open up <code>iex</code> we can visualize the image data. Run the following command in a terminal to open up <code>iex</code> with our project loaded:</p> <p>Next, we run the following code:</p> <div><pre><code><span>{</span><span>images</span><span>,</span> <span>labels</span><span>}</span> <span>=</span> <span>Digits</span><span>.</span><span>Model</span><span>.</span><span>download</span><span>()</span>

<span>images</span>
<span>|&gt;</span> <span>Digits</span><span>.</span><span>Model</span><span>.</span><span>transform_images</span><span>()</span>
<span>|&gt;</span> <span>Nx</span><span>.</span><span>slice_axis</span><span>(</span><span>0</span><span>,</span> <span>1</span><span>,</span> <span>0</span><span>)</span>
<span>|&gt;</span> <span>Nx</span><span>.</span><span>reshape</span><span>({</span><span>1</span><span>,</span> <span>1</span><span>,</span> <span>28</span><span>,</span> <span>28</span><span>})</span>
<span>|&gt;</span> <span>Nx</span><span>.</span><span>to_heatmap</span><span>()</span>
</code></pre></div><p>You should see the first handwritten digit of the dataset. This is what it looks like:</p> <p><img src="https://fly.io/phoenix-files/2022-05-05/example-digit-no-5.png?centered&amp;1/3&amp;card" alt="Sample image of number 5 digit heatmap"/></p> <p>We can see the corresponding label for the image too. Let&#39;s see how to do that.</p> <p>First, we pattern match the binary data and type from the downloaded label data.</p> <div><pre><code><span>{</span><span>binary</span><span>,</span> <span>type</span><span>,</span> <span>_</span><span>}</span> <span>=</span> <span>labels</span>
</code></pre></div><p>Then we convert the binary to a tensor and &#34;slice&#34; off the first item as our example.</p> <div><pre><code><span>binary</span>
<span>|&gt;</span> <span>Nx</span><span>.</span><span>from_binary</span><span>(</span><span>type</span><span>)</span>
<span>|&gt;</span> <span>Nx</span><span>.</span><span>new_axis</span><span>(</span><span>-</span><span>1</span><span>)</span>
<span>|&gt;</span> <span>Nx</span><span>.</span><span>slice_axis</span><span>(</span><span>0</span><span>,</span> <span>1</span><span>,</span> <span>0</span><span>)</span>
</code></pre></div><p>The first label should be a <code>5</code>. We&#39;ll refactor that code in our transform function to get the labels.</p> <div><pre><code><span>def</span> <span>transform_labels</span><span>({</span><span>binary</span><span>,</span> <span>type</span><span>,</span> <span>_</span><span>})</span> <span>do</span>
  <span>binary</span>
  <span>|&gt;</span> <span>Nx</span><span>.</span><span>from_binary</span><span>(</span><span>type</span><span>)</span>
  <span>|&gt;</span> <span>Nx</span><span>.</span><span>new_axis</span><span>(</span><span>-</span><span>1</span><span>)</span>
  <span>|&gt;</span> <span>Nx</span><span>.</span><span>equal</span><span>(</span><span>Nx</span><span>.</span><span>tensor</span><span>(</span><span>Enum</span><span>.</span><span>to_list</span><span>(</span><span>0</span><span>..</span><span>9</span><span>)))</span>
<span>end</span>
</code></pre></div><p>The labels of the training data are used as targets for the model&#39;s predictions. For each image, we know how it was labelled. During training, the model uses the labels to compare it&#39;s predictions with the actual correct result. The guessing is adjusted to give better results in the future.</p> <p>Currently, the labels are integers from 0 - 9. You can think of them as 10 different categories. In our case, the categories are integers, but when training a machine learning model, you might have categories such as colors, sizes, types of animals, etc.</p> <p>So we need to convert our categories into something that the machine learning model can understand. The way we do this is to convert the label into a tensor of size <code>{1, 10}</code>, where <code>10</code> is the number of categories you have.</p> <p>For example:</p> <div><pre><code><span>#Nx.Tensor&lt;</span>
  <span>u8</span><span>[</span><span>1</span><span>][</span><span>10</span><span>]</span>
  <span>[</span>
    <span>[</span><span>1</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>]</span>
  <span>]</span>
<span>&gt;</span>
</code></pre></div><p>In this example, the long list of numbers has a <code>1</code> is in the first position. This represents the first category. In our case, that is the number &#34;0&#34;, but it could also be the color &#34;red&#34;, the size &#34;small&#34;, or the type of animal &#34;dog&#34;.</p> <p>The second category would be:</p> <div><pre><code><span>#Nx.Tensor&lt;</span>
  <span>u8</span><span>[</span><span>1</span><span>][</span><span>10</span><span>]</span>
  <span>[</span>
    <span>[</span><span>0</span><span>,</span> <span>1</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>]</span>
  <span>]</span>
<span>&gt;</span>
</code></pre></div><p>And so on.</p> <p>This process is called one-hot encoding.</p> <p>You can see what the first label of the training data is when it&#39;s been one-hot encoded using the following chunk of code. (Still in <code>iex</code>):</p> <div><pre><code><span>labels</span>
<span>|&gt;</span> <span>Digits</span><span>.</span><span>Model</span><span>.</span><span>transform_labels</span><span>()</span>
<span>|&gt;</span> <span>Nx</span><span>.</span><span>slice_axis</span><span>(</span><span>0</span><span>,</span> <span>1</span><span>,</span> <span>0</span><span>)</span>
</code></pre></div><p>This should output the following tensor:</p> <div><pre><code><span>#Nx.Tensor&lt;</span>
  <span>u8</span><span>[</span><span>1</span><span>][</span><span>10</span><span>]</span>
  <span>[</span>
    <span>[</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>1</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>]</span>
  <span>]</span>
<span>&gt;</span>
</code></pre></div><p>Remember, we&#39;re working with the number <code>5</code> right now. This tensor is an array of zeros with a <code>1</code> in the index for the 5. Counting from 0, the <code>1</code> is in the 5th spot.</p> <p>Next, we convert the images and labels into batches. During training, we feed the data into the model in batches rather all at once. In this example we&#39;re using a batch size of 32. This means each batch will include 32 examples.</p> <div><pre><code><span>batch_size</span> <span>=</span> <span>32</span>

<span>images</span> <span>=</span>
  <span>images</span>
  <span>|&gt;</span> <span>Digits</span><span>.</span><span>Model</span><span>.</span><span>transform_images</span><span>()</span>
  <span>|&gt;</span> <span>Nx</span><span>.</span><span>to_batched_list</span><span>(</span><span>batch_size</span><span>)</span>

<span>labels</span> <span>=</span>
  <span>labels</span>
  <span>|&gt;</span> <span>Digits</span><span>.</span><span>Model</span><span>.</span><span>transform_labels</span><span>()</span>
  <span>|&gt;</span> <span>Nx</span><span>.</span><span>to_batched_list</span><span>(</span><span>batch_size</span><span>)</span>
</code></pre></div><p>Next, we zip the images and labels together using <code>Enum.zip</code>. Then we split the dataset into training, testing, and validation datasets. We need to use the majority of the data for training, and then a portion of the data to use to test the accuracy of the model. In this example we&#39;re using 80% of the data for training and validation, and the remaining 20% unseen data will be used for testing.</p> <div><pre><code><span>data</span> <span>=</span> <span>Enum</span><span>.</span><span>zip</span><span>(</span><span>images</span><span>,</span> <span>labels</span><span>)</span>

<span>training_count</span> <span>=</span> <span>floor</span><span>(</span><span>0.8</span> <span>*</span> <span>Enum</span><span>.</span><span>count</span><span>(</span><span>data</span><span>))</span>
<span>validation_count</span> <span>=</span> <span>floor</span><span>(</span><span>0.2</span> <span>*</span> <span>training_count</span><span>)</span>

<span>{</span><span>training_data</span><span>,</span> <span>test_data</span><span>}</span> <span>=</span> <span>Enum</span><span>.</span><span>split</span><span>(</span><span>data</span><span>,</span> <span>training_count</span><span>)</span>
<span>{</span><span>validation_data</span><span>,</span> <span>training_data</span><span>}</span> <span>=</span> <span>Enum</span><span>.</span><span>split</span><span>(</span><span>train</span><span>,</span> <span>validation_count</span><span>)</span>
</code></pre></div><p>Phew! That may seem pretty heavy but we&#39;ve already achieved a lot! We&#39;ve downloaded our training data, preprocessed it, and got it ready for building the model. During a real-life machine learning project you will likely spend a lot of time at acquiring, cleaning, and manipulating the data. We&#39;re now in a great position to build and train the model!</p> <h2 id="building-the-model"><a href="#building-the-model" aria-label="Anchor"></a>Building the Model</h2><p>Next up we&#39;ll use Axon to build the machine learning model. Add a new function to the <code>Digits.Model</code> module with the following code:</p> <div><pre><code><span>def</span> <span>new</span><span>({</span><span>channels</span><span>,</span> <span>height</span><span>,</span> <span>width</span><span>})</span> <span>do</span>
  <span>Axon</span><span>.</span><span>input</span><span>({</span><span>nil</span><span>,</span> <span>channels</span><span>,</span> <span>height</span><span>,</span> <span>width</span><span>})</span>
  <span>|&gt;</span> <span>Axon</span><span>.</span><span>flatten</span><span>()</span>
  <span>|&gt;</span> <span>Axon</span><span>.</span><span>dense</span><span>(</span><span>128</span><span>,</span> <span>activation:</span> <span>:relu</span><span>)</span>
  <span>|&gt;</span> <span>Axon</span><span>.</span><span>dense</span><span>(</span><span>10</span><span>,</span> <span>activation:</span> <span>:softmax</span><span>)</span>
<span>end</span>
</code></pre></div><p>First we need to set the input shape of the model to fit our training data. Next we flatten the previous layer and add a dense layer that uses relu as the activation function. Finally the output layer returns one of 10 labels (because our labels are 0 - 9).</p> <p>You can experiment with different model configurations to get different results.</p> <h2 id="training-the-model"><a href="#training-the-model" aria-label="Anchor"></a>Training the Model</h2><p>Now that we have the data and the model, we can start training. Add another function to <code>Digits.Model</code> to train the model:</p> <div><pre><code><span>def</span> <span>train</span><span>(</span><span>model</span><span>,</span> <span>training_data</span><span>,</span> <span>validation_data</span><span>)</span> <span>do</span>
  <span>model</span>
  <span>|&gt;</span> <span>Axon</span><span>.</span><span>Loop</span><span>.</span><span>trainer</span><span>(</span><span>:categorical_cross_entropy</span><span>,</span> <span>Axon</span><span>.</span><span>Optimizers</span><span>.</span><span>adam</span><span>(</span><span>0.01</span><span>))</span>
  <span>|&gt;</span> <span>Axon</span><span>.</span><span>Loop</span><span>.</span><span>metric</span><span>(</span><span>:accuracy</span><span>,</span> <span>&#34;Accuracy&#34;</span><span>)</span>
  <span>|&gt;</span> <span>Axon</span><span>.</span><span>Loop</span><span>.</span><span>validate</span><span>(</span><span>model</span><span>,</span> <span>validation_data</span><span>)</span>
  <span>|&gt;</span> <span>Axon</span><span>.</span><span>Loop</span><span>.</span><span>run</span><span>(</span><span>training_data</span><span>,</span> <span>compiler:</span> <span>EXLA</span><span>,</span> <span>epochs:</span> <span>10</span><span>)</span>
<span>end</span>
</code></pre></div><p>We&#39;re using categorical cross entropy because we&#39;re matching multiple labels and the &#34;adam&#34; optimizer because it gives fairly good results. We&#39;ll track a single accuracy metric, and we&#39;ll also validate the model with our validation data from earlier to ensure the model is not over-fitting on the training data.</p> <p>Finally we&#39;ll use EXLA as the compiler and we&#39;ll train for 10 epochs. An epoch is one cycle through the data, so this means we&#39;ll cycle through the data 10 times during training.</p> <h2 id="testing-our-model"><a href="#testing-our-model" aria-label="Anchor"></a>Testing Our Model</h2><p>We can also test our model after training to get an idea of how well it performs. Add the following function to <code>Digits.Model</code>:</p> <div><pre><code><span>def</span> <span>test</span><span>(</span><span>model</span><span>,</span> <span>state</span><span>,</span> <span>test_data</span><span>)</span> <span>do</span>
  <span>model</span>
  <span>|&gt;</span> <span>Axon</span><span>.</span><span>Loop</span><span>.</span><span>evaluator</span><span>(</span><span>state</span><span>)</span>
  <span>|&gt;</span> <span>Axon</span><span>.</span><span>Loop</span><span>.</span><span>metric</span><span>(</span><span>:accuracy</span><span>,</span> <span>&#34;Accuracy&#34;</span><span>)</span>
  <span>|&gt;</span> <span>Axon</span><span>.</span><span>Loop</span><span>.</span><span>run</span><span>(</span><span>test_data</span><span>)</span>
<span>end</span>
</code></pre></div><p>This tests the model using previously unseen data to check the accuracy of the predictions.</p> <h2 id="saving-and-loading-our-model"><a href="#saving-and-loading-our-model" aria-label="Anchor"></a>Saving and Loading Our Model</h2><p>The final thing to do is to add the ability to save and load the model. Our model is just an Elixir struct, so saving and loading it is simply a case of using Erlang&#39;s <code>binary_to_term/1</code> and <code>term_to_binary/1</code> functions:</p> <div><pre><code><span>def</span> <span>save!</span><span>(</span><span>model</span><span>,</span> <span>state</span><span>)</span> <span>do</span>
  <span>contents</span> <span>=</span> <span>:erlang</span><span>.</span><span>term_to_binary</span><span>({</span><span>model</span><span>,</span> <span>state</span><span>})</span>

  <span>File</span><span>.</span><span>write!</span><span>(</span><span>path</span><span>(),</span> <span>contents</span><span>)</span>
<span>end</span>

<span>def</span> <span>load!</span> <span>do</span>
  <span>path</span><span>()</span>
  <span>|&gt;</span> <span>File</span><span>.</span><span>read!</span><span>()</span>
  <span>|&gt;</span> <span>:erlang</span><span>.</span><span>binary_to_term</span><span>()</span>
<span>end</span>

<span>def</span> <span>path</span> <span>do</span>
  <span>Path</span><span>.</span><span>join</span><span>(</span><span>Application</span><span>.</span><span>app_dir</span><span>(</span><span>:digits</span><span>,</span> <span>&#34;priv&#34;</span><span>),</span> <span>&#34;model.axon&#34;</span><span>)</span>
<span>end</span>
</code></pre></div><h2 id="running-the-model"><a href="#running-the-model" aria-label="Anchor"></a>Running the Model</h2><p>Now that we&#39;ve written all the code to transform the data, train, and test our machine learning model, we&#39;ll write a mix command to put it all together:</p> <div><pre><code><span>defmodule</span> <span>Mix</span><span>.</span><span>Tasks</span><span>.</span><span>Train</span> <span>do</span>
  <span>use</span> <span>Mix</span><span>.</span><span>Task</span>

  <span>@requirements</span> <span>[</span><span>&#34;app.start&#34;</span><span>]</span>

  <span>alias</span> <span>Digits</span>

  <span>def</span> <span>run</span><span>(</span><span>_</span><span>)</span> <span>do</span>
    <span>EXLA</span><span>.</span><span>set_as_nx_default</span><span>([</span><span>:tpu</span><span>,</span> <span>:cuda</span><span>,</span> <span>:rocm</span><span>,</span> <span>:host</span><span>])</span>

    <span>{</span><span>images</span><span>,</span> <span>labels</span><span>}</span> <span>=</span> <span>Digits</span><span>.</span><span>Model</span><span>.</span><span>download</span><span>()</span>

    <span>images</span> <span>=</span>
      <span>images</span>
      <span>|&gt;</span> <span>Digits</span><span>.</span><span>Model</span><span>.</span><span>transform_images</span><span>()</span>
      <span>|&gt;</span> <span>Nx</span><span>.</span><span>to_batched_list</span><span>(</span><span>32</span><span>)</span>

    <span>labels</span> <span>=</span>
      <span>labels</span>
      <span>|&gt;</span> <span>Digits</span><span>.</span><span>Model</span><span>.</span><span>transform_labels</span><span>()</span>
      <span>|&gt;</span> <span>Nx</span><span>.</span><span>to_batched_list</span><span>(</span><span>32</span><span>)</span>

    <span>data</span> <span>=</span> <span>Enum</span><span>.</span><span>zip</span><span>(</span><span>images</span><span>,</span> <span>labels</span><span>)</span>

    <span>training_count</span> <span>=</span> <span>floor</span><span>(</span><span>0.8</span> <span>*</span> <span>Enum</span><span>.</span><span>count</span><span>(</span><span>data</span><span>))</span>
    <span>validation_count</span> <span>=</span> <span>floor</span><span>(</span><span>0.2</span> <span>*</span> <span>training_count</span><span>)</span>

    <span>{</span><span>training_data</span><span>,</span> <span>test_data</span><span>}</span> <span>=</span> <span>Enum</span><span>.</span><span>split</span><span>(</span><span>data</span><span>,</span> <span>training_count</span><span>)</span>
    <span>{</span><span>validation_data</span><span>,</span> <span>training_data</span><span>}</span> <span>=</span> <span>Enum</span><span>.</span><span>split</span><span>(</span><span>training_data</span><span>,</span> <span>validation_count</span><span>)</span>

    <span>model</span> <span>=</span> <span>Digits</span><span>.</span><span>Model</span><span>.</span><span>new</span><span>({</span><span>1</span><span>,</span> <span>28</span><span>,</span> <span>28</span><span>})</span>

    <span>Mix</span><span>.</span><span>Shell</span><span>.</span><span>IO</span><span>.</span><span>info</span><span>(</span><span>&#34;training...&#34;</span><span>)</span>

    <span>state</span> <span>=</span> <span>Digits</span><span>.</span><span>Model</span><span>.</span><span>train</span><span>(</span><span>model</span><span>,</span> <span>training_data</span><span>,</span> <span>validation_data</span><span>)</span>

    <span>Mix</span><span>.</span><span>Shell</span><span>.</span><span>IO</span><span>.</span><span>info</span><span>(</span><span>&#34;testing...&#34;</span><span>)</span>

    <span>Digits</span><span>.</span><span>Model</span><span>.</span><span>test</span><span>(</span><span>model</span><span>,</span> <span>state</span><span>,</span> <span>test_data</span><span>)</span>

    <span>Digits</span><span>.</span><span>Model</span><span>.</span><span>save!</span><span>(</span><span>model</span><span>,</span> <span>state</span><span>)</span>

    <span>:ok</span>
  <span>end</span>
<span>end</span>
</code></pre></div><p>We can run the training with the following command:</p> <h2 id="setting-up-the-liveview"><a href="#setting-up-the-liveview" aria-label="Anchor"></a>Setting Up the LiveView</h2><p>Now that we have a trained machine learning model, we can set up a LiveView to accept new handwritten digits, and then display the predicted results.</p> <p>First, we add a new live route to our router file in <code>lib/digits_web/router.ex</code> :</p> <div><pre><code><span>scope</span> <span>&#34;/&#34;</span><span>,</span> <span>DigitsWeb</span> <span>do</span>
  <span>pipe_through</span> <span>:browser</span>

  <span>live</span> <span>&#34;/&#34;</span><span>,</span> <span>PageLive</span><span>,</span> <span>:index</span>
<span>end</span>
</code></pre></div><p>Next, we create a new file under <code>lib/digits_web/live</code> called <code>page_live.ex</code>. This is our LiveView module where all the interactivity happens:</p> <div><pre><code><span>defmodule</span> <span>DigitsWeb</span><span>.</span><span>PageLive</span> <span>do</span>
  <span>@moduledoc</span> <span>&#34;&#34;&#34;
  PageLive LiveView
  &#34;&#34;&#34;</span>

  <span>use</span> <span>DigitsWeb</span><span>,</span> <span>:live_view</span>
<span>end</span>
</code></pre></div><p>When a user submits a new handwritten digit, the machine learning model makes a prediction on what digit was written and then the LiveView displays the prediction to the user. However, when the LiveView is first loaded, there isn&#39;t a prediction to display. So, first, we need to initiate the <code>prediction</code> assign value as <code>nil</code> inside the <code>mount/3</code> callback:</p> <div><pre><code><span>def</span> <span>mount</span><span>(</span><span>_params</span><span>,</span> <span>_session</span><span>,</span> <span>socket</span><span>)</span> <span>do</span>
  <span>{</span><span>:ok</span><span>,</span> <span>assign</span><span>(</span><span>socket</span><span>,</span> <span>%{</span><span>prediction:</span> <span>nil</span><span>})}</span>
<span>end</span>
</code></pre></div><p>Next, the <code>render/1</code> function is responsible for rendering the LiveView:</p> <div><pre><code><span>def</span> <span>render</span><span>(</span><span>assigns</span><span>)</span> <span>do</span>
  <span>~H&#34;&#34;</span><span>&#34;
  &lt;div id=&#34;</span><span>wrapper</span><span>&#34; phx-update=&#34;</span><span>ignore</span><span>&#34;&gt;
    &lt;div id=&#34;</span><span>canvas</span><span>&#34; phx-hook=&#34;</span><span>Draw</span><span>&#34;&gt;&lt;/div&gt;
  &lt;/div&gt;

  &lt;div&gt;
    &lt;button phx-click=&#34;</span><span>reset</span><span>&#34;&gt;Reset&lt;/button&gt;
    &lt;button phx-click=&#34;</span><span>predict</span><span>&#34;&gt;Predict&lt;/button&gt;
  &lt;/div&gt;

  &lt;%= if @prediction do %&gt;
  &lt;div&gt;
    &lt;div&gt;
      Prediction:
    &lt;/div&gt;
    &lt;div&gt;
      &lt;%= @prediction %&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;% end %&gt;
  &#34;&#34;&#34;</span>
<span>end</span>
</code></pre></div><p>Notice above that we have a <code>div</code> with the <code>id</code> of &#34;canvas&#34;. This will have an HTML canvas attached. The <code>phx-hook</code> uses Javascript to let us interact with the canvas. The canvas <code>div</code> is wrapped in another <code>div</code> with the <code>phx-update=&#34;ignore&#34;</code> because we don&#39;t want Phoenix to update it.</p> <p>Next are two buttons, one to reset the canvas and one to make a prediction from what the user drew. Each of these buttons are wired up to <code>phx-click</code> triggers.</p> <p>Finally, if we have a <code>prediction</code>, it is displayed.</p> <figure> <figcaption> <p> Fly.io is a great way to run your Phoenix LiveView app close to your users. It&#39;s really easy to get started. You can be running in minutes.</p><p><a href="https://fly.io/docs/getting-started/elixir/"> Deploy a Phoenix app today!  <span>→</span> </a></p> </figcaption><p><img src="https://fly.io/public/images/cta-cat.jpg" srcset="/public/images/cta-cat@2x.jpg 2x" alt=""/></p></figure><h2 id="adding-the-canvas"><a href="#adding-the-canvas" aria-label="Anchor"></a>Adding the Canvas</h2><p>Next, we need some input from the user. We could let the user upload images using Phoenix&#39;s LiveView upload functionality, but a better (and way cooler) experience is to let the user draw new examples directly into the LiveView.</p> <p>There&#39;s a handy NPM package called <a href="https://www.npmjs.com/package/draw-on-canvas">draw-on-canvas</a> that make this part easy.</p> <p>To install it, <code>cd</code> into the <code>assets</code> directory and run the following command in a terminal:</p> <p>This installs the <code>draw-on-canvas</code> into the project.</p> <p>Now we connect the <code>draw-on-canvas</code> package to our LiveView via a hook. Open up <code>assets/js/app.js</code> and import the <code>draw-on-canvas</code> package:</p> <div><pre><code><span>import</span> <span>Draw</span> <span>from</span> <span>&#39;</span><span>draw-on-canvas</span><span>&#39;</span>
</code></pre></div><p>Let&#39;s create a new <code>Hooks</code> object:</p> <p>Remember to register the hook object in the <code>LiveSocket</code>:</p> <div><pre><code><span>let</span> <span>liveSocket</span> <span>=</span> <span>new</span> <span>LiveSocket</span><span>(</span><span>&#34;</span><span>/live</span><span>&#34;</span><span>,</span> <span>Socket</span><span>,</span> <span>{</span>
  <span>params</span><span>:</span> <span>{</span><span>_csrf_token</span><span>:</span> <span>csrfToken</span><span>},</span>
  <span>hooks</span><span>:</span> <span>Hooks</span>
<span>})</span>
</code></pre></div><p>Next we add a new <code>Draw</code> hook:</p> <p>We need to implement the <code>mounted</code> function, which is called when the hook is mounted. This is where we set up the canvas:</p> <div><pre><code><span>Hooks</span><span>.</span><span>Draw</span> <span>=</span> <span>{</span>
  <span>mounted</span><span>()</span> <span>{</span>
    <span>this</span><span>.</span><span>draw</span> <span>=</span> <span>new</span> <span>Draw</span><span>(</span><span>this</span><span>.</span><span>el</span><span>,</span> <span>384</span><span>,</span> <span>384</span><span>,</span> <span>{</span>
      <span>backgroundColor</span><span>:</span> <span>&#34;</span><span>black</span><span>&#34;</span><span>,</span>
      <span>strokeColor</span><span>:</span> <span>&#34;</span><span>white</span><span>&#34;</span><span>,</span>
      <span>strokeWeight</span><span>:</span> <span>10</span>
    <span>})</span>
  <span>}</span>
<span>}</span>
</code></pre></div><p>When we open the app in a browser, we should see a black square canvas that we can draw on!</p> <h2 id="interacting-with-the-canvas"><a href="#interacting-with-the-canvas" aria-label="Anchor"></a>Interacting With the Canvas</h2><p>Remember back in our <code>PageLive</code> module, we added two buttons for interacting with the canvas.</p> <p>The first button is used to reset the canvas. When the button is pressed we send a message to the client to reset the canvas. The <code>push_event</code> function makes this easy.</p> <p>Our new &#34;reset&#34; event handler in <code>PageLive</code> looks like this:</p> <div><pre><code><span>def</span> <span>handle_event</span><span>(</span><span>&#34;reset&#34;</span><span>,</span> <span>_params</span><span>,</span> <span>socket</span><span>)</span> <span>do</span>
  <span>{</span><span>:noreply</span><span>,</span>
    <span>socket</span>
    <span>|&gt;</span> <span>assign</span><span>(</span><span>prediction:</span> <span>nil</span><span>)</span>
    <span>|&gt;</span> <span>push_event</span><span>(</span><span>&#34;reset&#34;</span><span>,</span> <span>%{})</span>
  <span>}</span>
<span>end</span>
</code></pre></div><p>When the reset button is clicked, the <code>phx-click</code> trigger sends the <code>reset</code> event to the server. We then push an event called <code>reset</code> to the client. We also set the prediction to <code>nil</code> in the socket assigns.</p> <p>On the Javascript side, we add a <code>handleEvent</code>, that listens for the <code>reset</code> event, and resets the canvas:</p> <div><pre><code><span>this</span><span>.</span><span>handleEvent</span><span>(</span><span>&#34;</span><span>reset</span><span>&#34;</span><span>,</span> <span>()</span> <span>=&gt;</span> <span>{</span>
  <span>this</span><span>.</span><span>draw</span><span>.</span><span>reset</span><span>()</span>
<span>})</span>
</code></pre></div><p>Next, let&#39;s make our &#34;predict&#34; button work. We want to grab the contents of the canvas as an image. Again, we send a message to the client from the <code>PageLive</code> LiveView module:</p> <div><pre><code><span>def</span> <span>handle_event</span><span>(</span><span>&#34;predict&#34;</span><span>,</span> <span>_params</span><span>,</span> <span>socket</span><span>)</span> <span>do</span>
  <span>{</span><span>:noreply</span><span>,</span> <span>push_event</span><span>(</span><span>socket</span><span>,</span> <span>&#34;predict&#34;</span><span>,</span> <span>%{})}</span>
<span>end</span>
</code></pre></div><p>In the <code>mounted</code> callback, we add another <code>handleEvent</code>. This grabs the contents of the canvas as a data URL and sends it to the server using <code>pushEvent</code>:</p> <div><pre><code><span>this</span><span>.</span><span>handleEvent</span><span>(</span><span>&#34;</span><span>predict</span><span>&#34;</span><span>,</span> <span>()</span> <span>=&gt;</span> <span>{</span>
  <span>this</span><span>.</span><span>pushEvent</span><span>(</span><span>&#34;</span><span>image</span><span>&#34;</span><span>,</span> <span>this</span><span>.</span><span>draw</span><span>.</span><span>canvas</span><span>.</span><span>toDataURL</span><span>(</span><span>&#39;</span><span>image/png</span><span>&#39;</span><span>))</span>
<span>})</span>
</code></pre></div><h2 id="making-predictions"><a href="#making-predictions" aria-label="Anchor"></a>Making Predictions</h2><p>Now that we hooked up the buttons to reset the canvas and send up the canvas contents to make a prediction, we will use the image from the canvas as a new input to our machine learning model.</p> <p>We can accept the image data URL from the client using another <code>handle_event/3</code> callback function:</p> <div><pre><code><span>def</span> <span>handle_event</span><span>(</span><span>&#34;image&#34;</span><span>,</span> <span>&#34;data:image/png;base64,&#34;</span> <span>&lt;&gt;</span> <span>raw</span><span>,</span> <span>socket</span><span>)</span> <span>do</span>
  <span>name</span> <span>=</span> <span>Base</span><span>.</span><span>url_encode64</span><span>(</span><span>:crypto</span><span>.</span><span>strong_rand_bytes</span><span>(</span><span>10</span><span>),</span> <span>padding:</span> <span>false</span><span>)</span>
  <span>path</span> <span>=</span> <span>Path</span><span>.</span><span>join</span><span>(</span><span>System</span><span>.</span><span>tmp_dir!</span><span>(),</span> <span>&#34;</span><span>#{</span><span>name</span><span>}</span><span>.png&#34;</span><span>)</span>

  <span>File</span><span>.</span><span>write!</span><span>(</span><span>path</span><span>,</span> <span>Base</span><span>.</span><span>decode64!</span><span>(</span><span>raw</span><span>))</span>

  <span>prediction</span> <span>=</span> <span>Digits</span><span>.</span><span>Model</span><span>.</span><span>predict</span><span>(</span><span>path</span><span>)</span>

  <span>File</span><span>.</span><span>rm!</span><span>(</span><span>path</span><span>)</span>

  <span>{</span><span>:noreply</span><span>,</span> <span>assign</span><span>(</span><span>socket</span><span>,</span> <span>prediction:</span> <span>prediction</span><span>)}</span>
<span>end</span>
</code></pre></div><p>In this function, we use a binary pattern matching on the <code>params</code> to get the image data. Next, we generate a random file name and create a path to a temporary directory for storing the image. Then we decode the image data and write it to the path.</p> <p>Next we pass the path into the <code>Digits.Model.predict/1</code> function and return a prediction. The prediction result is a number between 0 and 9. We&#39;ll write that function next.</p> <p>Finally, we delete the image file and assign the prediction to the socket for display in our LiveView.</p> <p>Before we can use the user&#39;s drawing with our model, we need to prepare the image. We need to:</p> <ul> <li>Convert it to grayscale to reduce the number channels from 3 to 1 </li><li>Resize it to 28 x 28 </li></ul> <p>The <code>Evision</code> library can do these changes for us. Let&#39;s add it as a dependency in our <code>mix.exs</code> file now:</p> <div><pre><code><span>{</span><span>:evision</span><span>,</span> <span>&#34;~&gt; 0.1.0-dev&#34;</span><span>,</span> <span>github:</span> <span>&#34;cocoa-xu/evision&#34;</span><span>,</span> <span>branch:</span> <span>&#34;main&#34;</span><span>}</span>
</code></pre></div><p>Install the dependency using:</p> <p>In the <code>Digits.Model</code> module, let&#39;s add a new function for making a prediction.</p> <div><pre><code><span>def</span> <span>predict</span><span>(</span><span>path</span><span>)</span> <span>do</span>
  <span>{</span><span>:ok</span><span>,</span> <span>mat</span><span>}</span> <span>=</span> <span>Evision</span><span>.</span><span>imread</span><span>(</span><span>path</span><span>,</span> <span>flags:</span> <span>Evision</span><span>.</span><span>cv_IMREAD_GRAYSCALE</span><span>)</span>
  <span>{</span><span>:ok</span><span>,</span> <span>mat</span><span>}</span> <span>=</span> <span>Evision</span><span>.</span><span>resize</span><span>(</span><span>mat</span><span>,</span> <span>[</span><span>28</span><span>,</span> <span>28</span><span>])</span>

  <span>data</span> <span>=</span>
    <span>Evision</span><span>.</span><span>Nx</span><span>.</span><span>to_nx</span><span>(</span><span>mat</span><span>)</span>
    <span>|&gt;</span> <span>Nx</span><span>.</span><span>reshape</span><span>({</span><span>1</span><span>,</span> <span>28</span><span>,</span> <span>28</span><span>})</span>
    <span>|&gt;</span> <span>List</span><span>.</span><span>wrap</span><span>()</span>
    <span>|&gt;</span> <span>Nx</span><span>.</span><span>stack</span><span>()</span>

  <span>{</span><span>model</span><span>,</span> <span>state</span><span>}</span> <span>=</span> <span>load!</span><span>()</span>

  <span>model</span>
  <span>|&gt;</span> <span>Axon</span><span>.</span><span>predict</span><span>(</span><span>state</span><span>,</span> <span>data</span><span>,</span> <span>compiler:</span> <span>EXLA</span><span>)</span>
  <span>|&gt;</span> <span>Nx</span><span>.</span><span>argmax</span><span>()</span>
  <span>|&gt;</span> <span>Nx</span><span>.</span><span>to_number</span><span>()</span>
<span>end</span>
</code></pre></div><p>First, we read the image path and convert it to grayscale. This reduces the number of channels from 3 to 1. Then we resize the image to <code>28</code> x <code>28</code>.</p> <p>We also need to convert the image data to an Nx tensor and reshape it to an expected correct shape. Our machine learning model expects a &#34;batch&#34; of inputs, and so we&#39;ll wrap the tensor using <code>List.wrap/1</code> and then stack it using <code>Nx.stack/1</code>.</p> <p>Next, we load the <code>model</code> and the <code>state</code>, using the <code>load!/0</code> function from earlier. Ideally you wouldn&#39;t be loading the <code>model</code> and <code>state</code> for each prediction, but it&#39;s fine for our basic example.</p> <p>We pass the <code>model</code>, <code>state</code> and <code>data</code> into the <code>Axon.predict/4</code> function. One thing to note is, you will need to add <code>require Axon</code> to the <code>Digits.Model</code> module because <code>Axon.predict/4</code> is actually a macro.</p> <p>The <code>Axon.predict/4</code> function returns a prediction in the form of a one-hot encoded tensor. We use the <code>Nx.argmax/1</code> function to convert it to a tensor that contains a single scalar value between 0 and 9, and then we use <code>Nx.to_number/1</code> to return the value as a number.</p> <p>Our predicted number is set as the <code>prediction</code> is the LiveView assigns, displaying it to the user.</p> <h2 id="we-built-an-end-to-end-machine-learning-application-in-elixir"><a href="#we-built-an-end-to-end-machine-learning-application-in-elixir" aria-label="Anchor"></a>We Built an End-to-end Machine Learning Application in Elixir!</h2><p>Wow! Check out what we just did!</p> <p>We built an end-to-end machine learning application using Elixir! We trained a model from scratch. We used LiveView for interactive, real-time application input from the user. We ran predictions and displayed the results interactively.</p> <p>One of the most amazing things here was that we did it all using Elixir and didn&#39;t need external machine learning tools or languages. Machine learning in Elixir is still maturing, but I hope this inspires you to try something new in your own project.</p> <p>Full code for this tutorial is at <a href="https://github.com/philipbrown/handwritten-digits">philipbrown/handwritten-digits-elixir</a>.</p>  </section> <dl> <dt> Previous post  ↓ </dt> <dd> <a href="https://fly.io/phoenix-files/dates-formatting-with-hooks/"> Formatting the user&#39;s local date-times using Hooks </a> </dd> </dl> </article></div>
  </body>
</html>
