<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.mpi-hd.mpg.de/personalhomes/fwerner/research/2021/09/grpc-for-ipc/">Original</a>
    <h1>Using gRPC for (local) inter-process communication – F. Werner&#39;s Research Page</h1>
    
    <div id="readability-page-1" class="page"><div>
<article>

<div><p>Using a full-featured RPC framework for IPC seems like overkill when the processes run on the same machine. However, if your project anyway exposes RPCs for public APIs or would benefit from a schema-based serialisation layer it makes sense to use only one tool that combines these—also for IPC.</p>
<h2 id="microservices-for-beginners">Microservices for beginners</h2>
<p>For the FlashCam software running on the DAQ servers we converged on the following high-level architecture:</p>
<figure><svg viewBox="0 0 2130 862"><use href="FlashCam-DAQ-Architecture.svg#figure_1"></use></svg></figure>
<p>Each bright box corresponds to a software process running on the DAQ server. The middle row are those processes that directly talk to the camera hardware via Ethernet using their hardware-specific protocols. Each of these control &amp; monitoring processes expose a small set of functions that other processes may call to change or monitor the state. The two processes in the lower row are somewhat special: they do not connect to any hardware, but rather act as independent connectors between subsystems to run control loops. “Thermal” monitors the subsystem temperatures and adjusts cooling and ventilation parameters to stabilise the camera; “PDP-Readout-Connector” monitors pixel health, deactivates the trigger contribution of tripped or broken pixels and reactivates the trigger contribution of recovered pixels.<sidenote><span>This way the corresponding logic resides in a single, well-scoped subsystem and the Readout and PDP subsystems do not need to know anything about each other.</span></sidenote>
</p>
<p>The “State Supervisor” orchestrates these subsystems and exposes an abstraction of the camera functionality via an RPC interface—essentially as a small set of states to hide the details of the hardware subsystems. The State Supervisor acts as the <em>sole entry point</em> for external control &amp; monitoring; only a camera expert may control the subsystems individually in engineering mode.</p>
<p>All processes send their telemetry (log events, metrics, eventually trace events) to one single collector that dumps these to disk in a time-ordered fashion, e.g., one file per night for next-day analysis by camera experts. One aggregate telemetry stream per camera (as opposed to one telemetry stream per subsystem) is easier to manage and follow. The operators may subscribe to a subset of the telemetry, such as warnings and errors. With only a couple of hundred MByte of telemetry per night we do not expect this process to become a bottleneck.</p>
<p>We decided to maximise decoupling by splitting the subsystems into their own processes (as opposed to a monolith) for the following reasons:</p>
<ul>
<li>
<p>subsystems may be developed independently by the corresponding experts</p>
</li>
<li>
<p>subsystems may be deployed individually, e.g., in hardware test setups or during software upgrades</p>
</li>
<li>
<p>subsystems are testable in isolation using exactly the same interface as in operation</p>
</li>
</ul>
<p>Such a modular architecture has potential drawbacks that need to be taken into account:</p>
<ul>
<li>
<p>to limit uncontrolled growth and foster subsystem homogeneity we provide a common framework for telemetry and inter-process communication and a set of tools for structuring metrics and APIs</p>
</li>
<li>
<p>to have one source of truth we keep the code of all subsystems in one monorepo, which also simplifies managing dependencies and building, testing and deploying the software</p>
</li>
<li>
<p>to ensure subsystem compatibility &amp; consistency we require an IPC framework that checks procedure calls and arguments <em>at compile time</em></p>
</li>
</ul>
<h2 id="why-grpc">Why gRPC?</h2>
<p>To minimise dependencies we aimed at using the same library for inter-process communication as for the remote interfaces. Ideally, the RPC layer does not incur a significant latency overhead compared to traditional means of IPC and enforces compile-time consistency via schemas. Having a serialisation layer that is independent of the RPC layer and that supports schema evolution makes sure that the in-flight telemetry messages may also be stored to disk in the same format and that we can evolve subsystems while retaining backwards compatibility, e.g., for the monitoring data.</p>
<p>There are a few RPC frameworks that fit these needs and combine the benefits of schema-based serialisation with RPC functionality. For our projects (which are mainly based on C/C++), three popular, mature options seem to exist: Apache Thrift, Cap’n Proto, gRPC. For FlashCam we settled on gRPC because its (default) serialisation layer, protobuf, is already being used in CTA. gRPC has the added benefits of providing efficient streaming connections (for our telemetry data) and some very useful tooling, such as the excellent <a href="https://buf.build">Buf</a> linter and breaking-change detector.</p>
<h2 id="latency-overhead-of-grpc-for-local-ipc">Latency overhead of gRPC for local IPC</h2>
<p>Our State Supervisor needs to handle state changes of ~10 subsystems in an ordered fashion; its code becomes much simpler when using a single thread and synchronous processing, but latencies of the individual calls add up and may become critical if they’re in the milliseconds-regime. Since I couldn’t find any measurements on gRPC’s <em>unary call latency</em> over Unix domain sockets, I chose to measure the latency distribution.</p>
<p>In our scenario of local IPC, some obvious tuning options exist:<sidenote><span>Compression seems to be disabled by default, otherwise this would’ve been an obvious knob to turn (off).</span></sidenote>
</p>
<ul>
<li>
<p>data is exchanged via a Unix domain socket (<code>unix://</code> address) instead of a TCP socket</p>
</li>
<li>
<p>server and client may run on the same CPU core or separate cores; we will test both to see the difference</p>
</li>
</ul>
<p>The test system is an AMD EPYC 7402P-based server running CentOS 8 with the “latency-performance” profile and gRPC v1.40.0; “other core” below means running the client on cores <em>not</em> sharing an L3 cache with the server. We compare the performance with the most simple traditional IPC method I could think of: exchanging C structs over a Unix domain socket using blocking I/O—which should have near-ideal performance for a sockets-based approach.<sidenote><span>Shared-memory IPC should be faster because it happens in userspace only. But it’s not straightforward (for me) to scale this to a server supporting multiple clients—I’d be interested in a neat example, though!</span></sidenote>
</p>
<p>The histograms of the latency distributions of 1 million IPC calls per combination are plotted below:</p>
<figure><svg viewBox="0 0 368.39952 224.39952"><use href="gRPC-Unary-Call-Latency.svg#figure_1"></use></svg></figure>
<p>The median and upper percentiles are tabulated below:</p>
<table>
<thead>
<tr>
<th>IPC technology</th>
<th>Server/client thread distribution</th>
<th>Unary call latency median</th>
<th>95<sup>th</sup> percentile</th>
<th>99<sup>th</sup> percentile</th>
</tr>
</thead>
<tbody>
<tr>
<td>UDS</td>
<td>same core</td>
<td>4 µs</td>
<td>5 µs</td>
<td>6 µs</td>
</tr>
<tr>
<td></td>
<td>other core</td>
<td>11 µs</td>
<td>12 µs</td>
<td>13 µs</td>
</tr>
<tr>
<td>gRPC</td>
<td>same core</td>
<td>167 µs</td>
<td>178 µs</td>
<td>200 µs</td>
</tr>
<tr>
<td></td>
<td>other core</td>
<td>116 µs</td>
<td>129 µs</td>
<td>142 µs</td>
</tr>
</tbody>
</table>
<p>When using Unix domain sockets and blocking I/O on the same core, the kernel seems to be able to immediately switch the context to the reading thread—hence, this represents the optimal but somewhat contrived case. It seems more realistic to compare the “other core” latencies, where gRPC is about a factor of 10 slower than blocking I/O over Unix domain sockets.</p>
<h2 id="conclusions">Conclusions</h2>
<p>The ~100 µs unary-call latency overhead of gRPC for local IPC is entirely acceptable for our purposes; the overhead is outweighed by the benefits of using only a single library for IPC and RPC, and having well-defined, strongly-typed interfaces that are checked for consistency at compile-time and have clear evolution strategies.</p>
<p>For the connections to the telemetry collector, congestion from handling multiple clients may be a concern in situations such as a run start, where all subsystems are being configured and produce a burst of messages. Our prototype telemetry collector currently handles up to ~120k–150k telemetry messages per second for up to 3 concurrent clients and a maximum aggregate throughput of ~400k messages per second for our ~10 clients—also sufficient for our purposes.</p>
<h2 id="literature">Literature</h2>
<p><a href="https://docs.microsoft.com/en-us/aspnet/core/grpc/performance">Performance best practices with gRPC</a> mentions that converting unary calls to bidirectional streaming could improve performance. I explicitly decided against that to keep code simple to grok.</p>
<p><a href="https://buf.build/blog/api-design-is-stuck-in-the-past">API design is stuck in the past</a> explains the advantages of controlled, schema-driven development for RPC interfaces.</p>
<h2 id="code-appendix">Code appendix</h2>
<p>The complete code for running the two microbenchmarks is shown below.</p>
<details>
<summary><code>Makefile</code></summary>
<div><pre tabindex="0"><code data-lang="make"><span>CXX</span> <span>=</span> g++
<span>CPPFLAGS</span> <span>+=</span> <span>`</span>pkg-config --cflags protobuf grpc<span>`</span>
<span>CXXFLAGS</span> <span>+=</span> -std<span>=</span>c++17 -O2 -march<span>=</span>native -mtune<span>=</span>native

<span>SYSTEM</span> <span>?=</span> <span>$(</span>shell uname | cut -f <span>1</span> -d_<span>)</span>
<span>LDFLAGS</span> <span>+=</span> <span>`</span>pkg-config --libs protobuf grpc++ grpc<span>`</span>
ifeq (<span>$(</span><span>SYSTEM</span><span>)</span>,Darwin)
<span>LDFLAGS</span> <span>+=</span> -lgrpc++_reflection
else
<span>LDFLAGS</span> <span>+=</span> -Wl,--no-as-needed -lgrpc++_reflection -Wl,--as-needed
endif
<span>LDFLAGS</span> <span>+=</span> -ldl

<span>all</span><span>:</span> uds-ipc-latency grpc-ipc-latency

<span>uds-ipc-latency</span><span>:</span> uds-ipc-latency.o
	<span>$(</span>CXX<span>)</span> $^ <span>$(</span>LDFLAGS<span>)</span> -o <span>$@</span>

<span>grpc-ipc-latency</span><span>:</span> trivial.ipc.pb.o trivial.ipc.grpc.pb.o grpc-ipc-latency.o
	<span>$(</span>CXX<span>)</span> $^ <span>$(</span>LDFLAGS<span>)</span> -o <span>$@</span>

<span>.PRECIOUS</span><span>:</span> %.grpc.pb.cc
<span>%.grpc.pb.cc</span><span>:</span> %.proto
	protoc --grpc_out<span>=</span>. --plugin<span>=</span>protoc-gen-grpc<span>=</span><span>`</span>which grpc_cpp_plugin<span>`</span> $&lt;

<span>.PRECIOUS</span><span>:</span> %.pb.cc
<span>%.pb.cc</span><span>:</span> %.proto
	protoc --cpp_out<span>=</span>. $&lt;

<span>clean</span><span>:</span>
	rm -f *.o *.pb.cc *.pb.h uds-ipc-latency grpc-ipc-latency

</code></pre></div></details>
<details>
<summary><code>uds-ipc-latency.cpp</code></summary>
<div><pre tabindex="0"><code data-lang="c++"><span>// uds-ipc-latency.cpp - Measure latency of unary IPC calls over a Unix domain socket using blocking I/O.
</span><span>//
</span><span>// Compile: g++ -std=c++17 -O2 -march=native -mtune=native -o uds-ipc-latency uds-ipc-latency.cpp
</span><span>// Run on 1st and 2nd core: ./uds-ipc-latency 0x1 0x2 &gt; uds-ipc-latencies-nsec.txt
</span><span></span><span>#include</span> <span>&lt;algorithm&gt;</span><span>
</span><span>#include</span> <span>&lt;chrono&gt;</span><span>
</span><span>#include</span> <span>&lt;cstdio&gt;</span><span>
</span><span>#include</span> <span>&lt;cstring&gt;</span><span>
</span><span>#include</span> <span>&lt;vector&gt;</span><span>
</span><span></span>
<span>#include</span> <span>&lt;sys/socket.h&gt;</span><span>
</span><span>#include</span> <span>&lt;sys/errno.h&gt;</span><span>
</span><span>#include</span> <span>&lt;unistd.h&gt;</span><span>
</span><span></span>
<span>#if __linux__
</span><span>#include</span> <span>&lt;sched.h&gt;</span><span>
</span><span>#endif
</span><span></span>
<span>using</span> <span>namespace</span> std;

<span>struct</span> <span>DummyMsg</span> {
    <span>int64_t</span> i;
};

<span>void</span> <span>die</span>(<span>const</span> <span>char</span> <span>*</span>msg) {
    fprintf(stderr, <span>&#34;%s: %s (%d)</span><span>\n</span><span>&#34;</span>, msg, strerror(errno), errno);
    exit(<span>1</span>);
}

<span>void</span> <span>call</span>(<span>int</span> socket, DummyMsg <span>&amp;</span>msg) {
    <span>if</span> (<span>int</span> rc <span>=</span> write(socket, <span>&amp;</span>msg, <span>sizeof</span>(msg)); rc <span>!=</span> <span>sizeof</span>(msg))
        die(<span>&#34;write() failed&#34;</span>);

    <span>if</span> (<span>int</span> rc <span>=</span> read(socket, <span>&amp;</span>msg, <span>sizeof</span>(msg)); rc <span>!=</span> <span>sizeof</span>(msg))
        die(<span>&#34;read() failed&#34;</span>);
}

<span>int</span> <span>main</span>(<span>int</span> argc, <span>const</span> <span>char</span> <span>*</span>argv[])
{
<span>#if __linux__
</span><span></span>    cpu_set_t parent_mask, child_mask;
    CPU_ZERO(<span>&amp;</span>parent_mask);
    CPU_ZERO(<span>&amp;</span>child_mask);
    <span>if</span> (argc <span>==</span> <span>3</span>) {
        sscanf(argv[<span>1</span>], <span>&#34;%x&#34;</span>, <span>&amp;</span>parent_mask);
        sscanf(argv[<span>2</span>], <span>&#34;%x&#34;</span>, <span>&amp;</span>child_mask);
    }
<span>#endif
</span><span></span>
    <span>int</span> sockets[<span>2</span>];
    <span>if</span> (<span>int</span> rc <span>=</span> socketpair(AF_UNIX, SOCK_STREAM, <span>0</span>, sockets); rc <span>==</span> <span>-</span><span>1</span>)
        die(<span>&#34;socketpair() failed&#34;</span>);

    pid_t pid <span>=</span> fork();
    <span>if</span> (pid <span>==</span> <span>-</span><span>1</span>)
        die(<span>&#34;fork() failed&#34;</span>);

    <span>if</span> (pid <span>==</span> <span>0</span>) {  <span>// child
</span><span></span>        close(sockets[<span>0</span>]); <span>// close parent&#39;s socket
</span><span></span>        <span>#if __linux__
</span><span></span>        <span>if</span> (<span>int</span> rc <span>=</span> sched_setaffinity(<span>0</span>, <span>sizeof</span>(child_mask), <span>&amp;</span>child_mask); rc <span>==</span> <span>-</span><span>1</span>)
            die(<span>&#34;sched_setaffinity() failed&#34;</span>);
        <span>#endif
</span><span></span>
        DummyMsg msg{.i <span>=</span> <span>0</span>};
        <span>for</span> (<span>int</span> i <span>=</span> <span>0</span>; i <span>&lt;</span> <span>1000</span>; i<span>++</span>)
            call(sockets[<span>1</span>], msg);  <span>// warmup
</span><span></span>
        std<span>::</span>vector<span>&lt;</span><span>int</span><span>&gt;</span> latencies;
        latencies.reserve(<span>1000000</span>);
        <span>for</span> (<span>int</span> i <span>=</span> <span>0</span>; i <span>&lt;</span> <span>1000000</span>; i<span>++</span>) {
            <span>auto</span> start <span>=</span> chrono<span>::</span>high_resolution_clock<span>::</span>now();
            call(sockets[<span>1</span>], msg);
            <span>auto</span> end <span>=</span> chrono<span>::</span>high_resolution_clock<span>::</span>now();

            latencies.push_back(chrono<span>::</span>duration_cast<span>&lt;</span>chrono<span>::</span>nanoseconds<span>&gt;</span>(end <span>-</span> start).count());
        }

        <span>for</span> (<span>auto</span> l : latencies)
            printf(<span>&#34;%i</span><span>\n</span><span>&#34;</span>, l);

        close(sockets[<span>1</span>]);
    } <span>else</span> {  <span>// parent
</span><span></span>        close(sockets[<span>1</span>]);  <span>// close child&#39;s socket
</span><span></span>
        <span>#if __linux__
</span><span></span>        <span>if</span> (<span>int</span> rc <span>=</span> sched_setaffinity(<span>0</span>, <span>sizeof</span>(parent_mask), <span>&amp;</span>parent_mask); rc <span>==</span> <span>-</span><span>1</span>)
            die(<span>&#34;sched_setaffinity() failed&#34;</span>);
        <span>#endif
</span><span></span>
        DummyMsg msg;
        <span>while</span> (<span>1</span>) {
            <span>if</span> (<span>int</span> rc <span>=</span> read(sockets[<span>0</span>], <span>&amp;</span>msg, <span>sizeof</span>(msg)); rc <span>!=</span> <span>sizeof</span>(msg))
                die(<span>&#34;read() failed&#34;</span>);

            msg.i<span>++</span>;

            <span>if</span> (<span>int</span> rc <span>=</span> write(sockets[<span>0</span>], <span>&amp;</span>msg, <span>sizeof</span>(msg)); rc <span>!=</span> <span>sizeof</span>(msg))
                die(<span>&#34;write() failed&#34;</span>);
        }

        close(sockets[<span>0</span>]);
    }
}

</code></pre></div></details>
<details>
<summary><code>grpc-ipc-latency.cpp</code></summary>
<div><pre tabindex="0"><code data-lang="c++"><span>// grpc-ipc-latency.cpp - Measure latency of unary IPC calls over a Unix domain socket using gRPC.
</span><span>//
</span><span>// Compile: g++ -std=c++17 -O2 -march=native -mtune=native -o grpc-ipc-latency grpc-ipc-latency.cpp
</span><span>// Run on 1st and 2nd core: ./grpc-ipc-latency 0x1 0x2 &gt; grpc-ipc-latencies-nsec.txt
</span><span></span><span>#include</span> <span>&lt;algorithm&gt;</span><span>
</span><span>#include</span> <span>&lt;chrono&gt;</span><span>
</span><span>#include</span> <span>&lt;cstdio&gt;</span><span>
</span><span>#include</span> <span>&lt;cstring&gt;</span><span>
</span><span>#include</span> <span>&lt;thread&gt;</span><span>
</span><span>#include</span> <span>&lt;vector&gt;</span><span>
</span><span></span>
<span>#include</span> <span>&lt;sys/socket.h&gt;</span><span>
</span><span>#include</span> <span>&lt;sys/errno.h&gt;</span><span>
</span><span>#include</span> <span>&lt;unistd.h&gt;</span><span>
</span><span></span>
<span>#if __linux__
</span><span>#include</span> <span>&lt;sched.h&gt;</span><span>
</span><span>#endif
</span><span></span>
<span>#include</span> <span>&lt;grpcpp/grpcpp.h&gt;</span><span>
</span><span></span>
<span>#include</span> <span>&#34;trivial.ipc.grpc.pb.h&#34;</span><span>
</span><span></span>
<span>using</span> <span>namespace</span> std;
<span>using</span> <span>namespace</span> trivial<span>::</span>ipc;
<span>using</span> <span>namespace</span> grpc;
<span>using</span> <span>namespace</span> std<span>::</span>chrono_literals;

<span>void</span> <span>die</span>(<span>const</span> <span>char</span> <span>*</span>msg) {
    fprintf(stderr, <span>&#34;%s: %s (%d)</span><span>\n</span><span>&#34;</span>, msg, strerror(errno), errno);
    exit(<span>1</span>);
}

<span>class</span> <span>RPCServiceImpl</span> <span>final</span> <span>:</span> <span>public</span> RPCService<span>::</span>Service {
  Status <span>UnaryCall</span>(ServerContext<span>*</span> <span>/*context*/</span>, <span>const</span> UnaryCallRequest<span>*</span> request,
                   UnaryCallReply<span>*</span> reply) <span>override</span> {
    reply<span>-&gt;</span>set_i(request<span>-&gt;</span>i() <span>+</span> <span>1</span>);
    <span>return</span> Status<span>::</span>OK;
  }
};

<span>int</span> <span>main</span>(<span>int</span> argc, <span>const</span> <span>char</span> <span>*</span>argv[])
{
<span>#if __linux__
</span><span></span>    cpu_set_t parent_mask, child_mask;
    CPU_ZERO(<span>&amp;</span>parent_mask);
    CPU_ZERO(<span>&amp;</span>child_mask);
    <span>if</span> (argc <span>==</span> <span>3</span>) {
        sscanf(argv[<span>1</span>], <span>&#34;%x&#34;</span>, <span>&amp;</span>parent_mask);
        sscanf(argv[<span>2</span>], <span>&#34;%x&#34;</span>, <span>&amp;</span>child_mask);
    }
<span>#endif
</span><span></span>
    std<span>::</span>string address{<span>&#34;unix:///tmp/test.socket&#34;</span>};

    pid_t pid <span>=</span> fork();
    <span>if</span> (pid <span>==</span> <span>-</span><span>1</span>)
        die(<span>&#34;fork() failed&#34;</span>);

    <span>if</span> (pid <span>==</span> <span>0</span>) {  <span>// child
</span><span></span>        <span>#if __linux__
</span><span></span>        <span>if</span> (<span>int</span> rc <span>=</span> sched_setaffinity(<span>0</span>, <span>sizeof</span>(child_mask), <span>&amp;</span>child_mask); rc <span>==</span> <span>-</span><span>1</span>)
            die(<span>&#34;sched_setaffinity() failed&#34;</span>);
        <span>#endif
</span><span></span>
        std<span>::</span>this_thread<span>::</span>sleep_for(<span>100</span>ms);  <span>// wait for setup
</span><span></span>
        <span>auto</span> client <span>=</span> RPCService<span>::</span>NewStub(grpc<span>::</span>CreateChannel(address, grpc<span>::</span>InsecureChannelCredentials()));

        UnaryCallRequest req{};
        req.set_i(<span>0</span>);

        UnaryCallReply rep{};
        <span>for</span> (<span>int</span> i <span>=</span> <span>0</span>; i <span>&lt;</span> <span>1000</span>; i<span>++</span>) {  <span>// warmup
</span><span></span>            ClientContext ctx{};
            <span>if</span> (grpc<span>::</span>Status status <span>=</span> client<span>-&gt;</span>UnaryCall(<span>&amp;</span>ctx, req, <span>&amp;</span>rep); <span>!</span>status.ok())
                die(<span>&#34;UnaryCall failed&#34;</span>);
        }

        std<span>::</span>vector<span>&lt;</span><span>int</span><span>&gt;</span> latencies;
        latencies.reserve(<span>1000000</span>);
        <span>for</span> (<span>int</span> i <span>=</span> <span>0</span>; i <span>&lt;</span> <span>1000000</span>; i<span>++</span>) {
            <span>auto</span> start <span>=</span> chrono<span>::</span>high_resolution_clock<span>::</span>now();
            ClientContext ctx{};
            <span>if</span> (grpc<span>::</span>Status status <span>=</span> client<span>-&gt;</span>UnaryCall(<span>&amp;</span>ctx, req, <span>&amp;</span>rep); <span>!</span>status.ok())
                die(<span>&#34;UnaryCall failed&#34;</span>);
            <span>auto</span> end <span>=</span> chrono<span>::</span>high_resolution_clock<span>::</span>now();

            latencies.push_back(chrono<span>::</span>duration_cast<span>&lt;</span>chrono<span>::</span>nanoseconds<span>&gt;</span>(end <span>-</span> start).count());
        }

        <span>for</span> (<span>auto</span> l : latencies)
            printf(<span>&#34;%i</span><span>\n</span><span>&#34;</span>, l);
    } <span>else</span> {  <span>// parent
</span><span></span>        <span>#if __linux__
</span><span></span>        <span>if</span> (<span>int</span> rc <span>=</span> sched_setaffinity(<span>0</span>, <span>sizeof</span>(parent_mask), <span>&amp;</span>parent_mask); rc <span>==</span> <span>-</span><span>1</span>)
            die(<span>&#34;sched_setaffinity() failed&#34;</span>);
        <span>#endif
</span><span></span>
        RPCServiceImpl service;
        ServerBuilder builder;
        builder.AddListeningPort(address, grpc<span>::</span>InsecureServerCredentials());
        builder.RegisterService(<span>&amp;</span>service);

        std<span>::</span>unique_ptr<span>&lt;</span>Server<span>&gt;</span> server(builder.BuildAndStart());
        server<span>-&gt;</span>Wait();
    }
}

</code></pre></div></details>
<details>
<summary><code>trivial.ipc.proto</code></summary>
<div><pre tabindex="0"><code data-lang="proto">syntax <span>=</span> <span>&#34;proto3&#34;</span>;

<span>package</span> trivial<span>.</span>ipc;

<span>service</span> RPCService {
  <span>rpc</span> UnaryCall (UnaryCallRequest) <span>returns</span> (UnaryCallReply) {}
}

<span>message</span> <span>UnaryCallRequest</span> {
  <span>fixed64</span> i <span>=</span> <span>1</span>;
}

<span>message</span> <span>UnaryCallReply</span> {
  <span>fixed64</span> i <span>=</span> <span>1</span>;
}

</code></pre></div></details>
</div>

</article>
</div></div>
  </body>
</html>
