<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.rfleury.com/p/multi-threading-and-mutation">Original</a>
    <h1>Multi-Threading and Mutation</h1>
    
    <div id="readability-page-1" class="page"><div><div><article><div class=""><div><div dir="auto"><p><span>On day one, to even begin programming anything, programmers must understand how to write text files, and use an interpreter, or compiler, or assembler, to turn them into executing programs. Soon after, they must roughly understand what memory is, and how to use addresses and pointers (even if they’re working in various high level languages in which those concepts masquerade under other names). This naturally follows into a basic understanding of memory allocation. An important next step is learning to write code to operate on batches of data, rather than single elements of data, for both performance and simplicity. They can then understand how to </span><a href="https://www.rfleury.com/p/untangling-lifetimes-the-arena-allocator" rel="">more simply manage memory</a><span>, by being organized about these batches, and performing the allocations themselves as a batch, upfront. They can become more familiar with the underlying mechanisms of the CPU, how it obtains memory, performs computations, and writes memory.</span></p><p><span>At each of these milestones, the programmer’s understanding of the underlying hardware, and how to gracefully use it, broadens. And because modern hardware offers several independently-executing processor units, one of the next most crucial milestones in this journey is writing code to execute </span><em>in</em><span> </span><em>parallel</em><span>, in multiple simultaneously-executing instruction streams, rather than just a single instruction stream. This is critical for both software performance </span><em>and</em><span> utility. When writing user-mode code on a multitasking operating system, like myself and most developers, this is done with multi-threading.</span></p><p><span>Many of these milestones introduce new constraints which apply pressure on code to be shaped in a particular way. If a programmer solves for only </span><em>some of these constraints</em><span>, it’s like building a piece of furniture and tightening one particular screw fully, before even placing all other screws. Code, much like </span><a href="https://www.rfleury.com/p/you-get-what-you-measure" rel="">many other things</a><span>, is partly a function of its environment—that is, the constraints to which it’s regularly exposed.</span></p><p><span>Multi-threading is one such milestone. Single-threaded code can be built with a different set of constraints than multi-threaded code, and as such, it is often extremely difficult to later add the “screw” of multithreading to a mostly-fully-built piece of “furniture”. In other words, if code needs to be multi-threaded, then it should be built under those constraints </span><em>as early as possible</em><span>; more and more of it will need to be rewritten the </span><em>later those constraints are introduced</em><span>.</span></p><p>Sometimes, it makes more sense to willingly accept the cost of rewriting, especially if it’s difficult to predict what the correct multi-threaded design for a problem will be, and a single-threaded version must be first written to explore the problem space. I think there is often too much reluctance toward rewriting systems, and too many resources put into “reusability” (before the exact circumstances of that reuse are fully understood), but I digress.</p><p><span>For me, multi-threading was initially an extremely intimidating milestone to tackle, because it introduces </span><em>so many</em><span> of these additional constraints. It seems to add an entirely new dimension to programming, and many techniques one learns to work adequately in single-threaded space suddenly become invalid in multi-threaded space.</span></p><p><span>But as I’ve learned over the past couple of years, this doesn’t mean that, at the end of the day, it always needs to be </span><em>difficult</em><span>. It can instead be quite easy, after internalizing these additional constraints, and designing systems accordingly. Designing such systems can obviously be tricky and dependent on one’s own ingenuity, but multi-threading being intimidating was more a function of its </span><em>opacity</em><span> rather than its </span><em>intrinsic difficulty</em><span>. In other words, hard problems are still hard, but when I was first thinking about multithreading, there were many problems that </span><em>seemed like they should be easy</em><span> that were also hard.</span></p><p><span>Anytime someone writes a shader, for example, they’re doing multi-threaded programming. </span><a href="https://www.shadertoy.com/" rel="">Shadertoy</a><span> proves that this can be simple for something like tens (if not hundreds) of thousands of programmers, and it mostly happens invisibly, because the underlying primitives are well-designed. I learned that I can also design underlying primitives, for a given problem, to make multithreaded programming simple.</span></p><p><span>I’ve </span><a href="https://www.rfleury.com/p/a-taxonomy-of-computation-shapes" rel="">written before</a><span> about some lessons I’ve learned which have helped me internalize these additional constraints, and design multi-threaded systems. Recently, I’ve been doing much more multi-threaded programming, and I want to share more of those lessons in this post.</span></p><p><span>Generally, when someone is being taught multithreaded programming, they are taught about synchronization mechanisms—for instance, atomic CPU operations, and the higher-level abstractions that they are often used to implement, like mutexes, condition variables, semaphores, and so on. And while understanding synchronization mechanisms is important, the true task of writing multithreaded systems is not figuring out how to synchronize, it’s how to </span><em>not synchronize</em><span>. One of the major benefits of multi-threaded code is, obviously, that it allows multiple tasks to happen simultaneously. The more synchronization a multithreaded system requires, the more this benefit dissolves. Of course, </span><em>some amount</em><span> of synchronization must exist in order for two systems to work together, and </span><em>at those synchronization points</em><span>, getting the tricky details of synchronization mechanisms is critical—but ideally, the vast majority of multi-threaded code is executing </span><em>without synchronization at all</em><span>.</span></p><p><span>One intrinsic characteristic of multi-threaded code is that simultaneous reads can happen without any synchronization. Writes, on the other hand, can require synchronization (with both other writes </span><em>and</em><span> reads)—for one touched region in memory, it is really only ever well-defined to have a single write at a time, and thus there must be some synchronized order of writes, and reads cannot occur </span><em>while</em><span> a write occurs. This means either </span><em>reads</em><span> or </span><em>non-conflicting writes </em><span>lend themselves better to multi-threaded code than </span><em>potentially-conflicting writes</em><span>.</span></p><p><span>Thus, designing multi-threaded systems which execute cleanly in parallel requires careful attention to where </span><em>reads</em><span> occur, where </span><em>writes</em><span>—</span><em>mutations—</em><span>occur, and </span><em>what is mutated</em><span>. Writes should not conflict with other threads also performing writes as often as possible, lest the system requires additional synchronization. In other words, the mutations performed by each thread should be </span><em>bucketed</em><span> whenever possible.</span></p><p><span>A simple example of </span><em>bucketed mutations</em><span> would be when many threads allocate and mutate a local variable on the stack. This is trivially conflict-free and thus requires no synchronization, because the term “the stack” actually refers to </span><em>a per-thread stack</em><span>, and thus each thread mutates a completely different region in memory:</span></p><pre><code>void ThreadEntryPoint(...)
{
  U64 x = 0;
  for(U64 idx = 0; idx &lt; 1000; idx += 1)
  {
    x += idx; // will *never* conflict with other threads
  }
}

void EntryPoint(...)
{
  LaunchThread(ThreadEntryPoint, ...);
  LaunchThread(ThreadEntryPoint, ...);
}</code></pre><p><span>And a simple example of </span><em>non-bucketed mutations</em><span> would be when many threads write into the same fixed address in memory, e.g. through a </span><code>static</code><span> variable:</span></p><pre><code><code>void ThreadEntryPoint(...)
{
  static U64 x = 0;
  for(U64 idx = 0; idx &lt; 1000; idx += 1)
  {
    x += idx; // will *always* conflict with other threads
  }
}

void EntryPoint(...)
{
  LaunchThread(ThreadEntryPoint, ...);
  LaunchThread(ThreadEntryPoint, ...);
}</code></code></pre><p><span>In the above example, the conflicting write—the non-bucketed mutation—is not properly synchronized, and so the value of </span><code>x</code><span> is not reliably well-defined, and thus of much use.</span></p><p><span>A common example of mutation is mutating some data to allocate memory. Because allocations are a subset of mutations, they have the same characteristics in multi-threaded systems that other mutations do. So </span><em>bucketing mutations</em><span> wherever possible implies the rule to </span><em>bucket allocations</em><span> wherever possible.</span></p><p><span>Bucketing allocations for multi-threaded purposes often leads naturally to many opportunities to bucket allocations </span><em>by lifetime</em><span>. As such, the </span><a href="https://www.rfleury.com/p/untangling-lifetimes-the-arena-allocator" rel="">arena allocator</a><span> is an excellent tool for bucketing allocations.</span></p><p><span>Many people have asked me about synchronization mechanisms being baked into arena allocators, such that the lowest level arena-based allocation operation—</span><code>ArenaPush</code><span>—is thread-safe. This is an understandable question, because other allocators—e.g. </span><code>malloc</code><span>—are designed to be callable from several threads at once.</span></p><p><span>But making the basic arena operations thread-safe is putting the cart before the horse, and my reasoning for preferring </span><em>read-only access</em><span> and </span><em>bucketed mutations</em><span> in multi-threaded systems hopefully clears up why—it is </span><em>assuming</em><span> that the correct architecture requires synchronized access to the same arena. But the much more preferable alternative, when it is indeed possible for a given problem, is to use each arena </span><em>as a per-thread bucketed allocation</em><span> (and thus </span><em>bucketed mutation</em><span>) mechanism, such that only one thread trivially accesses an arena at a time with no synchronization.</span></p><p><span>An arena can be, of course, used </span><em>along with </em><span>synchronization mechanisms, such that an arena is not per-thread but rather per-data-structure, or per-hash-table-stripe, and so on, but baking in that synchronization </span><em>at the lowest level</em><span> is a misunderstanding of the ideal multi-threaded system, where threads execute </span><em>almost entirely</em><span> without synchronization (and thus almost entirely with read-only access to shared regions, or non-conflicting reads and writes). Using synchronization mechanisms with an arena is a compromise on that ideal. This is perfectly acceptable in many circumstances, since it may be for a </span><em>different </em><span>ideal (e.g. storing a heavy resource once, saving both memory and compute time, thus requiring synchronized access to a shared cache), but in any case, the arena implementation is </span><em>more</em><span> </span><em>flexible</em><span>, and functions well for </span><em>both cases</em><span>, when synchronization is user-defined, often resulting in no synchronization whatsoever.</span></p><p><span>In single-threaded space, it is often convenient or useful to combine </span><em>several codepaths</em><span> and produce </span><em><a href="https://www.rfleury.com/i/112467756/effective-codepaths-vs-codepaths" rel="">one effective codepath</a></em><span>. But sometimes, this is done such that </span><em>one low-level codepath</em><span> in an </span><em>effective codepath </em><span>contains a mutation, and </span><em>another low-level codepath</em><span> in the same </span><em>effective codepath</em><span> contains only a read. Take the following example, from a previous post:</span></p><pre><code>// retrieves value associated with `key` - if it does not
// exist inside `table`, then inserts it with initial value
// of `default_val`
Val ValFromKey(Table *table, Key *key, Val default_val);

Key key = ...;
Val val = ValFromKey(table, &amp;key, default_val);</code></pre><p>In that previous post, I wrote about how this kind of immediate-mode API can be extremely useful in collapsing the number of low-level codepaths required in a specific problem, particularly the number of low-level codepaths responsible for maintaining complex state machines.</p><p><span>But in multi-threaded systems, this kind of API requires extra analysis. The </span><em>effective codepath</em><span> must be treated as having the mutational properties of </span><em>all</em><span> of its potential </span><em>low-level codepaths</em><span>. Thus, the effective codepath which calls </span><code>ValFromKey</code><span> can only be understood </span><em>as mutational</em><span> of its </span><code>table</code><span> argument. If that table is a shared cache, perhaps shared across threads, then it has potentially conflicting mutations, thus requiring synchronization.</span></p><p><span>As I discussed earlier, this may be a perfectly suitable design—for example, if I replaced </span><code>Val</code><span> with </span><code>TextureHandle</code><span>, and </span><code>ValFromKey</code><span> with </span><code>TextureFromKey</code><span>, and this API was used to do quick lookups into a shared texture cache given a key, and I expect that </span><em>mostly</em><span> to consist of several in-flight reads after textures are loaded (and thus not requiring synchronization with writes), then synchronization is a perfectly reasonable trade.</span></p><p><span>But the details can change. Suppose that this effective codepath is used as a helper mechanism in </span><em>two</em><span> other overarching codepaths. In the first case, the call to </span><code>ValFromKey</code><span> is </span><em>mutational</em><span> in 99% of cases. In the second task, the call to </span><code>ValFromKey</code><span> is 100% non-mutational, and thus only doing a read-only lookup.</span></p><p><span>This manifested recently on a project I work on, where a </span><code>ValFromKey</code><span> mechanism was being used to build a large deduplicated hash table of strings, and the same mechanism was later being used to look up nodes in that hash table and gather information from them. The system I was working on was originally written as single-threaded code, and I was doing a pass over the system to improve its performance, particularly by moving independent streams of work to execute simultaneously, with no synchronization, on multiple threads.</span></p><p><span>Of course, in the single-threaded context, this code functions perfectly correctly—but by blurring the line between mutational writes and read-only access, it gave both a 99% mutational low-level codepath and a 100% read-only codepath the same name. Of course, if I verified that indeed the latter case was 100% read-only, I could simply </span><em>call it anyways</em><span> as a read-only effective codepath, but this is awfully close to “cheating”—it’d only take a small number of completely innocent changes to invisibly break the system with new race conditions, and so I found it much more reasonable to explicitly separate the “mutate explicitly” and “read-only” codepaths, and use each accordingly.</span></p><p><span>After doing so, because all allocations in the system are explicit to callers with </span><code>Arena</code><span> parameters, I instead have an API like the following:</span></p><pre><code><code>void TableInsert(Arena *arena, Table *table, Key *key, Val *val);
Val *TableLookup(Table *table, Key *key);</code></code></pre><p><span>With this style of API, the caller is in control of the allocation bucket via the </span><code>Arena</code><span> parameter. As such, the fact that </span><code>TableLookup</code><span> is read-only is explicit (as there is no such parameter), and thus guaranteed to function correctly in parallel with other codepaths also executing </span><code>TableLookup</code><span>.</span></p><p><span>In the aforementioned problem, that allowed the </span><em>second</em><span> codepath to be massively improved in performance—the previously single-threaded code doing 100% read-only lookups into a table could now be reorganized to execute completely in parallel.</span></p><p><span>But what about the case of </span><code>TableInsert</code><span>? It’s easy to understand the analysis of looking at the codepath responsible for building the table, and concluding that: each call to </span><code>TableInsert</code><span> mutates </span><code>arena</code><span> and </span><code>table</code><span>, which conflicts with other calls to </span><code>TableInsert</code><span>, and therefore all </span><code>TableInsert</code><span>s must be synchronized.</span></p><p><span>And again, in some cases this may be completely reasonable. And in those cases, as I’ve found, it isn’t necessarily the end of the world. I’ve found that it’s often easy to implement a much cleverer synchronization mechanisms than—for instance—just throwing a lock around </span><code>table</code><span> and </span><code>arena</code><span>. For instance, if </span><code>table</code><span> is a hash table, instead of taking an </span><code>Arena</code><span>, </span><code>TableInsert</code><span> can take a different mechanism—I’ll call it a </span><code>Guard</code><span>—such that the insertion mechanism can map a </span><code>(Guard, Hash)</code><span> pair to an </span><code>Arena</code><span> and a read-write lock, where the </span><code>Guard</code><span> can have several </span><code>Arena</code><span>s and locks, subdividing the hash table. Thus, in order to actually require a writing lock, multiple threads need to not only be writing to the same table, but to the same “stripe” in the same table.</span></p><p><span>This already provides an improvement over the single lock and arena, but beyond this, it’s often well within reason to do </span><em>much better</em><span> than that, with carefully-designed atomic locking operations rather than the heavier-handed locking abstractions. But a full investigation of such techniques deserves its own post. Clever locking mechanisms, from what I can tell, seem like they’re useful in getting another 20%, 30%, or 40% out of code which already has synchronization built in, but they’re still much slower than code which requires </span><em>no synchronization whatsoever</em><span>.</span></p><p><span>But a simple tweak to the problem allows all calls to </span><code>TableInsert</code><span> to be bucketed, and thus non-conflicting. Instead of having a </span><em>single table</em><span>, I can also simply have </span><em>many tables</em><span>, and then have a separate </span><em>“join” step</em><span>, which allows me to combine </span><em>the many tables</em><span> into a </span><em>single joined table</em><span>. In many cases, a multi-threaded “build” step, with each thread operating without synchronization (and on separate arenas), followed by a “join” step, may be much more efficient than a multi-threaded “build” step which requires synchronized access to a single data structure.</span></p><p><span>Assume </span><code>Table</code><span> is a simple linked-list-chaining hash table:</span></p><pre><code>struct Node
{
  Node *next;
  Key key;
  Val val;
};

struct Slot
{
  Node *first;
  Node *last;
};

struct Table
{
  U64 slots_count;
  Slot *slots;
};</code></pre><p><span>A “join” operation for two </span><code>Table</code><span>s with the same </span><code>slots_count</code><span> can then be easily written as a linked-list concatenation for each slot.</span></p><pre><code>Table *dst = ...;
Table *src = ...;
for(U64 idx = 0; idx &lt; slots_count; idx += 1)
{
  if(dst-&gt;slots[idx].last &amp;&amp; src-&gt;slots[idx].first)
  {
    dst-&gt;slots[idx].last-&gt;next = src-&gt;slots[idx].first;
    dst-&gt;slots[idx].last = src-&gt;slots[idx].last;
  }
  else if(src-&gt;slots[idx].first)
  {
    MemoryCopyStruct(&amp;dst-&gt;slots[idx], &amp;src-&gt;slots[idx]);
  }
}</code></pre><p><span>If type-system-enforcement of the same </span><code>slots_count</code><span> is desired, then the API can be slightly tweaked to the following:</span></p><pre><code>struct Table
{
  // `slots_count` is omitted - chosen/passed by user
  Slot *slots;
};

void TableInsert(Arena *arena, Table *table, U64 slots_count, Key *key, Val *val);
Val *TableLookup(Table *table, U64 slots_count, Key *key);
void TableJoin(Table *dst, Table *src, U64 slots_count);</code></pre><p>A “join” operation can also be extended with a number of other features, like sorting each bucket to ensure deterministic results. It can also be easily parallelized, as each slot’s “join” operation is entirely independent from the joining work for all other slots. As such, the mutations for the “join” operation are bucketed.</p><p><span>Similar to my feeling after learning simpler </span><a href="https://www.rfleury.com/p/untangling-lifetimes-the-arena-allocator" rel="">memory management</a><span> techniques, I’ve learned that multi-threaded programming becomes significantly easier when I adopt a careful, grounded, and organized approach, and when I’m closely familiar with a given problem’s details, and meticulously untangle operations by their lower level properties, rather than relying purely on stories told at higher levels of abstraction. In this case, those details include </span><em>reads</em><span>, </span><em>conflicting writes</em><span>, and </span><em>non-conflicting writes</em><span>.</span></p><p>I hope this post provided similar insight to you.</p><p>If you enjoyed this post, please consider subscribing. Thanks for reading.</p><p>-Ryan</p></div></div></div></article></div></div></div>
  </body>
</html>
