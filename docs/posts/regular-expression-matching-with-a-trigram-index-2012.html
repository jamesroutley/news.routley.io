<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://swtch.com/~rsc/regexp/regexp4.html">Original</a>
    <h1>Regular Expression Matching with a Trigram Index (2012)</h1>
    
    <div id="readability-page-1" class="page">


<h2>
<a href="https://swtch.com/~rsc/">Russ Cox</a>
</h2>

<!--

-->

<h2 id="intro">Introduction</h2>

<p>
In the summer of 2006, I was lucky enough to be an intern at Google.
At the time, Google had an internal tool called gsearch that acted as if it ran
grep over all the files in the Google source tree and printed the results.
Of course, that implementation would be fairly slow, so what gsearch
actually did was talk to a bunch of
servers that kept different pieces of the source tree in memory: each
machine did a grep through its memory and then gsearch merged the
results and printed them.  Jeff Dean, my intern host and one of the
authors of gsearch, suggested that it would be cool to build a web interface
that, in effect, let you run gsearch over the world&#39;s public source code.
I thought that sounded fun, so that&#39;s what I did that summer.
Due primarily to an excess of optimism in our original schedule,
the launch slipped to October, but <a href="http://googlecode.blogspot.com/2006/10/search-worlds-public-source-code.html">on October 5, 2006 we did
launch</a> (by then I was back at school but still a part-time intern).
</p>

<p>
I built the earliest demos using Ken Thompson&#39;s Plan 9 grep,
because I happened to have it lying around in library form.
The plan had been to switch to a “real” regexp library,
namely PCRE, probably behind a newly written, code reviewed parser,
since PCRE&#39;s parser was a well-known source of security bugs.
The only problem was my then-recent discovery that <a href="https://swtch.com/~rsc/regexp/regexp1.html">none of the
popular regexp implementations - not Perl, not Python,
not PCRE - used real automata</a>.
This was a surprise to me, and even to Rob Pike,
the author of the Plan 9 regular expression library.
(Ken was not yet at Google to be consulted.)
I had learned about regular expressions and automata from
the Dragon Book, from theory classes in college, and from
reading Rob&#39;s and Ken&#39;s code.  The idea that you wouldn&#39;t use
the guaranteed linear time algorithm had never occurred to me.
But it turned out that Rob&#39;s code in particular used an algorithm
only a few people had ever known, and the others had
<a href="https://swtch.com/~rsc/regexp/regexp2.html#ahu74">forgotten
about it years earlier</a>. We launched with the Plan 9 grep code;
a few years later I did replace it, with <a href="https://swtch.com/~rsc/regexp/regexp3.html">RE2</a>.
</p>

<p>
Code Search was Google&#39;s first and only search engine to accept
regular expression queries, which was geekily great but a very small niche.
The sad fact is that many programmers can&#39;t write regular expressions,
let alone correct ones. When we started Code Search, a Google search for
“regular expression search engine” turned up sites where you typed
“phone number” and got back “<code>\(\d{3}\) \d{3}-\d{4}</code>”.
</p>

<p>
<a href="http://google-opensource.blogspot.com/2010/03/re2-principled-approach-to-regular.html">Google open sourced the regular expression engine</a> I wrote for Code Search,
<a href="https://github.com/google/re2/">RE2</a>, in March 2010.
Code Search and RE2 have been a great vehicle for educating people about
how to do regular expression search safely.  In fact, Tom Christiansen recently
told me that even people in the Perl community use it
(<code>perl -Mre::engine::RE2</code>), to run regexp search engines
(the real kind) on the web without opening themselves up to trivial denial
of service attacks.
</p>

<p>
In October 2011, Google announced that it would <a href="http://googleblog.blogspot.com/2011/10/fall-sweep.html">shut down Code Search</a> as
part of its efforts to <a href="http://googleblog.blogspot.com/2011/09/fall-spring-clean.html">refocus on higher-impact products</a>,
and now Code Search is no longer online.
To mark the occasion, I thought it would be
appropriate to write a little about how Code Search worked.
The actual Code Search was built on top of Google&#39;s world-class document indexing
and retrieval tools; this article is accompanied by an implementation that
works well enough to index and search large code bases on a single computer.
</p>

<h2 id="word">Indexed Word Search</h2>

<p>
Before we can get to regular expression search, it helps to know a little about
how word-based full-text search is implemented.  The key data structure is
called a posting list or inverted index, which lists, for every possible search term,
the documents that contain that term.
</p>

<p>For example, consider these three very
short documents:
</p>

<pre>(1) Google Code Search
(2) Google Code Project Hosting
(3) Google Web Search
</pre>

<p>
The inverted index for these three documents looks like:
</p>

<pre>Code: {1, 2}
Google: {1, 2, 3}
Hosting: {2}
Project: {2}
Search: {1, 3}
Web: {3}
</pre>

<p>
To find all the documents that contain both Code and Search, you
load the index entry for Code {1, 2} and intersect it with the list for
Search {1, 3}, producing the list {1}.  To find documents that contain
Code or Search (or both), you union the lists instead of intersecting them.
Since the lists are sorted, these operations run in linear time.
</p>

<p>
To support phrases, full-text search implementations usually record each
occurrence of a word in the posting list, along with its position:
</p>

<pre>Code: {(1, 2), (2, 2)}
Google: {(1, 1), (2, 1), (3, 1)}
Hosting: {(2, 4)}
Project: {(2, 3)}
Search: {(1, 3), (3, 4)}
Web: {(3, 2)}
</pre>

<p>
To find the phrase “Code Search”, an implementation first loads the
list for Code and then scans the list for Search to find
entries that are one word past entries in the Code list.
The (1, 2) entry in the Code list and the (1, 3) entry in
the Search list are from the same document (1) and have
consecutive word numbers (2 and 3), so document 1 contains the
phrase “Code Search”.
</p>

<p>
An alternate way to support phrases is to treat them as <SPAN size="-1">AND</SPAN> queries
to identify a set of candidate documents and then filter out
non-matching documents after loading the document bodies
from disk.  In practice, phrases built out of common words
like “to be or not to be” make this approach unattractive.
Storing the position information in the index entries makes the
index bigger but avoids loading a document from disk unless
it is guaranteed to be a match.
</p>

<h2 id="regexp">Indexed Regular Expression Search</h2>

<p>
There is too much source code in the world for Code Search
to have kept it all in memory and run a regexp search over it all
for every query, no matter how fast the regexp engine.
Instead, Code Search used an inverted index to identify candidate
documents to be searched, scored, ranked, and eventually
shown as results.
</p>

<p>
Regular expression matches do not always line up
nicely on word boundaries, so the inverted index cannot
be based on words like in the previous example.
Instead, we can use an old information retrieval trick and
build an index of <i>n</i>-grams, substrings of length <i>n</i>.
This sounds more general than it is.
In practice, there are too few distinct 2-grams and too
many distinct 4-grams, so 3-grams (trigrams) it is.
</p>

<p>
Continuing the example from the last section, the document set:
</p>

<pre>(1) Google Code Search
(2) Google Code Project Hosting
(3) Google Web Search
</pre>

<p>
has this trigram index:
</p>

<pre>_Co: {1, 2}     Sea: {1, 3}     e_W: {3}        ogl: {1, 2, 3}
_Ho: {2}        Web: {3}        ear: {1, 3}     oje: {2}
_Pr: {2}        arc: {1, 3}     eb_: {3}        oog: {1, 2, 3}
_Se: {1, 3}     b_S: {3}        ect: {2}        ost: {2}
_We: {3}        ct_: {2}        gle: {1, 2, 3}  rch: {1, 3}
Cod: {1, 2}     de_: {1, 2}     ing: {2}        roj: {2}
Goo: {1, 2, 3}  e_C: {1, 2}     jec: {2}        sti: {2}
Hos: {2}        e_P: {2}        le_: {1, 2, 3}  t_H: {2}
Pro: {2}        e_S: {1}        ode: {1, 1}     tin: {2}
</pre>

<p>
(The _ character serves here as a visible representation of a space.)
</p>

<p>
Given a regular expression such as <code>/Google.*Search/</code>,
we can build a query of <SPAN size="-1">AND</SPAN>s and <SPAN size="-1">OR</SPAN>s that gives the trigrams that
must be present in any text matching the regular expression.
In this case, the query is
</p>

<p>
<code>Goo</code> <SPAN size="-1">AND</SPAN> <code>oog</code> <SPAN size="-1">AND</SPAN> <code>ogl</code> <SPAN size="-1">AND</SPAN> <code>gle</code> <SPAN size="-1">AND</SPAN> <code>Sea</code> <SPAN size="-1">AND</SPAN> <code>ear</code> <SPAN size="-1">AND</SPAN> <code>arc</code> <SPAN size="-1">AND</SPAN> <code>rch</code>
</p>

<p>
We can run this query against the trigram index to identify
a set of candidate documents and then run the full regular expression
search against only those documents.
</p>

<p>
The conversion to a query is not simply a matter of pulling out the
text strings and turning them into <SPAN size="-1">AND</SPAN> expressions, although that
is part of it.  A regular expression using the | operator will lead to a
query with <SPAN size="-1">OR</SPAN> terms, and parenthesized subexpressions complicate
the process of turning strings into <SPAN size="-1">AND</SPAN> expressions.
</p>

<p>
The full rules compute five results from each regular expression <i>r</i>:
whether the empty string is a matching string,
the exact set of matching strings or an indication that the exact set is unknown,
a set of prefixes of all matching strings,
a set of suffixes of all matching strings,
and a match query like the above, often describing the middle of the string.
The rules follow from the meaning of the regular expressions:
</p>

<table>
<tbody><tr><td colspan="4">‘’ (empty string)
</td></tr><tr><td></td><td>
   emptyable(‘’) </td><td>= </td><td> true
</td></tr><tr><td></td><td>
   exact(‘’) </td><td>= </td><td> {‘’}
</td></tr><tr><td></td><td>
   prefix(‘’) </td><td>= </td><td> {‘’}
</td></tr><tr><td></td><td>
   suffix(‘’) </td><td>= </td><td> {‘’}
</td></tr><tr><td></td><td>
   match(‘’) </td><td>= </td><td> <SPAN size="-1">ANY</SPAN> (special query: match all documents)

</td></tr><tr><td colspan="4"><code>c</code> (single character)
</td></tr><tr><td></td><td>
   emptyable(<code>c</code>) </td><td>= </td><td> false
</td></tr><tr><td></td><td>
   exact(<code>c</code>) </td><td>= </td><td> {<code>c</code>}
</td></tr><tr><td></td><td>
   prefix(<code>c</code>) </td><td>= </td><td> {<code>c</code>}
</td></tr><tr><td></td><td>
   suffix(<code>c</code>) </td><td>= </td><td> {<code>c</code>}
</td></tr><tr><td></td><td>
   match(<code>c</code>) </td><td>= </td><td> <SPAN size="-1">ANY</SPAN>

</td></tr><tr><td colspan="4"><i>e</i>?  (zero or one)
</td></tr><tr><td></td><td>
   emptyable(<i>e</i>?) </td><td>= </td><td> true
</td></tr><tr><td></td><td>
   exact(<i>e</i>?) </td><td>= </td><td> exact(<i>e</i>) ∪ {‘’}
</td></tr><tr><td></td><td>
   prefix(<i>e</i>?) </td><td>= </td><td> {‘’}
</td></tr><tr><td></td><td>
   suffix(<i>e</i>?) </td><td>= </td><td> {‘’}
</td></tr><tr><td></td><td>
   match(<i>e</i>?) </td><td>= </td><td> <SPAN size="-1">ANY</SPAN>

</td></tr><tr><td colspan="4"><i>e</i>*  (zero or more)
</td></tr><tr><td></td><td>
   emptyable(<i>e</i>*) </td><td>= </td><td> true
</td></tr><tr><td></td><td>
   exact(<i>e</i>*) </td><td>= </td><td> unknown
</td></tr><tr><td></td><td>
   prefix(<i>e</i>*) </td><td>= </td><td> {‘’}
</td></tr><tr><td></td><td>
   suffix(<i>e</i>*) </td><td>= </td><td> {‘’}
</td></tr><tr><td></td><td>
   match(<i>e</i>*) </td><td>= </td><td> <SPAN size="-1">ANY</SPAN>

</td></tr><tr><td colspan="4"><i>e</i>+  (one or more)
</td></tr><tr><td></td><td>
   emptyable(<i>e</i>+) </td><td>= </td><td> emptyable(<i>e</i>)
</td></tr><tr><td></td><td>
   exact(<i>e</i>+) </td><td>= </td><td> unknown
</td></tr><tr><td></td><td>
   prefix(<i>e</i>+) </td><td>= </td><td> prefix(<i>e</i>)
</td></tr><tr><td></td><td>
   suffix(<i>e</i>+) </td><td>= </td><td> suffix(<i>e</i>)
</td></tr><tr><td></td><td>
   match(<i>e</i>+) </td><td>= </td><td> match(<i>e</i>)

</td></tr><tr><td colspan="4"><i>e</i><sub>1</sub> | <i>e</i><sub>2</sub> (alternation)
</td></tr><tr><td></td><td>
   emptyable(<i>e</i><sub>1</sub> | <i>e</i><sub>2</sub>) </td><td>= </td><td> emptyable(<i>e</i><sub>1</sub>) or emptyable(<i>e</i><sub>2</sub>)
</td></tr><tr><td></td><td>
   exact(<i>e</i><sub>1</sub> | <i>e</i><sub>2</sub>) </td><td>= </td><td> exact(<i>e</i><sub>1</sub>) ∪ exact(<i>e</i><sub>2</sub>)
</td></tr><tr><td></td><td>
   prefix(<i>e</i><sub>1</sub> | <i>e</i><sub>2</sub>) </td><td>= </td><td> prefix(<i>e</i><sub>1</sub>) ∪ prefix(<i>e</i><sub>2</sub>)
</td></tr><tr><td></td><td>
   suffix(<i>e</i><sub>1</sub> | <i>e</i><sub>2</sub>) </td><td>= </td><td> suffix(<i>e</i><sub>1</sub>) ∪ suffix(<i>e</i><sub>2</sub>)
</td></tr><tr><td></td><td>
   match(<i>e</i><sub>1</sub> | <i>e</i><sub>2</sub>) </td><td>= </td><td> match(<i>e</i><sub>1</sub>) <SPAN size="-1">OR</SPAN> match(<i>e</i><sub>2</sub>)

</td></tr><tr><td colspan="4"><i>e</i><sub>1</sub> <i>e</i><sub>2</sub> (concatenation)
</td></tr><tr><td></td><td>
   emptyable(<i>e</i><sub>1</sub><i>e</i><sub>2</sub>) </td><td>= </td><td> emptyable(<i>e</i><sub>1</sub>) and emptyable(<i>e</i><sub>2</sub>)
</td></tr><tr><td></td><td>
   exact(<i>e</i><sub>1</sub><i>e</i><sub>2</sub>) </td><td>= </td><td>exact(<i>e</i><sub>1</sub>) × exact(<i>e</i><sub>2</sub>), if both are known
</td></tr><tr><td></td><td>
   </td><td> </td><td>or       unknown, otherwise
</td></tr><tr><td></td><td>
   prefix(<i>e</i><sub>1</sub><i>e</i><sub>2</sub>) </td><td>= </td><td> exact(<i>e</i><sub>1</sub>) × prefix(<i>e</i><sub>2</sub>), if exact(<i>e</i><sub>1</sub>) is known
</td></tr><tr><td></td><td>
   </td><td> </td><td>or       prefix(<i>e</i><sub>1</sub>) ∪ prefix(<i>e</i><sub>2</sub>), if emptyable(<i>e</i><sub>1</sub>)
</td></tr><tr><td></td><td>
   </td><td> </td><td>or       prefix(<i>e</i><sub>1</sub>), otherwise
</td></tr><tr><td></td><td>
   suffix(<i>e</i><sub>1</sub><i>e</i><sub>2</sub>) </td><td>= </td><td> suffix(<i>e</i><sub>1</sub>) × exact(<i>e</i><sub>2</sub>), if exact(<i>e</i><sub>2</sub>) is known
</td></tr><tr><td></td><td>
   </td><td> </td><td>or       suffix(<i>e</i><sub>2</sub>) ∪ suffix(<i>e</i><sub>1</sub>), if emptyable(<i>e</i><sub>2</sub>)
</td></tr><tr><td></td><td>
   </td><td> </td><td>or       suffix(<i>e</i><sub>2</sub>), otherwise
</td></tr><tr><td></td><td>
   match(<i>e</i><sub>1</sub><i>e</i><sub>2</sub>) </td><td>= </td><td> match(<i>e</i><sub>1</sub>) <SPAN size="-1">AND</SPAN> match(<i>e</i><sub>2</sub>)
</td></tr></tbody></table>

<p>
The rules as described above are correct but would not produce
very interesting match queries, and the various string sets could
get exponentially large depending on the regular expression.
At each step, we can apply some simplifications to keep the
computed information manageable.
First, we need a function to compute trigrams.
</p>

<p>
The trigrams function applied to a single string can be <SPAN size="-1">ANY</SPAN>, if the string has fewer than three characters,
or else the <SPAN size="-1">AND</SPAN> of all the trigrams in the string.
The trigrams function applied to a set of strings is the <SPAN size="-1">OR</SPAN> of the trigrams function applied to each
of the strings.
</p>

<p>
(Single string)
</p>
<ul>
<li>trigrams(<code>ab</code>) = <SPAN size="-1">ANY</SPAN>
</li><li>trigrams(<code>abc</code>) = <code>abc</code>
</li><li>trigrams(<code>abcd</code>) = <code>abc</code> <SPAN size="-1">AND</SPAN> <code>bcd</code>
</li><li>trigrams(<code>wxyz</code>) = <code>wxy</code> <SPAN size="-1">AND</SPAN> <code>xyz</code>
</li></ul>

<p>
(Set of strings)
</p>
<ul>
<li>trigrams({<code>ab</code>}) = trigrams(<code>ab</code>) = <SPAN size="-1">ANY</SPAN>
</li><li>trigrams({<code>abcd</code>}) = trigrams(<code>abcd</code>) = <code>abc</code> <SPAN size="-1">AND</SPAN> <code>bcd</code>
</li><li>trigrams({<code>ab</code>, <code>abcd</code>}) = trigrams(<code>ab</code>) <SPAN size="-1">OR</SPAN> trigrams(<code>abcd</code>) = <SPAN size="-1">ANY</SPAN> <SPAN size="-1">OR</SPAN> (<code>abc</code> <SPAN size="-1">AND</SPAN> <code>bcd</code>) = <SPAN size="-1">ANY</SPAN>
</li><li>trigrams({<code>abcd</code>, <code>wxyz</code>}) = trigrams(<code>abcd</code>) <SPAN size="-1">OR</SPAN> trigrams(<code>wxyz</code>) = (<code>abc</code> <SPAN size="-1">AND</SPAN> <code>bcd</code>) <SPAN size="-1">OR</SPAN> (<code>wxy</code> <SPAN size="-1">AND</SPAN> <code>xyz</code>)
</li></ul>

<p>
Using the trigrams function, we can define transformations that apply
at any step during the analysis, to any regular expression <i>e</i>.
These transformations preserve the validity of the computed information
and can be applied as needed to keep the result manageable:
</p>

<p>
(Information-saving)
</p>
<ul>
<li>At any time, set match(<i>e</i>) = match(<i>e</i>) <SPAN size="-1">AND</SPAN> trigrams(prefix(<i>e</i>)).
</li><li>At any time, set match(<i>e</i>) = match(<i>e</i>) <SPAN size="-1">AND</SPAN> trigrams(suffix(<i>e</i>)).
</li><li>At any time, set match(<i>e</i>) = match(<i>e</i>) <SPAN size="-1">AND</SPAN> trigrams(exact(<i>e</i>)).
</li></ul>

<p>
(Information-discarding)
</p>

<ul>
<li>If prefix(<i>e</i>) contains both <i>s</i> and <i>t</i> where <i>s</i> is a prefix of <i>t</i>, discard <i>t</i>.
</li><li>If suffix(<i>e</i>) contains both <i>s</i> and <i>t</i> where <i>s</i> is a suffix of <i>t</i>, discard <i>t</i>.
</li><li>If prefix(<i>e</i>) is too large, chop the last character off the longest strings in prefix(<i>e</i>).
</li><li>If suffix(<i>e</i>) is too large, chop the first character off the longest strings in suffix(<i>e</i>).
</li><li>If exact(<i>e</i>) is too large, set exact(<i>e</i>) = unknown.
</li></ul>

<p>
The best way to apply these transformation is to use the information-saving
transformations immediately before applying an information-discarding transformation.
Effectively, these transformations move information that is about to be
discarded from the prefix, suffix, and exact sets into the match query itself.
On a related note, a little more information can be squeezed from the
concatenation analysis: if <i>e</i><sub>1</sub><i>e</i><sub>2</sub> is not exact,
match(<i>e</i><sub>1</sub><i>e</i><sub>2</sub>) can also require trigrams(suffix(<i>e</i><sub>1</sub>) × prefix(<i>e</i><sub>2</sub>)).
</p>

<p>
In addition to these transformations, it helps to apply basic
Boolean simplifications to the match query as it is constructed:
“<code>abc</code> <SPAN size="-1">OR</SPAN> (<code>abc</code> <SPAN size="-1">AND</SPAN> <code>def</code>)” is more expensive but no
more precise than “<code>abc</code>”.
</p>

<h2 id="impl">Implementation</h2>

<p>
To demonstrate these ideas, I have published a basic implementation,
written in Go, at <a href="https://github.com/google/codesearch">github.com/google/codesearch</a>.
If you have <a href="http://go.dev/">Go installed</a> you can run
</p>
<pre>go install github.com/google/codesearch/cmd/...
</pre>
<p>
to install binaries named <code>cgrep</code>, <code>cindex</code>, and <code>csearch</code>.
If you don&#39;t have Go installed,
you can <a href="https://go.dev/dl">download binary packages</a>
from the Go web site.
</p>

<p>
The first step is to run <code>cindex</code> with a list of
directories or files to include in the index:
</p>

<pre>cindex /usr/include $HOME/src
</pre>

<p>
By default <code>cindex</code> adds to the existing index, if one exists, so the last command
is equivalent to the pair of commands:
</p>

<pre>cindex /usr/include
cindex $HOME/src
</pre>

<p>
With no arguments, <code>cindex</code> refreshes an existing index, so after running the previous commands,
</p>

<pre>cindex
</pre>

<p>
will rescan <code>/usr/include</code> and <code>$HOME/src</code>
and rewrite the index.  Run <code>cindex</code> <code>-help</code> for more details.

</p><p>
<a href="https://github.com/google/codesearch/blob/master/cmd/cindex/cindex.go">The indexer</a> assumes that files are encoded in UTF-8.  It rejects
files that are unlikely to be interesting, such as those that contain
invalid UTF-8, or that have very long lines, or that have a very large
number of distinct trigrams.
</p>

<p>
The index file contains a list of paths (for reindexing, as described above),
a list of indexed files, and then the same posting lists we saw at the
beginning of this article, one for each trigram.
In practice, this index tends to be around 20% of the size of the files being indexed.
For example, indexing the <a href="http://www.kernel.org/pub/linux/kernel/v3.0/">Linux 3.1.3 kernel sources</a>, a total of 420 MB, creates a 77 MB index.  Of course, the index is organized so that only a small fraction needs to be read for any particular search.
</p>

<p>
Once the index has been written, run <code>csearch</code> to search:
</p>

<pre>csearch [-c] [-f <i>fileregexp</i>] [-h] [-i] [-l] [-n] <i>regexp</i>
</pre>

<p>
The regexp syntax is RE2&#39;s, which is to say
<a href="https://github.com/google/re2/wiki/Syntax">basically Perl&#39;s, but without backreferences</a>.
(RE2 does support capturing parentheses.
Backreferences are when those submatches are substituted back into the regular expression during the match.)
The boolean command-line flags are like <code>grep</code>&#39;s,
except that as is standard for Go programs, there is no
distinction between short and long options, so options
cannot be combined: it is <code>csearch -i -n</code>, not <code>csearch -in</code>.
The new <code>-f</code> flag causes <code>csearch</code> to
consider only files with paths matching <i>fileregexp</i>.
</p>

<pre>$ csearch -f /usr/include DATAKIT
/usr/include/bsm/audit_domain.h:#define	BSM_PF_DATAKIT		9
/usr/include/gssapi/gssapi.h:#define GSS_C_AF_DATAKIT    9
/usr/include/sys/socket.h:#define	AF_DATAKIT	9		/* datakit protocols */
/usr/include/sys/socket.h:#define	PF_DATAKIT	AF_DATAKIT
$
</pre>

<p>
The <code>-verbose</code> flag causes <code>csearch</code> to report
statistics about the search.  The <code>-brute</code> flag bypasses the trigram index,
searching every file listed in the index instead of using a precise trigram query.
In the case of the Datakit query,
it turns out that the trigram query narrows the search down to just
three files, all of which are matches.
</p>

<pre>$ time csearch -verbose -f /usr/include DATAKIT
2011/12/10 00:23:24 query: &#34;AKI&#34; &#34;ATA&#34; &#34;DAT&#34; &#34;KIT&#34; &#34;TAK&#34;
2011/12/10 00:23:24 post query identified 3 possible files
/usr/include/bsm/audit_domain.h:#define	BSM_PF_DATAKIT		9
/usr/include/gssapi/gssapi.h:#define GSS_C_AF_DATAKIT    9
/usr/include/sys/socket.h:#define	AF_DATAKIT	9		/* datakit protocols */
/usr/include/sys/socket.h:#define	PF_DATAKIT	AF_DATAKIT
0.00u 0.00s 0.00r
$
</pre>

<p>
In contrast, without the index we&#39;d have to search 2,739 files:
</p>

<pre>$ time csearch -brute -verbose -f /usr/include DATAKIT
2011/12/10 00:25:02 post query identified 2739 possible files
/usr/include/bsm/audit_domain.h:#define	BSM_PF_DATAKIT		9
/usr/include/gssapi/gssapi.h:#define GSS_C_AF_DATAKIT    9
/usr/include/sys/socket.h:#define	AF_DATAKIT	9		/* datakit protocols */
/usr/include/sys/socket.h:#define	PF_DATAKIT	AF_DATAKIT
0.08u 0.03s 0.11r  # brute force
$
</pre>

<p>
(I am using <code>/usr/include</code> on an OS X Lion laptop.
You may get slightly different results on your system.)
</p>

<p>
As a larger example, we can search for <code>hello world</code> in the Linux 3.1.3 kernel.
The trigram index narrows the search from 36,972 files to 25 files and cuts the time required
for the search by about 100x.
</p>

<pre>$ time csearch -verbose -c &#39;hello world&#39;
2011/12/10 00:31:16 query: &#34; wo&#34; &#34;ell&#34; &#34;hel&#34; &#34;llo&#34; &#34;lo &#34; &#34;o w&#34; &#34;orl&#34; &#34;rld&#34; &#34;wor&#34;
2011/12/10 00:31:16 post query identified 25 possible files
/Users/rsc/pub/linux-3.1.3/Documentation/filesystems/ramfs-rootfs-initramfs.txt: 2
/Users/rsc/pub/linux-3.1.3/Documentation/s390/Debugging390.txt: 3
/Users/rsc/pub/linux-3.1.3/arch/blackfin/kernel/kgdb_test.c: 1
/Users/rsc/pub/linux-3.1.3/arch/frv/kernel/gdb-stub.c: 1
/Users/rsc/pub/linux-3.1.3/arch/mn10300/kernel/gdb-stub.c: 1
/Users/rsc/pub/linux-3.1.3/drivers/media/video/msp3400-driver.c: 1
0.01u 0.00s 0.01r
$

$ time csearch -brute -verbose -h &#39;hello world&#39;
2011/12/10 00:31:38 query: &#34; wo&#34; &#34;ell&#34; &#34;hel&#34; &#34;llo&#34; &#34;lo &#34; &#34;o w&#34; &#34;orl&#34; &#34;rld&#34; &#34;wor&#34;
2011/12/10 00:31:38 post query identified 36972 possible files
/Users/rsc/pub/linux-3.1.3/Documentation/filesystems/ramfs-rootfs-initramfs.txt: 2
/Users/rsc/pub/linux-3.1.3/Documentation/s390/Debugging390.txt: 3
/Users/rsc/pub/linux-3.1.3/arch/blackfin/kernel/kgdb_test.c: 1
/Users/rsc/pub/linux-3.1.3/arch/frv/kernel/gdb-stub.c: 1
/Users/rsc/pub/linux-3.1.3/arch/mn10300/kernel/gdb-stub.c: 1
/Users/rsc/pub/linux-3.1.3/drivers/media/video/msp3400-driver.c: 1
1.26u 0.42s 1.96r  # brute force
$
</pre>

<p>
For case-insensitive searches, the less precise query means that the speedup
is not as great, but it is still an order of magnitude better than brute force:
</p>

<pre>$ time csearch -verbose -i -c &#39;hello world&#39;
2011/12/10 00:42:22 query: (&#34;HEL&#34;|&#34;HEl&#34;|&#34;HeL&#34;|&#34;Hel&#34;|&#34;hEL&#34;|&#34;hEl&#34;|&#34;heL&#34;|&#34;hel&#34;)
  (&#34;ELL&#34;|&#34;ELl&#34;|&#34;ElL&#34;|&#34;Ell&#34;|&#34;eLL&#34;|&#34;eLl&#34;|&#34;elL&#34;|&#34;ell&#34;)
  (&#34;LLO&#34;|&#34;LLo&#34;|&#34;LlO&#34;|&#34;Llo&#34;|&#34;lLO&#34;|&#34;lLo&#34;|&#34;llO&#34;|&#34;llo&#34;)
  (&#34;LO &#34;|&#34;Lo &#34;|&#34;lO &#34;|&#34;lo &#34;) (&#34;O W&#34;|&#34;O w&#34;|&#34;o W&#34;|&#34;o w&#34;) (&#34; WO&#34;|&#34; Wo&#34;|&#34; wO&#34;|&#34; wo&#34;)
  (&#34;WOR&#34;|&#34;WOr&#34;|&#34;WoR&#34;|&#34;Wor&#34;|&#34;wOR&#34;|&#34;wOr&#34;|&#34;woR&#34;|&#34;wor&#34;)
  (&#34;ORL&#34;|&#34;ORl&#34;|&#34;OrL&#34;|&#34;Orl&#34;|&#34;oRL&#34;|&#34;oRl&#34;|&#34;orL&#34;|&#34;orl&#34;)
  (&#34;RLD&#34;|&#34;RLd&#34;|&#34;RlD&#34;|&#34;Rld&#34;|&#34;rLD&#34;|&#34;rLd&#34;|&#34;rlD&#34;|&#34;rld&#34;)
2011/12/10 00:42:22 post query identified 599 possible files
/Users/rsc/pub/linux-3.1.3/Documentation/filesystems/ramfs-rootfs-initramfs.txt: 3
/Users/rsc/pub/linux-3.1.3/Documentation/java.txt: 1
/Users/rsc/pub/linux-3.1.3/Documentation/s390/Debugging390.txt: 3
/Users/rsc/pub/linux-3.1.3/arch/blackfin/kernel/kgdb_test.c: 1
/Users/rsc/pub/linux-3.1.3/arch/frv/kernel/gdb-stub.c: 1
/Users/rsc/pub/linux-3.1.3/arch/mn10300/kernel/gdb-stub.c: 1
/Users/rsc/pub/linux-3.1.3/arch/powerpc/platforms/powermac/udbg_scc.c: 1
/Users/rsc/pub/linux-3.1.3/drivers/media/video/msp3400-driver.c: 1
/Users/rsc/pub/linux-3.1.3/drivers/net/sfc/selftest.c: 1
/Users/rsc/pub/linux-3.1.3/samples/kdb/kdb_hello.c: 2
0.07u 0.01s 0.08r
$

$ time csearch -brute -verbose -i -c &#39;hello world&#39;
2011/12/10 00:42:33 post query identified 36972 possible files
/Users/rsc/pub/linux-3.1.3/Documentation/filesystems/ramfs-rootfs-initramfs.txt: 3
/Users/rsc/pub/linux-3.1.3/Documentation/java.txt: 1
/Users/rsc/pub/linux-3.1.3/Documentation/s390/Debugging390.txt: 3
/Users/rsc/pub/linux-3.1.3/arch/blackfin/kernel/kgdb_test.c: 1
/Users/rsc/pub/linux-3.1.3/arch/frv/kernel/gdb-stub.c: 1
/Users/rsc/pub/linux-3.1.3/arch/mn10300/kernel/gdb-stub.c: 1
/Users/rsc/pub/linux-3.1.3/arch/powerpc/platforms/powermac/udbg_scc.c: 1
/Users/rsc/pub/linux-3.1.3/drivers/media/video/msp3400-driver.c: 1
/Users/rsc/pub/linux-3.1.3/drivers/net/sfc/selftest.c: 1
/Users/rsc/pub/linux-3.1.3/samples/kdb/kdb_hello.c: 2
1.24u 0.34s 1.59r  # brute force
$
</pre>

<p>
To minimize I/O and take advantage of operating system caching,
<code>csearch</code> uses <code>mmap</code> to map the index into memory and
in doing so read directly from the operating system&#39;s file cache.
This makes <code>csearch</code> run quickly on repeated runs without using
a server process.
</p>

<p>
The source code for these tools is at
<a href="https://github.com/google/codesearch/">github.com/google/codesearch</a>.
The files illustrating the techniques from this article are
<a href="https://github.com/google/codesearch/blob/master/index/regexp.go"><code>index/regexp.go</code></a> (regexp to query),
<a href="https://github.com/google/codesearch/blob/master/index/read.go"><code>index/read.go</code></a> (reading from index),
and
<a href="https://github.com/google/codesearch/blob/master/index/write.go"><code>index/write.go</code></a> (writing an index).
</p>

<p>
The code also includes, in <a href="https://github.com/google/codesearch/blob/master/regexp/match.go"><code>regexp/match.go</code></a>, a custom DFA-based matching engine tuned just for the <code>csearch</code> tool.
The only thing this engine is good for is implementing
grep, but that it does very quickly.  Because it can call on
the <a href="https://go.dev/pkg/regexp/syntax">standard Go package</a>
to parse regular expressions and boil them down to basic operations,
the new matcher is under 500 lines of code.
</p>

<h2 id="history">History</h2>

<p>
Neither <i>n</i>-grams nor their application to pattern matching is new.

Shannon used <i>n</i>-grams in his seminal 1948 paper “A Mathematical Theory of
Communication” [<a href="#1">1</a>] to analyze the information content of English text.
Even then, <i>n</i>-grams were old hat.  Shannon cites Pratt&#39;s 1939 book
<i>Secret and Urgent</i> [<a href="#2">2</a>], and one imagines that Shannon used <i>n</i>-gram analysis during his wartime cryptography work.
</p>

<p>
Zobel, Moffat, and Sacks-Davis&#39;s 1993 paper “Searching Large Lexicons
for Partially Specified Terms using Compressed Inverted Files” [<a href="#3">3</a>]
describes how to use an inverted index of the <i>n</i>-grams in a set of words
(a lexicon) to map a pattern like <code>fro*n</code> to matches like
<code>frozen</code> or <code>frogspawn</code>.  Witten, Moffat,
and Bell&#39;s 1994 classic book <i>Managing Gigabytes</i> [<a href="#4">4</a>] summarized
the same approach.
Zobel et al.&#39;s paper mentions that the technique can be applied to a richer
pattern language than just <code>*</code> wildcards, but only demonstrates
a simple character class:
</p>

<blockquote>
Note that <i>n</i>-grams can be used to support other kinds of pattern matching.
For example, if <i>n</i> = 3 and patterns contain sequences such as <code>ab[cd]e</code>,
where the square brackets denote that the character between <code>b</code> and
<code>e</code> must be either <code>c</code> or <code>d</code>, then matches
can be found by looking for strings containing either <code>abc</code> and <code>bce</code> or <code>abd</code> and <code>bde</code>.
</blockquote>

<p>
The main difference between the Zobel paper and our implementation is that a gigabyte
is no longer a large amount of data, so instead of applying <i>n</i>-gram indexing
and pattern matching to just the word list, we have the computational resources to
apply it to an entire document set.
The trigram query generation rules given above generate the query
</p>

<pre>(abc <SPAN size="-1">AND</SPAN> bce) <SPAN size="-1">OR</SPAN> (abd <SPAN size="-1">AND</SPAN> bde)
</pre>

<p>
for the regular expression <code>ab[cd]e</code>.  If the implementation
chose to apply the simplifying transformations more aggressively, it would use
a smaller memory footprint but arrive at
</p>

<pre>(abc <SPAN size="-1">OR</SPAN> abd) <SPAN size="-1">AND</SPAN> (bce <SPAN size="-1">OR</SPAN> bde)
</pre>

<p>
instead.  This second query is equally valid but less precise, demonstrating
the tradeoff between memory usage and precision.
</p>

<h2 id="summary">Summary</h2>

<p>
Regular expression matching over large numbers of small documents
can be made fast by using an index of the trigrams present in each
document.
The use of trigrams for this purpose is not new, but neither is it well known.
</p>

<p>
Despite all their
<a href="https://github.com/google/re2/wiki/Syntax">apparent syntactic complexity</a>,
regular expressions in the mathematical sense of the term
can always be reduced to the few cases (empty string, single character,
repetition, concatenation, and alternation) considered above.
This underlying simplicity makes it possible to implement efficient
search algorithms
like the ones in the <a href="https://swtch.com/~rsc/regexp/regexp1.html">first</a> <a href="https://swtch.com/~rsc/regexp/regexp2.html">three</a> <a href="https://swtch.com/~rsc/regexp/regexp3.html">articles</a> in this series.
The analysis above, which converts a regular expression into a trigram query,
is the heart of the indexed matcher, and it is made possible
by the same simplicity.
</p>

<p>
If you miss Google Code Search and want to run fast indexed regular expression
searches over your local code, give the
<a href="https://github.com/google/codesearch/">standalone programs</a>
a try.
</p>

<h2 id="ack">Acknowledgements</h2>

<p>
Thanks to everyone at Google who worked on or supported Code Search over the past five years.  There are too many to name individually, but it was truly a team effort and a lot of fun.
</p>

<h2 id="refs">References</h2>

<p>
[<a name="1"></a>1] Claude Shannon, “<a href="http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf">A Mathematical Theory of Communication</a>,” <i>Bell System Technical Journal</i>, July, October 1948.
</p>

<p>
[<a name="2"></a>2] Fletcher Pratt, <i><a href="http://books.google.com/books?id=SnS7bwAACAAJ">Secret and Urgent: The Story of Codes and Ciphers</a></i>, 1939.  Reprinted by Aegean Park Press, 1996.
</p>

<p>
[<a name="3"></a>3] Justin Zobel, Alistair Moffat, and Ron Sacks-Davis, “<a href="http://www.vldb.org/conf/1993/P290.PDF">Searching Large Lexicons for Partially Specified Terms using Compressed Inverted Files,</a>”
<i>Proceedings of the 19th VLDB Conference</i>, 1993.
</p>

<p>
[<a name="4"></a>4] Ian Witten, Alistair Moffat, and Timothy Bell, <i><a href="http://books.google.com/books?id=2F74jyPl48EC">Managing Gigabytes: Compressing and Indexing Documents and Images</a></i>, 1994.  Second Edition, 1999.
</p>

<!--
<p class=lp-left>
Discussion on <a href="XXX">XXX</a>.
</p>
-->







</div>
  </body>
</html>
