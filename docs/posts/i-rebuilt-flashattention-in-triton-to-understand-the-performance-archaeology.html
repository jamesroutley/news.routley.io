<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://aminediro.com/posts/flash_attn/">Original</a>
    <h1>I rebuilt FlashAttention in Triton to understand the performance archaeology</h1>
    
    <div id="readability-page-1" class="page"><div>
                <blockquote>
<p>⏲️ Estimated reading time ~45min.</p>
</blockquote>

<p>Flash Attention has become one of the most impactful optimizations in modern deep learning. Since the original paper was published in 2022, we’ve seen four major versions—each squeezing more performance out of increasingly powerful hardware. But here’s the thing: reading papers is one thing, understanding <em>why</em> these optimizations were made is another entirely.</p>
<p>My goal here is simple: start from first principles, implement FlashAttention v1 exactly as described in the paper, profile it, find the bottlenecks, and see how far we can push it. We’ll build intuition by iterating on the algorithm, discovering through profiling exactly why v2, v3, and v4 were necessary. Think of this as archaeology—digging through the performance layers to understand what each version was really solving.</p>
<p>So let’s put ourselves in the shoes of that mythical Stanford grad student. You’ve finally finished configuring your neovim and archlinux setup (a multi-year endeavor, naturally). You open up a fresh LLaMA model to peek under the hood. Text goes in, gets tokenized, embedded, then flows through a stack of transformer blocks. Standard stuff. But then you look closer at the attention mechanism—three projections and then… there it is:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span>scores <span>=</span> torch<span>.</span>matmul(q, k<span>.</span>transpose(<span>3</span>, <span>2</span>)) <span>/</span> math<span>.</span>sqrt(self<span>.</span>head_dim)
</span></span><span><span>scores <span>=</span> F<span>.</span>softmax(scores, dim<span>=-</span><span>1</span>)
</span></span><span><span>output <span>=</span> torch<span>.</span>matmul(scores, v)  <span># (B, N_h, T, D_h)</span>
</span></span></code></pre></div><p>This monstrosity of a code is staring you right in the face. For those who don’t immediately see the problem with these 4 lines, let me add some annotations on how this would normally execute in PyTorch (without compilation).</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># 1. We load q `(B, N_h, S, D_h)` and k `(B, N_h, S, D_h)`</span>
</span></span><span><span><span># 2. We compute Q.Kt and write it back to HBM. Note score is `(B, N_h, S, S)`.</span>
</span></span><span><span>scores <span>=</span> torch<span>.</span>matmul(q, k<span>.</span>transpose(<span>3</span>, <span>2</span>)) <span>/</span> math<span>.</span>sqrt(self<span>.</span>head_dim)
</span></span><span><span><span># 3. Reload the scores tensor to compute the softmax and write it back to HBM</span>
</span></span><span><span>scores <span>=</span> F<span>.</span>softmax(scores, dim<span>=-</span><span>1</span>)
</span></span><span><span><span># 4. Load v `(B, N_h, S, D_h)`, load the scores from HBM</span>
</span></span><span><span><span># 5. Compute scores@v and write it back to HBM.</span>
</span></span><span><span>output <span>=</span> torch<span>.</span>matmul(scores, v)  <span># `(B, N_h, T, D_h)`</span>
</span></span></code></pre></div><p>Do you see it now? Well, we have three tensors <code>q</code>, <code>k</code>, <code>v</code> each of dimension <code>(B, N_h, T, D_h)</code><sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>. The output tensor of the attention mechanism is <code>(B, N_h, T, D_h)</code>, and somehow in the middle we had to materialize a <code>(B, N_h, S, S)</code> tensor for funsies. The attention mechanism has a critical bottleneck: <strong>quadratic memory complexity</strong>. Let’s take a standard training sequence length <code>S=8192</code>. Computing attention naively requires <code>O(S²)</code> memory to store the full attention matrix, which means consuming several gigabytes of GPU memory. Crucially, in modern transformers, <code>S &gt;&gt; D_h</code> (sequence length is much larger than head dimension) - we typically have <code>S=8192</code> or more while <code>D_h=64</code> or <code>128</code>. This massive asymmetry is what makes Flash Attention algorithm possible. The second big issue here is the back and forth to HBM. Modern GPUs have compute throughput vastly exceeding memory bandwidth. Repeatedly reading Q, K, V, and scores from slow High Bandwidth Memory (HBM) is going to greatly impact performance (more on this later).</p>
<p>The whole idea of Flash Attention is to bypass these intermediate steps—i.e., go from tensors q, k, v to the output tensor directly and compute the attention in one go, with minimal memory footprint (materializing only the tensors we need) and minimal back and forth to HBM, and hopefully getting to <code>O(S)</code> memory complexity.</p>
<blockquote>
<p><strong>The Plan</strong>: Start with FlashAttention v1 from the original paper. Implement it faithfully in Triton, profile with NVIDIA’s tooling, identify the bottlenecks, then iterate. Each optimization will teach us something about the GPU memory hierarchy and why the subsequent versions (v2, v3, v4) introduced the changes they did. Four versions represents an enormous amount of engineering work—let’s see how much of that journey we can reconstruct by just following the profiler’s breadcrumbs.</p>
</blockquote>
<h2 id="setup">Setup and Hardware</h2>
<p>First things first, I’ll use <code>triton</code> to implement the attention kernels. I wanted to implement them in CUDA but I thought this was a good opportunity to learn <code>triton</code>. I’m always skeptical about DSLs and abstractions that promise “write once, run fast everywhere,” but like my very wise friend <a href="https://fleetwood.dev/">Chris Fleetwood</a> said, you can’t go wrong learning something shipped inside PyTorch. Running <code>triton</code> is also extremely straightforward and reduces the mess of boilerplate and C++ code that you have to write when implementing CUDA kernels, especially if you want to call them from Python. I actually went back and reimplemented Flash Attention in CUDA to have a bit more control, but that will come later (a little teasing).</p>
<p>I’ll compare these kernels directly with a reference PyTorch implementation, first to make sure the kernel is doing what it’s supposed to, and second to profile the kernel against the baseline.</p>
<h3 id="why-triton-block-level-programming-without-thread-hell">Why Triton? Block-level Programming Without Thread Hell</h3>
<p>Triton is a Python-based DSL for writing GPU kernels. The pitch is simple: write Python-like code at the block level, and the compiler handles individual thread management, memory coalescing, and other low-level optimizations. What makes Triton fundamentally different from CUDA is the abstraction level - in CUDA, you explicitly program individual threads of a block. In Triton, you write your kernel thinking about blocks/tiles of data a little bit how you’d write torch code, and the compiler figures out how to map that to threads. It handles threads data access pattern, synchronization barriers and work splitting. My skepticism comes from the fact that I don’t really trust these “magic” compilers for running parallel code. Usually, the promised performance falls apart quickly. But Triton has a few things going for it:</p>
<ol>
<li>It’s maintained by OpenAI and ships with PyTorch 2.0+</li>
<li>The generated PTX is easily inspectable, so you can see what it’s actually doing</li>
<li>It handles the tedious bits (pointer arithmetic, bounds checking) while still giving you control over the algorithm. You write your kernel at block level and the compilers issues the correct code to load your tensors in.</li>
</ol>
<h3 id="setup-1">Setup</h3>
<p>I’m running this on my personal machine:</p>
<ul>
<li><strong>OS</strong>: Arch Linux (btw)</li>
<li><strong>GPU</strong>: NVIDIA GeForce RTX 2070 (8 GB VRAM)</li>
<li><strong>Driver</strong>: 580.95.05</li>
<li><strong>CUDA</strong>: 13.0</li>
<li><strong>Triton</strong>: 3.5</li>
</ul>
<p>Here are the GPU specs from <code>cudaGetDeviceProperties</code>. These numbers will be useful once we start profiling our kernel to detect performance issues:</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Name</strong></td>
<td>NVIDIA GeForce RTX 2070</td>
</tr>
<tr>
<td><strong>Compute Capability</strong></td>
<td>7.5</td>
</tr>
<tr>
<td><strong>Total Memory</strong></td>
<td>8 GB</td>
</tr>
<tr>
<td><strong>MultiProcessor Count (SM_count)</strong></td>
<td>36</td>
</tr>
<tr>
<td><strong>Max Threads per MultiProcessor</strong></td>
<td>1024</td>
</tr>
<tr>
<td><strong>Warp Size</strong></td>
<td>32</td>
</tr>
<tr>
<td><strong>L2 Cache Size</strong></td>
<td>4 MB</td>
</tr>
<tr>
<td><strong>Shared Memory per Block</strong></td>
<td>48 KB</td>
</tr>
<tr>
<td><strong>Shared Memory per MultiProcessor</strong></td>
<td>64 KB</td>
</tr>
<tr>
<td><strong>Registers per MultiProcessor</strong></td>
<td>65536</td>
</tr>
</tbody>
</table>
<h3 id="profiling-tools">Profiling Tools</h3>
<p>For profiling, I use three tools at different granularities:</p>
<ol>
<li>
<p><strong><code>torch.profiler</code></strong>: Quick and dirty. Good for seeing wall-clock time and basic GPU utilization. I use this for initial sanity checks.</p>
</li>
<li>
<p><strong>NVIDIA Nsight Systems (<code>nsys</code>)</strong>: System-wide profiler. Shows CPU/GPU timeline, kernel launches, memory transfers. Great for spotting gaps where the GPU is idle.</p>
</li>
<li>
<p><strong>NVIDIA Nsight Compute (<code>ncu</code>)</strong>: The heavy hitter. Gives you everything: occupancy, memory throughput, warp stalls, instruction mix, bank conflicts. This is what I’ll use for deep-diving into kernel performance. You can run it with:</p>
</li>
</ol>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sudo ncu --set full --kernel-name <span>&#34;attn_kernel&#34;</span> -o profile_output -f python script.py
</span></span></code></pre></div><p>Then open the <code>.ncu-rep</code> file with <code>ncu-ui</code> for the full analysis.</p>
<h2 id="the-flash-attention-algorithm">The Flash Attention Algorithm</h2>
<p>For modern GPUs, compute throughput vastly exceeds memory bandwidth - an A100 can do ~300 TFLOPs but only has ~2 TB/s of memory bandwidth. This massive compute-to-memory ratio means that naive algorithms spending most of their time waiting for memory, not computing. Flash Attention restructures the computation to maximize arithmetic intensity (FLOPs per byte transferred).</p>
<p>Let’s build the intuition behind the core ideas from walking back from the solution.
The goal is to one shot (single kernel) build the output tensor.</p>
<p>We are in GPU programming land. If you want to minimize transfers to main memory—like any good GPU engineer who has read <a href="https://siboehm.com/articles/22/CUDA-MMM">Simon’s matmul blog post</a> (stop reading this post and go read it. I am serious. It’s the best matmul writeup on the internet.)—you’d immediately think about computing blocks of output in parallel and writing each block to memory.</p>
<p>This is commonly known as <strong>tiling</strong>, and every GPU kernel out there uses this trick. The idea is simple but powerful: instead of streaming data directly from slow global memory for each operation, you load a tile (a small block of data) into fast shared memory once, reuse it across multiple computations within a thread block, and only write the final results back. This dramatically reduces memory bandwidth bottlenecks by exploiting data locality and the GPU’s memory hierarchy—exactly what you need when global memory access is orders of magnitude slower than compute.</p>
<p>Let’s first look at how the output tensor is computed based on the attention formula. Let’s focus on a single batch and head <code>(B, N_h)</code> to clearly see the shapes of the matmuls and the elements. Also, we’ll omit the causal masking here (although it’s not really complicated to add it to the attention score before softmax).</p>

<p><img src="https://aminediro.com/posts/flash_attn/images/flashattn_output.png" alt="output tensor"/>
  <img src="https://aminediro.com/posts/flash_attn/images/dark_flashattn_output.png" alt="output tensor"/>
</p>
<p>If you have been following along, you’re probably scratching your head wondering why we are sharding the <code>V</code> tensor <strong>row-wise</strong>. Sorry for pulling a quick one here, but this formulation of the output (using the rows of V instead of the columns) will actually help us build the flash attention algorithm. For each row <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>O</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">O_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>O</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span> of the output:</p>
<p><span><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>O</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><munder><mo>∑</mo><mi>j</mi></munder><msup><mi>e</mi><msub><mi>S</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></msup><msub><mi>V</mi><mi>j</mi></msub></mrow><mrow><munder><mo>∑</mo><mi>j</mi></munder><msup><mi>e</mi><msub><mi>S</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></msup></mrow></mfrac><mspace width="1em"></mspace><mtext>where</mtext><mspace width="1em"></mspace><msub><mi>S</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><msub><mi>Q</mi><mi>i</mi></msub><mo>⋅</mo><msubsup><mi>K</mi><mi>j</mi><mi mathvariant="normal">⊤</mi></msubsup></mrow><annotation encoding="application/x-tex">
O_i = \frac{\sum_j e^{S_{ij}} V_j}{\sum_j e^{S_{ij}}} \quad\text{where}\quad S_{ij} = Q_i \cdot K_j^{\top}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>O</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>∑</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>S</span><span><span><span><span><span><span></span><span><span><span>ij</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span><span></span><span></span></span><span><span></span><span><span><span>∑</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>S</span><span><span><span><span><span><span></span><span><span><span>ij</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span></span><span><span>where</span></span><span></span><span><span>S</span><span><span><span><span><span><span></span><span><span><span>ij</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>Q</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>⋅</span><span></span></span><span><span></span><span><span>K</span><span><span><span><span><span><span></span><span><span>j</span></span></span><span><span></span><span><span><span>⊤</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></p>
<p>Still, why split <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">V_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span> row-wise and <strong>not column-wise</strong>? First of all, sharding on the <code>D</code> dim makes less. Like I pointed out earlier, <code>D</code> is usually 64 or 128 whereas <code>S</code> sequence length could be 8192 or more depending on the context window. If we tried to split <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>V</span></span></span></span></span> column-wise (along <code>D</code>), the math would look like this:</p>
<p><span><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">…</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>P</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">…</mo></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>×</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>V</mi><mtext>left</mtext></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>V</mi><mtext>right</mtext></msub></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>P</mi><mo>⋅</mo><msub><mi>V</mi><mtext>left</mtext></msub></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>P</mi><mo>⋅</mo><msub><mi>V</mi><mtext>right</mtext></msub></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">
\begin{bmatrix}
\dots &amp; P_{ij} &amp; \dots
\end{bmatrix}
\times
\begin{bmatrix}
V_{\text{left}} &amp; V_{\text{right}}
\end{bmatrix}
=
\begin{bmatrix}
P \cdot V_{\text{left}} &amp; P \cdot V_{\text{right}}
\end{bmatrix}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>[</span></span><span><span><span><span><span><span><span><span></span><span><span>…</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span></span><span><span><span><span><span><span></span><span><span><span>P</span><span><span><span><span><span><span></span><span><span><span>ij</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span></span><span><span><span><span><span><span></span><span><span>…</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span><span><span>]</span></span></span><span></span><span>×</span><span></span></span><span><span></span><span><span><span>[</span></span><span><span><span><span><span><span><span><span></span><span><span><span>V</span><span><span><span><span><span><span></span><span><span><span><span>left</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span></span><span><span><span><span><span><span></span><span><span><span>V</span><span><span><span><span><span><span></span><span><span><span><span>right</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span><span><span>]</span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span><span>[</span></span><span><span><span><span><span><span><span><span></span><span><span>P</span><span></span><span>⋅</span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span><span><span>left</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span></span><span><span><span><span><span><span></span><span><span>P</span><span></span><span>⋅</span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span><span><span>right</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span><span><span>]</span></span></span></span></span></span></span></span></p>
<p>This would force us to load the entire score matrix <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>Q</mi><mi mathvariant="normal">.</mi><msup><mi>K</mi><mi>t</mi></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P=softmax(Q.K^t)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>P</span><span></span><span>=</span><span></span></span><span><span></span><span>so</span><span>f</span><span>t</span><span>ma</span><span>x</span><span>(</span><span>Q</span><span>.</span><span><span>K</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span></span></span></span></span><span>)</span></span></span></span></span> just to compute the left half of the output. But we can’t store the entire <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>P</span></span></span></span></span> matrix—that’s the exact problem FlashAttention solves! By splitting row-wise:</p>
<p><span><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnlines="solid" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>P</mi><mrow><mi>i</mi><mn>0</mn></mrow></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>P</mi><mrow><mi>i</mi><mn>1</mn></mrow></msub></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>×</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em" rowlines="solid"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>V</mi><mn>0</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>V</mi><mn>1</mn></msub></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>=</mo><mo stretchy="false">(</mo><msub><mi>P</mi><mrow><mi>i</mi><mn>0</mn></mrow></msub><mo>⋅</mo><msub><mi>V</mi><mn>0</mn></msub><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">(</mo><msub><mi>P</mi><mrow><mi>i</mi><mn>1</mn></mrow></msub><mo>⋅</mo><msub><mi>V</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">
\left[ \begin{array}{c|c} P_{i0} &amp; P_{i1} \end{array} \right] \times \begin{bmatrix} V_0 \\ \hline V_1 \end{bmatrix} = (P_{i0} \cdot V_0) + (P_{i1} \cdot V_1)
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>[</span></span><span><span><span></span><span><span><span><span><span><span></span><span><span><span>P</span><span><span><span><span><span><span></span><span><span><span>i</span><span>0</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span></span><span></span><span><span><span><span><span><span></span><span><span><span>P</span><span><span><span><span><span><span></span><span><span><span>i</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span></span><span><span>]</span></span></span><span></span><span>×</span><span></span></span><span><span></span><span><span><span>[</span></span><span><span><span><span><span><span></span><span><span><span><span><span><span><span></span><span><span><span>V</span><span><span><span><span><span><span></span><span><span>0</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><span><span>V</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span><span><span></span><span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span><span>]</span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>(</span><span><span>P</span><span><span><span><span><span><span></span><span><span><span>i</span><span>0</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>⋅</span><span></span></span><span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>0</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span></span><span>+</span><span></span></span><span><span></span><span>(</span><span><span>P</span><span><span><span><span><span><span></span><span><span><span>i</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>⋅</span><span></span></span><span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span></span>
</p>
<blockquote>
<p>Now, the genius piece of flash attention is this: if we found a way to build <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>O</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">O_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>O</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span> iteratively for a given <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>i</span></span></span></span></span>, we could calculate a small chunk <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{ij}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>P</span><span><span><span><span><span><span></span><span><span><span>ij</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>, multiply by <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">V_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>, add it to the sum, and discard <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{ij}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>P</span><span><span><span><span><span><span></span><span><span><span>ij</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>. This means we are only allocating a small block of memory!</p>
</blockquote>
<p>But why does this matter anyway? Isn’t it still mathematically equivalent if we compute the whole <code>P_i</code> row? Well, if you live in some mythical mathematical idea world, yes. But this code runs on hardware, and when and how we access memory does matter a LOT!</p>
<h3 id="quick-detour-into-gpu-memory-land">Quick Detour into GPU Memory Land</h3>
<p>Understanding GPU memory is crucial, especially because GPUs are <a href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_threads"><strong>SIMT</strong> (Single Instruction, Multiple Thread)</a> machines. If you’re familiar with SIMD programming on the CPU side, you usually need to use explicit vector instructions to pack and align vectors before executing instructions. SIMT, on the other hand, lets you write scalar thread code that the GPU transparently executes as a lockstep vector, coalescing loads automatically when memory is well-aligned.</p>
<p>GPUs execute thousands of lightweight threads in lockstep (called warps). Multiple warps form a block, and blocks are scheduled into SMs. Looking back at the <a href="#setup">Setup section</a>, we can see that my RTX 2070 has 36 SMs (streaming multiprocessors), and each SM can handle 1024 threads (32 warps) concurrently.</p>
<p>GPU cores are simple and rely on this massive parallelism to <strong>hide latency</strong>—whenever one warp stalls waiting on memory, the scheduler swaps in another warp that already has its data “in flight.” This only works if kernels expose enough parallelism and if memory accesses are structured to take advantage of locality<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>. A core part of the CUDA programming model is that it exposes the <a href="https://modal.com/gpu-glossary/device-software/memory-hierarchy">memory hierarchy directly</a>, forcing the programmer (or compiler) to reason explicitly about where data lives and how far it is from the compute units. Memory proximity matters enormously:</p>
<ul>
<li>
<p><strong>DRAM/HBM (High Bandwidth Memory)</strong>: I don’t have HBM on my RTX 2070, but even on high-end cards like the A100 with large capacity (~80GB), this memory is relatively far from the SMs. Despite impressive raw bandwidth (~1–2 TB/s), latency is high, so naive repeated reads kill performance.</p>
</li>
<li>
<p><strong>L2 Cache:</strong> A chip-wide cache that helps buffer global memory traffic. Still far slower than on-SM memories. L2 cache behavior is typically leveraged implicitly through access patterns rather than explicit optimization.</p>
</li>
<li>
<p><strong>L1 / Shared Memory (SRAM)</strong>: On-SM, extremely fast, and explicitly managed in CUDA. This is where SRAM truly shines compared to HBM. First, the physics: SRAM sits directly on the SM, mere micrometers from the compute units (vs millimeters for HBM - that’s 1000x closer!). It uses 6-transistor flip-flop circuits that hold state without refresh cycles, unlike HBM’s capacitor-based cells. This means SRAM access takes only ~20-30 cycles vs ~200-600 for HBM.</p>
<p>The bandwidth story is even more compelling: each SM gets its own ~164KB of SRAM (48 Kb for my poor RTX 2070) with ~1-2 TB/s bandwidth. With 108 SMs on an A100, that’s theoretically ~100+ TB/s aggregate SRAM bandwidth across the chip, compared to “only” ~2 TB/s to HBM. But here’s the real beauty - SRAM is explicitly programmer-controlled! Unlike HBM which goes through L2 cache, L1 cache, and complex replacement policies you can’t control, with SRAM you orchestrate the exact choreography of data movement with computation. No cache thrashing, no surprise evictions, just <strong>deterministic high-speed access</strong> exactly when you need it.</p>
</li>
<li>
<p><strong>Registers</strong>: The closest memory to the compute units. Tiny (~256KB per SM) but insanely fast (100+ TB/s effective). The compiler allocates these, and register pressure directly impacts occupancy (how many warps can run concurrently, which in turn affects latency hiding).</p>
</li>
</ul>
<blockquote>
<p>Look at this diagram of <strong>memory hierarchy with bandwidth and memory size</strong> taken from Flash Attention v1 very closely, and try to internalize it. It is extremely important to take I/O into account when designing highly performant algorithms.</p>
</blockquote>
<p><img src="https://aminediro.com/posts/flash_attn/images/ta.png" alt="Memory hierarchy and bandwidth"/></p>
<p>If you’re still following along with me, the main insight is to restructure attention so that all intermediate activations fit within these fast memories—registers and shared memory—minimizing slow HBM traffic and maximizing warp-level parallelism. This is why building the output block by block using the same size <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{ij}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>P</span><span><span><span><span><span><span></span><span><span><span>ij</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span> is a core pillar of fast attention. But it is still unclear at this stage how we could build the output incrementally.</p>
<h3 id="online-softmax-mathematics">Online Softmax Mathematics</h3>
<p>Before we go deeper into how we could build these blocks, we need one last quick detour into something I omitted until now for simplification reasons. The last lego block we need in our toolbox before building flash attention is <strong>softmax numerical stability</strong>.</p>
<p><strong>Numerically stable softmax</strong></p>
<p>Until now I wrote the softmax function for a vector <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi>z</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>z</mi><mi>n</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">z = (z_1, \dots, z_n)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>z</span><span></span><span>=</span><span></span></span><span><span></span><span>(</span><span><span>z</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span><span>z</span><span><span><span><span><span><span></span><span><span>n</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span> as:</p>
<p><span><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">softmax</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mfrac><msup><mi>e</mi><msub><mi>z</mi><mi>i</mi></msub></msup><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mi>e</mi><msub><mi>z</mi><mi>j</mi></msub></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">
\operatorname{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>softmax</span></span><span>(</span><span><span>z</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>∑</span><span><span><span><span><span><span></span><span><span><span>j</span><span>=</span><span>1</span></span></span></span><span><span></span><span><span><span>n</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>z</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span><span></span><span></span></span><span><span></span><span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>z</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span></span></span></span></span></span></p>
<p>But what happens if input values <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>z</span></span></span></span></span> are large (e.g., <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>=</mo><mn>1000</mn></mrow><annotation encoding="application/x-tex">z_i = 1000</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>z</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>1000</span></span></span></span></span>)? Computing <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>e</mi><mn>1000</mn></msup></mrow><annotation encoding="application/x-tex">e^{1000}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span>1000</span></span></span></span></span></span></span></span></span></span></span></span></span> will exceed the maximum value a floating-point number can hold, resulting in <code>inf</code> (infinity) or <code>NaN</code> (Not a Number) errors. Conversely, underflow can happen if values are very negative—the denominator might vanish to zero, leading to division by zero errors. The trick is to <em>numerically stabilize</em> by subtracting the maximum logit.</p>
<p>The key property is that <strong>Softmax is invariant to adding or subtracting a constant ( c )</strong>:</p>
<p><span><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">softmax</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo>+</mo><mi>c</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="normal">softmax</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">
\operatorname{softmax}(z_i + c) = \operatorname{softmax}(z_i)
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>softmax</span></span><span>(</span><span><span>z</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>+</span><span></span></span><span><span></span><span>c</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span>softmax</span></span><span>(</span><span><span>z</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span></span></p>
<p>For numerical stability, we usually subtract the maximum value (<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>=</mo><msub><mrow><mi>max</mi><mo>⁡</mo></mrow><mi>j</mi></msub><msub><mi>z</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">m = \max_j z_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>m</span><span></span><span>=</span><span></span></span><span><span></span><span><span>max</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span><span>z</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>). Setting ( c = -m ) ensures that the exponentials are at most 1, preventing overflow :</p>
<p><span><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">softmax</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mfrac><msup><mi>e</mi><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>−</mo><mi>m</mi></mrow></msup><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mi>e</mi><mrow><msub><mi>z</mi><mi>j</mi></msub><mo>−</mo><mi>m</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">
\operatorname{softmax}(z_i) = \frac{e^{z_i - m}}{\sum_{j=1}^{n} e^{z_j - m}}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>softmax</span></span><span>(</span><span><span>z</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>∑</span><span><span><span><span><span><span></span><span><span><span>j</span><span>=</span><span>1</span></span></span></span><span><span></span><span><span><span>n</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>z</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>−</span><span>m</span></span></span></span></span></span></span></span></span></span></span><span><span></span><span></span></span><span><span></span><span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>z</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>−</span><span>m</span></span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span></span></span></span></span></span></p>
<p>So let’s go back to incrementally computing a row of the output <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>O</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">O_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>O</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>. Let’s start slow to build the intuition. We take row <code>i=0</code> and <code>j=0</code>. This means we are focusing on the first block of <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mn>00</mn></msub><mo>=</mo><msub><mi>Q</mi><mn>0</mn></msub><mi mathvariant="normal">@</mi><msubsup><mi>K</mi><mn>0</mn><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">P_{00}=Q_0@K_0^t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>P</span><span><span><span><span><span><span></span><span><span><span>00</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>Q</span><span><span><span><span><span><span></span><span><span>0</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>@</span><span><span>K</span><span><span><span><span><span><span></span><span><span>0</span></span></span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span> and the first row of the value tensor <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">V_0</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>0</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>.</p>
<p><img src="https://aminediro.com/posts/flash_attn/images/flashattn_row_0.png" alt="output tensor"/>
  <img src="https://aminediro.com/posts/flash_attn/images/dark_flashattn_row_0.png" alt="output tensor"/>
</p>
<p>If you already see an issue with the current softmax, bear with me a minute. Remember that each element of the score matrix is computed as <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mfrac><msup><mi>e</mi><mrow><msub><mi>Q</mi><mi>i</mi></msub><mo>⋅</mo><msubsup><mi>K</mi><mi>j</mi><mi>T</mi></msubsup><mo>−</mo><msub><mi>m</mi><mi>i</mi></msub></mrow></msup><mrow><msub><mo>∑</mo><mi>j</mi></msub><msup><mi>e</mi><mrow><msub><mi>Q</mi><mi>i</mi></msub><mo>⋅</mo><msubsup><mi>K</mi><mi>j</mi><mi>T</mi></msubsup><mo>−</mo><msub><mi>m</mi><mi>i</mi></msub></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex"> P_{ij} = \frac{e^{Q_i \cdot K_j^T - m_i}}{\sum_j e^{Q_i \cdot K_j^T- m_i}} </annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>P</span><span><span><span><span><span><span></span><span><span><span>ij</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span><span>∑</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>Q</span><span><span><span><span><span><span></span><span>i</span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>⋅</span><span><span>K</span><span><span><span><span><span><span></span><span>j</span></span><span><span></span><span>T</span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>−</span><span><span>m</span><span><span><span><span><span><span></span><span>i</span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span><span></span><span></span></span><span><span></span><span><span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>Q</span><span><span><span><span><span><span></span><span>i</span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>⋅</span><span><span>K</span><span><span><span><span><span><span></span><span>j</span></span><span><span></span><span>T</span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>−</span><span><span>m</span><span><span><span><span><span><span></span><span>i</span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span></span></span></span></span>. So the denominator is clearly broken (we need to compute the whole row to compute this sum).</p>
<p>For now we only have <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mn>0</mn></msub><mo>⋅</mo><msubsup><mi>K</mi><mn>0</mn><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">Q_0\cdot K_0^t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>Q</span><span><span><span><span><span><span></span><span><span>0</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>⋅</span><span></span></span><span><span></span><span><span>K</span><span><span><span><span><span><span></span><span><span>0</span></span></span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span> in the row, so the first value we computed is clearly broken. Similarly, <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>=</mo><msub><mi>m</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">m_i=m_0</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>m</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>m</span><span><span><span><span><span><span></span><span><span>0</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span> here represents the maximum value of the row. For now we’ve only computed one value, so the max is also broken. Let’s continue to the second value of the output row and see how we can fix what is broken:</p>
<p><img src="https://aminediro.com/posts/flash_attn/images/flashattn_row_1.png" alt="output tensor"/>
  <img src="https://aminediro.com/posts/flash_attn/images/dark_flashattn_row_1.png" alt="output tensor"/>
</p>
<p>When we see a new block, we can’t just throw away our previous work. Instead, we need to <strong>rescale</strong> what we’ve already computed. How though? If we simplify the problem to only having two values in this row (<code>D=2</code>) and we have computed the first one, the correct output value should be :</p>
<p><span><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>O</mi><mn>1</mn></msub><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mrow><msub><mi>Q</mi><mn>1</mn></msub><mo>⋅</mo><msubsup><mi>K</mi><mn>1</mn><mi>T</mi></msubsup><mo>−</mo><msub><mi>m</mi><mn>1</mn></msub></mrow></msup><mo>⋅</mo><msub><mi>V</mi><mn>1</mn></msub><mo>+</mo><msup><mi>e</mi><mrow><msub><mi>Q</mi><mn>1</mn></msub><mo>⋅</mo><msubsup><mi>K</mi><mn>2</mn><mi>T</mi></msubsup><mo>−</mo><msub><mi>m</mi><mn>1</mn></msub></mrow></msup><mo>⋅</mo><msub><mi>V</mi><mn>2</mn></msub></mrow><mrow><msup><mi>e</mi><mrow><msub><mi>Q</mi><mn>1</mn></msub><mo>⋅</mo><msubsup><mi>K</mi><mn>1</mn><mi>T</mi></msubsup><mo>−</mo><msub><mi>m</mi><mn>1</mn></msub></mrow></msup><mo>+</mo><msup><mi>e</mi><mrow><msub><mi>Q</mi><mn>1</mn></msub><mo>⋅</mo><msubsup><mi>K</mi><mn>2</mn><mi>T</mi></msubsup><mo>−</mo><msub><mi>m</mi><mn>1</mn></msub></mrow></msup></mrow></mfrac><mspace width="1em"></mspace><mtext>where</mtext><mspace width="1em"></mspace><msub><mi>m</mi><mn>1</mn></msub><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>Q</mi><mn>1</mn></msub><mo>⋅</mo><msubsup><mi>K</mi><mn>1</mn><mi>T</mi></msubsup><mo separator="true">,</mo><msub><mi>Q</mi><mn>1</mn></msub><mo>⋅</mo><msubsup><mi>K</mi><mn>2</mn><mi>T</mi></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">
O_1 = \frac{e^{Q_1 \cdot K_1^T - m_1} \cdot V_1 + e^{Q_1 \cdot K_2^T - m_1} \cdot V_2}{e^{Q_1 \cdot K_1^T - m_1} + e^{Q_1 \cdot K_2^T - m_1}} \quad \text{where} \quad m_1 = \max(Q_1 \cdot K_1^T, Q_1 \cdot K_2^T)
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>O</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>Q</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>⋅</span><span><span>K</span><span><span><span><span><span><span></span><span><span>1</span></span></span><span><span></span><span><span>T</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>−</span><span><span>m</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span></span><span>+</span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>Q</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>⋅</span><span><span>K</span><span><span><span><span><span><span></span><span><span>2</span></span></span><span><span></span><span><span>T</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>−</span><span><span>m</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span><span></span><span></span></span><span><span></span><span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>Q</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>⋅</span><span><span>K</span><span><span><span><span><span><span></span><span><span>1</span></span></span><span><span></span><span><span>T</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>−</span><span><span>m</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span></span><span>⋅</span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>+</span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>Q</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>⋅</span><span><span>K</span><span><span><span><span><span><span></span><span><span>2</span></span></span><span><span></span><span><span>T</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>−</span><span><span>m</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span></span><span>⋅</span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span></span><span><span>where</span></span><span></span><span><span>m</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>max</span><span>(</span><span><span>Q</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>⋅</span><span></span></span><span><span></span><span><span>K</span><span><span><span><span><span><span></span><span><span>1</span></span></span><span><span></span><span><span>T</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>Q</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>⋅</span><span></span></span><span><span></span><span><span>K</span><span><span><span><span><span><span></span><span><span>2</span></span></span><span><span></span><span><span>T</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span></span></p>
<p>Thanks to the exponential multiplicative property we can easily build these from the previous work and currently computed block. This is done using <strong>running correction factors</strong>:</p>
<p><span><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="center" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi>m</mi><mn>1</mn></msub><mo>=</mo><msub><mi>Q</mi><mn>1</mn></msub><mo>⋅</mo><msubsup><mi>K</mi><mn>1</mn><mi>T</mi></msubsup><mo separator="true">,</mo><mspace width="1em"></mspace><msub><mi>m</mi><mn>2</mn></msub><mo>=</mo><msub><mi>Q</mi><mn>1</mn></msub><mo>⋅</mo><msubsup><mi>K</mi><mn>2</mn><mi>T</mi></msubsup><mo separator="true">,</mo><mspace width="1em"></mspace><msub><mi>m</mi><mtext>new</mtext></msub><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>m</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>m</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mspace width="1em"></mspace><mi>α</mi><mo>=</mo><msup><mi>e</mi><mrow><msub><mi>m</mi><mn>1</mn></msub><mo>−</mo><msub><mi>m</mi><mtext>new</mtext></msub></mrow></msup><mo separator="true">,</mo><mspace width="1em"></mspace><mi>β</mi><mo>=</mo><msup><mi>e</mi><mrow><msub><mi>m</mi><mn>2</mn></msub><mo>−</mo><msub><mi>m</mi><mtext>new</mtext></msub></mrow></msup></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{gathered}
m_1 = Q_1 \cdot K_1^T, \quad m_2 = Q_1 \cdot K_2^T, \quad m_{\text{new}} = \max(m_1, m_2) \quad
\alpha = e^{m_1 - m_{\text{new}}}, \quad \beta = e^{m_2 - m_{\text{new}}}
\end{gathered}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span><span><span></span><span><span><span>m</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span><span><span>Q</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>⋅</span><span></span><span><span>K</span><span><span><span><span><span><span></span><span><span>1</span></span></span><span><span></span><span><span>T</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span></span><span><span>m</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span><span><span>Q</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>⋅</span><span></span><span><span>K</span><span><span><span><span><span><span></span><span><span>2</span></span></span><span><span></span><span><span>T</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span></span><span><span>m</span><span><span><span><span><span><span></span><span><span><span><span>new</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span><span>max</span><span>(</span><span><span>m</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>m</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span></span><span>α</span><span></span><span>=</span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>m</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>−</span><span><span>m</span><span><span><span><span><span><span></span><span><span><span><span>new</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span>,</span><span></span><span></span><span>β</span><span></span><span>=</span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>m</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>−</span><span><span>m</span><span><span><span><span><span><span></span><span><span><span><span>new</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></p>
<p>These <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>α</span></span></span></span></span> and <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>β</span></span></span></span></span> factors let us rescale our previous exponentials to the new maximum. Think of it as “adjusting the baseline” - when we find a new maximum or a new sum, we scale down the previous values accordingly.</p>
<p><span><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>l</mi><mtext>new</mtext></msub><mo>=</mo><mi>α</mi><msub><mi>l</mi><mn>1</mn></msub><mo>+</mo><mi>β</mi><msub><mi>l</mi><mn>2</mn></msub><mo>=</mo><msup><mi>e</mi><mrow><msub><mi>m</mi><mn>1</mn></msub><mo>−</mo><msub><mi>m</mi><mtext>new</mtext></msub></mrow></msup><msub><mi>l</mi><mn>1</mn></msub><mo>+</mo><msup><mi>e</mi><mrow><msub><mi>m</mi><mn>2</mn></msub><mo>−</mo><msub><mi>m</mi><mtext>new</mtext></msub></mrow></msup><msub><mi>l</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">
l_{\text{new}} = \alpha l_1 + \beta l_2 = e^{m_1 - m_{\text{new}}} l_1 + e^{m_2 - m_{\text{new}}} l_2
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>l</span><span><span><span><span><span><span></span><span><span><span><span>new</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>α</span><span><span>l</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>+</span><span></span></span><span><span></span><span>β</span><span><span>l</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>m</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>−</span><span><span>m</span><span><span><span><span><span><span></span><span><span><span><span>new</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span><span>l</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>+</span><span></span></span><span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>m</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>−</span><span><span>m</span><span><span><span><span><span><span></span><span><span><span><span>new</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span><span>l</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></p>
<p><span><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><menclose notation="box"><mstyle scriptlevel="0" displaystyle="false"><mstyle scriptlevel="0" displaystyle="false"><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi>l</mi><mtext>new</mtext></msub><mo>=</mo><msup><mi>e</mi><mrow><msub><mi>Q</mi><mn>1</mn></msub><mo>⋅</mo><msubsup><mi>K</mi><mn>1</mn><mi>T</mi></msubsup><mo>−</mo><msub><mi>m</mi><mtext>new</mtext></msub></mrow></msup><mo>+</mo><msup><mi>e</mi><mrow><msub><mi>Q</mi><mn>1</mn></msub><mo>⋅</mo><msubsup><mi>K</mi><mn>2</mn><mi>T</mi></msubsup><mo>−</mo><msub><mi>m</mi><mtext>new</mtext></msub></mrow></msup></mrow></mstyle></mstyle></mstyle></menclose></mrow><annotation encoding="application/x-tex">
\boxed{l_{\text{new}} = e^{Q_1 \cdot K_1^T - m_{\text{new}}} + e^{Q_1 \cdot K_2^T - m_{\text{new}}}}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span></span><span><span><span><span><span>l</span><span><span><span><span><span><span></span><span><span><span><span>new</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>Q</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>⋅</span><span><span>K</span><span><span><span><span><span><span></span><span><span>1</span></span></span><span><span></span><span><span>T</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>−</span><span><span>m</span><span><span><span><span><span><span></span><span><span><span><span>new</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span></span><span>+</span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>Q</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>⋅</span><span><span>K</span><span><span><span><span><span><span></span><span><span>2</span></span></span><span><span></span><span><span>T</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>−</span><span><span>m</span><span><span><span><span><span><span></span><span><span><span><span>new</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span><span></span><span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></p>
<p>Now here’s where the magic happens. We also need to update our output <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>O</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">O_1</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>O</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>. Remember, we had computed <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>O</mi><mn>1</mn><mtext>prev</mtext></msubsup></mrow><annotation encoding="application/x-tex">O_1^{\text{prev}}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>O</span><span><span><span><span><span><span></span><span><span>1</span></span></span><span><span></span><span><span><span><span>prev</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span> using only the first block, but now we need to incorporate the second block. The update formula rescales the old output and adds the contribution from the new block:</p>
<p><span><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msubsup><mi>O</mi><mn>1</mn><mtext>new</mtext></msubsup><mo>=</mo><mfrac><mrow><mi>α</mi><mo>⋅</mo><msub><mi>l</mi><mn>1</mn></msub><mo>×</mo><msubsup><mi>O</mi><mn>1</mn><mtext>prev</mtext></msubsup><mo>+</mo><mi>β</mi><mo>⋅</mo><msup><mi>e</mi><mrow><msub><mi>Q</mi><mn>1</mn></msub><mo>⋅</mo><msubsup><mi>K</mi><mn>2</mn><mi>T</mi></msubsup><mo>−</mo><msub><mi>m</mi><mn>2</mn></msub></mrow></msup><mo>⋅</mo><msub><mi>V</mi><mn>2</mn></msub></mrow><msub><mi>l</mi><mtext>new</mtext></msub></mfrac></mrow><annotation encoding="application/x-tex">
O_1^{\text{new}} = \frac{\alpha \cdot l_1 \times O_1^{\text{prev}} + \beta \cdot e^{Q_1 \cdot K_2^T - m_2} \cdot V_2}{l_{\text{new}}}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>O</span><span><span><span><span><span><span></span><span><span>1</span></span></span><span><span></span><span><span><span><span>new</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>l</span><span><span><span><span><span><span></span><span><span><span><span>new</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span></span></span><span><span></span><span><span>α</span><span></span><span>⋅</span><span></span><span><span>l</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>×</span><span></span><span><span>O</span><span><span><span><span><span><span></span><span><span>1</span></span></span><span><span></span><span><span><span><span>prev</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>+</span><span></span><span>β</span><span></span><span>⋅</span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>Q</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>⋅</span><span><span>K</span><span><span><span><span><span><span></span><span><span>2</span></span></span><span><span></span><span><span>T</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>−</span><span><span>m</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span></span><span>⋅</span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span></span></span></span></span></span></p>
<p>Let’s expand this to see what’s really happening. The old output was <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>O</mi><mn>1</mn><mtext>prev</mtext></msubsup><mo>=</mo><mfrac><msup><mi>e</mi><mrow><msub><mi>Q</mi><mn>1</mn></msub><msubsup><mi>K</mi><mn>1</mn><mi>T</mi></msubsup><mo>−</mo><msub><mi>m</mi><mn>1</mn></msub></mrow></msup><msub><mi>l</mi><mn>1</mn></msub></mfrac><mo>⋅</mo><msub><mi>V</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">O_1^{\text{prev}} = \frac{e^{Q_1 K_1^T - m_1}}{l_1} \cdot V_1</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>O</span><span><span><span><span><span><span></span><span><span>1</span></span></span><span><span></span><span><span><span><span>prev</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span><span>l</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span><span></span><span></span></span><span><span></span><span><span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>Q</span><span><span><span><span><span><span></span><span>1</span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>K</span><span><span><span><span><span><span></span><span>1</span></span><span><span></span><span>T</span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>−</span><span><span>m</span><span><span><span><span><span><span></span><span>1</span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span></span><span>⋅</span><span></span></span><span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>, so:</p>
<p><span><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo>=</mo><mfrac><mn>1</mn><msub><mi>l</mi><mtext>new</mtext></msub></mfrac><mrow><mo fence="true">(</mo><msup><mi>e</mi><mrow><msub><mi>m</mi><mn>1</mn></msub><mo>−</mo><msub><mi>m</mi><mtext>new</mtext></msub></mrow></msup><mo>⋅</mo><menclose notation="updiagonalstrike"><msub><mi>l</mi><mn>1</mn></msub></menclose><mo>⋅</mo><mfrac><msup><mi>e</mi><mrow><msub><mi>Q</mi><mn>1</mn></msub><msubsup><mi>K</mi><mn>1</mn><mi>T</mi></msubsup><mo>−</mo><msub><mi>m</mi><mn>1</mn></msub></mrow></msup><menclose notation="updiagonalstrike"><msub><mi>l</mi><mn>1</mn></msub></menclose></mfrac><mo>⋅</mo><msub><mi>V</mi><mn>1</mn></msub><mo>+</mo><msup><mi>e</mi><mrow><msub><mi>m</mi><mn>2</mn></msub><mo>−</mo><msub><mi>m</mi><mtext>new</mtext></msub></mrow></msup><mo>⋅</mo><msup><mi>e</mi><mrow><msub><mi>Q</mi><mn>1</mn></msub><mo>⋅</mo><msubsup><mi>K</mi><mn>2</mn><mi>T</mi></msubsup><mo>−</mo><msub><mi>m</mi><mn>2</mn></msub></mrow></msup><mo>⋅</mo><msub><mi>V</mi><mn>2</mn></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">
= \frac{1}{l_{\text{new}}} \left( e^{m_1 - m_{\text{new}}} \cdot \cancel{l_1} \cdot \frac{e^{Q_1 K_1^T - m_1}}{\cancel{l_1}} \cdot V_1 + e^{m_2 - m_{\text{new}}} \cdot e^{Q_1 \cdot K_2^T - m_2} \cdot V_2 \right)
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>=</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>l</span><span><span><span><span><span><span></span><span><span><span><span>new</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span></span></span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span></span><span><span><span>(</span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>m</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>−</span><span><span>m</span><span><span><span><span><span><span></span><span><span><span><span>new</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span></span><span>⋅</span><span></span><span><span><span><span><span><span></span><span><span><span>l</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><svg width="100%" height="0.8444em"><line x1="0" y1="100%" x2="100%" y2="0" stroke-width="0.046em"></line></svg></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span>⋅</span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span><span><span><span><span></span><span><span><span>l</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><svg width="100%" height="0.8444em"><line x1="0" y1="100%" x2="100%" y2="0" stroke-width="0.046em"></line></svg></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span><span><span></span><span></span></span><span><span></span><span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>Q</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>K</span><span><span><span><span><span><span></span><span><span>1</span></span></span><span><span></span><span><span>T</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>−</span><span><span>m</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span></span><span>⋅</span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>+</span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>m</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>−</span><span><span>m</span><span><span><span><span><span><span></span><span><span><span><span>new</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span></span><span>⋅</span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>Q</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>⋅</span><span><span>K</span><span><span><span><span><span><span></span><span><span>2</span></span></span><span><span></span><span><span>T</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>−</span><span><span>m</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span></span><span>⋅</span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>)</span></span></span></span></span></span></span></span></p>
<p>Simplifying by combining the exponentials:</p>
<p><span><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo>=</mo><mfrac><mn>1</mn><msub><mi>l</mi><mtext>new</mtext></msub></mfrac><mrow><mo fence="true">(</mo><msup><mi>e</mi><mrow><msub><mi>Q</mi><mn>1</mn></msub><mo>⋅</mo><msubsup><mi>K</mi><mn>1</mn><mi>T</mi></msubsup><mo>−</mo><msub><mi>m</mi><mtext>new</mtext></msub></mrow></msup><mo>⋅</mo><msub><mi>V</mi><mn>1</mn></msub><mo>+</mo><msup><mi>e</mi><mrow><msub><mi>Q</mi><mn>1</mn></msub><msubsup><mi>K</mi><mn>2</mn><mi>T</mi></msubsup><mo>−</mo><msub><mi>m</mi><mtext>new</mtext></msub></mrow></msup><mo>⋅</mo><msub><mi>V</mi><mn>2</mn></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">
= \frac{1}{l_{\text{new}}} \left( e^{Q_1 \cdot K_1^T - m_{\text{new}}} \cdot V_1 + e^{Q_1 K_2^T - m_{\text{new}}} \cdot V_2 \right)
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>=</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>l</span><span><span><span><span><span><span></span><span><span><span><span>new</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span></span></span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span></span><span><span><span>(</span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>Q</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>⋅</span><span><span>K</span><span><span><span><span><span><span></span><span><span>1</span></span></span><span><span></span><span><span>T</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>−</span><span><span>m</span><span><span><span><span><span><span></span><span><span><span><span>new</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span></span><span>⋅</span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>+</span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>Q</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>K</span><span><span><span><span><span><span></span><span><span>2</span></span></span><span><span></span><span><span>T</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>−</span><span><span>m</span><span><span><span><span><span><span></span><span><span><span><span>new</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span></span><span>⋅</span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>)</span></span></span></span></span></span></span></span></p>
<p>And voilà! We get exactly what we’d expect - the proper softmax formula:</p>
<p><span><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo>=</mo><mfrac><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mn>2</mn></munderover><msup><mi>e</mi><mrow><msub><mi>Q</mi><mn>1</mn></msub><mo>⋅</mo><msubsup><mi>K</mi><mi>j</mi><mi>T</mi></msubsup><mo>−</mo><msub><mi>m</mi><mtext>new</mtext></msub></mrow></msup><msub><mi>V</mi><mi>j</mi></msub></mrow><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mn>2</mn></munderover><msup><mi>e</mi><mrow><msub><mi>Q</mi><mn>1</mn></msub><mo>⋅</mo><msubsup><mi>K</mi><mi>j</mi><mi>T</mi></msubsup><mo>−</mo><msub><mi>m</mi><mtext>new</mtext></msub></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">
= \frac{\sum_{j=1}^{2} e^{Q_1 \cdot K_j^T - m_{\text{new}}} V_j}{\sum_{j=1}^{2} e^{Q_1 \cdot K_j^T - m_{\text{new}}}}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>=</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>∑</span><span><span><span><span><span><span></span><span><span><span>j</span><span>=</span><span>1</span></span></span></span><span><span></span><span><span><span>2</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>Q</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>⋅</span><span><span>K</span><span><span><span><span><span><span></span><span><span>j</span></span></span><span><span></span><span><span>T</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>−</span><span><span>m</span><span><span><span><span><span><span></span><span><span><span><span>new</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span><span></span><span></span></span><span><span></span><span><span><span>∑</span><span><span><span><span><span><span></span><span><span><span>j</span><span>=</span><span>1</span></span></span></span><span><span></span><span><span><span>2</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>Q</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>⋅</span><span><span>K</span><span><span><span><span><span><span></span><span><span>j</span></span></span><span><span></span><span><span>T</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>−</span><span><span>m</span><span><span><span><span><span><span></span><span><span><span><span>new</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span></span></span></span></span></span></p>
<p>The beauty of this approach is that we never had to store the full attention matrix. We just kept updating our running statistics (<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>m</span></span></span></span></span> and <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>l</span></span></span></span></span>) and our output incrementally!</p>
<blockquote>
<p>This is the core insight of Flash Attention’s <strong>online softmax</strong> - we can compute exact attention without materializing the entire attention matrix in memory.</p>
</blockquote>
<h3 id="core-idea-tiling-and-online-softmax">Core Idea: Tiling and Online Softmax</h3>
<p>TLDR, Flash Attention solves the memory problem through <strong>two key insights</strong>:</p>
<ol>
<li><strong>Block-wise Computation</strong>: Instead of computing the full attention matrix, process Q, K, V in small blocks that fit in fast on-chip SRAM. We have been working with a single row of the output matrix, but we could easily generalize to computing a chunk of <code>n</code> rows of the matrix. For a single chunk <code>n</code> of the output, we would need <code>n</code> rows of the query matrix <code>Q</code> and all <code>j</code> rows of <code>K</code> and <code>V</code>. More concretely, the algorithm divides the sequence into blocks of size <code>Bc</code> and processes them iteratively.</li>
<li><strong>Online Softmax</strong>: Compute softmax incrementally without materializing the full attention matrix. This is usually the unintuitive part of the flash attention algorithm. Hopefully, if you have been following along, you can see the core idea behind the online softmax. The key is keeping track of running statistics (max value <code>m</code> and sum <code>l</code>) to compute the correct softmax normalization.</li>
</ol>
<p>Let’s roll up our sleeves and implement the algorithm!</p>
<hr/>
<h2 id="implementing-flash-attention-v1">Implementing Flash Attention v1</h2>
<p>Here is the triton implementation of the FA1 algorithm<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup> straight out of the paper. I tried to follow the algorithm as closely as possible to have a good baseline that we can work from:</p>
<p><span><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mstyle mathsize="0.9em"><menclose notation="left right"><mtable rowspacing="0.16em" columnalign="left" columnspacing="1em" rowlines="solid none none none none none none none none none none none none none none none none none none"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mrow><mtext mathvariant="bold">Algorithm</mtext><mtext> </mtext><mtext mathvariant="bold">1</mtext><mtext> </mtext></mrow><mtext>FlashAttention</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mrow><mtext mathvariant="bold">Require:</mtext><mtext> </mtext></mrow><mtext>Matrices </mtext><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>N</mi><mo>×</mo><mi>d</mi></mrow></msup><mtext> in HBM, on-chip SRAM of size </mtext><mi>M</mi><mi mathvariant="normal">.</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>1.</mn><mspace width="1em"></mspace><mtext>Set block sizes </mtext><msub><mi>B</mi><mi>c</mi></msub><mo>=</mo><mo stretchy="false">⌈</mo><mfrac><mi>M</mi><mrow><mn>4</mn><mi>d</mi></mrow></mfrac><mo stretchy="false">⌉</mo><mo separator="true">,</mo><mtext> </mtext><msub><mi>B</mi><mi>r</mi></msub><mo>=</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mo stretchy="false">⌈</mo><mfrac><mi>M</mi><mrow><mn>4</mn><mi>d</mi></mrow></mfrac><mo stretchy="false">⌉</mo><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>2.</mn><mspace width="1em"></mspace><mtext>Initialize </mtext><mi>O</mi><mo>=</mo><msub><mn mathvariant="bold">0</mn><mrow><mi>N</mi><mo>×</mo><mi>d</mi></mrow></msub><mo separator="true">,</mo><mtext> </mtext><mi mathvariant="normal">ℓ</mi><mo>=</mo><msub><mn mathvariant="bold">0</mn><mi>N</mi></msub><mo separator="true">,</mo><mtext> </mtext><mi>m</mi><mo>=</mo><mo stretchy="false">(</mo><mo>−</mo><mi mathvariant="normal">∞</mi><msub><mo stretchy="false">)</mo><mi>N</mi></msub><mi mathvariant="normal">.</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>3.</mn><mspace width="1em"></mspace><mtext>Divide </mtext><mi>Q</mi><mtext> into </mtext><msub><mi>T</mi><mi>r</mi></msub><mtext> blocks </mtext><msub><mi>Q</mi><mi>i</mi></msub><mo separator="true">,</mo><mtext> and </mtext><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mtext> into </mtext><msub><mi>T</mi><mi>c</mi></msub><mtext> blocks </mtext><msub><mi>K</mi><mi>j</mi></msub><mo separator="true">,</mo><msub><mi>V</mi><mi>j</mi></msub><mi mathvariant="normal">.</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>4.</mn><mspace width="1em"></mspace><mtext>Divide </mtext><mi>O</mi><mo separator="true">,</mo><mi mathvariant="normal">ℓ</mi><mo separator="true">,</mo><mi>m</mi><mtext> into blocks </mtext><msub><mi>O</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi mathvariant="normal">ℓ</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>m</mi><mi>i</mi></msub><mi mathvariant="normal">.</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>5.</mn><mspace width="1em"></mspace><mrow><mtext mathvariant="bold">for</mtext><mtext> </mtext></mrow><mn>1</mn><mo>≤</mo><mi>j</mi><mo>≤</mo><msub><mi>T</mi><mi>c</mi></msub><mrow><mtext> </mtext><mtext mathvariant="bold">do</mtext></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>6.</mn><mspace width="2em"></mspace><mtext>Load </mtext><msub><mi>K</mi><mi>j</mi></msub><mo separator="true">,</mo><msub><mi>V</mi><mi>j</mi></msub><mtext> from HBM to SRAM.</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>7.</mn><mspace width="2em"></mspace><mrow><mtext mathvariant="bold">for</mtext><mtext> </mtext></mrow><mn>1</mn><mo>≤</mo><mi>i</mi><mo>≤</mo><msub><mi>T</mi><mi>r</mi></msub><mrow><mtext> </mtext><mtext mathvariant="bold">do</mtext></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>8.</mn><mspace width="2em"></mspace><mspace width="1em"></mspace><mtext>Load </mtext><msub><mi>Q</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>O</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi mathvariant="normal">ℓ</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>m</mi><mi>i</mi></msub><mtext> from HBM to SRAM.</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>9.</mn><mspace width="2em"></mspace><mspace width="1em"></mspace><mtext>Compute </mtext><msub><mi>S</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><msub><mi>Q</mi><mi>i</mi></msub><msubsup><mi>K</mi><mi>j</mi><mi mathvariant="normal">⊤</mi></msubsup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><msub><mi>B</mi><mi>r</mi></msub><mo>×</mo><msub><mi>B</mi><mi>c</mi></msub></mrow></msup><mi mathvariant="normal">.</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>10.</mn><mspace width="2em"></mspace><mspace width="1em"></mspace><msub><mover accent="true"><mi>m</mi><mo>~</mo></mover><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mrow><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">w</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mrow><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mtext> </mtext><msub><mover accent="true"><mi>P</mi><mo>~</mo></mover><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>−</mo><msub><mover accent="true"><mi>m</mi><mo>~</mo></mover><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mtext> </mtext><msub><mover accent="true"><mi mathvariant="normal">ℓ</mi><mo>~</mo></mover><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mrow><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">w</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">m</mi></mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>P</mi><mo>~</mo></mover><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>11.</mn><mspace width="2em"></mspace><mspace width="1em"></mspace><msubsup><mi>m</mi><mi>i</mi><mtext>new</mtext></msubsup><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>m</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mover accent="true"><mi>m</mi><mo>~</mo></mover><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mtext> </mtext><msubsup><mi mathvariant="normal">ℓ</mi><mi>i</mi><mtext>new</mtext></msubsup><mo>=</mo><msup><mi>e</mi><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>−</mo><msubsup><mi>m</mi><mi>i</mi><mtext>new</mtext></msubsup></mrow></msup><msub><mi mathvariant="normal">ℓ</mi><mi>i</mi></msub><mo>+</mo><msup><mi>e</mi><mrow><msub><mover accent="true"><mi>m</mi><mo>~</mo></mover><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>−</mo><msubsup><mi>m</mi><mi>i</mi><mtext>new</mtext></msubsup></mrow></msup><msub><mover accent="true"><mi mathvariant="normal">ℓ</mi><mo>~</mo></mover><mrow><mi>i</mi><mi>j</mi></mrow></msub><mi mathvariant="normal">.</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>12.</mn><mspace width="2em"></mspace><mspace width="1em"></mspace><msub><mi>O</mi><mi>i</mi></msub><mo>←</mo><mrow><mi mathvariant="normal">d</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">g</mi></mrow><mo stretchy="false">(</mo><msubsup><mi mathvariant="normal">ℓ</mi><mi>i</mi><mtext>new</mtext></msubsup><msup><mo stretchy="false">)</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo fence="true">(</mo><mrow><mi mathvariant="normal">d</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">g</mi></mrow><mo stretchy="false">(</mo><msub><mi mathvariant="normal">ℓ</mi><mi>i</mi></msub><mo stretchy="false">)</mo><msup><mi>e</mi><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>−</mo><msubsup><mi>m</mi><mi>i</mi><mtext>new</mtext></msubsup></mrow></msup><msub><mi>O</mi><mi>i</mi></msub><mo>+</mo><msup><mi>e</mi><mrow><msub><mover accent="true"><mi>m</mi><mo>~</mo></mover><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>−</mo><msubsup><mi>m</mi><mi>i</mi><mtext>new</mtext></msubsup></mrow></msup><msub><mover accent="true"><mi>P</mi><mo>~</mo></mover><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mi>V</mi><mi>j</mi></msub><mo fence="true">)</mo></mrow><mi mathvariant="normal">.</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>13.</mn><mspace width="2em"></mspace><mspace width="1em"></mspace><msub><mi mathvariant="normal">ℓ</mi><mi>i</mi></msub><mo>←</mo><msubsup><mi mathvariant="normal">ℓ</mi><mi>i</mi><mtext>new</mtext></msubsup><mo separator="true">,</mo><mtext> </mtext><msub><mi>m</mi><mi>i</mi></msub><mo>←</mo><msubsup><mi>m</mi><mi>i</mi><mtext>new</mtext></msubsup><mi mathvariant="normal">.</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>14.</mn><mspace width="2em"></mspace><mrow><mtext mathvariant="bold">end</mtext><mtext> </mtext><mtext mathvariant="bold">for</mtext></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>15.</mn><mspace width="1em"></mspace><mrow><mtext mathvariant="bold">end</mtext><mtext> </mtext><mtext mathvariant="bold">for</mtext></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>16.</mn><mspace width="1em"></mspace><mrow><mtext mathvariant="bold">return</mtext><mtext> </mtext></mrow><mi>O</mi><mi mathvariant="normal">.</mi></mrow></mstyle></mtd></mtr></mtable></menclose></mstyle></mrow><annotation encoding="application/x-tex">
\small
\begin{array}{l}
\hline
\textbf{Algorithm 1 } \text{FlashAttention} \\
\hline
\\
\textbf{Require: } \text{Matrices } Q, K, V \in \mathbb{R}^{N \times d} \text{ in HBM, on-chip SRAM of size } M. \\
\\
1. \quad \text{Set block sizes } B_c = \lceil \frac{M}{4d} \rceil, \ B_r = \min(\lceil \frac{M}{4d} \rceil, d). \\
2. \quad \text{Initialize } O = \mathbf{0}_{N \times d}, \ \ell = \mathbf{0}_N, \ m = (-\infty)_N. \\
3. \quad \text{Divide } Q \text{ into } T_r \text{ blocks } Q_i, \text{ and } K, V \text{ into } T_c \text{ blocks } K_j, V_j. \\
4. \quad \text{Divide } O, \ell, m \text{ into blocks } O_i, \ell_i, m_i. \\
5. \quad \textbf{for } 1 \le j \le T_c \textbf{ do} \\
6. \qquad \text{Load } K_j, V_j \text{ from HBM to SRAM.} \\
7. \qquad \textbf{for } 1 \le i \le T_r \textbf{ do} \\
8. \qquad \quad \text{Load } Q_i, O_i, \ell_i, m_i \text{ from HBM to SRAM.} \\
9. \qquad \quad \text{Compute } S_{ij} = Q_i K_j^\top \in \mathbb{R}^{B_r \times B_c}. \\
10.\qquad \quad \tilde{m}_{ij} = \mathrm{rowmax}(S_{ij}), \ \tilde{P}_{ij} = \exp(S_{ij} - \tilde{m}_{ij}), \ \tilde{\ell}_{ij} = \mathrm{rowsum}(\tilde{P}_{ij}). \\
11.\qquad \quad m_i^{\text{new}} = \max(m_i, \tilde{m}_{ij}), \ \ell_i^{\text{new}} = e^{m_i - m_i^{\text{new}}} \ell_i + e^{\tilde{m}_{ij} - m_i^{\text{new}}} \tilde{\ell}_{ij}. \\
12.\qquad \quad O_i \leftarrow \mathrm{diag}(\ell_i^{\text{new}})^{-1} \left( \mathrm{diag}(\ell_i) e^{m_i - m_i^{\text{new}}} O_i + e^{\tilde{m}_{ij} - m_i^{\text{new}}} \tilde{P}_{ij} V_j \right). \\
13.\qquad \quad \ell_i \leftarrow \ell_i^{\text{new}}, \ m_i \leftarrow m_i^{\text{new}}. \\
14.\qquad \textbf{end for} \\
15.\quad \textbf{end for} \\
16.\quad \textbf{return } O. \\
\hline
\end{array}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>Algorithm 1 </span></span><span><span>FlashAttention</span></span></span></span><span><span></span><span></span></span><span><span></span><span><span><span>Require: </span></span><span><span>Matrices </span></span><span>Q</span><span>,</span><span></span><span>K</span><span>,</span><span></span><span>V</span><span></span><span>∈</span><span></span><span><span>R</span><span><span><span><span><span><span></span><span><span><span>N</span><span>×</span><span>d</span></span></span></span></span></span></span></span></span><span><span> in HBM, on-chip SRAM of size </span></span><span>M</span><span>.</span></span></span><span><span></span><span></span></span><span><span></span><span><span>1.</span><span></span><span><span>Set block sizes </span></span><span><span>B</span><span><span><span><span><span><span></span><span><span>c</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span><span>⌈</span><span><span></span><span><span><span><span><span><span></span><span><span><span>4</span><span>d</span></span></span></span><span><span></span><span></span></span><span><span></span><span><span><span>M</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span>⌉</span><span>,</span><span> </span><span></span><span><span>B</span><span><span><span><span><span><span></span><span><span>r</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span><span>min</span><span>(⌈</span><span><span></span><span><span><span><span><span><span></span><span><span><span>4</span><span>d</span></span></span></span><span><span></span><span></span></span><span><span></span><span><span><span>M</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span>⌉</span><span>,</span><span></span><span>d</span><span>)</span><span>.</span></span></span><span><span></span><span><span>2.</span><span></span><span><span>Initialize </span></span><span>O</span><span></span><span>=</span><span></span><span><span>0</span><span><span><span><span><span><span></span><span><span><span>N</span><span>×</span><span>d</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span> </span><span></span><span>ℓ</span><span></span><span>=</span><span></span><span><span>0</span><span><span><span><span><span><span></span><span><span>N</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span> </span><span></span><span>m</span><span></span><span>=</span><span></span><span>(</span><span>−</span><span>∞</span><span><span>)</span><span><span><span><span><span><span></span><span><span>N</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>.</span></span></span><span><span></span><span><span>3.</span><span></span><span><span>Divide </span></span><span>Q</span><span><span> into </span></span><span><span>T</span><span><span><span><span><span><span></span><span><span>r</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span> blocks </span></span><span><span>Q</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span> and </span></span><span>K</span><span>,</span><span></span><span>V</span><span><span> into </span></span><span><span>T</span><span><span><span><span><span><span></span><span><span>c</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span> blocks </span></span><span><span>K</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>.</span></span></span><span><span></span><span><span>4.</span><span></span><span><span>Divide </span></span><span>O</span><span>,</span><span></span><span>ℓ</span><span>,</span><span></span><span>m</span><span><span> into blocks </span></span><span><span>O</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>ℓ</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>m</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>.</span></span></span><span><span></span><span><span>5.</span><span></span><span><span>for </span></span><span>1</span><span></span><span>≤</span><span></span><span>j</span><span></span><span>≤</span><span></span><span><span>T</span><span><span><span><span><span><span></span><span><span>c</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span> do</span></span></span></span><span><span></span><span><span>6.</span><span></span><span><span>Load </span></span><span><span>K</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span> from HBM to SRAM.</span></span></span></span><span><span></span><span><span>7.</span><span></span><span><span>for </span></span><span>1</span><span></span><span>≤</span><span></span><span>i</span><span></span><span>≤</span><span></span><span><span>T</span><span><span><span><span><span><span></span><span><span>r</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span> do</span></span></span></span><span><span></span><span><span>8.</span><span></span><span></span><span><span>Load </span></span><span><span>Q</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>O</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>ℓ</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>m</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span> from HBM to SRAM.</span></span></span></span><span><span></span><span><span>9.</span><span></span><span></span><span><span>Compute </span></span><span><span>S</span><span><span><span><span><span><span></span><span><span><span>ij</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span><span><span>Q</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>K</span><span><span><span><span><span><span></span><span><span>j</span></span></span><span><span></span><span><span>⊤</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>∈</span><span></span><span><span>R</span><span><span><span><span><span><span></span><span><span><span><span>B</span><span><span><span><span><span><span></span><span><span>r</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>×</span><span><span>B</span><span><span><span><span><span><span></span><span><span>c</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span>.</span></span></span><span><span></span><span><span>10.</span><span></span><span></span><span><span><span><span><span><span><span></span><span>m</span></span><span><span></span><span><span>~</span></span></span></span></span></span></span><span><span><span><span><span><span></span><span><span><span>ij</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span><span><span>rowmax</span></span><span>(</span><span><span>S</span><span><span><span><span><span><span></span><span><span><span>ij</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span>,</span><span> </span><span></span><span><span><span><span><span><span><span></span><span>P</span></span><span><span></span><span><span>~</span></span></span></span></span></span></span><span><span><span><span><span><span></span><span><span><span>ij</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span><span>exp</span><span>(</span><span><span>S</span><span><span><span><span><span><span></span><span><span><span>ij</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>−</span><span></span><span><span><span><span><span><span><span></span><span>m</span></span><span><span></span><span><span>~</span></span></span></span></span></span></span><span><span><span><span><span><span></span><span><span><span>ij</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span>,</span><span> </span><span></span><span><span><span><span><span><span><span></span><span>ℓ</span></span><span><span></span><span><span>~</span></span></span></span></span></span></span><span><span><span><span><span><span></span><span><span><span>ij</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span><span><span>rowsum</span></span><span>(</span><span><span><span><span><span><span><span></span><span>P</span></span><span><span></span><span><span>~</span></span></span></span></span></span></span><span><span><span><span><span><span></span><span><span><span>ij</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span>.</span></span></span><span><span></span><span><span>11.</span><span></span><span></span><span><span>m</span><span><span><span><span><span><span></span><span><span>i</span></span></span><span><span></span><span><span><span><span>new</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span><span>max</span><span>(</span><span><span>m</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span><span><span><span><span><span></span><span>m</span></span><span><span></span><span><span>~</span></span></span></span></span></span></span><span><span><span><span><span><span></span><span><span><span>ij</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span>,</span><span> </span><span></span><span><span>ℓ</span><span><span><span><span><span><span></span><span><span>i</span></span></span><span><span></span><span><span><span><span>new</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>m</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>−</span><span><span>m</span><span><span><span><span><span><span></span><span><span>i</span></span></span><span><span></span><span><span><span><span>new</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span><span>ℓ</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>+</span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span><span><span><span><span><span></span><span>m</span></span><span><span></span><span><span>~</span></span></span></span></span></span></span><span><span><span><span><span><span></span><span><span><span>ij</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>−</span><span><span>m</span><span><span><span><span><span><span></span><span><span>i</span></span></span><span><span></span><span><span><span><span>new</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span><span><span><span><span><span><span></span><span>ℓ</span></span><span><span></span><span><span>~</span></span></span></span></span></span></span><span><span><span><span><span><span></span><span><span><span>ij</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>.</span></span></span><span><span></span><span><span>12.</span><span></span><span></span><span><span>O</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>←</span><span></span><span><span>diag</span></span><span>(</span><span><span>ℓ</span><span><span><span><span><span><span></span><span><span>i</span></span></span><span><span></span><span><span><span><span>new</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>)</span><span><span><span><span><span><span></span><span><span><span>−</span><span>1</span></span></span></span></span></span></span></span></span><span></span><span><span><span>(</span></span><span><span>diag</span></span><span>(</span><span><span>ℓ</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>m</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>−</span><span><span>m</span><span><span><span><span><span><span></span><span><span>i</span></span></span><span><span></span><span><span><span><span>new</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span><span>O</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>+</span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span><span><span><span><span><span></span><span>m</span></span><span><span></span><span><span>~</span></span></span></span></span></span></span><span><span><span><span><span><span></span><span><span><span>ij</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>−</span><span><span>m</span><span><span><span><span><span><span></span><span><span>i</span></span></span><span><span></span><span><span><span><span>new</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span><span><span><span><span><span><span></span><span>P</span></span><span><span></span><span><span>~</span></span></span></span></span></span></span><span><span><span><span><span><span></span><span><span><span>ij</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>)</span></span></span><span></span><span>.</span></span></span><span><span></span><span><span>13.</span><span></span><span></span><span><span>ℓ</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>←</span><span></span><span><span>ℓ</span><span><span><span><span><span><span></span><span><span>i</span></span></span><span><span></span><span><span><span><span>new</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span> </span><span></span><span><span>m</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>←</span><span></span><span><span>m</span><span><span><span><span><span><span></span><span><span>i</span></span></span><span><span></span><span><span><span><span>new</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>.</span></span></span><span><span></span><span><span>14.</span><span></span><span><span>end for</span></span></span></span><span><span></span><span><span>15.</span><span></span><span><span>end for</span></span></span></span><span><span></span><span><span>16.</span><span></span><span><span>return </span></span><span>O</span><span>.</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span></span><span><span></span><span></span></span><span><span></span><span></span></span><span><span></span><span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>
</p>
<p>We recognize the usual suspects from the math we derived earlier. I will only focus on the forward pass without causal masking to keep focus on the core algorithm. Let’s start with the simple reference implementation in pytorch:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># q,k,v are of shape: `(B, N_h, S, D_h)`</span>
</span></span><span><span><span># B: batch_size</span>
</span></span><span><span><span># N_h: num heads</span>
</span></span><span><span><span># S: sequence length</span>
</span></span><span><span><span># D_h: head dim</span>
</span></span><span><span><span>def</span> <span>simple_attn</span>(q, k, v):
</span></span><span><span>    att <span>=</span> q <span>@</span> k<span>.</span>transpose(<span>-</span><span>2</span>, <span>-</span><span>1</span>) <span>*</span> (<span>1.0</span> <span>/</span> math<span>.</span>sqrt(k<span>.</span>size(<span>-</span><span>1</span>)))
</span></span><span><span>    att <span>=</span> F<span>.</span>softmax(att, dim<span>=-</span><span>1</span>)
</span></span><span><span>    y <span>=</span> att <span>@</span> v
</span></span><span><span>    <span>return</span> y
</span></span></code></pre></div><p>My first implementation <a href="https://github.com/AmineDiro/nano-llama-flash/blob/0d2b59e74574b32d03c25dca43c1c34c2735433e/kernels/triton_flash_att.py"><code>kernels/triton_flash_att.py</code></a> follows the original Flash Attention paper very closely:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>@triton.jit</span>
</span></span><span><span><span>def</span> <span>attn_kernel</span>(Q, K, V, O, S, D, Tc, Tr, Bc, Br, softmax_scale, l, m):
</span></span><span><span>    <span># ... [SETUP offsets here]</span>
</span></span><span><span>    <span># Outer loop over K, V blocks</span>
</span></span><span><span>    <span>for</span> j <span>in</span> range(<span>0</span>, Tc):
</span></span><span><span>        <span># Load K_j, V_j from HBM to SRAM</span>
</span></span><span><span>        kj <span>=</span> tl<span>.</span>load(k_ptr <span>+</span> offset_j)  <span># (Bc, D)</span>
</span></span><span><span>        vj <span>=</span> tl<span>.</span>load(v_ptr <span>+</span> offset_j)  <span># (Bc, D)</span>
</span></span><span><span>
</span></span><span><span>        <span># Inner loop over Q blocks</span>
</span></span><span><span>        <span>for</span> i <span>in</span> range(<span>0</span>, Tr):
</span></span><span><span>            <span># Load Q_i and previous O_i, l_i, m_i</span>
</span></span><span><span>            qi <span>=</span> tl<span>.</span>load(q_ptr <span>+</span> offset_i)  <span># `(Bc, D)`</span>
</span></span><span><span>            prev_oi <span>=</span> tl<span>.</span>load(o_ptr <span>+</span> offset_i)
</span></span><span><span>            prev_li <span>=</span> tl<span>.</span>load(l_ptr <span>+</span> S_i_offset)
</span></span><span><span>            prev_mi <span>=</span> tl<span>.</span>load(m_ptr <span>+</span> S_i_offset)
</span></span><span><span>
</span></span><span><span>            <span># Compute attention scores for this block</span>
</span></span><span><span>            Sij <span>=</span> tl<span>.</span>dot(qi, tl<span>.</span>trans(kj)) <span>*</span> softmax_scale  <span># (Bc, Bc)</span>
</span></span><span><span>
</span></span><span><span>            <span># Online softmax update</span>
</span></span><span><span>            mij <span>=</span> tl<span>.</span>max(Sij, <span>1</span>)  <span># Row-wise max</span>
</span></span><span><span>            pij <span>=</span> tl<span>.</span>exp(Sij <span>-</span> mij[:, <span>None</span>])
</span></span><span><span>            lij <span>=</span> tl<span>.</span>sum(pij, <span>1</span>)  <span># Row-wise sum</span>
</span></span><span><span>
</span></span><span><span>            <span># Update running statistics</span>
</span></span><span><span>            mi_new <span>=</span> tl<span>.</span>maximum(prev_mi, mij)
</span></span><span><span>            alpha <span>=</span> tl<span>.</span>exp(prev_mi <span>-</span> mi_new)
</span></span><span><span>            beta <span>=</span> tl<span>.</span>exp(mij <span>-</span> mi_new)
</span></span><span><span>            li_new <span>=</span> prev_li <span>*</span> alpha <span>+</span> lij <span>*</span> beta
</span></span><span><span>
</span></span><span><span>            <span># Update output</span>
</span></span><span><span>            oi_new <span>=</span> (alpha[:, <span>None</span>] <span>*</span> prev_li[:, <span>None</span>] <span>*</span> prev_oi
</span></span><span><span>                      <span>+</span> beta[:, <span>None</span>] <span>*</span> tl<span>.</span>dot(pij, vj)) <span>/</span> li_new[:, <span>None</span>]
</span></span><span><span>
</span></span><span><span>            <span># Write back to HBM</span>
</span></span><span><span>            tl<span>.</span>store(o_ptr <span>+</span> offset_i, oi_new)
</span></span><span><span>            tl<span>.</span>store(m_ptr <span>+</span> S_i_offset, mi_new)
</span></span><span><span>            tl<span>.</span>store(l_ptr <span>+</span> S_i_offset, li_new)
</span></span></code></pre></div><p>Some notes about this implementation :</p>
<ul>
<li><strong>Double loop</strong>: Outer loop over K/V blocks, inner loop over Q blocks. Notice we are reloading block <code>qi = tl.load(q_ptr + offset_i)  # (Bc, D)</code> in the inner loop which is <em>pretty</em> inefficient.</li>
<li>We dispatch this kernel on a grid of <code>B x N_h</code> blocks. This means that each block has to find the correct matrix offset in <code>Q</code>,<code>K</code>,<code>V</code> and compute a full of the output <code>O</code> for the batch and attention head. I immediately thought that we are wasting compute here as each row <code>O</code> can be computed separately from each other. But don’t worry I’ll come back to this in our V2.</li>
<li>Another wasteful thing I threw in is allocating <code>l</code> and <code>m</code> as <code>l = torch.zeros(B, N_h, S).cuda() ; m = torch.full((B, N_h, S), float(&#34;-inf&#34;)).cuda()</code>. If you recall the memory access bit earlier, we are effectively loading rows using <code>S_i_offset</code> from HBM which isn’t great. What we would want is for each block to allocate these accumulators locally in SRAM and avoid allocating them entirely on HBM.</li>
<li>Notice also that I threw in another simplification <code>Br = Bc = 32</code>. This means that we are splitting <code>Q</code> and <code>K</code> and <code>V</code> into same size chunks. This isn’t the greatest idea because we might need to have different values for optimization reason. It is just another simplification I threw in for the implementation to be straightforward.</li>
</ul>
<p>Before looking at the profiling of this v1 naive implementation let’s first look at another requirement I brushed over quickly but that is really important: <strong>SRAM limit</strong>. Because we are living in the real world we can’t have infinite SRAM on GPUs (sigh! ..), we need to have a ballpark estimation of how much SRAM we need to allocate and if this fits into my device SRAM.</p>
<p>Quick back-of-the-enveloppe calculation, the kernel at minimum needs to store in SRAM simultaneously:</p>
<ul>
<li>Query block Q_i: <code>Bc × D</code> floats</li>
<li>Key block K_j: <code>Bc × D</code> floats</li>
<li>Value block V_j: <code>Bc × D</code> floats</li>
<li>Attention scores S_ij: <code>Bc × Bc</code> floats</li>
<li>Running maximum and running sum: <code>2 × Bc</code> floats</li>
</ul>
<p>For <code>B=10; Bc = 32; N_h = 64; S = 64; D_h = 32</code>, Total SRAM: <code>2 × Bc + 3 × Bc × D + Bc²</code> floats = <code>2 × 32 + 3 × 32 × 64 + 32² = 7,232</code> floats = <code>7,232 × 4 bytes ≈ 28KB</code>. This fits comfortably in the ~48KB of shared memory available per thread block. It does mean, though, that we are <strong>limited to ~2 blocks per SM</strong>. If you’re confused about this limit, don’t worry, I’ll explain this promptly once we get into profiling.</p>
<h3 id="profiling-the-v1-implementation">Profiling the v1 Implementation</h3>
<p>As I mentioned in the <a href="#setup">Setup and hardware</a> section, I’ll be using the Nsight Compute <code>ncu</code> tool from the start. It’s one of those tools that comes with a LOT of information, and it can be pretty overwhelming. I recommend watching this <a href="https://www.youtube.com/watch?v=F_BazucyCMw&amp;t=6910s">lecture</a> to get a global overview of all the <code>ncu-ui</code> sections. I also highly recommend reading <a href="https://modal.com/gpu-glossary/perf">Modal’s GPU glossary performance section</a> front to back to at least familiarize yourself with the key terms you need to understand GPU program performance, as well as gain a general understanding of how GPUs execute code. If you don’t know what a <code>warp scheduler</code> is, please go read that resource before continuing to the next section.</p>
<p>I ran the <code>ncu</code> profiler using this command to get the full set of timer and metrics:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span><span>#!/bin/bash
</span></span></span><span><span><span></span>
</span></span><span><span>sudo CUDA_HOME<span>=</span>/opt/cuda /usr/local/NVIDIA-Nsight-Compute/ncu <span>\
</span></span></span><span><span><span></span>    --set full <span>\
</span></span></span><span><span><span></span>    --kernel-name <span>&#34;attn_kernel&#34;</span> <span>\
</span></span></span><span><span><span></span>    --import-source yes <span>\
</span></span></span><span><span><span></span>    -o profile_flash_attn_v1 <span>\
</span></span></span><span><span><span></span>    -f .venv/bin/python3 kernels/triton_flash_att.py
</span></span></code></pre></div><p>Once the profiling is done, we can open it using <code>ncu-ui</code>, here is an annotated view of the profile once we open it:</p>
<p><img src="https://aminediro.com/posts/flash_attn/images/ncu_description.png" alt="output tensor"/>
  <img src="https://aminediro.com/posts/flash_attn/images/dark_ncu_description.png" alt="output tensor"/>
</p>
<p>Great, let’s drill down on the key metrics. First, we can see that our kernel took <code>166.47ms</code> to execute. Our kernel used a grid size of (10, 64, 1) matchs our triton grid of <code>(B, N_h)</code>. Each block has a size of <code>(128, 1, 1)</code>. Because we are using triton to write our kernel, there is no way to specify the block size directly (although there is a <code>num_warps</code> param that we could use). This means that triton chose to use <strong>4 warps</strong> per block to run the kernel.</p>
<p>Next, the profiler actually picked up of a very interesting problem that we identified earlier:</p>
<blockquote>
<p><em>The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 8. This kernel’s theoretical occupancy (25.0%) is limited by the required amount of shared memory.</em></p>
</blockquote>
<p>To understand this occupancy problem a little bit more, <code>ncu</code> provides a quite handy occupancy calculator tool. Let’s look at what the tool says :</p>
<p><img src="https://aminediro.com/posts/flash_attn/images/occupancy_v1.png" alt="occupancy v1"/></p>
<p>Because our shared memory requirement per block is quite high, we can only have 2 active blocks at a time per SM!
Low occupancy really hurts performance when there aren’t enough warps to hide the latency. But once occupancy is sufficient for latency hiding, increasing it further can degrade performance. Higher occupancy reduces resources per thread, potentially bottlenecking the kernel on registers or reducing the arithmetic intensity.</p>
<p>To fix this we need to change our SRAM requirement: SRAM only depends on <code>Bc</code> and <code>D_h</code>. So we can either have smaller chunks (although this could impact memory bandwidth), or have more attention heads to reduce each <code>D</code> (if we control model architecture). Either way, we can go back to tweaking this later. Let’s look at the detail section of the profile report to see what other surprises await us!</p>
<p><img src="https://aminediro.com/posts/flash_attn/images/memory_v1.png" alt="memory_v1"/></p>
<p>The whole idea behind Flash Attention is to avoid going to main memory (HBM) for intermediate steps. The <code>11.58 GB</code> reads and <code>5.54 GB</code> writes confirm a bottleneck. Quick math: In this naive implementation, we iterate over columns (<code>K</code>, <code>V</code>) in the outer loop and rows (<code>Q</code>, <code>O</code>) in the inner loop. This loop order forces us to reload the Accumulator (<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi></mrow><annotation encoding="application/x-tex">O</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span></span></span></span></span>) and Query (<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>Q</span></span></span></span></span>) from HBM for every single chunk of keys. With <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mi mathvariant="normal">/</mi><mi>B</mi><mi>c</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding="application/x-tex">S/Bc = 64</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>S</span><span>/</span><span>B</span><span>c</span><span></span><span>=</span><span></span></span><span><span></span><span>64</span></span></span></span></span> chunks, we are reading and writing the entire output matrix 64 times! <strong>Math</strong>: Size of <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo>≈</mo><mn>83</mn><mtext> MB</mtext></mrow><annotation encoding="application/x-tex">O \approx 83 \text{ MB}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span></span><span>≈</span><span></span></span><span><span></span><span>83</span><span><span> MB</span></span></span></span></span></span>. Reads <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>≈</mo><mn>64</mn><mo>×</mo><mo stretchy="false">(</mo><mi>Q</mi><mo>+</mo><mi>O</mi><mo stretchy="false">)</mo><mo>≈</mo><mn>10.6</mn><mtext> GB</mtext></mrow><annotation encoding="application/x-tex">\approx 64 \times (Q + O) \approx 10.6 \text{ GB}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>≈</span><span></span></span><span><span></span><span>64</span><span></span><span>×</span><span></span></span><span><span></span><span>(</span><span>Q</span><span></span><span>+</span><span></span></span><span><span></span><span>O</span><span>)</span><span></span><span>≈</span><span></span></span><span><span></span><span>10.6</span><span><span> GB</span></span></span></span></span></span>. Writes <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>≈</mo><mn>64</mn><mo>×</mo><mi>O</mi><mo>≈</mo><mn>5.3</mn><mtext> GB</mtext></mrow><annotation encoding="application/x-tex">\approx 64 \times O \approx 5.3 \text{ GB}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>≈</span><span></span></span><span><span></span><span>64</span><span></span><span>×</span><span></span></span><span><span></span><span>O</span><span></span><span>≈</span><span></span></span><span><span></span><span>5.3</span><span><span> GB</span></span></span></span></span></span>.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span>
</span></span><span><span>  <span>for</span> j <span>in</span> range(<span>0</span>, Tc):
</span></span><span><span>      <span># Load K_j, V_j from HBM</span>
</span></span><span><span>      <span># ....</span>
</span></span><span><span>      <span>for</span> i <span>in</span> range(<span>0</span>, Tr):
</span></span><span><span>          <span># ....</span>
</span></span><span><span>          <span># -&gt; Reading previous O_i (Bc,D)</span>
</span></span><span><span>          prev_oi <span>=</span> tl<span>.</span>load(o_ptr <span>+</span> offset_i)
</span></span><span><span>          <span># -&gt; Writing previous O_i (Bc,D)</span>
</span></span><span><span>          tl<span>.</span>store(o_ptr <span>+</span> offset_i, oi_new)
</span></span></code></pre></div><blockquote>
<p>We are treating HBM like a register, which explains the massive bandwidth usage!</p>
</blockquote>
<p>The last thing to look at before starting to look at the potential solutions we can implement is the <strong>source</strong> from the profiler.</p>
<p><img src="https://aminediro.com/posts/flash_attn/images/source_v1_division.png" alt="source_v1"/></p>
<p><code>ncu</code> marks problematic kernel code lines with a warning emoji ⚠️, which is quite helpful! The <code>div</code> operation is usually costly on CPUs, and it’s basically the same story for GPUs. CUDA implements floating-point division on GPUs via reciprocal + multiply ops: using <code>MUFU</code> (Multi-Function Unit) followed by one or two <code>FFMA</code> or <code>FMUL</code> instructions to refine precision. If we look at the <code>SASS</code> disassembly for this line:</p>
<div><pre tabindex="0"><code data-lang="asm"><span><span>     <span>MUFU.RCP</span> <span>R8</span>, <span>R8</span>
</span></span><span><span>     <span>...</span>
</span></span><span><span><span>@</span><span>P5</span>  <span>FMUL</span> <span>R29</span>, <span>R29</span>, <span>0</span><span>.25</span>
</span></span><span><span><span>@</span><span>P5</span>  <span>FMUL</span> <span>R15</span>, <span>R15</span>, <span>0</span><span>.25</span>
</span></span><span><span><span>@!</span><span>P4</span> <span>FMUL</span> <span>R10</span>, <span>R10</span>, <span>16777216</span>
</span></span><span><span><span>@!</span><span>P6</span> <span>FMUL</span> <span>R29</span>, <span>R29</span>, <span>16777216</span>
</span></span><span><span><span>@!</span><span>P4</span> <span>FMUL</span> <span>R23</span>, <span>R23</span>, <span>16777216</span>
</span></span><span><span><span>@!</span><span>P6</span> <span>FMUL</span> <span>R15</span>, <span>R15</span>, <span>16777216</span>
</span></span><span><span>     <span>FMUL</span> <span>R10</span>, <span>R9</span>, <span>R10</span>
</span></span><span><span>     <span>FMUL</span> <span>R29</span>, <span>R8</span>, <span>R29</span>
</span></span><span><span>     <span>FMUL</span> <span>R9</span>, <span>R9</span>, <span>R23</span>
</span></span><span><span>     <span>FMUL</span> <span>R8</span>, <span>R8</span>, <span>R15</span>
</span></span></code></pre></div><p>Because we are doing the division inside the hot loop we are doing a lot more work here and we can start thinking about how we can avoid this division all together.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span>  oi_new <span>=</span> (
</span></span><span><span>      alpha[:, <span>None</span>] <span>*</span> prev_li[:, <span>None</span>] <span>*</span> prev_oi
</span></span><span><span>      <span>+</span> beta[:, <span>None</span>] <span>*</span> tl<span>.</span>dot(pij, vj)
</span></span><span><span>  ) <span>/</span> li_new[:, <span>None</span>]
</span></span></code></pre></div><h3 id="next-implementation-plan">Next Implementation Plan</h3>
<p>Based on the ncu profiling data and our analysis of the memory traffic, we have identified three critical bottlenecks to address in the next iteration (V2).</p>
<ol>
<li>
<p><strong>Invert the Loop Order</strong>: The massive 11.58 GB of main memory reads and 5.54 GB of writes is our biggest performance killer. It stems from treating HBM as a temporary register for our output accumulator. Current loop forces us to repeatedly read/write the accumulator <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi></mrow><annotation encoding="application/x-tex">O</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span></span></span></span></span> and reload <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>Q</span></span></span></span></span> for every block of <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>K</span></span></span></span></span>. We must invert the loops: Parallelize the kernel over Queries (rows) so that each thread block handles a tile of <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>Q</span></span></span></span></span>:</p>
<ul>
<li><strong>Stationary Data:</strong> Rows of <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi></mrow><annotation encoding="application/x-tex">O</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span></span></span></span></span> and <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>Q</span></span></span></span></span> remain in SRAM/registers for the entire kernel.</li>
<li><strong>Streaming Data:</strong> <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>K</span></span></span></span></span> and <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>V</span></span></span></span></span> stream from HBM. Since all blocks share the same <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mi mathvariant="normal">/</mi><mi>V</mi></mrow><annotation encoding="application/x-tex">K/V</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>K</span><span>/</span><span>V</span></span></span></span></span>, the L2 cache will absorb most of the traffic (coalesced access).</li>
</ul>
</li>
<li>
<p><strong>Defer Normalization</strong>: Before describing the fix, let’s recall the true attention output: <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>O</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><msub><mo>∑</mo><mi>j</mi></msub><msup><mi>e</mi><msub><mi>S</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></msup><msub><mi>V</mi><mi>j</mi></msub></mrow><mrow><msub><mo>∑</mo><mi>j</mi></msub><msup><mi>e</mi><msub><mi>S</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">O_i = \frac{\sum_j e^{S_{ij}} V_j}{\sum_j e^{S_{ij}}}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>O</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span><span>∑</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>S</span><span><span><span><span><span><span></span><span><span>ij</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span><span></span><span></span></span><span><span></span><span><span><span><span>∑</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span><span>S</span><span><span><span><span><span><span></span><span><span>ij</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span></span></span></span></span>. Inside the kernel, we currently divide during every iteration of the loop: <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>O</mi><mtext>new</mtext></msub><mo>=</mo><mfrac><mo lspace="0em" rspace="0em">…</mo><msub><mi>l</mi><mtext>new</mtext></msub></mfrac></mrow><annotation encoding="application/x-tex">O_{\text{new}} = \frac{\dots}{l_{\text{new}}}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>O</span><span><span><span><span><span><span></span><span><span><span><span>new</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span><span>l</span><span><span><span><span><span><span></span><span><span><span><span>new</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span><span></span><span></span></span><span><span></span><span><span><span>…</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span></span></span></span></span>. This normalization step does not need to happen each time. We will store the unnormalized accumulated numerator and denominator in registers throughout the loop. We will perform the normalization <strong>once</strong>, at the end of the kernel, just before writing the result to HBM.</p>
</li>
<li>
<p><strong>Tuning Block Sizes</strong>: Theoretical occupancy is capped at 25% due to shared-memory pressure. Smaller <code>B_c</code> may reduce register/SRAM pressure and increase the number of active warps. But too small a tile can fragment memory access and reduce bandwidth efficiency. I’ll leave tweaking this value for later.</p>
</li>
</ol>
<h2 id="my-flash-attention-v2">(My) Flash Attention v2</h2>
<p>The v2 implementation (<a href="https://github.com/AmineDiro/nano-llama-flash/blob/main/kernels/triton_flash_att_v2.py"><code>kernels/triton_flash_att_v2.py</code></a>) makes critical changes for better performance:</p>
<h3 id="reorganized-loop-structure">Reorganized Loop Structure</h3>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>@triton.jit</span>
</span></span><span><span><span>def</span> <span>attn_kernel</span>(
</span></span><span><span>    Q, K, V, O,
</span></span><span><span>    S, stride_H, softmax_scale,
</span></span><span><span>    D: tl<span>.</span>constexpr, Tc: tl<span>.</span>constexpr, Bc: tl<span>.</span>constexpr
</span></span><span><span>  ):
</span></span><span><span>    <span># ... offset setup</span>
</span></span><span><span>    <span># Load query block ONCE at the start !!</span>
</span></span><span><span>    qi <span>=</span> tl<span>.</span>load(
</span></span><span><span>        q_ptr <span>+</span> offset_i, mask<span>=</span>mask[:, <span>None</span>], other<span>=</span><span>0.0</span>
</span></span><span><span>    )  <span># shape (Bc,D)</span>
</span></span><span><span>
</span></span><span><span>    <span># Block accumulator and running max in SRAM  !!</span>
</span></span><span><span>    prev_li <span>=</span> tl<span>.</span>zeros([Bc], dtype<span>=</span>tl<span>.</span>float32)
</span></span><span><span>    prev_mi <span>=</span> tl<span>.</span>zeros([Bc], dtype<span>=</span>tl<span>.</span>float32) <span>-</span> float(<span>&#34;inf&#34;</span>)
</span></span><span><span>    acc <span>=</span> tl<span>.</span>zeros([Bc, D], dtype<span>=</span>tl<span>.</span>float32)
</span></span><span><span>
</span></span><span><span>    <span>for</span> j <span>in</span> range(<span>0</span>, Tc):
</span></span><span><span>        <span># .. setup offset for K and V</span>
</span></span><span><span>        <span># Load K_j, V_j from HBM to SRAM</span>
</span></span><span><span>        kj <span>=</span> tl<span>.</span>load(k_ptr <span>+</span> offset_j)  <span># shape(Bc,D)</span>
</span></span><span><span>        vj <span>=</span> tl<span>.</span>load(v_ptr <span>+</span> offset_j)  <span># shape(Bc,D)</span>
</span></span><span><span>
</span></span><span><span>        <span># Compute Sij on SRAM : Q_i * K_j.T / sqrt(D)</span>
</span></span><span><span>        Sij <span>=</span> tl<span>.</span>dot(qi, tl<span>.</span>trans(kj)) <span>*</span> softmax_scale  <span># (Bc,Bc)</span>
</span></span><span><span>
</span></span><span><span>
</span></span><span><span>        <span># accumulators</span>
</span></span><span><span>        mij <span>=</span> tl<span>.</span>max(Sij, <span>1</span>) <span># Rowmax(Sij): (Bc,)</span>
</span></span><span><span>        pij <span>=</span> tl<span>.</span>exp(Sij <span>-</span> mij[:, <span>None</span>])  <span># (Bc,Bc)</span>
</span></span><span><span>        lij <span>=</span> tl<span>.</span>sum(pij, <span>1</span>)  <span># (Bc,)</span>
</span></span><span><span>
</span></span><span><span>        <span># Running maximum</span>
</span></span><span><span>        mi_new <span>=</span> tl<span>.</span>maximum(prev_mi, mij)
</span></span><span><span>
</span></span><span><span>        <span># Compute scaling factors using previous_max</span>
</span></span><span><span>        alpha <span>=</span> tl<span>.</span>exp(prev_mi <span>-</span> mi_new)
</span></span><span><span>        beta <span>=</span> tl<span>.</span>exp(mij <span>-</span> mi_new)
</span></span><span><span>
</span></span><span><span>        <span># Update running sum</span>
</span></span><span><span>        li_new <span>=</span> prev_li <span>*</span> alpha <span>+</span> lij <span>*</span> beta
</span></span><span><span>
</span></span><span><span>        <span># Update the output block</span>
</span></span><span><span>        acc <span>=</span> alpha[:, <span>None</span>] <span>*</span> acc <span>+</span> beta[:, <span>None</span>] <span>*</span> tl<span>.</span>dot(pij, vj)
</span></span><span><span>
</span></span><span><span>        prev_li <span>=</span> li_new
</span></span><span><span>        prev_mi <span>=</span> mi_new
</span></span><span><span>
</span></span><span><span>    <span># Divide by the  last accumulated sum !</span>
</span></span><span><span>    acc <span>=</span> acc <span>/</span> prev_li[:, <span>None</span>]
</span></span><span><span>    <span># Update in HBM</span>
</span></span><span><span>    tl<span>.</span>store(o_ptr <span>+</span> offset_i, acc)
</span></span></code></pre></div><p>I also adjusted the grid configuration:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span>grid <span>=</span> <span>lambda</span> META: (triton<span>.</span>cdiv(S, META[<span>&#34;Bc&#34;</span>]), B <span>*</span> N_h)
</span></span></code></pre></div><p>Where <strong>Dimension 0</strong>: Number of Q blocks = <code>S / Bc</code> and <strong>Dimension 1</strong>: Batch × Heads = <code>B × N_h</code>. To reiterate the main changes compared to v1:</p>
<ol>
<li><strong>Single loop</strong>: Each thread block processes one Q block, iterating over all K/V blocks. Each thread block is independent, allowing parallelism. With <code>B=10</code>, <code>N_h=64</code>, <code>S=1024</code>, <code>D_h=32</code> <code>Bc=32</code>, we launch <code>32 × 640 = 20,480</code> independent thread blocks.</li>
<li><strong>Load Q once</strong>: Query block is loaded once and reused across all iterations</li>
<li><strong>Register accumulation</strong>: Output accumulator <code>acc</code> stays in fast registers, no main memory writes until the end</li>
<li><strong>No intermediate main memory traffic</strong>: Notice that <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>l</span></span></span></span></span> and <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>m</span></span></span></span></span> are kept in registers, only final output written to main memory.</li>
</ol>
<h3 id="profiling-the-v2-implementation">Profiling the v2 Implementation</h3>
<p>Great, let’s profile our newly crafted kernel and see what <code>ncu</code> tells us!</p>
<p><img src="https://aminediro.com/posts/flash_attn/images/compare_v2_v1.png" alt="compare_v2_v1"/></p>
<p>Well, it is faster that the v1 but only <strong>6%</strong> faster. Let’s first look at the memory chart that was the biggest</p>
<p><img src="https://aminediro.com/posts/flash_attn/images/memory_v2_v1.png" alt="memory_v2_v1"/></p>
<p>Great! Rewriting reduced main memory reads to <strong>412.18 MB (-92.98%)</strong>. We are also writing <code>80MB</code> corresponding exactly to the <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi></mrow><annotation encoding="application/x-tex">O</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span></span></span></span></span> matrix size. But there is still a clear problem somewhere—we would expect a lot more speedup as we are now parallelizing 32x more. Let’s look at the occupancy; maybe it is somehow worse than v1 (sanity check here)?</p>
<p><img src="https://aminediro.com/posts/flash_attn/images/occupancy_v2.png" alt="occupancy_v2"/></p>
<p>We are at <strong>63%</strong> occupancy and are limited by shared memory with <strong>~12KB</strong> per block. So that’s clearly not the culprit. Let’s go back to the summary page of the profile; <code>ncu</code> picks up on these issues:</p>
<blockquote>
<ul>
<li><strong>Shared Load Bank Conflicts Est. Speedup: 63.57%</strong>: The memory access pattern for shared loads might not be optimal and causes on average a <strong>6.3 - way bank conflict</strong> across all 293601280 shared load requests. This results in 1174579308 bank conflicts, which represent 63.64% of the overall 1845667948 wavefronts for shared loads. Check the  Source Counters section for uncoalesced shared loads.</li>
<li><strong>Uncoalesced Shared Accesses Est. Speedup: 61.46%</strong>: This kernel has uncoalesced shared accesses resulting in a total of 1174405120 excessive wavefronts (62% of the total 1909063680 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.</li>
<li><strong>MIO Throttle Stalls Est. Speedup: 50.06%:</strong> On average, each warp of this workload spends 18.4 cycles being stalled waiting for the MIO (memory input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline pressure. This stall type represents about 50.7% of the total average of 36.2 cycles between issuing two instructions.</li>
</ul>
</blockquote>
<p>It seems to be a problem with how we access shared memory in our kernel. A lot of terms are thrown around here, so let’s first understand what <strong>bank conflicts, uncoalesced shared access, and L1 wavefronts shared load</strong> mean.</p>

<p>The profiler is screaming at us about bank conflicts and wavefronts. Before we can fix anything, we need to understand what’s actually happening at the hardware level. Let’s build intuition from the ground up.</p>
<p>Remember from the <a href="#quick-detour-into-gpu-memory-land">memory hierarchy section</a> that shared memory (SRAM) lives on-chip, right next to the compute units. It’s blazingly fast (~10-20 TB/s) but there’s a catch: shared memory isn’t a single monolithic block. It’s physically divided into <strong>32 memory banks</strong> that can be accessed simultaneously (remember how each warp has 32 threads).</p>
<p>Think of these banks like checkout lanes at a grocery store. If 32 customers each go to a different lane, everyone gets served in one “cycle.” But if multiple customers try to use the same lane, they have to wait in line - that’s a <strong>bank conflict</strong>.</p>
<p>The mapping from memory address to bank is straightforward:</p>
<p><span><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Bank number</mtext><mo>=</mo><mrow><mo fence="true">⌊</mo><mfrac><mtext>byte address</mtext><mn>4</mn></mfrac><mo fence="true">⌋</mo></mrow><mspace></mspace><mspace width="1em"></mspace><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">d</mi></mrow>  <mn>32</mn></mrow><annotation encoding="application/x-tex">
\text{Bank number} = \left\lfloor \frac{\text{byte address}}{4} \right\rfloor \mod 32
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>Bank number</span></span><span></span><span>=</span><span></span></span><span><span></span><span><span><span>⌊</span></span><span><span></span><span><span><span><span><span><span></span><span><span>4</span></span></span><span><span></span><span></span></span><span><span></span><span><span><span>byte address</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span><span>⌋</span></span></span><span></span><span></span><span></span></span><span><span></span><span><span><span>mod</span></span></span><span></span><span></span><span>32</span></span></span></span></span></span></p>
<p>For a <code>float32</code> array in shared memory, consecutive elements land in consecutive banks:</p>
<pre tabindex="0"><code>data[0]  → byte 0   → bank 0
data[1]  → byte 4   → bank 1
...
data[31] → byte 124 → bank 31
data[32] → byte 128 → bank 0   ← wraps around!
</code></pre><p>This is intentional! It means that when threads in a warp access consecutive array elements (a very common pattern), each thread hits a different bank.
Perfect parallelism !</p>
<h4 id="what-goes-wrong-the-strided-access-pattern">What Goes Wrong: The Strided Access Pattern</h4>
<p>Now here’s where things get spicy. Let’s look at which lines in our kernels this access get’s spicy. Thankfully <code>ncu</code> again show us the exact problematic line :</p>
<p><img src="https://aminediro.com/posts/flash_attn/images/transpose_v2.png" alt="tranpose_v2"/></p>
<p>So it turns out, that this line is the culprit :</p>
<div><pre tabindex="0"><code data-lang="python"><span><span>Sij <span>=</span> tl<span>.</span>dot(qi, tl<span>.</span>trans(kj)) <span>*</span> softmax_scale
</span></span></code></pre></div><p>But why though ? To understand why we are hitting shared memory bank conflict we’ll need to go a little bit deeper. How deep you ask? Well to the end of the world… But first, let’s continue understanding the remaining <code>ncu</code> reported issues.</p>
<h4 id="wavefronts-the-unit-of-memory-work">Wavefronts: The Unit of Memory Work</h4>
<p>The profiler keeps mentioning “wavefronts” - what are those? A <strong>wavefront</strong> (sometimes called a memory transaction) is a single memory operation that the hardware executes atomically. In the <strong>ideal case</strong>: 32 threads request shared memory, all hit different banks → <strong>1 wavefront</strong> → 1 cycle</p>
<ul>
<li><strong>N-way bank conflict</strong>: N threads want the same bank → <strong>N wavefronts</strong> → N cycles</li>
</ul>
<p>Now the profiler numbers start making sense:</p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Value</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>Shared load requests</td>
<td>293,601,280</td>
<td>Times our kernel asked for shared memory</td>
</tr>
<tr>
<td>Bank conflicts</td>
<td>1,174,579,308</td>
<td>Extra transactions due to serialization</td>
</tr>
<tr>
<td>Total wavefronts</td>
<td>1,845,667,948</td>
<td>Actual memory operations executed</td>
</tr>
<tr>
<td><strong>Conflict rate</strong></td>
<td><strong>63.64%</strong></td>
<td>Almost 2/3 of bandwidth wasted!</td>
</tr>
</tbody>
</table>
<p>The <strong>6.3-way average conflict</strong> means that on average, 6.3 threads are fighting for the same bank.</p>
<blockquote>
<p>The profiler is telling us we’re doing 6x more memory transactions than necessary. No wonder we only got only 6% speedup despite reducing main memory traffic by 93%!</p>
</blockquote>
<h4 id="uncoalesced-accesses-same-but-different-but-same">Uncoalesced Accesses: Same but different, but same</h4>
<p>The profiler also complains about **uncoalesced shared accesses&#34; with **62% excessive wavefronts**. This is essentially the same problem viewed from a different angle: <strong>Threads access scattered addresses</strong>. Our column reads are stride-<code>D</code> accesses, which the hardware cannot combine efficiently. The “excessive wavefronts” metric is counting how many extra transactions we’re issuing beyond the theoretical minimum.</p>
<h4 id="mio-throttle-the-pipeline-backs-up">MIO Throttle: The Pipeline Backs Up</h4>
<p>The third warning - <strong>MIO Throttle Stalls (50.06%)</strong> - is a consequence of the first two. MIO (Memory Input/Output) is the pipeline that handles:</p>
<ul>
<li>Shared memory operations</li>
<li>Special math instructions (like <code>exp</code>, <code>log</code>) (remember this for later)</li>
<li>Dynamic branches (this is the devil, avoid branches at all cost)</li>
</ul>
<p>When we’re issuing 6x more shared memory transactions than necessary, the MIO pipeline gets clogged. Warps have to stall waiting for the pipeline to clear, which is what the 50% stall rate is measuring.</p>
<h4 id="ptx-and-fun">PTX and fun</h4>
<p>OK, let’s go back and drill down on this single line:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span>Sij <span>=</span> tl<span>.</span>dot(qi, tl<span>.</span>trans(kj)) <span>*</span> softmax_scale
</span></span></code></pre></div><p>Well, we can’t just look at it, so let’s see what the Triton compiler generated for it. Triton compiles kernels down to PTX, and you can very easily map back the PTX code to Python line code:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>TRITON_CACHE_DIR<span>=</span><span>&#34;./triton_dump&#34;</span> python kernels/triton_flash_att_v2.py
</span></span><span><span>
</span></span><span><span>awk <span>&#39;/\.loc.* 83 /{p=1; print; next} /\.loc/{p=0} p&#39;</span> triton_dump/<span>[</span>ID<span>]</span>/attn_kernel.ptx &gt; attn_kernel_v2.ptx
</span></span></code></pre></div><p>Great! Let’s look at the assembly for the line <code>triton_flash_att_v2.py:83</code>:</p>
<div><pre tabindex="0"><code data-lang="asm"><span><span><span>.loc</span>	<span>1</span> <span>83</span> <span>34</span>                         <span>// triton_flash_att_v2.py:83:34
</span></span></span><span><span><span></span><span>bar.sync</span> 	<span>0</span><span>;
</span></span></span><span><span><span>// We are storing stuff in shared memory `st.`
</span></span></span><span><span><span></span><span>st.shared.v4.b32</span> 	[%r4], <span>{</span>%r100, %r101, %r102, %r103<span>}</span><span>;
</span></span></span><span><span><span></span><span>st.shared.v4.b32</span> 	[%r4<span>+</span><span>2048</span>], <span>{</span>%r104, %r105, %r106, %r107<span>}</span><span>;
</span></span></span><span><span><span></span><span>st.shared.v4.b32</span> 	[%r4<span>+</span><span>4096</span>], <span>{</span>%r108, %r109, %r110, %r111<span>}</span><span>;
</span></span></span><span><span><span></span><span>st.shared.v4.b32</span> 	[%r4<span>+</span><span>6144</span>], <span>{</span>%r112, %r113, %r114, %r115<span>}</span><span>;
</span></span></span><span><span><span></span>
</span></span><span><span>
</span></span><span><span><span>// Here we are loading something from shared memory `ld.`
</span></span></span><span><span><span></span><span>.loc</span>	<span>1</span> <span>83</span> <span>34</span>                         <span>// triton_flash_att_v2.py:83:34
</span></span></span><span><span><span></span><span>ld.shared.v4.b32</span> 	<span>{</span>%r388, %r389, %r390, %r391<span>}</span>, [%r8]<span>;
</span></span></span><span><span><span></span><span>ld.shared.v4.b32</span> 	<span>{</span>%r392, %r393, %r394, %r395<span>}</span>, [%r8<span>+</span><span>256</span>]<span>;
</span></span></span><span><span><span></span><span>ld.shared.v4.b32</span> 	<span>{</span>%r396, %r397, %r398, %r399<span>}</span>, [%r8<span>+</span><span>16</span>]<span>;
</span></span></span><span><span><span></span><span>ld.shared.v4.b32</span> 	<span>{</span>%r400, %r401, %r402, %r403<span>}</span>, [%r8<span>+</span><span>272</span>]<span>;
</span></span></span><span><span><span></span><span>ld.shared.v4.b32</span> 	<span>{</span>%r404, %r405, %r406, %r407<span>}</span>, [%r8<span>+</span><span>32</span>]<span>;
</span></span></span><span><span><span>// ... A lot more ld.shared.v4.b32 instruction
</span></span></span><span><span><span></span>
</span></span><span><span><span>// Then finally we do the acual matmul `tl.dot`
</span></span></span><span><span><span></span><span>.loc</span>	<span>1</span> <span>83</span> <span>25</span>                         <span>// triton_flash_att_v2.py:83:25
</span></span></span><span><span><span></span><span>fma.rn.f32</span> 	%r516, %r132, %r388, <span>0</span><span>f00000000</span><span>;
</span></span></span><span><span><span></span><span>fma.rn.f32</span> 	%r517, %r133, %r389, %r516<span>;
</span></span></span><span><span><span></span><span>fma.rn.f32</span> 	%r518, %r134, %r390, %r517<span>;
</span></span></span><span><span><span></span>
</span></span><span><span><span>// And this mul is the `.softmax_scale` we can ignore it
</span></span></span><span><span><span></span><span>.loc</span>	<span>1</span> <span>83</span> <span>41</span>                         <span>// triton_flash_att_v2.py:83:41
</span></span></span><span><span><span></span><span>mul.f32</span> 	%r1028, %r25, %r579<span>;
</span></span></span><span><span><span></span><span>mul.f32</span> 	%r1029, %r25, %r643<span>;
</span></span></span><span><span><span></span><span>mul.f32</span> 	%r1030, %r25, %r707<span>;
</span></span></span><span><span><span></span><span>mul.f32</span> 	%r1031, %r25, %r771<span>;
</span></span></span><span><span><span></span><span>mul.f32</span> 	%r1032, %r25, %r835<span>;
</span></span></span></code></pre></div><p>The storing PTX part is pretty straightforward: we are storing <code>K</code> from registers to shared memory in parallel with each of the 128 threads in the warp executing these instructions in parallel. <code>st.shared.v4.b32</code> is a vectorized instruction, meaning we store <code>4 * 4bytes = 64bytes</code> in one shot. Remember that we have <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">K_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>K</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span> of size <code>(Bc, D)</code>, meaning each chunk is <code>32 x 64 = 8192 bytes</code>. We are issuing 4 stores of 64 bytes per thread, and with 128 threads this amounts to exactly <code>8192 bytes</code>. Important detail: we are storing <code>K</code> in row-major order in shared memory, with a <strong>row stride of <code>256 bytes</code>.</strong> Here is how K is stored in shared memory with (a base offset of 0 to simplify):</p>
<p><img src="https://aminediro.com/posts/flash_attn/images/light_k_banks.png" alt="output tensor"/>
  <img src="https://aminediro.com/posts/flash_attn/images/dark_k_banks.png" alt="output tensor"/>
</p>
<p>Now, the next set of instructions are very interesting. <code>ld.shared.v4.b32</code> is a vectorized load of <code>4x 4bytes</code> wide from shared memory. Remember again, these instructions are executed in parallel by all threads in the warp, so to understand where the conflict happens, we need to look at what adresses each thread is loading from shared memory.</p>
<div><pre tabindex="0"><code data-lang="asm"><span><span><span>ld.shared.v4.b32</span> 	<span>{</span>%r388, %r389, %r390, %r391<span>}</span>, [%r8]<span>;
</span></span></span><span><span><span></span><span>ld.shared.v4.b32</span> 	<span>{</span>%r392, %r393, %r394, %r395<span>}</span>, [%r8<span>+</span><span>256</span>]<span>;
</span></span></span><span><span><span></span><span>ld.shared.v4.b32</span> 	<span>{</span>%r396, %r397, %r398, %r399<span>}</span>, [%r8<span>+</span><span>16</span>]<span>;
</span></span></span><span><span><span></span><span>ld.shared.v4.b32</span> 	<span>{</span>%r400, %r401, %r402, %r403<span>}</span>, [%r8<span>+</span><span>272</span>]<span>;
</span></span></span><span><span><span></span><span>ld.shared.v4.b32</span> 	<span>{</span>%r404, %r405, %r406, %r407<span>}</span>, [%r8<span>+</span><span>32</span>]<span>;
</span></span></span></code></pre></div><p>Looking at the pattern, it seems that we are doing multiple loads with an offset from a base register <code>%r8</code>, so to know from which memory address we are effectively loading, we need to figure out what this register holds for each thread. A quick <code>grep</code> across the PTX codebase and we find this:</p>
<div><pre tabindex="0"><code data-lang="asm"><span><span><span>mov.u32</span>   %r1, %tid.x<span>;              // Thread ID (0-127)
</span></span></span><span><span><span></span><span>and.b32</span>   %r77, %r1, <span>15</span><span>;            // tid &amp; 0xF (0-15)
</span></span></span><span><span><span></span><span>shl.b32</span>   %r93, %r77, <span>9</span><span>;            // shift left by 9
</span></span></span><span><span><span></span><span>add.s32</span>   %r8, %r91, %r93<span>;          // %r8 = smem_base + lane_group * 512
</span></span></span></code></pre></div><p>Nice! I thought it would be way more tedious. But let’s break down what these instructions are doing. Thankfully, we are in 2025 and LLMs exist to explain PTX; one prompt to Gemini later:</p>
<ul>
<li><code>%r1 = %tid.x</code>: Set <code>%r1</code> register to thread ID within the block (0 to 127)</li>
<li><code>%r77 = tid &amp; 15</code>: Extract low 4 bits (values 0-15, repeats for lanes 16-31)</li>
<li><code>%r93 = %r77 &lt;&lt; 9</code>: Multiply by 512 (= 2^9)</li>
<li><code>%r8 = smem_base + lane_group*512 = base + %r93</code>: Final shared memory address</li>
</ul>
<p>This means that for each thread in the warp we have:</p>
<p><span><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnalign="center center left" columnlines="solid solid" columnspacing="1em" rowlines="solid none none none none none none solid none none none"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>Lane</mtext></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>tid</mtext><mo>∧</mo><mn>15</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>r8 = base + offset</mtext></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>base</mtext><mo>+</mo><mn>0</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>base</mtext><mo>+</mo><mn>512</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>2</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>2</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>base</mtext><mo>+</mo><mn>1024</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>3</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>3</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>base</mtext><mo>+</mo><mn>1536</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">⋯</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">⋯</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">⋯</mo></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>14</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>14</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>base</mtext><mo>+</mo><mn>7168</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>15</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>15</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>base</mtext><mo>+</mo><mn>7680</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext mathvariant="bold">16</mtext></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext mathvariant="bold">0</mtext></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mrow><mtext mathvariant="bold">base</mtext><mtext> </mtext><mtext mathvariant="bold">+</mtext><mtext> </mtext><mtext mathvariant="bold">0</mtext></mrow><mo>←</mo><mtext>duplicate!</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>17</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>base</mtext><mo>+</mo><mn>512</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">⋯</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">⋯</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">⋯</mo></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>31</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>15</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>base</mtext><mo>+</mo><mn>7680</mn></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{array}{c|c|l}
\text{Lane} &amp; \text{tid} \land 15 &amp; \text{r8 = base + offset} \\
\hline
0  &amp; 0  &amp; \text{base} + 0 \\
1  &amp; 1  &amp; \text{base} + 512 \\
2  &amp; 2  &amp; \text{base} + 1024 \\
3  &amp; 3  &amp; \text{base} + 1536 \\
\cdots  &amp; \cdots  &amp; \cdots \\
14 &amp; 14 &amp; \text{base} + 7168 \\
15 &amp; 15 &amp; \text{base} + 7680 \\
\hline
\textbf{16} &amp; \textbf{0} &amp; \textbf{base + 0} \leftarrow \text{duplicate!} \\
17 &amp; 1  &amp; \text{base} + 512 \\
\cdots  &amp; \cdots  &amp; \cdots \\
31 &amp; 15 &amp; \text{base} + 7680 \\
\end{array}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>Lane</span></span></span></span><span><span></span><span><span>0</span></span></span><span><span></span><span><span>1</span></span></span><span><span></span><span><span>2</span></span></span><span><span></span><span><span>3</span></span></span><span><span></span><span><span>⋯</span></span></span><span><span></span><span><span>14</span></span></span><span><span></span><span><span>15</span></span></span><span><span></span><span><span><span>16</span></span></span></span><span><span></span><span><span>17</span></span></span><span><span></span><span><span>⋯</span></span></span><span><span></span><span><span>31</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span></span><span></span><span><span><span><span><span><span></span><span><span><span>tid</span></span><span></span><span>∧</span><span></span><span>15</span></span></span><span><span></span><span><span>0</span></span></span><span><span></span><span><span>1</span></span></span><span><span></span><span><span>2</span></span></span><span><span></span><span><span>3</span></span></span><span><span></span><span><span>⋯</span></span></span><span><span></span><span><span>14</span></span></span><span><span></span><span><span>15</span></span></span><span><span></span><span><span><span>0</span></span></span></span><span><span></span><span><span>1</span></span></span><span><span></span><span><span>⋯</span></span></span><span><span></span><span><span>15</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span></span><span></span><span><span><span><span><span><span></span><span><span><span>r8 = base + offset</span></span></span></span><span><span></span><span><span><span>base</span></span><span></span><span>+</span><span></span><span>0</span></span></span><span><span></span><span><span><span>base</span></span><span></span><span>+</span><span></span><span>512</span></span></span><span><span></span><span><span><span>base</span></span><span></span><span>+</span><span></span><span>1024</span></span></span><span><span></span><span><span><span>base</span></span><span></span><span>+</span><span></span><span>1536</span></span></span><span><span></span><span><span>⋯</span></span></span><span><span></span><span><span><span>base</span></span><span></span><span>+</span><span></span><span>7168</span></span></span><span><span></span><span><span><span>base</span></span><span></span><span>+</span><span></span><span>7680</span></span></span><span><span></span><span><span><span>base + 0</span></span><span></span><span>←</span><span></span><span><span>duplicate!</span></span></span></span><span><span></span><span><span><span>base</span></span><span></span><span>+</span><span></span><span>512</span></span></span><span><span></span><span><span>⋯</span></span></span><span><span></span><span><span><span>base</span></span><span></span><span>+</span><span></span><span>7680</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span></span><span><span></span><span></span></span><span><span></span><span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></p>
<blockquote>
<p><strong>Key insight</strong>: Due to masking <code>tid &amp; 15</code>, only <strong>16 unique base addresses</strong> exist. Lanes 16-31 duplicate lanes 0-15.</p>
</blockquote>
<p>So within a single warp of 32 threads, threads <code>0-15</code> get unique base addresses (0×512, 1×512, … 15×512) and threads <code>16-31</code> get duplicate addresses (same as lanes 0-15)! But bear in mind, we are not doing 2x the loading. When two lanes access the same address, the <strong>hardware broadcasts—it</strong>, fetches once and delivers to both. No conflict here. Let’s dig deeper to see where the conflict occurs.</p>
<p>Now that we have the base pointer address, let’s see what each thread loads. The PTX shows that we executed <strong>32 loads</strong> per thread to get data for the matrix multiply:</p>
<div><pre tabindex="0"><code data-lang="asm"><span><span><span>ld.shared.v4.b32</span>  [%r8]<span>;        // 4 floats at base + 0
</span></span></span><span><span><span></span><span>ld.shared.v4.b32</span>  [%r8<span>+</span><span>256</span>]<span>;    // 4 floats at base + 256 (next row)
</span></span></span><span><span><span></span><span>ld.shared.v4.b32</span>  [%r8<span>+</span><span>16</span>]<span>;     // 4 floats at base + 16 (next 4 cols)
</span></span></span><span><span><span></span><span>ld.shared.v4.b32</span>  [%r8<span>+</span><span>272</span>]<span>;    // 4 floats at base + 272 (next row, next 4 cols)
</span></span></span><span><span><span></span><span>ld.shared.v4.b32</span>  [%r8<span>+</span><span>32</span>]<span>;     // 4 floats at base + 32
</span></span></span><span><span><span></span><span>ld.shared.v4.b32</span>  [%r8<span>+</span><span>288</span>]<span>;    // 4 floats at base + 288
</span></span></span><span><span><span></span><span>ld.shared.v4.b32</span>  [%r8<span>+</span><span>48</span>]<span>;     // ...
</span></span></span><span><span><span></span><span>ld.shared.v4.b32</span>  [%r8<span>+</span><span>304</span>]<span>;
</span></span></span><span><span><span></span><span>ld.shared.v4.b32</span>  [%r8<span>+</span><span>64</span>]<span>;
</span></span></span><span><span><span></span><span>ld.shared.v4.b32</span>  [%r8<span>+</span><span>320</span>]<span>;
</span></span></span><span><span><span></span><span>ld.shared.v4.b32</span>  [%r8<span>+</span><span>80</span>]<span>;
</span></span></span><span><span><span></span><span>ld.shared.v4.b32</span>  [%r8<span>+</span><span>336</span>]<span>;
</span></span></span><span><span><span></span><span>ld.shared.v4.b32</span>  [%r8<span>+</span><span>96</span>]<span>;
</span></span></span><span><span><span></span><span>ld.shared.v4.b32</span>  [%r8<span>+</span><span>352</span>]<span>;
</span></span></span><span><span><span></span><span>ld.shared.v4.b32</span>  [%r8<span>+</span><span>112</span>]<span>;
</span></span></span><span><span><span></span><span>ld.shared.v4.b32</span>  [%r8<span>+</span><span>368</span>]<span>;
</span></span></span><span><span><span></span><span>...</span> (<span>32</span> <span>total</span> <span>loads</span>)
</span></span></code></pre></div><p>The offsets follow an <strong>interleaved pattern</strong>:</p>
<ul>
<li>Column 1: 0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240</li>
<li>Column 2: 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496</li>
</ul>
<p>Let’s draw over our previous shared memory schema to see which addresses of K are loaded by which threads and to clearly visualize this interleaved access pattern:</p>
<p><img src="https://aminediro.com/posts/flash_attn/images/conflict_shmem.png" alt="output tensor"/>
  <img src="https://aminediro.com/posts/flash_attn/images/dark_conflict_shmem.png" alt="output tensor"/>
</p>
<p>And there is the bank conflict right there!</p>
<p><span><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnalign="center left center center center center left" columnlines="solid solid none none none solid" columnspacing="1em" rowlines="solid none none none none none none none none none none none none none none none solid"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>Lane</mtext></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>Address</mtext></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>Bank 0</mtext></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>Bank 1</mtext></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>Bank 2</mtext></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>Bank 3</mtext></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>base</mtext><mo>+</mo><mn>0</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>base</mtext><mo>+</mo><mn>512</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>←</mo><mtext>conflict</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>2</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>base</mtext><mo>+</mo><mn>1024</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>←</mo><mtext>conflict</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>3</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>base</mtext><mo>+</mo><mn>1536</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>←</mo><mtext>conflict</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>4</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>base</mtext><mo>+</mo><mn>2048</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>←</mo><mtext>conflict</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>5</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>base</mtext><mo>+</mo><mn>2560</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>←</mo><mtext>conflict</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>6</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>base</mtext><mo>+</mo><mn>3072</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>←</mo><mtext>conflict</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>7</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>base</mtext><mo>+</mo><mn>3584</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>←</mo><mtext>conflict</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>8</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>base</mtext><mo>+</mo><mn>4096</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>←</mo><mtext>conflict</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>9</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>base</mtext><mo>+</mo><mn>4608</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>←</mo><mtext>conflict</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>10</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>base</mtext><mo>+</mo><mn>5120</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>←</mo><mtext>conflict</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>11</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>base</mtext><mo>+</mo><mn>5632</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>←</mo><mtext>conflict</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>12</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>base</mtext><mo>+</mo><mn>6144</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>←</mo><mtext>conflict</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>13</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>base</mtext><mo>+</mo><mn>6656</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>←</mo><mtext>conflict</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>14</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>base</mtext><mo>+</mo><mn>7168</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>←</mo><mtext>conflict</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>15</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>base</mtext><mo>+</mo><mn>7680</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">∙</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>←</mo><mtext>conflict</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>16</mn><mo>−</mo><mn>31</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>broadcast from lanes 0-15</mtext></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{array}{c|l|cccc|l}
\text{Lane} &amp; \text{Address} &amp; \text{Bank 0} &amp; \text{Bank 1} &amp; \text{Bank 2} &amp; \text{Bank 3} &amp; \\
\hline
0  &amp; \text{base} + 0      &amp; \bullet &amp; \bullet &amp; \bullet &amp; \bullet &amp; \\
1  &amp; \text{base} + 512    &amp; \bullet &amp; \bullet &amp; \bullet &amp; \bullet &amp; \leftarrow \text{conflict} \\
2  &amp; \text{base} + 1024   &amp; \bullet &amp; \bullet &amp; \bullet &amp; \bullet &amp; \leftarrow \text{conflict} \\
3  &amp; \text{base} + 1536   &amp; \bullet &amp; \bullet &amp; \bullet &amp; \bullet &amp; \leftarrow \text{conflict} \\
4  &amp; \text{base} + 2048   &amp; \bullet &amp; \bullet &amp; \bullet &amp; \bullet &amp; \leftarrow \text{conflict} \\
5  &amp; \text{base} + 2560   &amp; \bullet &amp; \bullet &amp; \bullet &amp; \bullet &amp; \leftarrow \text{conflict} \\
6  &amp; \text{base} + 3072   &amp; \bullet &amp; \bullet &amp; \bullet &amp; \bullet &amp; \leftarrow \text{conflict} \\
7  &amp; \text{base} + 3584   &amp; \bullet &amp; \bullet &amp; \bullet &amp; \bullet &amp; \leftarrow \text{conflict} \\
8  &amp; \text{base} + 4096   &amp; \bullet &amp; \bullet &amp; \bullet &amp; \bullet &amp; \leftarrow \text{conflict} \\
9  &amp; \text{base} + 4608   &amp; \bullet &amp; \bullet &amp; \bullet &amp; \bullet &amp; \leftarrow \text{conflict} \\
10 &amp; \text{base} + 5120   &amp; \bullet &amp; \bullet &amp; \bullet &amp; \bullet &amp; \leftarrow \text{conflict} \\
11 &amp; \text{base} + 5632   &amp; \bullet &amp; \bullet &amp; \bullet &amp; \bullet &amp; \leftarrow \text{conflict} \\
12 &amp; \text{base} + 6144   &amp; \bullet &amp; \bullet &amp; \bullet &amp; \bullet &amp; \leftarrow \text{conflict} \\
13 &amp; \text{base} + 6656   &amp; \bullet &amp; \bullet &amp; \bullet &amp; \bullet &amp; \leftarrow \text{conflict} \\
14 &amp; \text{base} + 7168   &amp; \bullet &amp; \bullet &amp; \bullet &amp; \bullet &amp; \leftarrow \text{conflict} \\
15 &amp; \text{base} + 7680   &amp; \bullet &amp; \bullet &amp; \bullet &amp; \bullet &amp; \leftarrow \text{conflict} \\
\hline
16-31 &amp; \text{broadcast from lanes 0-15} &amp; &amp; &amp; &amp; &amp; \\
\end{array}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>Lane</span></span></span></span><span><span></span><span><span>0</span></span></span><span><span></span><span><span>1</span></span></span><span><span></span><span><span>2</span></span></span><span><span></span><span><span>3</span></span></span><span><span></span><span><span>4</span></span></span><span><span></span><span><span>5</span></span></span><span><span></span><span><span>6</span></span></span><span><span></span><span><span>7</span></span></span><span><span></span><span><span>8</span></span></span><span><span></span><span><span>9</span></span></span><span><span></span><span><span>10</span></span></span><span><span></span><span><span>11</span></span></span><span><span></span><span><span>12</span></span></span><span><span></span><span><span>13</span></span></span><span><span></span><span><span>14</span></span></span><span><span></span><span><span>15</span></span></span><span><span></span><span><span>16</span><span></span><span>−</span><span></span><span>31</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span></span><span></span><span><span><span><span><span><span></span><span><span><span>Address</span></span></span></span><span><span></span><span><span><span>base</span></span><span></span><span>+</span><span></span><span>0</span></span></span><span><span></span><span><span><span>base</span></span><span></span><span>+</span><span></span><span>512</span></span></span><span><span></span><span><span><span>base</span></span><span></span><span>+</span><span></span><span>1024</span></span></span><span><span></span><span><span><span>base</span></span><span></span><span>+</span><span></span><span>1536</span></span></span><span><span></span><span><span><span>base</span></span><span></span><span>+</span><span></span><span>2048</span></span></span><span><span></span><span><span><span>base</span></span><span></span><span>+</span><span></span><span>2560</span></span></span><span><span></span><span><span><span>base</span></span><span></span><span>+</span><span></span><span>3072</span></span></span><span><span></span><span><span><span>base</span></span><span></span><span>+</span><span></span><span>3584</span></span></span><span><span></span><span><span><span>base</span></span><span></span><span>+</span><span></span><span>4096</span></span></span><span><span></span><span><span><span>base</span></span><span></span><span>+</span><span></span><span>4608</span></span></span><span><span></span><span><span><span>base</span></span><span></span><span>+</span><span></span><span>5120</span></span></span><span><span></span><span><span><span>base</span></span><span></span><span>+</span><span></span><span>5632</span></span></span><span><span></span><span><span><span>base</span></span><span></span><span>+</span><span></span><span>6144</span></span></span><span><span></span><span><span><span>base</span></span><span></span><span>+</span><span></span><span>6656</span></span></span><span><span></span><span><span><span>base</span></span><span></span><span>+</span><span></span><span>7168</span></span></span><span><span></span><span><span><span>base</span></span><span></span><span>+</span><span></span><span>7680</span></span></span><span><span></span><span><span><span>broadcast from lanes 0-15</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span></span><span></span><span><span><span><span><span><span></span><span><span><span>Bank 0</span></span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span></span><span><span><span><span><span><span></span><span><span><span>Bank 1</span></span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span></span><span><span><span><span><span><span></span><span><span><span>Bank 2</span></span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span></span><span><span><span><span><span><span></span><span><span><span>Bank 3</span></span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span><span>∙</span></span></span><span><span></span><span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span></span><span></span><span><span><span><span><span><span></span><span></span></span><span><span></span><span></span></span><span><span></span><span><span>←</span><span></span><span><span>conflict</span></span></span></span><span><span></span><span><span>←</span><span></span><span><span>conflict</span></span></span></span><span><span></span><span><span>←</span><span></span><span><span>conflict</span></span></span></span><span><span></span><span><span>←</span><span></span><span><span>conflict</span></span></span></span><span><span></span><span><span>←</span><span></span><span><span>conflict</span></span></span></span><span><span></span><span><span>←</span><span></span><span><span>conflict</span></span></span></span><span><span></span><span><span>←</span><span></span><span><span>conflict</span></span></span></span><span><span></span><span><span>←</span><span></span><span><span>conflict</span></span></span></span><span><span></span><span><span>←</span><span></span><span><span>conflict</span></span></span></span><span><span></span><span><span>←</span><span></span><span><span>conflict</span></span></span></span><span><span></span><span><span>←</span><span></span><span><span>conflict</span></span></span></span><span><span></span><span><span>←</span><span></span><span><span>conflict</span></span></span></span><span><span></span><span><span>←</span><span></span><span><span>conflict</span></span></span></span><span><span></span><span><span>←</span><span></span><span><span>conflict</span></span></span></span><span><span></span><span><span>←</span><span></span><span><span>conflict</span></span></span></span><span><span></span><span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span></span><span><span></span><span></span></span><span><span></span><span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></p>
<p><strong>Result</strong>: 16 requests to banks 0,1,2,3. Hardware serializes these into 16 phases. This means we have a theorical efficiency in this section of: 1/16 = 6.25% Waste: 15/16 = <strong>93.75%</strong> !!</p>
<p>One thing we could do is set <code>D = D+1</code> and see what would happen. Theoretically, the off-by-one stride would spread accesses across all banks (This is the idea behind <strong>padding</strong> - adding a dummy column to break the stride alignment).</p>
<p>Unfortunately, you can’t set <code>D</code> to non power of 2. So we end up doubling the dim. The padding breaks bank conflict but it ends up doing a lot more loads and being way slower than the version.
The implementation is <a href="https://github.com/AmineDiro/nano-llama-flash/blob/main/kernels/triton_flash_att_v2_padded.py"><code>kernels/triton_flash_att_v2_padded.py</code></a> is very close to v2 with the addition to a function that computes the padded head_dim</p>
<div><pre tabindex="0"><code data-lang="python"><span><span>
</span></span><span><span><span>def</span> <span>compute_padded_headdim</span>(D_h):
</span></span><span><span>    <span>&#34;&#34;&#34;
</span></span></span><span><span><span>    Compute padded head dimension to avoid bank conflicts.
</span></span></span><span><span><span>
</span></span></span><span><span><span>    The doubling provides extra padding that naturally breaks stride-based conflicts.
</span></span></span><span><span><span>    With stride=64 instead of 32, threads access different bank groups.
</span></span></span><span><span><span>    &#34;&#34;&#34;</span>
</span></span><span><span>    <span># Find next power of 2</span>
</span></span><span><span>    <span>if</span> D_h <span>&lt;=</span> <span>0</span>:
</span></span><span><span>        <span>return</span> <span>1</span>
</span></span><span><span>    <span># Check if already power of 2</span>
</span></span><span><span>    <span>if</span> (D_h <span>&amp;</span> (D_h <span>-</span> <span>1</span>)) <span>==</span> <span>0</span>:
</span></span><span><span>        <span># Already power of 2, double it</span>
</span></span><span><span>        <span>return</span> D_h <span>*</span> <span>2</span>
</span></span><span><span>    <span>else</span>:
</span></span><span><span>        <span># Round up to next power of 2</span>
</span></span><span><span>        <span>return</span> <span>1</span> <span>&lt;&lt;</span> (D_h <span>-</span> <span>1</span>)<span>.</span>bit_length()
</span></span></code></pre></div><p>We can see that bank conflicts got down to <strong>3.4 - way bank conflict across all 21135360 shared store requests</strong>. So I guess this worked 😂..</p>
<p>Back to the drawing board, Now that we understand the problem, we have several options:</p>
<ol>
<li><strong>Padding</strong>: Allocate K as <code>(Bc, D+1)</code> in shared memory but only use <code>D</code> columns - breaks the stride alignment -&gt; works, but additional work makes it slower.</li>
<li><strong>Pre-transpose K</strong>: Store K in column-major order before the kernel runs, so “column” reads become row reads</li>
<li><strong>Larger D</strong>: If we increase head dimension to something not divisible by 32, conflicts naturally reduce (but this changes the model architecture)</li>
<li><strong>Swizzling</strong>: Use a permuted memory layout that distributes bank accesses (this is what CUTLASS and newer Flash Attention versions do)</li>
</ol>
<p>In the next section, I’ll implement the <strong>transpose option</strong> and see how much performance we can claw back.</p>
<h2 id="my-flash-attention-v2-transpose">(My) Flash Attention v2 Transpose</h2>
<p>The v2 transpose kernel <a href="https://github.com/AmineDiro/nano-llama-flash/blob/main/kernels/triton_flash_att_v2_transpose.py"><code>kernels/triton_flash_att_v2_transpose.py</code></a> seems a little bit like cheating—we transpose the <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>K</span></span></span></span></span> matrix beforehand. A small but important detail is to make sure that the transpose isn’t just a view and force it to be contiguous in memory:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span>  k_trans <span>=</span> k<span>.</span>transpose(<span>-</span><span>1</span>, <span>-</span><span>2</span>)<span>.</span>contiguous() <span># IMPORTANT!</span>
</span></span><span><span>  attn_kernel[grid](
</span></span><span><span>    q, k_trans, v, o,
</span></span><span><span>    S,
</span></span><span><span>    q<span>.</span>stride(<span>1</span>), stride_k_d, stride_k_s,
</span></span><span><span>    <span>1</span> <span>/</span> math<span>.</span>sqrt(D_h),
</span></span><span><span>    D_h, Tc, Bc,
</span></span><span><span>  )
</span></span></code></pre></div><p>Now the only thing that changes is making sure we load the correct columns in the kernel. Note that I hoisted the softmax_scaling from the inside loop to save mul ops</p>
<div><pre tabindex="0"><code data-lang="python"><span><span>
</span></span><span><span><span>@triton.jit</span>
</span></span><span><span><span>def</span> <span>attn_kernel</span>(
</span></span><span><span>    Q, K, V, O,
</span></span><span><span>    S,
</span></span><span><span>    stride_H,
</span></span><span><span>    stride_k_d,
</span></span><span><span>    stride_k_s,
</span></span><span><span>    softmax_scale,
</span></span><span><span>    D: tl<span>.</span>constexpr,
</span></span><span><span>    Tc: tl<span>.</span>constexpr,
</span></span><span><span>    Bc: tl<span>.</span>constexpr,
</span></span><span><span>):
</span></span><span><span>    <span>#  ...</span>
</span></span><span><span>
</span></span><span><span>    <span># Pre-scale Q to save muls inside loop</span>
</span></span><span><span>    qi <span>=</span> qi <span>*</span> softmax_scale
</span></span><span><span>
</span></span><span><span>    <span>for</span> j <span>in</span> range(<span>0</span>, Tc):
</span></span><span><span>        <span># Pointer Math: (Row_Idx * Stride_Row) + (Col_Idx * Stride_Col)</span>
</span></span><span><span>        <span># Row_Idx is offs_d (0..D)</span>
</span></span><span><span>        <span># Col_Idx is current_cols (S dimension)</span>
</span></span><span><span>        offset_j_k <span>=</span> (offs_d[:, <span>None</span>] <span>*</span> stride_k_d) <span>+</span> (current_cols[<span>None</span>, :] <span>*</span> stride_k_s)
</span></span><span><span>
</span></span><span><span>        <span># Load K (D, Bc) directly!</span>
</span></span><span><span>        kj <span>=</span> tl<span>.</span>load(k_ptr <span>+</span> offset_j_k)
</span></span><span><span>
</span></span><span><span>        <span># ...</span>
</span></span><span><span>        Sij <span>=</span> tl<span>.</span>dot(qi, kj) <span># no transpose needed for kj !</span>
</span></span><span><span>
</span></span><span><span>    <span># ...</span>
</span></span></code></pre></div><p>Let’s profile it and look at the result in comparison to the previous two kernels:</p>
<p><img src="https://aminediro.com/posts/flash_attn/images/compare_v2_transpose.png" alt="compare_v2_transpose"/></p>
<p>Another key metric to track (we haven’t until now because we needed to fix obvious stuff), is to look at the GPU speed of light throughput. This is also known as <a href="https://modal.com/gpu-glossary/perf/roofline-model"><strong>roofline analysis</strong></a>.</p>
<p><img src="https://aminediro.com/posts/flash_attn/images/roofline_v2_transpose.png" alt="roofline_v2_trans"/></p>
<p>Our v2 transpose kernel has way higher arithmetic intensity thanks to removing shared memory access conflicts. This can also be corroborated by looking at the warp scheduler statistics. We can see +153% more eligible warps per cycle as they aren’t stalled by serial shared memory access.</p>
<p>Great! Kernel ran in <strong>34ms</strong> which is +145% improvement compared to the v1! Looking at <code>ncu</code> performance opportunities we can see that the next big blocker is :</p>
<blockquote>
<p><strong>Mio Throttle Stalls Est. Speedup: 43.97%</strong>: On average, each warp of this workload spends 6.7 cycles being stalled waiting for the MIO (memory input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline pressure. This stall type represents about 44.0% of the total average of 15.3 cycles between issuing two instructions.</p>
</blockquote>
<h3 id="still-having-an-mio-bottleneck">Still Having an MIO Bottleneck</h3>
<p>So we fixed bank conflicts and got a nice 145% speedup. What’s left? The MIO (Memory Input/Output) pipeline is now our bottleneck. Despite the name, this isn’t about main memory - MIO handles two things:</p>
<ul>
<li><strong>Shared memory access</strong>: Reading/writing our <code>qi</code>, <code>kj</code>, <code>vj</code> tiles</li>
<li><strong>Special math instructions</strong>: Transcendental functions like <code>tl.exp</code>, <code>tl.max</code>, <code>tl.log</code></li>
</ul>
<p>Every iteration of our inner loop calls <code>tl.exp</code> for the softmax and <code>tl.max</code> for numerical stability. These operations go through the SFU (Special Function Unit), which is much slower than the main FMA units. With <code>Bc=32</code>, we’re doing these expensive operations very frequently relative to the actual matmul work.</p>
<p>I tried a few things to reduce MIO pressure:</p>
<ol>
<li>
<p><strong>FP16</strong>: Wrote an <a href="https://github.com/AmineDiro/nano-llama-flash/blob/main/kernels/triton_flash_att_v2_transpose_fp16.py">FP16 kernel</a> hoping for a speedup by lowering shared memory requirements and increasing <code>D_h</code> and <code>Bc</code>. Ended up <em>slower</em> than FP32 because I kept fighting the compiler—it kept inserting extra shared memory accesses for type conversions, and I didn’t get any SRAM savings because FP16 precision sucks for accumulators and I needed to keep all the on-chip accumulators in FP32 anyway.</p>
</li>
<li>
<p><strong>Larger block sizes</strong>: Increasing <code>Bc</code> means fewer loop iterations, so fewer <code>exp</code>/<code>max</code> calls per output element. But my RTX 2070 doesn’t have enough shared memory to go much higher without killing occupancy.</p>
</li>
</ol>
<p>Anyway, the main take here is to try to reduce MIO stalls by having larger block reads and reducing all special function math instructions.</p>
<h3 id="the-core-tensor-core-problem">The core Tensor Core problem</h3>
<p>Looking at the instruction stats, something else jumped out:</p>
<p><img src="https://aminediro.com/posts/flash_attn/images/instruction_stats.png" alt="inst_stats"/></p>
<p>The kernel is dominated by <code>FFMA</code> (Fused Floating-point Multiply-Add) instructions. These are <strong>regular CUDA core operations, not tensor core operations</strong>. Tensor cores can do 4x4 matrix multiplies in a single cycle—<strong>they’re the whole reason modern GPUs are so fast at deep learning</strong>. But my <code>tl.dot</code> calls aren’t using them for some reason!</p>
<p>I fought with this for a while. Tried different memory layouts, alignment hints, different block sizes… Nothing worked. Turns out, on SM 7.5 (Turing architecture), Triton struggles to generate tensor core code. The compiler just falls back to regular <code>FMA</code> instructions which run on regular old CUDA cores, clogging up the pipe.</p>
<p>I guess I need to stop being poor and buy a newer GPU. More realistically, I’ll rent an H100 and see if the same code magically starts using tensor cores on SM 9.0…</p>
<h2 id="comparison-with-the-real-flash-attention-v2">Comparison with the Real Flash Attention v2</h2>
<p>At this point I’ve hit the limits of what I can optimize on my hardware.</p>
<p>Let’s open up the actual Flash Attention v2 paper<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup> and see what we got right and what we missed:</p>
<p><strong>1. Reducing Non-Matmul FLOPs</strong> ✅ (partially)</p>
<p>We did defer the final division to the end of the kernel. But FA2 goes further - it restructures the entire online softmax to minimize <code>exp</code> and <code>max</code> operations. GPUs have separate units for matmul (Tensor Cores, ~300 TFLOPs on A100) vs generic math (CUDA cores, ~20 TFLOPs). Every non-matmul operation is 15x slower, so minimizing them matters a lot.</p>
<p><strong>2. Parallelism Over Sequence Length</strong> ✅</p>
<p>We nailed this one. Our v2 kernel parallelizes over <code>(S/Bc, B*N_h)</code> instead of just <code>(B, N_h)</code>. This is exactly what FA2 does - split the query sequence into chunks and assign them to different thread blocks.</p>
<p><strong>3. Warp-Level Work Partitioning</strong> ⛔</p>
<p>This is where Triton abstracts too much away. FA2 carefully controls how warps within a block divide work: instead of “Split-K” (warps split the K/V dimension and sync to combine results), they use “Split-Q” (warps split the Q dimension and work independently). This removes expensive synchronization barriers.</p>
<p>In Triton, we don’t control warp-level scheduling - the compiler decides. We could inspect the generated PTX to see what it’s doing, but we can’t easily change it.</p>
<p><strong>4. Larger Head Dimensions</strong> 🤷</p>
<p>FA2 supports <code>D_h</code> up to 256 efficiently. Our kernel works with any head dimension, but we haven’t optimized the tiling specifically for larger values. With <code>D_h=128</code> or <code>256</code>, you’d want different block sizes and potentially different memory layouts. For implementation simplicity reasons, I kept <code>Bc==Br</code>, so we haven’t gotten to play with tweaking these parameters.</p>
<!-- ## Performance Comparison -->
<!---->
<!-- From my profiling notes, here are the key findings: -->
<!---->
<!-- When Flash Attention Wins -->
<!---->
<!-- - **Long sequences (S > 4096)**: Standard PyTorch attention OOMs, Flash Attention continues working -->
<!-- - **Memory-constrained scenarios**: Enables training with batch sizes impossible with standard attention -->
<!-- - **Very long sequences (S > 16K)**: Flash Attention becomes faster due to memory bandwidth bottleneck -->
<!---->
<!-- When cuBLAS/PyTorch Wins -->
<!---->
<!-- - **Short sequences (S < 2048)**: The overhead of tiling and online softmax isn't worth it -->
<!-- - **S = 8192 with Bc = 32**: cuBLAS highly optimized matmul is faster for medium-sized problems -->
<!---->
<!-- The Crossover Point -->
<!---->
<!-- The break-even point depends on: -->
<!---->
<!-- - GPU architecture (memory bandwidth vs compute ratio) -->
<!-- - Head dimension `D_h` -->
<!-- - Block size `Bc` -->
<!-- - Sequence length `S` -->
<!---->
<!-- On my RTX 2070 Super, the crossover is around S=4096-8192. -->
<h2 id="conclusion">Conclusion</h2>
<p>Don’t get it twisted—Flash Attention is a brilliant example of algorithm-hardware co-design. My goal here wasn’t to diminish the work of Tri Dao, but to walk through the reasoning myself. Every brilliant solution seems kind of obvious once you have it in front of you, but having the insight and technical chops to come up with it in the first place is a whole other thing.</p>
<p>I tried my best to demystify the core ideas behind Flash Attention, and show how understanding the GPU memory hierarchy lets you optimize iteratively: profile, rewrite, tweak, rinse and repeat.</p>
<p>If you’ve fallen asleep three times reading this and just woke up, here are the key takeaways:</p>
<ol>
<li><strong>Tiling</strong>: Process attention in blocks that fit in fast SRAM</li>
<li><strong>Online softmax</strong>: Compute softmax incrementally without materializing the full attention matrix</li>
<li><strong>Minimize HBM traffic</strong>: Load data once, keep accumulators in registers/SRAM, write only the final result</li>
</ol>
<p>There’s more to explore - FA3 brings asynchronous memory copies and FP8 support, FA4 pushes things even further with Blackwell-specific optimizations. But that’s for another post (and another GPU).</p>
<p>The full implementation is available in this <a href="https://github.com/AmineDiro/nano-llama-flash/tree/main/kernels">repository</a>, including the Triton kernels and profiling scripts.</p>
<h2 id="references">References</h2>


            </div></div>
  </body>
</html>
