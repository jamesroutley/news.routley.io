<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://liquidbrain.net/blog/canary-in-the-data-mine/">Original</a>
    <h1>Canary in the Data Mine: the Case for Data Filtration Transparency</h1>
    
    <div id="readability-page-1" class="page"><div itemprop="articleBody">
            <p>[<em>Note: this is written in a op-ed style to be a writing sample.</em>]</p>
<p>In November 2023, Stanford researcher David Thiel made a <a href="https://purl.stanford.edu/kh752sm9123">startling discovery</a>. The popular text-to-image AI model, Stable Diffusion 1.5, had been trained on a dataset containing child sexual abuse material (CSAM). His discovery sparked a public outcry. <a href="https://www.forbes.com/sites/alexandralevine/2023/12/20/stable-diffusion-child-sexual-abuse-material-stanford-internet-observatory/">Forbes</a> and <a href="https://www.cnn.com/2023/12/21/tech/child-sexual-abuse-material-ai-training-data/index.html">CNN</a> wrote negative articles, and the non-profit which hosted the dataset <a href="https://web.archive.org/web/20240108132400/https://laion.ai/notes/laion-maintanence/">took it offline temporarily</a> to remove the offending content.  </p>
<p>However, the damage had already been done. Stable Diffusion 1.5 was released as an open model, so existing copies could not be retracted. Today, you can still access the model online, through uploads by random strangers.</p>
<p>This case highlights a more general problem. Without clear standards for training data, AI developers risk embedding harmful content into their systems. And a lack of transparency about how developers filter data creates risks for researchers and users. AI developers need to be proactively transparent around data practices, before more problems emerge.  </p>
<h2 id="data-guzzlers">Data-guzzlers</h2>
<p>The central challenge with data arises out of how AI models are developed. AI systems, such as Stable Diffusion image models and OpenAI&#39;s GPT-4.5, are more grown than made. While software engineers build traditional software by writing specific instructions in code, AI developers instead train AI models by feeding them large amounts of data. The AI learns to understand and predict patterns present in that data. The more data you give the AI system, the better it is at internalizing those patterns and producing useful output.</p>
<p>Seeking to make the best model possible, developers compile massive datasets, often copying most of the public internet. However, the internet contains harmful or sensitive material, like CSAM, copyrighted content, and leaked evaluations. The AI models can then internalize and amplify these harmful outputs.</p>
<p>One particularly prominent example of sensitive data is copyrighted content. AI companies like OpenAI use copyrighted data from the internet, often without compensation agreements. In response, <a href="https://www.npr.org/2025/01/14/nx-s1-5258952/new-york-times-openai-microsoft">several media companies have sued</a> OpenAI for copyright infringement, in part <a href="https://hls.harvard.edu/today/does-chatgpt-violate-new-york-times-copyrights/">alleging</a> that ChatGPT sometimes repeats parts of its articles word for word.</p>
<p>AI companies have responded by paying media outlets for the licensed use of their data. In May 2024, OpenAI paid News Corp <a href="https://apnews.com/article/openai-news-corp-a49144d381796df5729c746f52fbef19">an estimated $250 million</a> for their content, which includes the Wall Street Journal. And, in a recent <a href="https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf#page=3">paper</a>, fellow AI developer Anthropic stated that they respect website&#39;s instructions for whether they allow crawling.</p>
<h2 id="cheating-on-the-test-and-getting-away-with-it">Cheating on the test ... and getting away with it:</h2>
<p>Data contamination poses a more subtle challenge for evaluating model quality. Currently, one of the primary ways that researchers and policymakers assess the strength of a released model is by looking at benchmarks, which are sets of easily-graded questions or tasks. These benchmarks are most accurate if AI models have not seen the questions -- or the answers -- beforehand. With publicly accessible datasets, however, there are no strict guarantees that developers will not train on that data.  </p>
<p>Because of this, some researchers include &#34;canary strings&#34; -- long sequences of random digits -- in their benchmark data. If a model is able to repeat the canary, it means that it has seen the data on the benchmark.</p>
<p>This approach has revealed that AI models such as OpenAI&#39;s GPT-4 and Anthropic&#39;s Claude were trained on data explicitly meant to be excluded from training. Both models <a href="https://www.alignmentforum.org/posts/kSmHMoaLKGcGgyWzs/big-bench-canary-contamination-in-gpt-4">have been shown</a> to know the canary string for &#34;BigBench,&#34; one of the largest and most important benchmarks for large language models (LLMs). While OpenAI notes this in their technical report on GPT-4 (<a href="https://cdn.openai.com/papers/gpt-4-system-card.pdf#page=3">footnote 5</a>), Anthropic has never publicly admitted to have included BigBench data in their training set.</p>
<p>Unlike CSAM and copyright, there has been no big backlash to this contamination, and no corresponding transparency. This leaves researchers in a bit of a tricky spot, because it&#39;s unclear whether benchmarks or other sensitive papers will end up in the training data. And there is no reliable way yet to determine that a given dataset has not been trained on. For instance, it is easy to train a model not to reproduce a canary string.</p>
<p>This lack of transparency poses real threats. Recent research from Anthropic suggests that AI systems learn unexpected lessons from the data they are trained on. Documents which detail training procedures can lead models to <a href="https://techcrunch.com/2024/12/18/new-anthropic-study-shows-ai-really-doesnt-want-to-be-forced-to-change-its-views/">fake being trained further</a>. Seeing papers which claim that AIs cheat a lot on exams <a href="https://alignment.anthropic.com/2025/reward-hacking-ooc/">make them more likely to cheat</a> themselves.</p>
<p>Ideally, safety papers with sensitive content would not end up in the training data if they made models more likely to behave dangerously. If filtering out such papers is not possible, AI companies should be upfront about this fact, so that benchmark creators and researchers could choose which data they share more carefully.</p>
<h2 id="transparency-the-time-is-now">Transparency: the time is now:</h2>
<p>Stable Diffusion-1.5&#39;s case demonstrates both the problem and potential solutions. A couple of months after Thiel broke the news, the leading AI companies partnered with Thorn, an anti-CSAM organization, to establish a <a href="https://www.thorn.org/blog/generative-ai-principles/">set of guidelines</a> to combat CSAM. Data filtration was the <a href="https://info.thorn.org/hubfs/thorn-safety-by-design-for-generative-AI.pdf?hsCtaAttrib=165037384300#page=12">first principle</a> on the list. Still, this progress only came after a harmful model had been released and couldn&#39;t be shut down.</p>
<p>AI companies should avoid repeating the mistake. They need to be proactively transparent about the types of sensitive data that they are using. If companies cannot guarantee the exclusion of certain types of data (like files with canary strings), they should state this explicitly, so that researchers and policymakers can make an informed decision.</p>
<p>It&#39;s time to be transparent about sensitive data, before the next harmful dataset becomes embedded in systems we cannot recall.</p>
        </div></div>
  </body>
</html>
