<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://simonwillison.net/2024/Oct/1/openai-devday-2024-live-blog/">Original</a>
    <h1>OpenAI DevDay 2024 live blog</h1>
    
    <div id="readability-page-1" class="page"><p>I’m at <a href="https://openai.com/devday/">OpenAI DevDay</a> in San Francisco, and I’m trying something new: a live blog, where this entry will be updated with new notes during the event.</p><div id="live-updates">

<p data-update-id="1"><strong>10:19</strong> The <strong>keynote</strong> is starting with a review of o1, and some examples of applications that use it.</p>

<p data-update-id="2"><strong>10:30</strong> They started with some demos of o1 being used in applications, and announced that the rate limit for o1 doubled to 10000 RPM (from 5000 RPM) - same as GPT-4 now.</p>

<p data-update-id="3"><strong>10:31</strong> The first big announcement: a realtime API, providing the ability to use WebSockets to implement voice input and output against their models.</p>

<p data-update-id="4"><strong>10:33</strong> What can you build with the new realtime API? The demonstrated an updated version of their Wanderlust travel agent demo. The demo uses voice as both input and output.</p>

<p data-update-id="5"><strong>10:37</strong> </p><p><img src="https://static.simonwillison.net/static/2024/wanderlust.jpg" alt="The image shows a large display screen mounted on a wall, showcasing a map application called Wanderlust. The screen has a pink background with a map of San Francisco on the right side.
The top of the screen shows macOS menu bar with icons and the time Tue Oct 1 10:35 AM.
The left side of the screen displays a conversation:
SF
Annotated map
San Francisco, here we are! How can I assist?
Yeah, could we take a look at Fort Mason, please?
Annotated map
There we go, Fort Mason! What do you need help with there?
Well, I have a few hundred awesome developers here in the room and I&#39;m thinking they might need a little snack. Do you have some chocolate shops around here?
Annotated map
Absolutely! Check out Ghirardelli Square, Ilan&#39;s Strawberries, and The Candy Store!
The right side shows a map of San Francisco, focusing on the Fort Mason area. Various landmarks and streets are labeled, including Aquatic Park, Ghirardelli Square, and Fisherman&#39;s Wharf.
At the bottom of the screen, there are icons for microphone input, text input, and map layers.
The screen is mounted above what appears to be wood paneling, and a small portion of a plant is visible in the lower-left corner of the image."/></p>

<p data-update-id="6"><strong>10:39</strong> ... and a demo of an AI assistant making a phone call to order food (thankfully to an OpenAI staff member on stage, not to an actual business!)</p>

<p data-update-id="7"><strong>10:41</strong> And a demo of the <a href="https://www.speak.com/">Speak</a> language learning app using the new Realtime API. The API is rolling out generally today.</p>

<p data-update-id="8"><strong>10:41</strong> Next up: model customization. They have fine-tuning for GPT-4o and 4o-mini now. Today they&#39;re announcing fine-tuning support for their vision models.</p>

<p data-update-id="9"><strong>10:42</strong> Now you can use images to fine-tune the model. They suggest this can be used for product recommendations, medical imaging, or even things like traffic sign detection or lane detection (Grab have been using it for that).</p>

<p data-update-id="10"><strong>10:42</strong> Fine-tuning with vision is available to every developer for GPT-4 (presumably they mean GPT-4o? Not clear.)</p>

<p data-update-id="11"><strong>10:43</strong> Next: price drops. Cost-per-token is already 99% cheaper than two years ago.</p>

<p data-update-id="12"><strong>10:44</strong> And today they&#39;re adding prompt caching - as seen previously in <a href="https://www.anthropic.com/news/prompt-caching">Claude</a> and <a href="https://ai.google.dev/gemini-api/docs/caching">Gemini</a>.</p>

<p data-update-id="13"><strong>10:44</strong> Their version of prompt caching is automatic! A 50% discount on tokens the model has seen before.</p>

<p data-update-id="14"><strong>10:45</strong> Model distillation: where a smaller model is taught by a larger model. Today they&#39;re announcing tools for model distillation - so you can fine-tune a 4o-mini model based on output from the larger models.</p>

<p data-update-id="15"><strong>10:46</strong> Two new tools: stored completions, which lets you store your interactions with the models permanently in the OpenAI platform, to use for fine-tuning and model distillation. That tool ships to all developers today.</p>

<p data-update-id="16"><strong>10:46</strong> Plus new evaluation tools, also shipping today.</p>

<p data-update-id="17"><strong>10:47</strong> Sam Altman will be here for the fireside chat in the afternoon, but won&#39;t be presenting keynotes before then.</p>

<p data-update-id="18"><strong>10:51</strong> And now a break. The schedule for the rest of the event has been updated - previously it said &#34;to be announced during the keynote&#34;, now we are seeing that 11-11:45am is &#34;Structured Outputs for reliable applications&#34; and 12-12:45pm is &#34;Powerful small models with distillation.</p>

<p data-update-id="19"><strong>10:52</strong> Then at 2-2:45pm &#34;Multimodel apps with the realtime API.</p>

<p data-update-id="20"><strong>11:05</strong> Next up: <strong>Structured outputs for reliable applications</strong>. I&#39;ve done a bunch of work with the OpenAI tools mechanism in the past for this, most notably my <a href="https://www.datasette.cloud/blog/2024/datasette-extract/">datasette-extract plugin</a> for loading unstructured text and images into structured SQLite database tables.</p>

<p data-update-id="21"><strong>11:06</strong> Atty Eleti and Michelle Pokrass are talking about <a href="https://platform.openai.com/docs/guides/structured-outputs">structured outputs</a>, the most recent evolution of that mechanism.</p>

<p data-update-id="22"><strong>11:07</strong> Atty starts with a review of GPT-4 apps like Duolingo, Klarna and Cursor, which &#34;connect to the outside world&#34; - and hence need structured outputs, typically using JSON.</p>

<p data-update-id="23"><strong>11:08</strong> Classic example of how asking for JSON in the past has often been unreliable - resulting in responses that start &#34;Here is the JSON you asked for...&#34;. Developers end up begging for &#34;Just the JSON please!&#34;.</p>

<p data-update-id="24"><strong>11:09</strong> Function calling launched in June 2023, and which helped a bit. In November - last year&#39;s DevDay - they released JSON mode that ensured valid data - but this could still hallucinate parameters or output the wrong type.</p>

<p data-update-id="25"><strong>11:10</strong> Structured Outputs, released in August this year, ensures that the output will exactly match a specified JSON schema. Michelle will explain how this works under the hood. The challenge was making sure the solution was performant for inference at scale.</p>

<p data-update-id="26"><strong>11:12</strong> Function calling continues to work the same way: you provide a tool with a type of function, then describe that function&#39;s parameters using JSON schema. Add <code>&#34;strict&#34;: true</code> to that JSON to turn on the new structured output mode for those functions.</p>

<p data-update-id="27"><strong>11:14</strong> And now a demo, with a function that describes data tables, their columns and operations that can be executed against them. Adding <code>&#34;strict&#34;: true</code> fixed a bug where the model used an operator that wasn&#39;t defined in the set of operators.</p>

<p data-update-id="28"><strong>11:15</strong> <code>&#34;response_format&#34;: {&#34;type&#34;: &#34;json_schema&#34;}</code> enables specifying a full JSON schema that&#39;s guaranteed to be followed by the structured outputs mode.</p>

<p data-update-id="29"><strong>11:17</strong> The demo imagines AI-enhanced glasses, using a neat <code>{&#34;voice_over&#34;: &#34;This is what the glasses say to you&#34;, &#34;display&#34;: &#34;4 feet tall&#34;}</code> output format which updates a display with a short string and specifies a longer string to be spoken out loud.</p>

<p data-update-id="30"><strong>11:18</strong> Next demo imagines a resume reviewing application, where you can drop a PDF resume directly onto a web form which then uses structured outputs to pull out the fields needed by the resume application.</p>

<p data-update-id="31"><strong>11:20</strong> The OpenAI library for JavaScript supports <a href="https://zod.dev/">Zod</a> for defining these schemas, and the Python library supports <a href="https://docs.pydantic.dev/">Pydantic</a>.</p>

<p data-update-id="32"><strong>11:21</strong> I&#39;m presuming that previous demo converted the PDF to images automatically - I don&#39;t think any of the OpenAI APIs accept PDF directly.</p>

<p data-update-id="33"><strong>11:23</strong> This next demo is much more interesting: defining a schema for a full dashboard interface, where different cards can represent charts or tables or rows - so now the tool can output a custom answer to a question with embedded charts and data. Hopefully the code will be published on GitHub after the talk.</p>

<p data-update-id="34"><strong>11:24</strong> Overall this is a pretty sophisticated demo of a custom chat UI with a whole assortment of custom tools built on top of function calling and structured output.</p>

<p data-update-id="35"><strong>11:25</strong> Atty emphasizes that prior to structured outputs reliability was a really big problem - any of these steps failing could break the entire application.</p>

<p data-update-id="36"><strong>11:25</strong> Next, Michelle is talking about the underlying implementation of structured outputs.</p>

<p data-update-id="37"><strong>11:26</strong> &#34;We took an approach that combined both research and engineering&#34;. It&#39;s more than just prompting - they used a technique called &#34;constrained decoding&#34;. (Sounds to me like the <a href="https://til.simonwillison.net/llms/llama-cpp-python-grammars">Llama.cpp grammars trick</a>).</p>

<p data-update-id="38"><strong>11:27</strong> As an example, consider handwritten number recognition - where there are only ten possible output labels, from 0 to 9. This is a classic machine learning image recognition task.</p>

<p data-update-id="39"><strong>11:28</strong> LLMs predict more than just digits through 0-9 - they output tokens, see <a href="https://simonwillison.net/2023/Jun/8/gpt-tokenizers/">my article Understanding GPT tokenizers</a> from last year.</p>

<p data-update-id="40"><strong>11:30</strong> For structured outputs, the trick is to limit which tokens can be produced next. The technique used here is called &#34;token masking&#34;. The LLM still generates probabilities for likely next tokens, but they then mask out any tokens that would not match the desired schema.</p>

<p data-update-id="41"><strong>11:31</strong> These masks have to be updated on every single inference step, so the operation has to be lightning fast to keep inference as fast as possible. Token sampling happens on a GPU in batches - which means the CPU can calculate the masks for the next step in parallel while the GPU is calculating probabilities.</p>

<p data-update-id="42"><strong>11:32</strong> These masks need to be calculated within 10ms. &#34;We wanted to pre-compute as much work as possible&#34; - to make mask computation more of a lookup. They build an index, derived from the JSON schema, to make fetching those masks as fast as possible.</p>

<p data-update-id="43"><strong>11:33</strong> JSON Schema is converted into a grammar, then a parser, then they iterate over ALL tokens and parse states and use that to create the index.</p>

<p data-update-id="44"><strong>11:33</strong> The indexs is a trie - a prefix-based data structure allowing for O1 lookups.</p>

<p data-update-id="45"><strong>11:34</strong> Generating the index is computationally expensive due to the need to go over all of the possible states. They do that just once and cache the index - which is why the first query to structured inputs can take a little time - sometimes up to 10 seconds - but following prompts are fast.</p>

<p data-update-id="46"><strong>11:35</strong> The open source community has used tricks to implement masks by turning a schema into a regular expression. But regular expressions don&#39;t cover recursive or deeply nested schemas - so they can&#39;t cover all of the features of JSON Schema.</p>

<p data-update-id="47"><strong>11:35</strong> Generative UI is a good example of a use-case that needs nested schemas - each component can have a list of children that might include more components. These cannot be converted to a regular expression due to that limit in terms of recursive nesting.</p>

<p data-update-id="48"><strong>11:37</strong> OpenAI wanted recursive JSON schema support, so they added a stack. They call this the CFG - Context Free Grammar - approach, which combines regular expressions and a stack. This is why it takes a little bit of time to build up the trie for inference.</p>

<p data-update-id="49"><strong>11:38</strong> So the trade-off here is the short delay when the schema is first encountered, which OpenAI think is worthwhile for the improved reliability.</p>

<p data-update-id="50"><strong>11:38</strong> Atty is now talking about the research side: how can we get the model to follow the schema in the most useful way possible?</p>

<p data-update-id="51"><strong>11:39</strong> If you force a model to output JSON fitting the grammar, you might end up with <code>{\n\n\n\n\n\n\n\n\n\n\n\n</code> until it hits the maximum of tokens.</p>

<p data-update-id="52"><strong>11:40</strong> OpenAI&#39;s internal evals showed that gpt-4o-2024-08-06 with Structured Outputs was far more accurate than prompting alone against the older models. They now get to 100% on that eval (not sure what that&#39;s measuring though).</p>

<p data-update-id="53"><strong>11:42</strong> One of the controversial API design decisions made was around <code>additionalProperties: true</code> - which is usually a default in JSON Schema. OpenAI have disallowed additional properties by default in their API, which differs from developer expectations.</p>

<p data-update-id="54"><strong>11:42</strong> OpenAI say explicit is better, so developers have to pass <code>additionalProperties: false</code> in their schemas.</p>

<p data-update-id="55"><strong>11:43</strong> All properties are required by default - no support for optional parameters (although they can be made nullable). Developers need to follow this rule in the JSON Schemas they send over as well.</p>

<p data-update-id="56"><strong>11:43</strong> Fields are generated in the same order that you defined them in the schema, even though JSON is supposed to ignore key order. This ensures you can implement things like chain-of-thought by adding those keys in the correct order in your schema design.</p>

<p data-update-id="57"><strong>11:45</strong> Looks like the documentation for <a href="https://platform.openai.com/docs/guides/realtime">the new Realtime API</a> is now available.</p>

<p data-update-id="58"><strong>11:46</strong> This session didn&#39;t present any new features - they were all in the documentation already - but the insight into how the Structured Output works under the hood was new.</p>

<p data-update-id="59"><strong>12:01</strong> Next up: <strong>Powerful small models with distillation</strong>, with John Allard and Steven Heidel.</p>

<p data-update-id="60"><strong>12:02</strong> Distillation &#34;allows you to create powerful, small models&#34;. They&#39;ll talk about why it matters, how it works and best practices and use cases - plus demos of the two new API platform features they are launching today.</p>

<p data-update-id="61"><strong>12:02</strong> Once you get an AI app working, the next step is figuring out how to get it to work at scale. You care about uptime, rate limits, latency and cost.</p>

<p data-update-id="62"><strong>12:04</strong> GPT-4o is 15x more expensive than GPT-4o mini, but it brings a large amount of additional &#34;knowledge&#34; - graduate level physics etc. It excels at the toughest knowledge benchmarks. Do you need that type of intelligence for your application?</p>

<p data-update-id="63"><strong>12:06</strong> Distillation: you fine-tune the small output on the outputs of the large model. You&#39;re compressing some of the capabilities of the large model into that smaller model.</p>

<p data-update-id="64"><strong>12:07</strong> Distillation involves three steps. The first and most important is to build task-specific evals for your application. You can&#39;t skip this step, because you can&#39;t improve what you can&#39;t measure.</p>

<p data-update-id="65"><strong>12:07</strong> The second step is to capture examples of what good performance looks like. Store example completions from a large model like GPT-4o and create a dataset.</p>

<p data-update-id="66"><strong>12:08</strong> The final step is the fine-tuning. Teach the small model how to replicate the responses from the large model by showing it many of those captured examples. We&#39;re trying to &#34;compress the intelligence&#34; of the large model into that small model.</p>

<p data-update-id="67"><strong>12:09</strong> A lot of people have done distillation on the OpenAI platform before, using the existing <a href="https://platform.openai.com/docs/guides/fine-tuning">fine-tuning mechanism</a>. Doing it that way is a lot of work though.</p>

<p data-update-id="68"><strong>12:10</strong> The two new features they are launching today will make distillation easier. The first is stored completions: a new parameter to the chat completions API that will let you opt-in to storing the full input and output to the model. You can apply tags as well, to help filter those later to create datasets for fine-tuning. <code>{&#34;store:&#34; true}</code></p>

<p data-update-id="69"><strong>12:11</strong> The second feature is a beta of an Evals product. This should allow you to do distillation end-to-end on the OpenAI platform.</p>

<p data-update-id="70"><strong>12:11</strong> Real-world use-case based on the Superhuman email app. That app has a &#34;quick reply&#34; feature that suggests options for a reply based on reading through the existing thread. How would you scale that feature to hundreds of millions of emails?</p>

<p data-update-id="71"><strong>12:13</strong> Aside: here&#39;s <a href="https://github.com/openai/openai-realtime-api-beta">openai/openai-realtime-api-beta</a> with example code for talking to the new Realtime API using JavaScript. <a href="https://github.com/openai/openai-realtime-console">openai/openai-realtime-console</a> is an example React app.</p>

<p data-update-id="72"><strong>12:14</strong> Using the Python client library for <code>client.chat.completions.create()</code> you can add <code>store=True, metadata={&#34;tag&#34;: &#34;test-set&#34;}</code> to store a prompt/response and add it to a tag.</p>

<p data-update-id="73"><strong>12:14</strong> A new UI at <a href="https://platform.openai.com/chat-completions">platform.openai.com/chat-completions</a> lets you browse through your stored completions.</p>

<p data-update-id="74"><strong>12:15</strong> Then in the new <a href="https://platform.openai.com/evalutions/create">/evalutions/create</a> interface you can add testing criteria and use that to create a new evaluation. (I don&#39;t have access to that page yet.)</p>

<p data-update-id="75"><strong>12:17</strong> Having created an eval it&#39;s easy to run that against other models - try it against GPT-4o mini and compare that with GPT-4o for example.</p>

<p data-update-id="76"><strong>12:20</strong> </p><p><img src="https://static.simonwillison.net/static/2024/eval.jpg" alt="Screenshot of a web interface showing evaluation results for an AI model named &#39;quick-reply-2-4o&#39;. The interface displays a table with columns for messages, output, and three evaluation metrics: &#39;repliesToRightPerson&#39;, &#39;repliesToMostPressingIssue&#39;, and &#39;repliesMakeSense&#39;. The table shows 8 rows of data, each representing a different conversation. Overall metrics at the top indicate 95%, 91%, and 97% success rates for the three evaluation criteria respectively. The interface appears to be part of a platform called &#39;Distillation Test&#39; in a &#39;DevDay Demo&#39; project."/></p>

<p data-update-id="77"><strong>12:21</strong> ... and now a demonstration of the fine-tuning UI, showing how a fine-tuned GPT-4o mini model on that data performs much better than 4o-mini on its own.</p>

<p data-update-id="78"><strong>12:23</strong> Is distillation right for your use-cases? That comes down to task generality against required precision. Great use-cases for distillation are tasks that cover a relatively narrow domain and have a relatively low precision requirement - a great fit for small models.</p>

<p data-update-id="79"><strong>12:23</strong> Tasks that have high precision needs but narrow generality work well too - that&#39;s a lot of forms of categorization. You may need a larger and more diverse data set to get that to work well. Same for broad generality and low precision.</p>

<p data-update-id="80"><strong>12:24</strong> Tasks with a broad generality and high need for precision are a poor fit for distillation - they need a full-powered large model.</p>

<p data-update-id="81"><strong>12:25</strong> Things to watch out for: Unevenly distributed or biased data points. Your training data should match the patterns of your production data. Also sparse examples which may result in blind spots in your data. A great example is fraud - if it&#39;s rare you might find that 1,000 samples have no instances of fraud at all!</p>

<p data-update-id="82"><strong>12:26</strong> Part of the value of distillation is you don&#39;t necessarily need human generated data or responses - but that doesn&#39;t mean you don&#39;t need to actively curate your distillation dataset. &#34;We tend to see distillation work best with the order of thousands rather than millions of examples.&#34;</p>

<p data-update-id="83"><strong>12:27</strong> Finally, take an iterative approach. Fine-tuning might not work on your first try - there are many variables to consider. It&#39;s important to start small with a few hundred examples and scale up once you know it&#39;s working based on your evals. Don&#39;t jump straight to millions of data points.</p>

<p data-update-id="84"><strong>12:28</strong> It strikes me that fine-tuning and distillation are strategically a great way of keeping people locked to one platform - if you build an application purely on top of prompt engineering it&#39;s much easier to swap between different LLM vendors than if you have fine-tuned a model.</p>

<p data-update-id="85"><strong>12:29</strong> They expect that it will become common for applications to be built using a collection of many different distilled small models, with a few large models for tasks that don&#39;t work well for distillation.</p>

<p data-update-id="86"><strong>12:30</strong> ... and now lunch - sessions resume at 2pm.</p>

<p data-update-id="87"><strong>12:32</strong> The system I built for this live blog is very simple - just <code>fetch()</code> calls polling an endpoint and updating a <code>&lt;div&gt;</code> using <code>innerHTML</code> - but the endpoint itself sets a 10s cache so Cloudflare should only let a hit through to the underlying app every 10s no matter how many people are viewing the page.</p>


</div></div>
  </body>
</html>
