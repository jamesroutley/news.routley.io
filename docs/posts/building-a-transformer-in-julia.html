<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://liorsinai.github.io/coding/2022/05/18/transformers.html">Original</a>
    <h1>Building a transformer in Julia</h1>
    
    <div id="readability-page-1" class="page"><div>
        <p><em>Building a transformer in Julia. This a very long post on the full process behind making a transformer work in Julia.</em></p>



<h3 id="table-of-contents">Table of Contents</h3>
<nav>
    <ol>
        <li><a href="#introduction">Introduction</a></li>
        <li><a href="#design">Design</a>
            <ul>
                <li><a href="#design-considerations">Design considerations</a></li>
                <li><a href="#inputs-and-outputs">Inputs and outputs</a></li>
                <li><a href="#architecture">Architecture</a></li>
                <li><a href="#attention">Attention</a></li>
            </ul>
        </li>
        <li><a href="#julia-implementation">Julia implementation</a>
            <ul>
                <li><a href="#project-setup">Project setup</a></li>
                <li><a href="#tokenizers">Tokenizers</a></li>
                <li><a href="#word-embeddings">Word embeddings</a></li>
                <li><a href="#position-encodings">Position encodings</a></li>
                <li><a href="#multiplication-with-higher-order-arrays">Multiplication with higher order arrays</a></li>
                <li><a href="#multi-head-attention">Multi-head attention</a></li>
                <li><a href="#encoder-blocks">Encoder blocks</a></li>
                <li><a href="#classifier">Classifier</a></li>
            </ul>
        </li>
        <li><a href="#use-case-amazon-reviews">Use case: Amazon reviews</a></li>
            <ul>
                <li><a href="#data-exploration">Data exploration</a></li>
                <li><a href="#pipeline">Pipeline</a></li>
                <li><a href="#evaluation">Evaluation</a></li>
            </ul>
        <li><a href="#conclusion">Conclusion </a></li>
    </ol>
</nav>

<h2 id="introduction">Introduction</h2>

<p>In December 2017 Google AI released their transformer architecture in the paper <a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a>. 
(It is a highly recommended read.)
They had achieved state of the art results on an English to German translation task using a mostly linear model that could be easily scaled up and parallelized. 
Since then it has come to dominate the machine learning space.
Many of the state of the art natural language processing (NLP) models today are transformer models.
Most of them have an incredibly similar architecture to the original and differ only on training regimes, datasets and sizes.</p>

<figure>
<img src="https://liorsinai.github.io/assets/posts/transformers/transformer_model_sizes_annotated.png" alt="architecture"/>
<figcaption></figcaption>
</figure>

<p>Transformers lend themselves to large models. The original transformer model was no light weight: it had 65 million parameters and could be scaled up to 213 million for marginal improvements.
But this is tiny in comparison to OpenAI’s <a href="https://arxiv.org/abs/2005.14165">GPT-3</a> with 175 billion parameters.
It is licensed over an <a href="https://openai.com/api/">API</a> and is responsible for generating text that has made its way around the internet and popular science articles. 
These large models are even shaping hardware.
The CEO of chip manufacturer Nvidia, Jensen Huang, focused a segment of his <a href="https://youtu.be/39ubNuxnrK8?t=772">2022 keynote</a> speech on transformers and their impact on his industry.
Nvidia have also released their own large transformer model with <a href="https://arxiv.org/abs/1909.08053">530 billon parameters</a> and have conducted tests with a <a href="https://github.com/NVIDIA/Megatron-LM">1 trillion parameter model</a>.</p>

<p>Despite being originally developed for NLP, they have come for computer vision too. 
This <a href="https://arxiv.org/abs/2010.11929">2020 paper</a> showed that they can compete with top convolutional neural networks (CNNs). 
More recently, DeepMind released an impressive model <a href="https://www.deepmind.com/publications/a-generalist-agent">Gato</a> that can perform multiple tasks such as “play Atari, caption images, chat, stack blocks with a real robot arm and much more”. It has 1.8 billion parameters.</p>

<h3 id="goals">Goals</h3>

<p>All this development in transformers has been over the past 5 years.
In that time they have mostly replaced the old favourite for NLP in academia and industry, recurrent neural networks (RNNs).
However that has not been long enough for pedagogy to adapt.
As of today, machine learning courses still teach RNNs for NLP.
This has created a gap and many blogs have sprung up to full it.
This blog post aims to be one of those.</p>

<p>I have two goals here:</p>
<ol>
  <li>Build a small working transformer in Julia code and train it on one use case.</li>
  <li>Detail the mathematics of the transformer for both forward equations and backpropagation.</li>
</ol>

<p>Julia has a small but growing user base. It is an elegant and fast language and I highly recommend it.
But even if you don’t know it well, I hope you will find this post accessible and that it will help improve your Julia. 
Mathematics on the other hand is a universal language and this should be accessible to anyone with university level maths.</p>

<p>The use case is a dataset of <a href="https://huggingface.co/datasets/amazon_reviews_multi">Amazon reviews from HuggingFace</a><sup id="fnref:amazon_multi" role="doc-noteref"><a href="#fn:amazon_multi" rel="footnote">1</a></sup>. Only the English subset of the dataset was used with 200,000 training samples and 5,000 test samples. The models were trained on two tasks:</p>
<ol>
  <li>Predict the star rating given the review text.</li>
  <li>Predict a positive or negative sentiment with 1-2 stars labelled negative, 4-5 stars labelled positive and 3 stars removed.</li>
</ol>

<p>Using a transformer for this task can be seen as excessive because it can be solved with simpler models e.g. a term frequency inverse document infrequency (TFIDF) model with 10,000 parameters. (You can see my Julia TFIDF model <a href="https://github.com/LiorSinai/TFIDF.jl">here</a>.) 
However because the task is simple it means we can limit the transformer model to around 250,000 parameters and we have a good baseline of the accuracy we can achieve.</p>

<p>For intuition and history behind transformers I recommend Peter Bloem’s excellent post <a href="http://peterbloem.nl/blog/transformers">Transformers from scratch</a>.
For code in a more popular framework I recommend Alexander Rush’s  <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">The annotated transformer</a> written in PyTorch.
Many transformer posts focus on another universal language, pictures. 
Amongst the most notable are Jay Alammar’s <a href="https://jalammar.github.io/illustrated-transformer/">the illustrated transformer</a> 
and a <a href="https://www.youtube.com/watch?v=XSSTuhyAmnI">video</a> by Ari Seff. I’ll use pictures too but it won’t be the primary medium.</p>

<p>This is not meant to be a full scale Julia solution.
For that, please see the <a href="https://github.com/chengchingwen/Transformers.jl">Transformers.jl</a> package. 
It has better optimizations, CUDA support, APIs for HuggingFace and more. 
My own repository with the code in this blog post can be accessed at <a href="https://github.com/LiorSinai/TransformersLite.jl">github.com/LiorSinai/TransformersLite.jl</a>.</p>

<p>Lastly, transformers are built on top of research and ideas of the last decade of machine learning research.
A background in neural networks is needed for this post.
For my part, I’ll briefly explain the ideas behind techniques like word embeddings, skip connections, regularization and so on
and provide some references.
I also encourage you to research more on your own.
It is also worth keeping in mind that machine learning is at its heart an empirical science, and a sufficient if maybe unsatisfactory answer for why most of these techniques are used is that they have given good results in the past.</p>

<h2 id="design">Design</h2>
<h3 id="design-considerations">Design considerations</h3>

<p>First it is important to look at the design considerations that the Google AI team prioritised.</p>

<p>In the current era of machine learning, the best outcomes have been achieved with bigger models and more data.
To facilitate this, a new architecture should have fast execution times and allow for parallel execution.</p>

<figure>
<img src="https://liorsinai.github.io/assets/posts/transformers/scale_linear.png" alt="scale linear - non-linear"/>
<figcaption></figcaption>
</figure>

<p>Let’s consider a scale of linear to non-linear.
In computer terms, fast means simple and simple means linear. On the other side is non-linear which is complex and slow.
Machine learning as used today is mostly linear. 
Anyone who first studies it is surely a little overwhelmed with all the linear algebra and matrices.
Some non-linearity is needed but research has found that one can get away with very little.
For example, only non-linearity in activation functions.
CNNs are more non-linear, mostly because of the strides of the kernels across images.</p>

<figure>
<img src="https://liorsinai.github.io/assets/posts/transformers/scale_parallel.png" alt="scale parallel - sequential"/>
<figcaption></figcaption>
</figure>

<p>The other scale is parallel to sequential. The previous favourite for NLP, RNNs, were very sequential.
This makes sense for language where words themselves are sequential and the location and order of words in a sentence is important.
Yet transformers are mostly parallel. This enables computers to distribute computation across GPU cores and across clusters.
To reconcile this with sequential language two techniques are used: position encoding and masking. 
In practice these “hacks” perform well enough to justify using a parallel model for sequential tasks.</p>

<h3 id="inputs-and-outputs">Inputs and outputs</h3>

<p>The input to a transformer is a sentence. For example “This camera works great!”. That is then processed into tokens.
Depending on the complexity desired, these tokens can represent words, punctuation, and subword pieces like suffixes. For examples: 
<code>[This, camera, work, ##s, great, !]</code>. A simpler approach is to remove case and punctuation: <code>[this, camera, works, great]</code>.</p>

<div>
	
	<div>
		<p>
		Julia uses column major format whereas Python uses row major format. In Julia word vectors are columns while in Python they are rows.
		Equations between the two formats will look backwards to each other.
		They need to be transposed and definitions also need to be transposed. 
		E.g. $K^TQ \rightarrow (K_c^TQ_c)^T=Q_c^TK_c= Q_r K_r^T$
		</p>
	</div>
</div>

<p>Each token is then associated with a series of weights called a word vector or a word embedding. For example, for our sentence:</p>
<figure>
<img src="https://liorsinai.github.io/assets/posts/transformers/inputs.png" alt="architecture"/>
<figcaption></figcaption>
</figure>
<p>Each word has $d_m$ weights associated with it. For typical transformer models $d_m$ ranges from 64 to 2048.
The main idea is that each weight represents some concept.
For example if we were to assign them manually we could go according to this scheme:</p>
<ul>
  <li>the first row is for how positive or negative the word is.</li>
  <li>the second row is for how common it is.</li>
  <li>the third row is for if it is a noun or not.</li>
</ul>

<p>In practice these weights will be assigned during training and it can be hard to interpret them.</p>

<p>The output of the transformer is another set of weights usually of the same size:</p>
<figure>
<img src="https://liorsinai.github.io/assets/posts/transformers/outputs.png" alt="architecture"/>
<figcaption></figcaption>
</figure>
<p>But now these weights have a different meaning. They are how each word relates to each other in the sentence and to the task at hand.
Where as the previous weights were unique to each word, these are particular to a sentence.
Change one word in the sentence and we might get a completely different matrix, depending on how important that word is.
The name transformer comes from the fact that it <em>transforms</em> a set of word embeddings to another set of embeddings.</p>

<p>The transformer takes advantage of linear algebra to calculate all the weights for all words at the same time.
Hence it is mostly a parallel computation. 
Without additional measures we could shuffle the words (columns) in the sentence and we would get shuffled versions of the same results.
To parallelize further, we can stack the embedding matrices of sentences of $N$ words into $B$ batches of $d_m\times N \times B$ dimensional arrays.</p>

<p>Because the output looks like the input we can feed it back into another transformer. 
Transformer models tend to have stacks of 6 to 24 of these layers.
With each layer it gets more difficult to interpret what the embedding weights actually mean. 
But stacking them has been shown to improve results.</p>

<p>The final output matrix is usually fed to a small neural network to output the final result.
This could be a single number in the case of sentiment analysis or a probability of each word in a vocabulary for a translation task.</p>

<h3 id="architecture">Architecture</h3>

<p>This is the famous schematic from the paper <a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a>:</p>

<figure>
<img src="https://liorsinai.github.io/assets/posts/transformers/architecture.png" alt="architecture"/>
<figcaption></figcaption>
</figure>

<p>The left side is called an encoder and the right side a decoder. 
You don’t need both sides. <a href="https://arxiv.org/abs/1810.04805">BERT</a> is an encoder-only model by Google. <a href="https://www.deepmind.com/publications/a-generalist-agent">Gato</a> is a decoder only transformer.
In this post I’ll focus only on the encoder which is sufficient for the classification task.</p>

<p>Each block in the schematic is associated with two sets of equations: the forward equations and the backwards equations.
I have compiled the encoder block equations into the table below.
The task for the remainder of the blog post will be to translate this maths into code.</p>
<figure>
<img id="equations_table" src="https://liorsinai.github.io/assets/posts/transformers/transformer_equations.png" alt="architecture"/>
<figcaption></figcaption>
</figure>
<p>Please see this table only as a guideline.
Some of the equations are incomplete so that they fit in this table.
Equations will be presented properly in each subsection.
$J_\alpha(Z)\equiv	\frac{\partial Z}{\partial \alpha}$.
There are 4 non-linear steps (softmax, 2×layer norm, RELU) and the other 8 are linear.</p>

<p>You’ll notice that the inputs are either 3D or 4D arrays. 
These are not standard in linear algebra so a <a href="#multiplication-with-higher-order-arrays">section</a> below is dedicated to getting comfortable with them.
In particular, in any programming language multiplying two higher order arrays will not work.
For example:</p>

<figure><pre><code data-lang="julia"><span>A</span> <span>=</span> <span>randn</span><span>(</span><span>3</span><span>,</span> <span>4</span><span>,</span> <span>2</span><span>,</span> <span>2</span><span>)</span>
<span>3</span><span>×4×2×2</span> <span>Array</span><span>{</span><span>Float64</span><span>,</span> <span>4</span><span>}</span><span>:</span>
<span>[</span><span>:</span><span>,</span> <span>:</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>]</span> <span>=</span>
 <span>0.539347</span>   <span>0.772838</span>  <span>0.793975</span>  <span>0.436097</span>
 <span>0.0890865</span>  <span>0.374346</span>  <span>0.462195</span>  <span>0.691458</span>
 <span>0.364314</span>   <span>0.701065</span>  <span>0.712357</span>  <span>0.801697</span>
<span>[</span><span>:</span><span>,</span> <span>:</span><span>,</span> <span>2</span><span>,</span> <span>1</span><span>]</span> <span>=</span>
 <span>0.587629</span>  <span>0.128034</span>  <span>0.908577</span>  <span>0.221286</span>
 <span>0.526123</span>  <span>0.788315</span>  <span>0.692201</span>  <span>0.99606</span>
 <span>0.510707</span>  <span>0.338502</span>  <span>0.832025</span>  <span>0.33279</span>
<span>[</span><span>:</span><span>,</span> <span>:</span><span>,</span> <span>1</span><span>,</span> <span>2</span><span>]</span> <span>=</span>
 <span>0.163337</span>  <span>0.991491</span>   <span>0.309396</span>  <span>0.155</span>
 <span>0.785946</span>  <span>0.0787799</span>  <span>0.160141</span>  <span>0.212985</span>
 <span>0.323122</span>  <span>0.806226</span>   <span>0.228209</span>  <span>0.205507</span>
<span>[</span><span>:</span><span>,</span> <span>:</span><span>,</span> <span>2</span><span>,</span> <span>2</span><span>]</span> <span>=</span>
 <span>0.0339063</span>  <span>0.402629</span>  <span>0.239698</span>   <span>0.471303</span>
 <span>0.787614</span>   <span>0.8888</span>    <span>0.0176223</span>  <span>0.957667</span>
 <span>0.352839</span>   <span>0.153378</span>  <span>0.829512</span>   <span>0.256615</span>
 <span>-</span><span>1.79658</span>    <span>1.45127</span>   <span>-</span><span>1.11244</span>
<span>B</span> <span>=</span> <span>randn</span><span>(</span><span>4</span><span>,</span> <span>3</span><span>,</span> <span>2</span><span>,</span> <span>2</span><span>);</span>
<span>A</span> <span>*</span> <span>B</span> <span># MethodError: no method matching *(::Array{Float64, 4}, ::Array{Float64, 4})</span></code></pre></figure>

<p>Multiplication simply isn’t defined for them. So we’ll have to write our own function to handle the multiplication here and also for the backpropagation. We’ll do this as a simple extension to 2D matrix multiplication.<sup id="fnref:tensors" role="doc-noteref"><a href="#fn:tensors" rel="footnote">2</a></sup></p>

<h3 id="attention">Attention</h3>

<p>The most important steps in the above table are the attention steps.
Combing them all into one and working with only 2D matrices, we get the definition for the scaled dot product attention:</p><p>

\[A = \text{softmax}\left(\frac{1}{\sqrt{d_h}}K^T Q\right)V\]

</p><p>Where $\text{softmax}$ is the function:</p><p>

\[\text{softmax}(z_i) = \frac{e^{z_i}}{\sum^N_r e^{z_r}}\]

</p><p>Attention is essentially a dot product of every column vector of the embedding matrix with some scaling.
To see this more clearly, substitute the equations for $K$ and $Q$ into $K^TQ$ and ignore the bias:</p><p>

\[K^T Q = (W_KX)(W_QX)^T = W_K XX^T W_Q^T\]

</p><p>I hope the $XX^T$ is recognisable as a dot product/inner product. 
The Google authors call it a different name, attention, and it is apparently all you need. 
It is very closely related to an older machine learning technique called <a href="https://www.machinelearningplus.com/nlp/cosine-similarity/">cosine similarity</a>.</p>

<p>Every word is multiplied with the embeddings of every other word, resulting in a small $N \times N$ matrix.
The hope is that the output looks something like:</p>
<figure>
<img src="https://liorsinai.github.io/assets/posts/transformers/attention.png" alt="architecture"/>
<figcaption></figcaption>
</figure>
<p>Reading down the columns we have an approximate weighting of how much every word thinks every other word is important.
Or in the paper’s terms, how much each word is paying attention to every other word.
Here “camera” thinks “great” is the most important word in the sentence.
Because of the weights this matrix is not symmetrical.
So “great” actually places less importance on “camera” than “camera” places on it.</p>

<p>This matrix is at the heart of transformer.
All the other layers work towards it or aim to use weights output by it.</p>

<h2 id="julia-implementation">Julia implementation</h2>
<h3 id="project-setup">Project setup</h3>

<p>We’ll be making use of the Flux framework along with the NNlib and ChainRulesCore packages.</p>

<p>An example output will be:</p>

<figure><pre><code data-lang="julia"><span>TransformerClassifier</span><span>(</span>
     <span>Embed</span><span>(</span><span>32</span><span>,</span> <span>7455</span><span>),</span>                   <span># 238_560 parameters</span>
     <span>PositionEncoding</span><span>(</span><span>32</span><span>),</span>
     <span>Dropout</span><span>(</span><span>0.1</span><span>),</span>
     <span>TransformerEncoderBlock</span><span>(</span>
          <span>MultiheadAttention</span><span>(</span><span>num_heads</span><span>=</span><span>4</span><span>,</span> <span>head_size</span><span>=</span><span>8</span><span>,</span> <span>32</span><span>=&gt;</span><span>32</span><span>)(</span>
               <span>denseQ</span> <span>=</span> <span>Dense</span><span>(</span><span>32</span> <span>=&gt;</span> <span>32</span><span>),</span>  <span># 1_056 parameters</span>
               <span>denseK</span> <span>=</span> <span>Dense</span><span>(</span><span>32</span> <span>=&gt;</span> <span>32</span><span>),</span>  <span># 1_056 parameters</span>
               <span>denseV</span> <span>=</span> <span>Dense</span><span>(</span><span>32</span> <span>=&gt;</span> <span>32</span><span>),</span>  <span># 1_056 parameters</span>
               <span>denseO</span> <span>=</span> <span>Dense</span><span>(</span><span>32</span> <span>=&gt;</span> <span>32</span><span>),</span>  <span># 1_056 parameters</span>
          <span>)</span>
          <span>Dropout</span><span>(</span><span>0.1</span><span>),</span>
          <span>LayerNorm</span><span>(</span><span>32</span><span>),</span>                <span># 64 parameters</span>
          <span>Dense</span><span>(</span><span>32</span> <span>=&gt;</span> <span>128</span><span>,</span> <span>relu</span><span>),</span>       <span># 4_224 parameters</span>
          <span>Dense</span><span>(</span><span>128</span> <span>=&gt;</span> <span>32</span><span>),</span>             <span># 4_128 parameters</span>
          <span>Dropout</span><span>(</span><span>0.1</span><span>),</span>
          <span>LayerNorm</span><span>(</span><span>32</span><span>),</span>                <span># 64 parameters</span>
     <span>)</span>
     <span>Dense</span><span>(</span><span>32</span> <span>=&gt;</span> <span>1</span><span>),</span>                    <span># 33 parameters</span>
     <span>FlattenLayer</span><span>(),</span>
     <span>Dense</span><span>(</span><span>50</span> <span>=&gt;</span> <span>5</span><span>),</span>                    <span># 255 parameters</span>
<span>)</span>                  <span># Total: 21 arrays, 251_552 parameters, 1.083 MiB</span></code></pre></figure>

<p>The <code>Dropout</code>, <code>LayerNorm</code> and <code>Dense</code> layers are already part of the Flux package.
We’ll be making <code>Embed</code>, <code>PositionEncoding</code>, <code>MultiheadAttention</code>, <code>TransformerEncoderBlock</code> and <code>FlattenLayer</code>.
We’ll also make a small index tokenizer to map tokens to word vectors.</p>

<p>The focus will be on the forward equations because Flux handles the backwards equation through automatic differentiation (AD).
Other than reducing our job in half, AD also means our forward and backwards equations will always be in sync. 
There will be collapsible blocks with backpropagation information ⇩.</p>

<p>To start, make a package in the Julia REPL:</p>
<figure>
    <code data-lang="julia-repl">
        <span>julia&gt;</span><span> cd(<span>&#34;path\\to\\project\\directory&#34;</span>)</span>
        </code>
</figure>

<p>The goal of using the package manager is that we can now use the super helpful Revise package,
which will dynamically update most changes during development without errors:</p>

<figure><pre><code data-lang="julia-repl">julia&gt; using Revise
julia&gt; using TransformersLite</code></pre></figure>

<p>You can see my final code at <a href="https://github.com/LiorSinai/TransformersLite.jl">github.com/LiorSinai/TransformersLite.jl</a>.
This is based loosely on the registered <a href="https://github.com/chengchingwen/Transformers.jl">Transformers.jl</a> package.</p>

<h3 id="tokenizers">Tokenizers</h3>

<p>The input is a sentence when we need to break up into tokens. 
This preprocessing step is a huge topic itself.
To avoid spending too much time here, I am going to provide functions for cleaning the text.
They put all text in lowercase, normalize unicode to ASCII e.g. “é” to “e” and “don’t” to “dont” and split sentences into words.
The regex for the latter is <code>[A-Za-z][A-Za-z]+\b</code> which finds all words with more than two ASCII letters without numbers.</p>

<figure><pre><code data-lang="julia"><span>using</span> <span>Unicode</span>
<span>function</span><span> clean</span><span>(</span><span>s</span><span>::</span><span>AbstractString</span><span>)</span>
    <span>s</span> <span>=</span> <span>lowercase</span><span>(</span><span>s</span><span>)</span>
    <span>s</span> <span>=</span> <span>Unicode</span><span>.</span><span>normalize</span><span>(</span><span>s</span><span>,</span> <span>:</span><span>NFD</span><span>)</span>
    <span>s</span> <span>=</span> <span>replace</span><span>(</span><span>s</span><span>,</span> <span>r</span><span>&#34;[&#39;`’</span><span>\u200d</span><span>\p{M}]&#34;</span> <span>=&gt;</span> <span>&#34;&#34;</span><span>)</span> <span># contractions, zero width joiner and marks from normalization</span>
    <span>s</span> <span>=</span> <span>replace</span><span>(</span><span>s</span><span>,</span> <span>r</span><span>&#34;</span><span>\n</span><span>&#34;</span> <span>=&gt;</span> <span>&#34; &#34;</span><span>)</span>
<span>end</span>

<span>function</span><span> preprocess</span><span>(</span><span>document</span><span>,</span> <span>tokenizer</span><span>;</span> <span>pattern</span> <span>=</span> <span>r</span><span>&#34;[A-Za-z][A-Za-z]+</span><span>\b</span><span>&#34;</span><span>,</span> <span>max_length</span><span>::</span><span>Union</span><span>{</span><span>Nothing</span><span>,</span> <span>Int</span><span>}</span><span>=</span><span>nothing</span><span>)</span>
    <span>document</span> <span>=</span> <span>clean</span><span>(</span><span>document</span><span>)</span>
    <span>words</span> <span>=</span> <span>map</span><span>(</span><span>m</span><span>-&gt;</span><span>string</span><span>(</span><span>m</span><span>.</span><span>match</span><span>),</span> <span>eachmatch</span><span>(</span><span>pattern</span><span>,</span> <span>document</span><span>))</span>
    <span>tokens</span> <span>=</span> <span>tokenizer</span><span>(</span><span>words</span><span>)</span>
    <span>if</span> <span>!</span><span>isnothing</span><span>(</span><span>max_length</span><span>)</span>
        <span>if</span> <span>length</span><span>(</span><span>tokens</span><span>)</span> <span>&gt;</span> <span>max_length</span>
            <span>tokens</span> <span>=</span> <span>tokens</span><span>[</span><span>1</span><span>:</span><span>max_length</span><span>]</span>
        <span>end</span>
    <span>end</span>
    <span>tokens</span>
<span>end</span></code></pre></figure>

<p>The <code>preprocess</code> function requires a tokenizer for subword tokenization.
I have made simple tokenizers at <a href="https://github.com/LiorSinai/TokenizersLite">github.com/LiorSinai/TokenizersLite.jl</a>.
You can also use the registered BytePairEncoding.jl package.
Or if you do not want subword tokenization use <code>tokenizer=identity</code>.
This is sufficient for the Amazon Reviews problem that we will investigate later.</p>

<p>Once we have tokens we do need to map them to word embeddings.
For this we’ll make a simple <code>IndexTokenizer</code>:</p>

<figure><pre><code data-lang="julia"><span>struct</span><span> IndexTokenizer</span><span>{</span><span>T</span><span>}</span>
    <span>vocabulary</span><span>::</span><span>Vector</span><span>{</span><span>T</span><span>}</span>
    <span>unksym</span><span>::</span><span>T</span>
    <span>unkidx</span><span>::</span><span>Int</span>
    <span>function</span><span> IndexTokenizer</span><span>(</span><span>vocab</span><span>::</span><span>Vector</span><span>{</span><span>T</span><span>},</span> <span>unksym</span><span>::</span><span>T</span><span>)</span> <span>where</span> <span>T</span>
        <span>if</span> <span>!</span><span>(</span><span>unksym</span> <span>∈</span> <span>vocab</span><span>)</span>
            <span>pushfirst!</span><span>(</span><span>vocab</span><span>,</span> <span>unksym</span><span>)</span>
            <span>unkidx</span> <span>=</span> <span>1</span>
        <span>else</span>
            <span>unkidx</span> <span>=</span> <span>findfirst</span><span>(</span><span>isequal</span><span>(</span><span>unksym</span><span>),</span> <span>vocab</span><span>)</span>
        <span>end</span>
        <span>new</span><span>{</span><span>T</span><span>}(</span><span>vocab</span><span>,</span> <span>unksym</span><span>,</span> <span>unkidx</span><span>)</span>
    <span>end</span>
<span>end</span>

<span>Base</span><span>.</span><span>length</span><span>(</span><span>tokenizer</span><span>::</span><span>IndexTokenizer</span><span>)</span> <span>=</span> <span>length</span><span>(</span><span>tokenizer</span><span>.</span><span>vocabulary</span><span>)</span>

<span>function</span><span> Base.show</span><span>(</span><span>io</span><span>::</span><span>IO</span><span>,</span> <span>tokenizer</span><span>::</span><span>IndexTokenizer</span><span>)</span> 
    <span>T</span> <span>=</span> <span>eltype</span><span>(</span><span>tokenizer</span><span>.</span><span>vocabulary</span><span>)</span>
    <span>print</span><span>(</span><span>io</span><span>,</span> <span>&#34;IndexTokenizer{</span><span>$(T)</span><span>}(length(vocabulary)=</span><span>$</span><span>(length(tokenizer)), unksym=</span><span>$</span><span>(tokenizer.unksym))&#34;</span><span>)</span>
<span>end</span></code></pre></figure>

<p>This <code>IndexTokenizer</code> takes in a list of tokens and an unknown symbol. 
The constructor function checks if the unknown symbol is in the list else it adds it to the front.</p>

<p>For the encoding process, we need to to replace a token with an index if it is in the vocabulary list and with the unknown symbol index (by default 1) if it is not:</p>

<figure><pre><code data-lang="julia"><span>encode</span><span>(</span><span>tokenizer</span><span>::</span><span>IndexTokenizer</span><span>{</span><span>T</span><span>},</span> <span>x</span><span>::</span><span>T</span><span>)</span> <span>where</span> <span>T</span> <span>=</span> <span>something</span><span>(</span>
	<span>findfirst</span><span>(</span><span>isequal</span><span>(</span><span>x</span><span>),</span> <span>tokenizer</span><span>.</span><span>vocabulary</span><span>),</span> <span>tokenizer</span><span>.</span><span>unkidx</span><span>)</span></code></pre></figure>

<p>This assumes we are giving a single token of type <code>T</code>. 
We also want to do multiple dispatch on sentences, which are <code>Vector{T}</code> and on batches of sentences, or <code>Vector{Vector{T}}</code>.
When with working with batches we’ll need all sentence to be the same length.
We truncate long sentences (already done in <code>preprocess</code>) and we can introduce a padding token for sentence longer than some maximum length.
Here the unknown token is used for padding:</p>

<figure><pre><code data-lang="julia"><span>function</span><span> encode</span><span>(</span><span>tokenizer</span><span>::</span><span>IndexTokenizer</span><span>{</span><span>T</span><span>},</span> <span>seq</span><span>::</span><span>AbstractVector</span><span>{</span><span>T</span><span>})</span> <span>where</span> <span>T</span>
    <span>map</span><span>(</span><span>x</span><span>-&gt;</span><span>encode</span><span>(</span><span>tokenizer</span><span>,</span> <span>x</span><span>),</span> <span>seq</span><span>)</span>
<span>end</span>

<span>function</span><span> encode</span><span>(</span><span>tokenizer</span><span>::</span><span>IndexTokenizer</span><span>{</span><span>T</span><span>},</span> <span>batch</span><span>::</span><span>AbstractVector</span><span>{</span><span>Vector</span><span>{</span><span>T</span><span>}})</span> <span>where</span> <span>T</span>
    <span>lengths</span> <span>=</span> <span>map</span><span>(</span><span>length</span><span>,</span> <span>batch</span><span>)</span>
    <span>indices</span> <span>=</span> <span>fill</span><span>(</span><span>tokenizer</span><span>.</span><span>unkidx</span><span>,</span> <span>maximum</span><span>(</span><span>lengths</span><span>),</span> <span>length</span><span>(</span><span>batch</span><span>))</span>
    <span>for</span> <span>(</span><span>i</span><span>,</span> <span>seq</span><span>)</span> <span>∈</span> <span>enumerate</span><span>(</span><span>batch</span><span>)</span>
        <span>for</span> <span>(</span><span>j</span><span>,</span> <span>x</span><span>)</span> <span>∈</span> <span>enumerate</span><span>(</span><span>seq</span><span>)</span>
            <span>@inbounds</span> <span>indices</span><span>[</span><span>j</span><span>,</span> <span>i</span><span>]</span> <span>=</span> <span>encode</span><span>(</span><span>tokenizer</span><span>,</span> <span>x</span><span>)</span>
        <span>end</span>
    <span>end</span>
    <span>indices</span>
<span>end</span></code></pre></figure>

<p>Lastly we can add a method to do multiple dispatch on the type <code>IndexTokenizer</code> itself, 
which turns this struct into a function:</p>

<figure><pre><code data-lang="julia"><span>(</span><span>tokenizer</span><span>::</span><span>IndexTokenizer</span><span>)(</span><span>x</span><span>)</span> <span>=</span> <span>encode</span><span>(</span><span>tokenizer</span><span>,</span> <span>x</span><span>)</span></code></pre></figure>

<p>In practice:</p>

<figure><pre><code data-lang="julia"><span>vocab</span> <span>=</span> <span>load_vocab</span><span>(</span><span>&#34;amazon_reviews_train_en.txt&#34;</span><span>)</span>
<span>indexer</span> <span>=</span> <span>IndexTokenizer</span><span>(</span><span>vocab</span><span>,</span> <span>&#34;[UNK]&#34;</span><span>)</span>

<span>text</span> <span>=</span> <span>&#34;This coffee from Kenya is really good.&#34;</span>
<span>tokens</span> <span>=</span> <span>preprocess</span><span>(</span><span>text</span><span>,</span> <span>identity</span><span>)</span> <span># [this,coffee,from,kenya,is,really,good]</span>
<span>indices</span> <span>=</span> <span>indexer</span><span>(</span><span>tokens</span><span>)</span> <span># [8,534,50,1,6,56,30]&#34;</span></code></pre></figure>

<p>The vocabulary file is at this <a href="https://github.com/LiorSinai/TransformersLite.jl/blob/main/vocab/amazon_reviews_train_en.txt">link</a> and the <code>load_vocab</code> function is:</p>

<figure><pre><code data-lang="julia"><span>function</span><span> load_vocab</span><span>(</span><span>filepath</span><span>)</span>
    <span>vocab</span> <span>=</span> <span>String</span><span>[]</span>
    <span>open</span><span>(</span><span>filepath</span><span>,</span> <span>&#34;r&#34;</span><span>)</span> <span>do</span> <span>file</span>
        <span>for</span> <span>line</span> <span>in</span> <span>eachline</span><span>(</span><span>file</span><span>)</span>
            <span>push!</span><span>(</span><span>vocab</span><span>,</span> <span>line</span><span>)</span>
        <span>end</span>
    <span>end</span>
    <span>vocab</span>
<span>end</span></code></pre></figure>

<p>The vocabulary is sorted from highest to lowest of the word counts in the original data . 
So if we limit the vocabulary e.g. <code>vocab[1:1000]</code> we can still be confident that it will have statistical significance.</p>

<h3 id="word-embeddings">Word embeddings</h3>
<p>Word embeddings were already introduced in the <a href="#inputs-and-outputs">Inputs and Outputs</a> section.
Here we’ll make a simple layer to store and retrieve them.</p>

<p>It it worth highlighting that the word embedding is unique to each model 
and will be trained from random values for each model.
This is not how humans work. 
Part of what makes language so useful is that we can have generic connotations and meanings for words and then derive more specific meaning from them in specific contexts. So for example, the word “good” always has the same “embedding” in any context.
But here we learn a different embedding for different models and even different training runs.</p>

<p>There are several justifications for this:</p>
<ol>
  <li>Word embeddings are task specific: for example in the Amazon review context “return” is a highly negative word associated with returning 
a defective product to the store. In other tasks it may be far more neutral.</li>
  <li>The tokenizer strategy from the previous section might change, or we might want to experiment with different tokenizers.</li>
  <li>We can tune the model dimension $d_m$ as a hyperparameter to make bigger or smaller models.</li>
</ol>

<p>This is somewhat unfortunate as it forms a massive part of our training. 
For the model I will use later it will be 95% of the trainable parameters.</p>

<p>The embedding layer is a struct that holds a matrix:</p>

<figure><pre><code data-lang="julia"><span>struct</span><span> Embed</span><span>{</span><span>W</span> <span>&lt;:</span> <span>AbstractArray</span><span>}</span>
    <span>embedding</span><span>::</span><span>W</span>
<span>end</span>

<span>Flux</span><span>.</span><span>@functor</span> <span>Embed</span> <span># tell Flux that this struct is trainable</span>

<span>Embed</span><span>(</span><span>output_dim</span><span>::</span><span>Int</span><span>,</span> <span>vocab_size</span><span>::</span><span>Int</span><span>)</span> <span>=</span> <span>Embed</span><span>(</span><span>randn</span><span>(</span><span>Float32</span><span>,</span> <span>output_dim</span><span>,</span> <span>vocab_size</span><span>))</span>

<span>Base</span><span>.</span><span>size</span><span>(</span><span>e</span><span>::</span><span>Embed</span><span>)</span> <span>=</span> <span>size</span><span>(</span><span>e</span><span>.</span><span>embedding</span><span>)</span>

<span>Base</span><span>.</span><span>show</span><span>(</span><span>io</span><span>::</span><span>IO</span><span>,</span> <span>e</span><span>::</span><span>Embed</span><span>)</span> <span>=</span> <span>print</span><span>(</span><span>io</span><span>,</span> <span>&#34;Embed(</span><span>$</span><span>(size(e.embedding)))&#34;</span><span>)</span>

<span>function</span><span> Base.show</span><span>(</span><span>io</span><span>::</span><span>IO</span><span>,</span> <span>m</span><span>::</span><span>MIME</span><span>&#34;text/plain&#34;</span><span>,</span> <span>e</span><span>::</span><span>Embed</span><span>)</span>
    <span>Flux</span><span>.</span><span>_layer_show</span><span>(</span><span>io</span><span>,</span> <span>e</span><span>)</span>
<span>end</span></code></pre></figure>

<p>We use <code>Float32</code> to reduce the size of the model and for performance benefits. We don’t need the extra accuracy provided by <code>Float64</code>.
We have a second show function for multimedia (MIME) types when we went prettier printing e.g. in the REPL and Jupyter notebooks.
The <code>Flux.@functor Embed</code> line is essential for Flux to be able to perform backpropagation.</p>

<p>For the forward pass we will use <code>NNlib.gather</code>:</p>

<figure><pre><code data-lang="julia"><span>using</span> <span>NNlib</span><span>:</span> <span>gather</span>
<span>function</span><span> </span><span>(</span><span>e</span><span>::</span><span>Embed</span><span>)(</span><span>x</span><span>::</span><span>AbstractArray</span><span>{</span><span>Int</span><span>})</span>
    <span>gather</span><span>(</span><span>e</span><span>.</span><span>embedding</span><span>,</span> <span>x</span><span>)</span>
<span>end</span></code></pre></figure>

<p>This is equivalent to <code>e.embedding[:, x]</code>. However using gather means that the <code>rrule</code> has already been defined for it.
See <a href="https://github.com/FluxML/NNlib.jl/blob/ff3ac6eb807e9b41f46f28f8b3287d19f4b722c7/src/gather.jl#L80">here</a>.</p>

<div>
	
	<div>
		<p>
		ChainRulesCore uses <code>rrule</code> to define backprogation rules.
		The old standard was the badly named <code>@adjoint</code> meaining the Jacobian adjoint meaning the conjugate transpose
		of the Jacobian. 
		This is different to the <code>adjoint</code> function which is the complex transpose of a matrix and the classical adjoint or adjugate which is the transpose of the cofactors of a matrix.
		</p>
	</div>
</div>

<p>The <code>rrule</code> is a reverse (backwards) rule that encodes the derivative for backpropagation. 
It is what makes the magic of automatic differentiation work.</p>

<p>The function <code>gather</code> does not have a formal derivative, but scatter is the opposite of it and is what we need to apply when we calculate the loss:</p>
<figure>
<img src="https://liorsinai.github.io/assets/posts/transformers/gather.png" alt="architecture"/>
<figcaption></figcaption>
</figure>

<p>At the end of backpropagation we need to distribute the error matrix amongst the original word embeddings.
This is what <code>scatter</code> does. Note that we use the red column twice, so we have two error columns directed towards it.
The <code>rrule</code> applies <code>+</code> as the reducing function; that is, the two errors are added together and then to the word embedding.</p>

<p>There is a cost to using a predefined function: it is very inefficient.
If we do a small experiment and call scatter we will see it results in a large matrix of mostly zeros:<sup id="fnref:sparse" role="doc-noteref"><a href="#fn:sparse" rel="footnote">3</a></sup></p>

<figure><pre><code data-lang="julia"><span>NNlib</span><span>.</span><span>scatter</span><span>(</span><span>+</span><span>,</span> <span>rand</span><span>(</span><span>8</span><span>,</span> <span>4</span><span>),</span> <span>[</span><span>1</span><span>,</span> <span>5</span><span>,</span> <span>11</span><span>,</span> <span>1</span><span>];</span> <span>dstsize</span><span>=</span><span>(</span><span>8</span><span>,</span> <span>15</span><span>))</span>
<span>8</span><span>×15</span> <span>Matrix</span><span>{</span><span>Float64</span><span>}</span><span>:</span>
 <span>1.62703</span>   <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.495725</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.237452</span>     <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>
 <span>0.979735</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.984499</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.145738</span>     <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>
 <span>0.892948</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.76959</span>   <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.714658</span>     <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>
 <span>1.45113</span>   <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.883492</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.52775</span>      <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>
 <span>0.702824</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.965256</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0966964</span>    <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>
 <span>1.16978</span>   <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.568429</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.000161501</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>
 <span>1.80566</span>   <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.271676</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.430018</span>     <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>
 <span>1.16445</span>   <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.911601</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.786343</span>     <span>0.0</span>  <span>0.0</span>  <span>0.0</span>  <span>0.0</span></code></pre></figure>

<h3 id="position-encodings">Position encodings</h3>

<p>As mentioned before, the matrix operations on the embedding matrix are parallel operations.
They do not take order into account.
One of the proposed methods proposed in the <a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a> paper to counteract this is to add constant value “position encodings” to the matrix.
The hope is that the model will learn these constant values and hence the relative and absolute positions.
For example a simple choice for each column $j$ is $\frac{j}{n_{max}}$:</p><p>

\[\begin{bmatrix} 
    \frac{1}{n_{max}} &amp; \frac{2}{n_{max}} &amp; \cdots &amp; \frac{n_{max}-1}{n_{max}} &amp;  1 \\
    \vdots            &amp;  \vdots           &amp; \ddots &amp; \vdots  &amp;  1 \\
    \frac{1}{n_{max}} &amp; \frac{2}{n_{max}} &amp; \cdots &amp; \frac{n_{max}-1}{n_{max}}  &amp;  1
\end{bmatrix}\]

</p><p>A problem with this encoding is that it is dependent on the parameter $n_{max}$, which fixes the sequence length.
Instead the authors propose a more convoluted solution but one that can be easily scaled to any sequence length:</p><p>

\[\begin{align} 
    PE(2i + 1, j) &amp;= \sin(j/(10^4)^{2i/d}) \\
    PE(2i + 2, j) &amp;= \cos(j/(10^4)^{2i/d})
\end{align}\]

</p><p>Plotted here on the left is a heatmap of the resultant matrix 
and on the right are the sine waves used for the odd numbered rows:</p>
<div>
    <p><img src="https://liorsinai.github.io/assets/posts/transformers/position_encodings.png" alt="architecture"/></p><div> 
        <p><img id="sineGraph" src="https://liorsinai.github.io/assets/posts/transformers/position_encoding_sin0.png" alt="sine graphs"/></p>
    </div>
</div>


<p>Each column has a unique pattern so the encoding does accomplishes its task.
To understand how, lets focus on the first row with $i=0$. 
This sine wave has a wavelength of $2\pi \approx 6.28$ and we sample it every $1$ timestep so it repeats every 6 blocks.
This leads to the 6 alternating colours in the top row: 3 light, then 3 dark, then repeat. 
So this sine wave can only distinguish between sequences of length 6 or less.
Now let’s move on to $i=1$. This sine wave has a period of $2\pi(10^4)^{2/32} \approx 11.17$ so it repeats approximately every 11 blocks in the 3rd row. 
We can now distinguish between sequences of up to length 11 and we can use the first row for greater precision.
As we add sine waves, we can distinguish between sequences of longer wave lengths.
In general the wavelengths are $2\pi(10^4)^{2i/d}$.</p>

<p>The remaining question, is why use both sine and cosine waves? 
The answer in the paper is: “We chose this function because we hypothesized it would allow the model to easily learn to attend by
relative positions, since for any fixed offset $k$, $PE_{j+k}$ can be represented as a linear function of
$PE_{j}$.” Here they are referring to the identities:</p><p>

\[\sin(\omega k + \omega j) = \sin(\omega k)\cos(\omega j) + \cos(\omega k)\sin(\omega j) \\
    \cos(\omega k + \omega j) = \cos(\omega k)\cos(\omega j) + \sin(\omega k)\sin(\omega j) \\\]

</p><p>which are linear for constant $\omega$ and $k$. 
For more detail please see <a href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/">here</a>.<sup id="fnref:linear_pe" role="doc-noteref"><a href="#fn:linear_pe" rel="footnote">4</a></sup></p>

<p>Now let’s code the <code>PositionEncoding</code> layer.
Since these values are constant, it is easiest to preallocate a matrix:</p>

<figure><pre><code data-lang="julia"><span>struct</span><span> PositionEncoding</span><span>{</span><span>W</span> <span>&lt;:</span> <span>AbstractArray</span><span>}</span>
    <span>encoding</span><span>::</span><span>W</span>
<span>end</span>

<span>function</span><span> PositionEncoding</span><span>(</span><span>dim_embedding</span><span>::</span><span>Int</span><span>,</span> <span>max_length</span><span>::</span><span>Int</span><span>=</span><span>1000</span><span>)</span>
    <span>W</span> <span>=</span> <span>make_position_encoding</span><span>(</span><span>dim_embedding</span><span>,</span> <span>max_length</span><span>)</span>
    <span>PositionEncoding</span><span>(</span><span>W</span><span>)</span>
<span>end</span>

<span>function</span><span> make_position_encoding</span><span>(</span><span>dim_embedding</span><span>::</span><span>Int</span><span>,</span> <span>seq_length</span><span>::</span><span>Int</span><span>,</span> <span>n</span><span>::</span><span>Int</span><span>=</span><span>10000</span><span>)</span>
    <span>encoding</span> <span>=</span> <span>Matrix</span><span>{</span><span>Float32</span><span>}(</span><span>undef</span><span>,</span> <span>dim_embedding</span><span>,</span> <span>seq_length</span><span>)</span>
    <span>for</span> <span>pos</span> <span>in</span> <span>1</span><span>:</span><span>seq_length</span>
        <span>for</span> <span>row</span> <span>in</span> <span>0</span><span>:</span><span>2</span><span>:</span><span>(</span><span>dim_embedding</span> <span>-</span> <span>1</span><span>)</span>
            <span>denom</span> <span>=</span> <span>1</span><span>/</span><span>(</span><span>n</span><span>^</span><span>(</span><span>row</span><span>/</span><span>dim_embedding</span><span>))</span>
            <span>encoding</span><span>[</span><span>row</span> <span>+</span> <span>1</span><span>,</span> <span>pos</span><span>]</span> <span>=</span> <span>sin</span><span>(</span><span>pos</span> <span>*</span> <span>denom</span><span>)</span>
            <span>encoding</span><span>[</span><span>row</span> <span>+</span> <span>2</span><span>,</span> <span>pos</span><span>]</span> <span>=</span> <span>cos</span><span>(</span><span>pos</span> <span>*</span> <span>denom</span><span>)</span>
        <span>end</span>
    <span>end</span>
    <span>encoding</span>    
<span>end</span>

<span>function</span><span> Base.show</span><span>(</span><span>io</span><span>::</span><span>IO</span><span>,</span> <span>pe</span><span>::</span><span>PositionEncoding</span><span>)</span>
    <span>print</span><span>(</span><span>io</span><span>,</span> <span>&#34;PositionEncoding(</span><span>$</span><span>(size(pe.encoding, 1)))&#34;</span><span>)</span>
<span>end</span></code></pre></figure>

<p>The forward pass then selects the required columns from the pre-allocated array:</p>

<figure><pre><code data-lang="julia"><span>(</span><span>pe</span><span>::</span><span>PositionEncoding</span><span>)(</span><span>x</span><span>::</span><span>AbstractArray</span><span>)</span> <span>=</span> <span>(</span><span>pe</span><span>::</span><span>PositionEncoding</span><span>)(</span><span>size</span><span>(</span><span>x</span><span>,</span> <span>2</span><span>))</span>
<span>function</span><span> </span><span>(</span><span>pe</span><span>::</span><span>PositionEncoding</span><span>)(</span><span>seq_length</span><span>)</span>
    <span>max_length</span> <span>=</span> <span>size</span><span>(</span><span>pe</span><span>.</span><span>encoding</span><span>,</span> <span>2</span><span>)</span>
    <span>if</span> <span>seq_length</span> <span>&gt;</span> <span>max_length</span>
        <span>error</span><span>(</span><span>&#34;sequence length of </span><span>$</span><span>seq_length exceeds maximum position encoding length of </span><span>$</span><span>max_length&#34;</span><span>)</span>
    <span>end</span>
    <span>pe</span><span>.</span><span>encoding</span><span>[</span><span>:</span><span>,</span> <span>Base</span><span>.</span><span>OneTo</span><span>(</span><span>seq_length</span><span>)]</span>
<span>end</span></code></pre></figure>

<p>Here an error is raised if the size of the pre-allocated matrix is exceeded.
We could instead calculate the extra columns required or resize the encoding matrix.
For now this only adds extra complexity.</p>

<p>Also note this layer only returns the encoding, so we need to add it separately:</p>

<figure><pre><code data-lang="julia"><span>X</span> <span>=</span> <span>rand</span><span>(</span><span>32</span><span>,</span> <span>100</span><span>,</span> <span>16</span><span>)</span>
<span>pe</span> <span>=</span> <span>PositionEncoding</span><span>(</span><span>32</span><span>)</span>
<span>Z</span> <span>=</span> <span>X</span> <span>.+</span> <span>pe</span><span>(</span><span>X</span><span>)</span> <span># broadcast the 2D encoding matrix to 3D</span></code></pre></figure>

<p>If desired we can move the addition into the forward pass e.g. for use in <code>Flux.chain</code>.</p>

<h3 id="multiplication-with-higher-order-arrays">Multiplication with higher order arrays</h3>

<p>Starting in 2D, matrix multiplication for $A\times B$ is defined as the sum of rows in $A$ multiplied with the columns in $B$:</p><p>

\[C_{ij} = \sum_r A_{ir} B_{rj}\]

</p><p>This can be written as a set of three loops (ignoring checks):</p>

<figure><pre><code data-lang="julia"><span>function</span><span> mul2d</span><span>(</span><span>A</span><span>&lt;:</span><span>AbstractMatrix</span><span>,</span> <span>B</span><span>&lt;:</span><span>AbstractMatrix</span><span>)</span>
	<span>n</span> <span>=</span> <span>size</span><span>(</span><span>A</span><span>,</span> <span>2</span><span>)</span> <span># == size(B, 1)</span>
	<span>C</span> <span>=</span> <span>zeros</span><span>(</span><span>size</span><span>(</span><span>A</span><span>,</span> <span>1</span><span>),</span> <span>size</span><span>(</span><span>B</span><span>,</span> <span>2</span><span>))</span>
	<span>for</span> <span>i</span> <span>in</span> <span>1</span><span>:</span><span>size</span><span>(</span><span>A</span><span>,</span> <span>1</span><span>)</span>
		<span>for</span> <span>j</span> <span>in</span> <span>1</span><span>:</span><span>size</span><span>(</span><span>B</span><span>,</span> <span>2</span><span>)</span>
			<span>for</span> <span>r</span> <span>in</span> <span>1</span><span>:</span><span>n</span>
				<span>C</span><span>[</span><span>i</span><span>,</span> <span>j</span><span>]</span> <span>+=</span> <span>A</span><span>[</span><span>i</span><span>,</span> <span>r</span><span>]</span> <span>*</span> <span>B</span><span>[</span><span>r</span><span>,</span> <span>j</span><span>]</span>
			<span>end</span>
		<span>end</span>
	<span>end</span>
	<span>C</span>
<span>end</span></code></pre></figure>

<p>Of course many programming languages already have this function built in and have highly optimised it. 
We can do a quick time test:</p>

<figure><pre><code data-lang="julia">    <span>A</span> <span>=</span> <span>randn</span><span>(</span><span>100</span><span>,</span> <span>100</span><span>);</span>
    <span>B</span> <span>=</span> <span>randn</span><span>(</span><span>100</span><span>,</span> <span>100</span><span>);</span>
    <span>@time</span> <span>mul2d</span><span>(</span><span>A</span><span>,</span> <span>B</span><span>);</span> <span># 0.002391s</span>
    <span>@time</span> <span>A</span> <span>*</span> <span>B</span><span>;</span>       <span># 0.000265s</span></code></pre></figure>

<p>The naive implementation is 9× slower.
One of the reasons is the indexing is very inefficient.
The code will start at the top and count down to the cell needed for each multiplication
when we could take advantage of the fact that the next cell is next door:</p>

<figure>
<img src="https://liorsinai.github.io/assets/posts/transformers/indexing.png" alt="indexing"/>
<figcaption></figcaption>
</figure>

<p>Later in machine learning came the idea of batch multiplication.
This is doing multiplications for a set of independent matrices simultaneously by grouping them into one large 3D array:</p><p>

\[C_{ijk} = \sum_r A_{irk} B_{rjk}\]

</p><p>We could write this as a set of four loops. 
Or since we know the inbuilt <code>*</code> is faster we can substitute that for the three inner loops:</p>

<figure><pre><code data-lang="julia"><span>function</span><span> mul3d</span><span>(</span><span>A</span><span>::</span><span>AbstractArray</span><span>{</span><span>T</span><span>,</span> <span>3</span><span>},</span> <span>B</span><span>::</span><span>AbstractArray</span><span>{</span><span>T</span><span>,</span> <span>3</span><span>})</span> <span>where</span> <span>T</span>
    <span>C</span> <span>=</span> <span>Array</span><span>{</span><span>Float64</span><span>}(</span><span>undef</span><span>,</span> <span>size</span><span>(</span><span>A</span><span>,</span> <span>1</span><span>),</span> <span>size</span><span>(</span><span>B</span><span>,</span> <span>2</span><span>),</span> <span>size</span><span>(</span><span>A</span><span>,</span> <span>3</span><span>))</span>
	<span>for</span> <span>k</span> <span>in</span> <span>1</span><span>:</span><span>size</span><span>(</span><span>A</span><span>,</span> <span>3</span><span>)</span>
		<span>C</span><span>[</span><span>:</span><span>,</span> <span>:</span><span>,</span> <span>k</span><span>]</span> <span>=</span> <span>A</span><span>[</span><span>:</span><span>,</span> <span>:</span><span>,</span> <span>k</span><span>]</span> <span>*</span> <span>B</span><span>[</span><span>:</span><span>,</span> <span>:</span><span>,</span> <span>k</span><span>]</span>
	<span>end</span>
	<span>C</span>
<span>end</span></code></pre></figure>

<p>But this doesn’t take advantage of of the fact that we are standardising the size of the matrices (all sequences are of the same length).
It is equivalent to using type <code>Vector{Matrix}</code> rather than <code>Array{T, 3}</code>.
NNlib has written a more optimised version called <code>batched_mul</code>. 
Doing a time test:</p>

<figure><pre><code data-lang="julia">    <span>Using</span> <span>NNlib</span>
    <span>A</span> <span>=</span> <span>randn</span><span>(</span><span>100</span><span>,</span> <span>100</span><span>,</span> <span>32</span><span>);</span>
    <span>B</span> <span>=</span> <span>randn</span><span>(</span><span>100</span><span>,</span> <span>100</span><span>,</span> <span>32</span><span>);</span>
    <span>@time</span> <span>mul3d</span><span>(</span><span>A</span><span>,</span> <span>B</span><span>);</span>       <span># 0.010918s</span>
    <span>@time</span> <span>batched_mul</span><span>(</span><span>A</span><span>,</span> <span>B</span><span>);</span> <span># 0.006588s</span></code></pre></figure>

<p>The NNlib function is about 1.5× faster.</p>

<p>For transformers we work with 4D arrays but they are sets of sets of independent matrices (repetition intended).
So multiplication is a set of five loops or two outer loops and matrix multiplication:</p><p>

\[C_{ijkl} = \sum_r A_{irkl} B_{rjkl}\]

</p><figure><pre><code data-lang="julia"><span>function</span><span> mul4d</span><span>(</span><span>A</span><span>::</span><span>AbstractArray</span><span>{</span><span>T</span><span>,</span> <span>4</span><span>},</span> <span>B</span><span>::</span><span>AbstractArray</span><span>{</span><span>T</span><span>,</span> <span>4</span><span>})</span> <span>where</span> <span>T</span>
    <span>C</span> <span>=</span> <span>Array</span><span>{</span><span>Float64</span><span>,</span> <span>4</span><span>}(</span><span>undef</span><span>,</span> <span>size</span><span>(</span><span>A</span><span>,</span> <span>1</span><span>),</span> <span>size</span><span>(</span><span>B</span><span>,</span> <span>2</span><span>),</span> <span>size</span><span>(</span><span>A</span><span>,</span> <span>3</span><span>),</span> <span>size</span><span>(</span><span>A</span><span>,</span> <span>4</span><span>))</span>
    <span>for</span> <span>l</span> <span>in</span> <span>1</span><span>:</span><span>size</span><span>(</span><span>A</span><span>,</span> <span>4</span><span>)</span>
        <span>for</span> <span>k</span> <span>in</span> <span>1</span><span>:</span><span>size</span><span>(</span><span>A</span><span>,</span> <span>3</span><span>)</span>
            <span>C</span><span>[</span><span>:</span><span>,</span> <span>:</span><span>,</span> <span>k</span><span>,</span> <span>l</span><span>]</span> <span>=</span> <span>A</span><span>[</span><span>:</span><span>,</span> <span>:</span><span>,</span> <span>k</span><span>,</span> <span>l</span><span>]</span> <span>*</span> <span>B</span><span>[</span><span>:</span><span>,</span> <span>:</span><span>,</span> <span>k</span><span>,</span> <span>l</span><span>]</span>
        <span>end</span>
    <span>end</span>
    <span>C</span>
<span>end</span></code></pre></figure>

<p>
  <a data-toggle="collapse" href="#EulerInterp" role="button" aria-expanded="false" aria-controls="collapseExample">
    Backpropagation for mul4d ⇩
  </a>
</p>
<div id="EulerInterp">
  <div><p>
    If we try getting gradients for <code>mul4d</code> it will not work:
    </p><pre><code>
    y, pull = Flux.pullback(mul4d, A, B);
    errors = randn(size(y)...);
    grads = pull(errors)
    </code></pre><p>
    The error is: &#34;Mutating arrays is not supported&#34;. So we will have to make an explicit <code>rrule</code> for it.
    </p></div>
</div>

<p>Making an optimised version of this is beyond the scope of this post. 
But what we can do is extend <code>batched_mul</code> by reshaping 4D $m\times n \times p \times q$ arrays into 3D $m\times n \times pq$ arrays:</p>

<figure><pre><code data-lang="julia"><span>import</span> <span>NNlib</span><span>.</span><span>batched_mul</span>
<span>function</span><span> batched_mul</span><span>(</span><span>A</span><span>::</span><span>AbstractArray</span><span>{</span><span>T</span><span>,</span> <span>4</span><span>},</span> <span>B</span><span>::</span><span>AbstractArray</span><span>{</span><span>T</span><span>,</span> <span>4</span><span>})</span> <span>where</span> <span>{</span><span>T</span><span>}</span>
    <span>if</span> <span>(</span><span>size</span><span>(</span><span>A</span><span>,</span> <span>2</span><span>)</span> <span>!=</span> <span>size</span><span>(</span><span>B</span><span>,</span> <span>1</span><span>))</span> <span>||</span> <span>(</span><span>size</span><span>(</span><span>A</span><span>,</span> <span>3</span><span>)</span> <span>!=</span> <span>size</span><span>(</span><span>B</span><span>,</span> <span>3</span><span>))</span> <span>||</span> <span>(</span><span>size</span><span>(</span><span>A</span><span>,</span> <span>4</span><span>)</span> <span>!=</span> <span>size</span><span>(</span><span>B</span><span>,</span> <span>4</span><span>))</span>
        <span>message</span> <span>=</span> <span>&#34;A has dimensions </span><span>$</span><span>(size(A)) but B has dimensions </span><span>$</span><span>(size(B))&#34;</span>
        <span>throw</span><span>(</span><span>DimensionMismatch</span><span>(</span><span>message</span><span>))</span>
    <span>end</span>
    <span>new_A</span> <span>=</span> <span>reshape</span><span>(</span><span>A</span><span>,</span> <span>size</span><span>(</span><span>A</span><span>,</span> <span>1</span><span>),</span> <span>size</span><span>(</span><span>A</span><span>,</span> <span>2</span><span>),</span> <span>:</span><span>)</span>
    <span>new_B</span> <span>=</span> <span>reshape</span><span>(</span><span>B</span><span>,</span> <span>size</span><span>(</span><span>B</span><span>,</span> <span>1</span><span>),</span> <span>size</span><span>(</span><span>B</span><span>,</span> <span>2</span><span>),</span> <span>:</span><span>)</span>
    <span>C</span> <span>=</span> <span>batched_mul</span><span>(</span><span>new_A</span><span>,</span> <span>new_B</span><span>)</span>
    <span>new_C</span> <span>=</span> <span>reshape</span><span>(</span><span>C</span><span>,</span> <span>(</span><span>size</span><span>(</span><span>C</span><span>,</span> <span>1</span><span>),</span> <span>size</span><span>(</span><span>C</span><span>,</span> <span>2</span><span>),</span> <span>size</span><span>(</span><span>A</span><span>,</span> <span>3</span><span>),</span> <span>size</span><span>(</span><span>A</span><span>,</span> <span>4</span><span>)))</span>
    <span>new_C</span>
<span>end</span></code></pre></figure>

<p>Doing a time test</p>

<figure><pre><code data-lang="julia">    <span>A</span> <span>=</span> <span>randn</span><span>(</span><span>50</span><span>,</span> <span>50</span><span>,</span> <span>12</span><span>,</span> <span>32</span><span>)</span>
    <span>B</span> <span>=</span> <span>randn</span><span>(</span><span>50</span><span>,</span> <span>50</span><span>,</span> <span>12</span><span>,</span> <span>32</span><span>)</span>
    <span>@time</span> <span>mul4d</span><span>(</span><span>A</span><span>,</span> <span>B</span><span>);</span>       <span># 0.016885</span>
    <span>@time</span> <span>batched_mul</span><span>(</span><span>A</span><span>,</span> <span>B</span><span>);</span> <span># 0.005216s</span></code></pre></figure>

<p>The <code>batched_mul</code> version is about 3× faster.</p>

<p>We don’t need to write a <code>rrule</code> for it because rules already exists for <code>reshape</code> and <code>batched_mul</code>.</p>

<h3 id="multi-head-attention">Multi-head attention</h3>

<p>We are finally at the heart of the transformer: multi-head attention.
At the end of this step we will have a <code>MultiheadAttention</code> layer:</p>

<figure><pre><code data-lang="julia"><span>MultiheadAttention</span><span>(</span><span>num_heads</span><span>=</span><span>4</span><span>,</span> <span>head_size</span><span>=</span><span>8</span><span>,</span> <span>32</span><span>=&gt;</span><span>32</span><span>)(</span>
    <span>denseQ</span> <span>=</span> <span>Dense</span><span>(</span><span>32</span> <span>=&gt;</span> <span>32</span><span>),</span>  <span># 1_056 parameters</span>
    <span>denseK</span> <span>=</span> <span>Dense</span><span>(</span><span>32</span> <span>=&gt;</span> <span>32</span><span>),</span>  <span># 1_056 parameters</span>
    <span>denseV</span> <span>=</span> <span>Dense</span><span>(</span><span>32</span> <span>=&gt;</span> <span>32</span><span>),</span>  <span># 1_056 parameters</span>
    <span>denseO</span> <span>=</span> <span>Dense</span><span>(</span><span>32</span> <span>=&gt;</span> <span>32</span><span>),</span>  <span># 1_056 parameters</span>
<span>)</span></code></pre></figure>

<p>The multi-head attention layer splits up the embedding matrix into multiple heads.
Each head will act on an embedding dimension of $d_h$ instead of the full $d_m$ as if the embedding was only $d_h$ in size. 
If we have $H$ heads then $d_m=Hd_h$.</p>

<p>First define a struct to hold all the dense layers and a parameter for $H$ called <code>nhead</code>:</p>

<figure><pre><code data-lang="julia"><span>struct</span><span> MultiheadAttention</span><span>{</span><span>Q</span><span>&lt;:</span><span>Dense</span><span>,</span> <span>K</span><span>&lt;:</span><span>Dense</span><span>,</span> <span>V</span><span>&lt;:</span><span>Dense</span><span>,</span> <span>O</span><span>&lt;:</span><span>Dense</span><span>}</span>
    <span>nhead</span><span>::</span><span>Int</span>
    <span>denseQ</span><span>::</span><span>Q</span>
    <span>denseK</span><span>::</span><span>K</span>
    <span>denseV</span><span>::</span><span>V</span>
    <span>denseO</span><span>::</span><span>O</span>
<span>end</span>

<span>Flux</span><span>.</span><span>@functor</span> <span>MultiheadAttention</span> <span>(</span><span>denseQ</span><span>,</span> <span>denseK</span><span>,</span> <span>denseV</span><span>,</span> <span>denseO</span><span>,</span> <span>)</span> <span># tell Flux which parameters are trainable</span></code></pre></figure>

<p>We would like $d_m$ to be divisible by $H$ but the maths will work if it is not.
So if the user supplies $d_h$ accept it as valid:</p>

<figure><pre><code data-lang="julia"><span>function</span><span> MultiheadAttention</span><span>(</span><span>nhead</span><span>::</span><span>Int</span><span>,</span> <span>dm</span><span>::</span><span>Int</span><span>,</span> <span>dh</span><span>::</span><span>Int</span><span>,</span> <span>dout</span><span>::</span><span>Int</span><span>)</span>
    <span>MultiheadAttention</span><span>(</span>
        <span>nhead</span><span>,</span>
        <span>Dense</span><span>(</span><span>dm</span><span>,</span> <span>dh</span><span>*</span><span>nhead</span><span>),</span>
        <span>Dense</span><span>(</span><span>dm</span><span>,</span> <span>dh</span><span>*</span><span>nhead</span><span>),</span>
        <span>Dense</span><span>(</span><span>dm</span><span>,</span> <span>dh</span><span>*</span><span>nhead</span><span>),</span>
        <span>Dense</span><span>(</span><span>dh</span><span>*</span><span>nhead</span><span>,</span> <span>dout</span><span>),</span>
    <span>)</span>
<span>end</span>

<span>function</span><span> MultiheadAttention</span><span>(</span><span>nhead</span><span>::</span><span>Int</span><span>,</span> <span>dm</span><span>::</span><span>Int</span><span>,</span> <span>dout</span><span>::</span><span>Int</span><span>)</span>
    <span>if</span> <span>dm</span> <span>%</span> <span>nhead</span> <span>!=</span> <span>0</span> 
        <span>error</span><span>(</span><span>&#34;embedding dimension=</span><span>$</span><span>dm is not divisible by number of heads=</span><span>$</span><span>nhead&#34;</span><span>)</span>
    <span>end</span>
    <span>MultiheadAttention</span><span>(</span><span>nhead</span><span>,</span> <span>dm</span><span>,</span> <span>div</span><span>(</span><span>dm</span><span>,</span> <span>nhead</span><span>),</span> <span>dout</span><span>)</span>
<span>end</span></code></pre></figure>

<p>Define utility functions for printing:</p>

<figure><pre><code data-lang="julia"><span>function</span><span> Base.show</span><span>(</span><span>io</span><span>::</span><span>IO</span><span>,</span> <span>mha</span><span>::</span><span>MultiheadAttention</span><span>)</span>
    <span>dh</span> <span>=</span> <span>div</span><span>(</span><span>size</span><span>(</span><span>mha</span><span>.</span><span>denseQ</span><span>.</span><span>weight</span><span>)[</span><span>1</span><span>],</span> <span>mha</span><span>.</span><span>nhead</span><span>)</span>
    <span>dm</span> <span>=</span> <span>size</span><span>(</span><span>mha</span><span>.</span><span>denseQ</span><span>.</span><span>weight</span><span>)[</span><span>2</span><span>]</span>
    <span>dout</span> <span>=</span> <span>size</span><span>(</span><span>mha</span><span>.</span><span>denseO</span><span>.</span><span>weight</span><span>)[</span><span>1</span><span>]</span>
    <span>print</span><span>(</span><span>io</span><span>,</span> <span>&#34;MultiheadAttention(&#34;</span><span>)</span>
    <span>print</span><span>(</span><span>io</span><span>,</span> <span>&#34;num_heads=</span><span>$</span><span>(mha.nhead), &#34;</span><span>)</span>
    <span>print</span><span>(</span><span>io</span><span>,</span> <span>&#34;head_size=</span><span>$(dh)</span><span>, &#34;</span><span>)</span>
    <span>print</span><span>(</span><span>io</span><span>,</span> <span>&#34;</span><span>$(dm)</span><span>=&gt;</span><span>$(dout)</span><span>&#34;</span><span>)</span>
    <span>print</span><span>(</span><span>io</span><span>,</span> <span>&#34;)&#34;</span><span>)</span>
<span>end</span>

<span>function</span><span> Base.show</span><span>(</span><span>io</span><span>::</span><span>IO</span><span>,</span> <span>m</span><span>::</span><span>MIME</span><span>&#34;text/plain&#34;</span><span>,</span> <span>mha</span><span>::</span><span>MultiheadAttention</span><span>)</span>
    <span>_show_multiheadattention</span><span>(</span><span>io</span><span>,</span> <span>mha</span><span>)</span>
<span>end</span>

<span>function</span><span> _show_multiheadattention</span><span>(</span><span>io</span><span>::</span><span>IO</span><span>,</span> <span>mha</span><span>::</span><span>MultiheadAttention</span><span>;</span> <span>indent</span><span>=</span><span>0</span><span>)</span>
    <span>inner_indent</span> <span>=</span> <span>indent</span> <span>+</span> <span>5</span>
    <span>print</span><span>(</span><span>io</span><span>,</span> <span>&#34; &#34;</span><span>^</span><span>indent</span><span>,</span> <span>mha</span><span>)</span>
    <span>print</span><span>(</span><span>io</span><span>,</span><span>&#34;(&#34;</span><span>)</span>
    <span>print</span><span>(</span><span>io</span><span>,</span> <span>&#34;</span><span>\n</span><span>&#34;</span><span>)</span>
    <span>Flux</span><span>.</span><span>_layer_show</span><span>(</span><span>io</span><span>,</span> <span>mha</span><span>.</span><span>denseQ</span><span>,</span> <span>inner_indent</span><span>,</span> <span>&#34;denseQ&#34;</span><span>)</span>
    <span>Flux</span><span>.</span><span>_layer_show</span><span>(</span><span>io</span><span>,</span> <span>mha</span><span>.</span><span>denseK</span><span>,</span> <span>inner_indent</span><span>,</span> <span>&#34;denseK&#34;</span><span>)</span>
    <span>Flux</span><span>.</span><span>_layer_show</span><span>(</span><span>io</span><span>,</span> <span>mha</span><span>.</span><span>denseV</span><span>,</span> <span>inner_indent</span><span>,</span> <span>&#34;denseV&#34;</span><span>)</span>
    <span>Flux</span><span>.</span><span>_layer_show</span><span>(</span><span>io</span><span>,</span> <span>mha</span><span>.</span><span>denseO</span><span>,</span> <span>inner_indent</span><span>,</span> <span>&#34;denseO&#34;</span><span>)</span>
    <span>print</span><span>(</span><span>io</span><span>,</span> <span>&#34; &#34;</span><span>^</span><span>indent</span><span>,</span> <span>&#34;)&#34;</span><span>)</span>
    <span>if</span> <span>indent</span><span>==</span><span>0</span>
        <span>Flux</span><span>.</span><span>_big_finale</span><span>(</span><span>io</span><span>,</span> <span>mha</span><span>)</span>
    <span>else</span> 
        <span>println</span><span>(</span><span>io</span><span>,</span> <span>&#34;&#34;</span><span>)</span>
    <span>end</span>
<span>end</span></code></pre></figure>

<p>Now let’s start with the forward pass. 
We first calculate a query, key and value from the input matrix.
These terms are kind of archaic.
They refer to a database model where the user makes a query (text in a search box), this is mapped to keys (video titles) 
and a value is returned (video). 
Or for a more direct programming metaphor: a hashmap where the query is hashed to a key to retrieve a value.
The matrix multiplications here represent a softer version of this where we are returning a weighting of the values.
The same matrix is used as the query, key and value, which can be interpreted as a self-reflective lookup, analogous to asking a query what it thinks is most important about itself.</p>

<p>But the names aren’t so important. 
The query, key and value are each calculated using the dense matrices we stored in the struct based on the input matrices:</p>

<figure><pre><code data-lang="julia"><span>function</span><span> </span><span>(</span><span>mha</span><span>::</span><span>MultiheadAttention</span><span>)(</span><span>query</span><span>::</span><span>A1</span><span>,</span> <span>key</span><span>::</span><span>A2</span><span>,</span> <span>value</span><span>::</span><span>A3</span><span>)</span> <span>where</span> <span>{</span>
    <span>T</span><span>,</span> <span>A1</span> <span>&lt;:</span> <span>AbstractArray</span><span>{</span><span>T</span><span>,</span> <span>3</span><span>},</span> <span>A2</span> <span>&lt;:</span> <span>AbstractArray</span><span>{</span><span>T</span><span>,</span> <span>3</span><span>},</span> <span>A3</span> <span>&lt;:</span> <span>AbstractArray</span><span>{</span><span>T</span><span>,</span> <span>3</span><span>}}</span>
    <span># batch multiplication version. Input is dm × N × B</span>
    <span>qs</span> <span>=</span> <span>size</span><span>(</span><span>query</span><span>)</span>
    <span>ks</span> <span>=</span> <span>size</span><span>(</span><span>key</span><span>)</span>
    <span>vs</span> <span>=</span> <span>size</span><span>(</span><span>value</span><span>)</span>

    <span>#size(Q) == (dh*nhead, N, B)</span>
    <span>Q</span> <span>=</span> <span>mha</span><span>.</span><span>denseQ</span><span>(</span><span>query</span><span>)</span>
    <span>K</span> <span>=</span> <span>mha</span><span>.</span><span>denseK</span><span>(</span><span>key</span><span>)</span>
    <span>V</span> <span>=</span> <span>mha</span><span>.</span><span>denseV</span><span>(</span><span>value</span><span>)</span></code></pre></figure>

<p>In the above <a href="#multiplication-with-higher-order-arrays">section</a> I went into great detail about handling multiplication for higher order arrays. How does <code>Dense</code> handle the 3D input?</p>

<figure><pre><code data-lang="julia"><span>(</span><span>a</span><span>::</span><span>Dense</span><span>)(</span><span>x</span><span>::</span><span>AbstractArray</span><span>)</span> <span>=</span> 
  <span>reshape</span><span>(</span><span>a</span><span>(</span><span>reshape</span><span>(</span><span>x</span><span>,</span> <span>size</span><span>(</span><span>x</span><span>,</span><span>1</span><span>),</span> <span>:</span><span>)),</span> <span>:</span><span>,</span> <span>size</span><span>(</span><span>x</span><span>)[</span><span>2</span><span>:</span><span>end</span><span>]</span><span>...</span><span>)</span></code></pre></figure>

<p>It turns the 3D $d_m \times N \times B$ input into a 2D $d_m \times NB$ matrix, does the multiplication, then transforms it back again.
This solution is valid because the weights for the dense layer are 2D.</p>

<p>We now need to split $Q$, $K$ and $V$ from $d_m \times N \times B$ to $d_h \times N \times H \times B$ matrices.
This is done in two steps:</p>
<ol>
  <li>$d_h \times H \times N \times B$ (break $d_m$ into $d_h$ and $H$)</li>
  <li>$d_h \times N \times H \times B$ (swap the 2nd and 3rd dimensions)</li>
</ol>

<figure><pre><code data-lang="julia">    <span>Q</span> <span>=</span> <span>permutedims</span><span>(</span><span>reshape</span><span>(</span><span>Q</span><span>,</span> <span>dh</span><span>,</span> <span>mha</span><span>.</span><span>nhead</span><span>,</span> <span>qs</span><span>[</span><span>2</span><span>],</span> <span>qs</span><span>[</span><span>3</span><span>]),</span> <span>[</span><span>1</span><span>,</span> <span>3</span><span>,</span> <span>2</span><span>,</span> <span>4</span><span>])</span>
    <span>K</span> <span>=</span> <span>permutedims</span><span>(</span><span>reshape</span><span>(</span><span>K</span><span>,</span> <span>dh</span><span>,</span> <span>mha</span><span>.</span><span>nhead</span><span>,</span> <span>ks</span><span>[</span><span>2</span><span>],</span> <span>ks</span><span>[</span><span>3</span><span>]),</span> <span>[</span><span>1</span><span>,</span> <span>3</span><span>,</span> <span>2</span><span>,</span> <span>4</span><span>])</span>
    <span>V</span> <span>=</span> <span>permutedims</span><span>(</span><span>reshape</span><span>(</span><span>V</span><span>,</span> <span>dh</span><span>,</span> <span>mha</span><span>.</span><span>nhead</span><span>,</span> <span>vs</span><span>[</span><span>2</span><span>],</span> <span>vs</span><span>[</span><span>3</span><span>]),</span> <span>[</span><span>1</span><span>,</span> <span>3</span><span>,</span> <span>2</span><span>,</span> <span>4</span><span>])</span></code></pre></figure>

<p>Then we calculate the scaled dot attention for each head, combine results, and pass it through the output dense  layer:</p>

<figure><pre><code data-lang="julia">    <span>A</span> <span>=</span> <span>scaled_dot_attention</span><span>(</span><span>Q</span><span>,</span> <span>K</span><span>,</span> <span>V</span><span>)</span>
    <span>A</span> <span>=</span> <span>permutedims</span><span>(</span><span>A</span><span>,</span> <span>[</span><span>1</span><span>,</span> <span>3</span><span>,</span> <span>2</span><span>,</span> <span>4</span><span>])</span>
    <span>A</span> <span>=</span> <span>reshape</span><span>(</span><span>A</span><span>,</span> <span>dm</span><span>,</span> <span>size</span><span>(</span><span>A</span><span>,</span> <span>3</span><span>),</span> <span>size</span><span>(</span><span>A</span><span>,</span> <span>4</span><span>))</span>
    <span>mha</span><span>.</span><span>denseO</span><span>(</span><span>A</span><span>)</span>
<span>end</span></code></pre></figure>

<p>Using the <code>batched_mul</code> function from the previous section it is straightforward to calculate attention:<sup id="fnref:permutedims" role="doc-noteref"><a href="#fn:permutedims" rel="footnote">5</a></sup></p>

<figure><pre><code data-lang="julia"><span>function</span><span> scaled_dot_attention</span><span>(</span><span>query</span><span>::</span><span>A1</span><span>,</span> <span>key</span><span>::</span><span>A2</span><span>,</span> <span>value</span><span>::</span><span>A3</span><span>)</span> <span>where</span> <span>{</span>
    <span>T</span><span>,</span> <span>A1</span> <span>&lt;:</span> <span>AbstractArray</span><span>{</span><span>T</span><span>,</span> <span>4</span><span>},</span> <span>A2</span> <span>&lt;:</span> <span>AbstractArray</span><span>{</span><span>T</span><span>,</span> <span>4</span><span>},</span> <span>A3</span> <span>&lt;:</span> <span>AbstractArray</span><span>{</span><span>T</span><span>,</span> <span>4</span><span>}}</span>
    <span># Batched version. Input is (dh, N, nhead, B)</span>
    <span>dh</span> <span>=</span> <span>size</span><span>(</span><span>query</span><span>,</span> <span>1</span><span>)</span>
    <span>keyT</span> <span>=</span> <span>permutedims</span><span>(</span><span>key</span><span>,</span> <span>(</span><span>2</span><span>,</span> <span>1</span><span>,</span> <span>3</span><span>,</span> <span>4</span><span>))</span>
    <span>score</span> <span>=</span> <span>one</span><span>(</span><span>T</span><span>)</span><span>/</span><span>convert</span><span>(</span><span>T</span><span>,</span> <span>sqrt</span><span>(</span><span>dh</span><span>))</span> <span>.*</span> <span>batched_mul</span><span>(</span><span>keyT</span><span>,</span> <span>query</span><span>)</span>
    <span>score</span> <span>=</span> <span>softmax</span><span>(</span><span>score</span><span>;</span> <span>dims</span><span>=</span><span>1</span><span>)</span> <span>#size(score) == (N, N, nhead, B)</span>
    <span>batched_mul</span><span>(</span><span>value</span><span>,</span> <span>score</span><span>)</span> <span>#size(attention) == (dh, N, nhead, B)</span>
<span>end</span></code></pre></figure>

<p>The softmax function (and its rrule) are provided by NNlib. For backpropagation information please see this 
<a href="https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/">link</a>.</p>

<p>
  <a data-toggle="collapse" href="#EulerInterp" role="button" aria-expanded="false" aria-controls="collapseExample">
    Notes on backpropagation in the attention layer ⇩
  </a>
</p>
<div id="EulerInterp">
  <p>
    For a 2D matrix $Z=AB$ we have the backwards functions defined as:
    $$
    \frac{\partial L}{\partial A} = \frac{\partial L}{\partial Z} B^T \\
    \frac{\partial L}{\partial B} = A^T \frac{\partial L}{\partial Z}
    $$
    Here we have a case of $Z=A^TB$.  
    This requires finding transposes of these results:
    $$
    \frac{\partial L}{\partial A} = \left(\frac{\partial L}{\partial A^T}\right)^T = B \frac{\partial L}{\partial Z}^T \\
    \frac{\partial L}{\partial B} = (A^T)^T \frac{\partial L}{\partial Z} = A\frac{\partial L}{\partial Z}
    $$
    We don&#39;t need to define the rrule because Flux will combine the rules for <code>permutedims</code> and <code>batched_mul</code> to get the same result.
  </p>
</div>

<p>The one last thing we need to do is make it work for a single embedding instead of a batch.
For code reuse the best solution is to make a single embedding a batch of one:</p>

<figure><pre><code data-lang="julia"><span>function</span><span> </span><span>(</span><span>mha</span><span>::</span><span>MultiheadAttention</span><span>)(</span><span>query</span><span>::</span><span>A1</span><span>,</span> <span>key</span><span>::</span><span>A2</span><span>,</span> <span>value</span><span>::</span><span>A3</span><span>)</span> <span>where</span> <span>{</span>
    <span>T</span><span>,</span> <span>A1</span> <span>&lt;:</span> <span>AbstractMatrix</span><span>{</span><span>T</span><span>},</span> <span>A2</span> <span>&lt;:</span> <span>AbstractMatrix</span><span>{</span><span>T</span><span>},</span> <span>A3</span> <span>&lt;:</span> <span>AbstractMatrix</span><span>{</span><span>T</span><span>}}</span>
    <span># single sample version. Input is dm × N</span>
    <span>query</span> <span>=</span> <span>reshape</span><span>(</span><span>query</span><span>,</span> <span>size</span><span>(</span><span>query</span><span>,</span> <span>1</span><span>),</span> <span>size</span><span>(</span><span>query</span><span>,</span> <span>2</span><span>),</span> <span>1</span><span>)</span>
    <span>key</span>   <span>=</span> <span>reshape</span><span>(</span><span>key</span><span>,</span> <span>size</span><span>(</span><span>key</span><span>,</span> <span>1</span><span>),</span> <span>size</span><span>(</span><span>key</span><span>,</span> <span>2</span><span>),</span> <span>1</span><span>)</span>
    <span>value</span> <span>=</span> <span>reshape</span><span>(</span><span>value</span><span>,</span> <span>size</span><span>(</span><span>value</span><span>,</span> <span>1</span><span>),</span> <span>size</span><span>(</span><span>value</span><span>,</span> <span>2</span><span>),</span> <span>1</span><span>)</span>
    <span>A</span> <span>=</span> <span>mha</span><span>(</span><span>query</span><span>,</span> <span>key</span><span>,</span> <span>value</span><span>)</span>
    <span>reshape</span><span>(</span><span>A</span><span>,</span> <span>size</span><span>(</span><span>A</span><span>,</span> <span>1</span><span>),</span> <span>size</span><span>(</span><span>A</span><span>,</span> <span>2</span><span>))</span>
<span>end</span></code></pre></figure>

<h3 id="encoder-blocks">Encoder blocks</h3>

<p>We still need to complete the rest of the equations in the <a href="#equations_table">table</a>.
Thankfully the rest of the layers are provided by Flux. We wrap them in an <code>TransformerEncoderBlock</code>:</p>

<figure><pre><code data-lang="julia"><span>struct</span><span> TransformerEncoderBlock</span><span>{</span><span>MA</span><span>&lt;:</span><span>MultiheadAttention</span><span>,</span> <span>L1</span><span>&lt;:</span><span>LayerNorm</span><span>,</span> <span>D1</span><span>&lt;:</span><span>Dense</span><span>,</span> <span>D2</span><span>&lt;:</span><span>Dense</span><span>,</span> <span>L2</span><span>&lt;:</span><span>LayerNorm</span><span>,</span> <span>DO</span><span>&lt;:</span><span>Dropout</span><span>}</span>
    <span>multihead_attention</span><span>::</span><span>MA</span>
    <span>layer_norm_attention</span><span>::</span><span>L1</span>
    <span>dense1</span><span>::</span><span>D1</span>
    <span>dense2</span><span>::</span><span>D2</span>
    <span>layer_norm_feedforward</span><span>::</span><span>L2</span>
    <span>dropout</span><span>::</span><span>DO</span>
<span>end</span>
<span>Flux</span><span>.</span><span>@functor</span> <span>TransformerEncoderBlock</span> <span># make whole TransformerEncoder trainable</span></code></pre></figure>

<p>This layer includes drop out regularization which wasn’t in the table but it is part of the original paper.
During training this layer randomly sets some weights to zero.
This interferes with training but makes it less likely to overfit.
Have a look at these graphs from the training for the sentiment analysis task:</p>
<figure>
<img src="https://liorsinai.github.io/assets/posts/transformers/dropout.png" alt="dropout history"/>
<figcaption></figcaption>
</figure>
<p>With 10% of dropout there is a much smaller gap between the validation and training accuracies.</p>

<p>Layer norm is a normalization over each layer. 
In the embedding matrix that is each word, so each column in each batch of matrices.
There are other kinds of normalization like batch normalization, which is a normalization across batches.
Interestingly layer norm was only popularised after batch normalization in this <a href="https://arxiv.org/abs/1607.06450">2016 paper</a>.</p>

<p>The actual function used for layer norm is:</p><p>

\[a_{nb}\frac{X_{nb}-\mu_{nb}}{\sigma_{nb}+\epsilon} + b_{nb}\]

</p><p>For every column $n$ of every batch $b$. This has two parameters in $a_{nb}$ and $b_{nb}$. 
They are not so important and you can turn them off with <code>LayerNorm(d, affine=false)</code>.
$\epsilon$ is a small constant value for numerical stability.
For backpropagation information please see 
<a href="https://liorsinai.github.io/mathematics/2022/05/18/layernorm.html">my post</a>.</p>

<p>Because the inputs and outputs are similar we only need four parameters to define the whole block:</p>

<figure><pre><code data-lang="julia"><span>TransformerEncoderBlock</span><span>(</span><span>nhead</span><span>::</span><span>Int</span><span>,</span> <span>dm</span><span>::</span><span>Int</span><span>,</span> <span>dhid</span><span>::</span><span>Int</span><span>;</span> <span>pdrop</span><span>::</span><span>Float64</span><span>=</span><span>0.1</span><span>)</span> <span>=</span> <span>TransformerEncoderBlock</span><span>(</span>
    <span>MultiheadAttention</span><span>(</span><span>nhead</span><span>,</span> <span>dm</span><span>,</span> <span>dm</span><span>),</span>
    <span>LayerNorm</span><span>(</span><span>dm</span><span>),</span>
    <span>Dense</span><span>(</span><span>dm</span><span>,</span> <span>dhid</span><span>,</span> <span>relu</span><span>),</span>
    <span>Dense</span><span>(</span><span>dhid</span><span>,</span> <span>dm</span><span>),</span>
    <span>LayerNorm</span><span>(</span><span>dm</span><span>),</span>
    <span>Dropout</span><span>(</span><span>pdrop</span><span>)</span>
<span>)</span></code></pre></figure>

<p>Printing functions:</p>

<figure><pre><code data-lang="julia"><span>function</span><span> Base.show</span><span>(</span><span>io</span><span>::</span><span>IO</span><span>,</span> <span>te</span><span>::</span><span>TransformerEncoderBlock</span><span>)</span>
    <span>print</span><span>(</span><span>io</span><span>,</span> <span>&#34;TransformerEncoderBlock(&#34;</span><span>)</span>
    <span>print</span><span>(</span><span>io</span><span>,</span> <span>te</span><span>.</span><span>multihead_attention</span><span>)</span>
    <span>print</span><span>(</span><span>io</span><span>,</span> <span>&#34;, &#34;</span><span>,</span> <span>te</span><span>.</span><span>layer_norm_attention</span><span>)</span>
    <span>print</span><span>(</span><span>io</span><span>,</span> <span>&#34;, &#34;</span><span>,</span> <span>te</span><span>.</span><span>dense1</span><span>)</span>
    <span>print</span><span>(</span><span>io</span><span>,</span> <span>&#34;, &#34;</span><span>,</span> <span>te</span><span>.</span><span>dense2</span><span>)</span>
    <span>print</span><span>(</span><span>io</span><span>,</span> <span>&#34;, &#34;</span><span>,</span> <span>te</span><span>.</span><span>layer_norm_feedforward</span><span>)</span>
    <span>print</span><span>(</span><span>io</span><span>,</span> <span>&#34;)&#34;</span><span>)</span>
<span>end</span>

<span>function</span><span> Base.show</span><span>(</span><span>io</span><span>::</span><span>IO</span><span>,</span> <span>m</span><span>::</span><span>MIME</span><span>&#34;text/plain&#34;</span><span>,</span> <span>te</span><span>::</span><span>TransformerEncoderBlock</span><span>)</span>
    <span>_show_transformer_encoder</span><span>(</span><span>io</span><span>,</span> <span>te</span><span>)</span>
<span>end</span>

<span>function</span><span> _show_transformer_encoder</span><span>(</span><span>io</span><span>::</span><span>IO</span><span>,</span> <span>t</span><span>::</span><span>TransformerEncoderBlock</span><span>;</span> <span>indent</span><span>=</span><span>0</span><span>)</span>
    <span>inner_indent</span> <span>=</span> <span>indent</span> <span>+</span> <span>5</span>
    <span>print</span><span>(</span><span>io</span><span>,</span> <span>&#34; &#34;</span><span>^</span><span>indent</span><span>,</span> <span>&#34;TransformerEncoderBlock&#34;</span><span>)</span>
    <span>print</span><span>(</span><span>io</span><span>,</span> <span>&#34;(&#34;</span><span>)</span>
    <span>print</span><span>(</span><span>io</span><span>,</span> <span>&#34;</span><span>\n</span><span>&#34;</span><span>)</span>
    <span>_show_multiheadattention</span><span>(</span><span>io</span><span>,</span> <span>t</span><span>.</span><span>multihead_attention</span><span>,</span> <span>indent</span><span>=</span><span>inner_indent</span><span>)</span>
    <span>Flux</span><span>.</span><span>_layer_show</span><span>(</span><span>io</span><span>,</span> <span>t</span><span>.</span><span>dropout</span><span>,</span> <span>inner_indent</span><span>)</span>
    <span>Flux</span><span>.</span><span>_layer_show</span><span>(</span><span>io</span><span>,</span> <span>t</span><span>.</span><span>layer_norm_attention</span><span>,</span> <span>inner_indent</span><span>)</span>
    <span>Flux</span><span>.</span><span>_layer_show</span><span>(</span><span>io</span><span>,</span> <span>t</span><span>.</span><span>dense1</span><span>,</span> <span>inner_indent</span><span>)</span>
    <span>Flux</span><span>.</span><span>_layer_show</span><span>(</span><span>io</span><span>,</span> <span>t</span><span>.</span><span>dense2</span><span>,</span> <span>inner_indent</span><span>)</span>
    <span>Flux</span><span>.</span><span>_layer_show</span><span>(</span><span>io</span><span>,</span> <span>t</span><span>.</span><span>dropout</span><span>,</span> <span>inner_indent</span><span>)</span>
    <span>Flux</span><span>.</span><span>_layer_show</span><span>(</span><span>io</span><span>,</span> <span>t</span><span>.</span><span>layer_norm_attention</span><span>,</span> <span>inner_indent</span><span>)</span>
    <span>print</span><span>(</span><span>io</span><span>,</span> <span>&#34; &#34;</span><span>^</span><span>indent</span><span>,</span> <span>&#34;)&#34;</span><span>)</span>
    <span>if</span> <span>indent</span><span>==</span><span>0</span>
        <span>Flux</span><span>.</span><span>_big_finale</span><span>(</span><span>io</span><span>,</span> <span>t</span><span>)</span>
    <span>else</span>
        <span>println</span><span>(</span><span>io</span><span>,</span> <span>&#34;&#34;</span><span>)</span>
    <span>end</span>
<span>end</span></code></pre></figure>

<p>Lastly, the forward pass:</p>

<figure><pre><code data-lang="julia"><span>function</span><span> </span><span>(</span><span>t</span><span>::</span><span>TransformerEncoderBlock</span><span>)(</span><span>x</span><span>::</span><span>A</span><span>)</span> <span>where</span> <span>{</span><span>T</span><span>,</span> <span>N</span><span>,</span> <span>A</span><span>&lt;:</span><span>AbstractArray</span><span>{</span><span>T</span><span>,</span> <span>N</span><span>}}</span>
    <span>a</span> <span>=</span> <span>t</span><span>.</span><span>multihead_attention</span><span>(</span><span>x</span><span>,</span> <span>x</span><span>,</span> <span>x</span><span>)</span>
    <span>a</span> <span>=</span> <span>t</span><span>.</span><span>dropout</span><span>(</span><span>a</span><span>)</span>
    <span>res_a</span> <span>=</span> <span>x</span> <span>+</span> <span>a</span> <span># skip connection</span>
    <span>res_a</span> <span>=</span> <span>t</span><span>.</span><span>layer_norm_attention</span><span>(</span><span>res_a</span><span>)</span>
    <span>z_ff</span> <span>=</span> <span>t</span><span>.</span><span>dense1</span><span>(</span><span>res_a</span><span>)</span>
    <span>z_ff</span> <span>=</span> <span>t</span><span>.</span><span>dense2</span><span>(</span><span>z_ff</span><span>)</span>
    <span>z_ff</span> <span>=</span> <span>t</span><span>.</span><span>dropout</span><span>(</span><span>z_ff</span><span>)</span>
    <span>res_ff</span> <span>=</span> <span>res_a</span> <span>+</span> <span>z_ff</span> <span># skip connection</span>
    <span>res_ff</span> <span>=</span> <span>t</span><span>.</span><span>layer_norm_feedforward</span><span>(</span><span>res_ff</span><span>)</span>
    <span>res_ff</span>
<span>end</span></code></pre></figure>

<p>Skip connections are short-circuits.
They look like they are undoing all the hard work of the previous layer.
However these have proved very useful for neural networks with many layers
because they carry a strong signal both on the forward pass and with the gradient on the backwards pass.</p>

<h3 id="classifier">Classifier</h3>

<p>At last, our model is almost ready for use.
There is just one last question, how to use the output embedding matrix?
We could take a mean across each word embedding and then pass that to a dense layer.
Or we can take a dense layer across each word, reduce it down to one dimension, and pass that to a dense layer.
Or we could flatten the whole array into a $d_m N \times 1$ column.</p>

<p>My preference is to do an aggregation on each word first and then on the sentence.
Here is a simple flatten layer which we will need to put in between:</p>

<figure><pre><code data-lang="julia"><span>struct</span><span> FlattenLayer</span> <span>end</span>

<span>Flux</span><span>.</span><span>@functor</span> <span>FlattenLayer</span>

<span>function</span><span> </span><span>(</span><span>f</span><span>::</span><span>FlattenLayer</span><span>)(</span><span>x</span><span>::</span><span>AbstractArray</span><span>{</span><span>T</span><span>,</span> <span>3</span><span>})</span> <span>where</span> <span>T</span>
  <span>reshape</span><span>(</span><span>x</span><span>,</span> <span>:</span><span>,</span> <span>size</span><span>(</span><span>x</span><span>,</span> <span>3</span><span>))</span> <span># same as Flux.flatten</span>
<span>end</span>

<span>function</span><span> </span><span>(</span><span>f</span><span>::</span><span>FlattenLayer</span><span>)(</span><span>x</span><span>::</span><span>AbstractArray</span><span>{</span><span>T</span><span>,</span> <span>2</span><span>})</span> <span>where</span> <span>T</span>
    <span>reshape</span><span>(</span><span>x</span><span>,</span> <span>:</span><span>,</span> <span>1</span><span>)</span> <span># returns a column vector</span>
<span>end</span>

<span>function</span><span> Base.show</span><span>(</span><span>io</span><span>::</span><span>IO</span><span>,</span> <span>f</span><span>::</span><span>FlattenLayer</span><span>)</span>
  <span>print</span><span>(</span><span>io</span><span>,</span> <span>&#34;FlattenLayer()&#34;</span><span>)</span>
<span>end</span></code></pre></figure>

<p>We can now make our model with Flux chain:</p>

<figure><pre><code data-lang="julia"><span>dim_embedding</span> <span>=</span> <span>32</span>
<span>pdrop</span> <span>=</span> <span>0.1</span>
<span>add_position_encoding</span><span>(</span><span>x</span><span>)</span> <span>=</span> <span>x</span> <span>.+</span> <span>position_encoding</span><span>(</span><span>x</span><span>)</span>
<span>model</span> <span>=</span> <span>Chain</span><span>(</span>
    <span>Embed</span><span>(</span><span>dim_embedding</span><span>,</span> <span>length</span><span>(</span><span>indexer</span><span>)),</span> 
    <span>add_position_encoding</span><span>,</span> <span># can also make anonymous</span>
    <span>Dropout</span><span>(</span><span>pdrop</span><span>),</span>
    <span>TransformerEncoderBlock</span><span>(</span><span>4</span><span>,</span> <span>dim_embedding</span><span>,</span> <span>dim_embedding</span> <span>*</span> <span>4</span><span>;</span> <span>pdrop</span><span>=</span><span>pdrop</span><span>),</span>
    <span>Dense</span><span>(</span><span>dim_embedding</span><span>,</span> <span>1</span><span>),</span>
    <span>TransformersLite</span><span>.</span><span>FlattenLayer</span><span>(),</span>
    <span>Dense</span><span>(</span><span>max_length</span><span>,</span> <span>nlabels</span><span>)</span>
     <span>)</span></code></pre></figure>

<p>Or here is the whole model wrapped in a struct with nicer printing and names:</p>

<figure><pre><code data-lang="julia"><span>struct</span><span> TransformerClassifier</span><span>{</span>
    <span>E</span><span>&lt;:</span><span>Embed</span><span>,</span> 
    <span>PE</span><span>&lt;:</span><span>PositionEncoding</span><span>,</span> 
    <span>DO</span><span>&lt;:</span><span>Dropout</span><span>,</span> 
    <span>TEB</span><span>&lt;:</span><span>Vector</span><span>{</span><span>TransformerEncoderBlock</span><span>},</span> 
    <span>A</span><span>,</span> 
    <span>f</span><span>&lt;:</span><span>FlattenLayer</span><span>,</span> 
    <span>D</span><span>&lt;:</span><span>Dense</span>
    <span>}</span>
    <span>embed</span><span>::</span><span>E</span>
    <span>position_encoding</span><span>::</span><span>PE</span>
    <span>dropout</span><span>::</span><span>DO</span>
    <span>encoder_layers</span><span>::</span><span>TEB</span>
    <span>agg_layer</span><span>::</span><span>A</span>
    <span>flatten_layer</span><span>::</span><span>f</span>
    <span>classifier</span><span>::</span><span>D</span>
<span>end</span>

<span>Flux</span><span>.</span><span>@functor</span> <span>TransformerClassifier</span>

<span>function</span><span> Base.show</span><span>(</span><span>io</span><span>::</span><span>IO</span><span>,</span> <span>m</span><span>::</span><span>MIME</span><span>&#34;text/plain&#34;</span><span>,</span> <span>t</span><span>::</span><span>TransformerClassifier</span><span>)</span>
    <span>_show_transformer_classifier</span><span>(</span><span>io</span><span>,</span> <span>t</span><span>)</span>
<span>end</span>

<span>function</span><span> _show_transformer_classifier</span><span>(</span><span>io</span><span>::</span><span>IO</span><span>,</span> <span>t</span><span>::</span><span>TransformerClassifier</span><span>;</span> <span>indent</span><span>=</span><span>0</span><span>)</span>
    <span>inner_indent</span> <span>=</span> <span>5</span>
    <span>print</span><span>(</span><span>io</span><span>,</span> <span>&#34; &#34;</span><span>^</span><span>indent</span><span>,</span> <span>&#34;TransformerClassifier&#34;</span><span>)</span>
    <span>print</span><span>(</span><span>io</span><span>,</span> <span>&#34;(&#34;</span><span>)</span>
    <span>print</span><span>(</span><span>io</span><span>,</span> <span>&#34;</span><span>\n</span><span>&#34;</span><span>)</span>
    <span>Flux</span><span>.</span><span>_layer_show</span><span>(</span><span>io</span><span>,</span> <span>t</span><span>.</span><span>embed</span><span>,</span> <span>inner_indent</span><span>)</span>
    <span>Flux</span><span>.</span><span>_layer_show</span><span>(</span><span>io</span><span>,</span> <span>t</span><span>.</span><span>position_encoding</span><span>,</span> <span>inner_indent</span><span>)</span>
    <span>Flux</span><span>.</span><span>_layer_show</span><span>(</span><span>io</span><span>,</span> <span>t</span><span>.</span><span>dropout</span><span>,</span> <span>inner_indent</span><span>)</span>
    <span>for</span> <span>e</span> <span>in</span> <span>t</span><span>.</span><span>encoder_layers</span>
        <span>_show_transformer_encoder</span><span>(</span><span>io</span><span>,</span> <span>e</span><span>,</span> <span>indent</span><span>=</span><span>inner_indent</span><span>)</span>
    <span>end</span>
    <span>Flux</span><span>.</span><span>_layer_show</span><span>(</span><span>io</span><span>,</span> <span>t</span><span>.</span><span>agg_layer</span><span>,</span> <span>inner_indent</span><span>)</span>
    <span>Flux</span><span>.</span><span>_layer_show</span><span>(</span><span>io</span><span>,</span> <span>t</span><span>.</span><span>flatten_layer</span><span>,</span> <span>inner_indent</span><span>)</span>
    <span>Flux</span><span>.</span><span>_layer_show</span><span>(</span><span>io</span><span>,</span> <span>t</span><span>.</span><span>classifier</span><span>,</span> <span>inner_indent</span><span>)</span>
    <span>print</span><span>(</span><span>io</span><span>,</span> <span>&#34; &#34;</span><span>^</span><span>indent</span><span>,</span> <span>&#34;)&#34;</span><span>)</span>
    <span>if</span> <span>indent</span><span>==</span><span>0</span>
        <span>Flux</span><span>.</span><span>_big_finale</span><span>(</span><span>io</span><span>,</span> <span>t</span><span>)</span>
    <span>end</span>
<span>end</span>

<span>function</span><span> </span><span>(</span><span>t</span><span>::</span><span>TransformerClassifier</span><span>)(</span><span>x</span><span>::</span><span>A</span><span>)</span> <span>where</span> <span>{</span><span>T</span><span>,</span> <span>N</span><span>,</span> <span>A</span><span>&lt;:</span><span>AbstractArray</span><span>{</span><span>T</span><span>,</span> <span>N</span><span>}}</span>
    <span>x</span> <span>=</span> <span>t</span><span>.</span><span>embed</span><span>(</span><span>x</span><span>)</span>
    <span>x</span> <span>=</span> <span>x</span> <span>.+</span> <span>t</span><span>.</span><span>position_encoding</span><span>(</span><span>x</span><span>)</span>
    <span>x</span> <span>=</span> <span>t</span><span>.</span><span>dropout</span><span>(</span><span>x</span><span>)</span>
    <span>for</span> <span>e</span> <span>in</span> <span>t</span><span>.</span><span>encoder_layers</span>
        <span>x</span> <span>=</span> <span>e</span><span>(</span><span>x</span><span>)</span>
    <span>end</span>
    <span>x</span> <span>=</span> <span>t</span><span>.</span><span>agg_layer</span><span>(</span><span>x</span><span>)</span>
    <span>x</span> <span>=</span> <span>t</span><span>.</span><span>flatten_layer</span><span>(</span><span>x</span><span>)</span>
    <span>x</span> <span>=</span> <span>t</span><span>.</span><span>classifier</span><span>(</span><span>x</span><span>)</span>
    <span>x</span>
<span>end</span></code></pre></figure>

<p>An example of a small model:</p>

<figure><pre><code data-lang="julia"><span>model</span> <span>=</span> <span>TransformersLite</span><span>.</span><span>TransformerClassifier</span><span>(</span>
    <span>Embed</span><span>(</span><span>dim_embedding</span><span>,</span> <span>length</span><span>(</span><span>indexer</span><span>)),</span> 
    <span>PositionEncoding</span><span>(</span><span>dim_embedding</span><span>),</span> 
    <span>Dropout</span><span>(</span><span>pdrop</span><span>),</span>
    <span>TransformerEncoderBlock</span><span>[</span>
        <span>TransformerEncoderBlock</span><span>(</span><span>4</span><span>,</span> <span>dim_embedding</span><span>,</span> <span>dim_embedding</span> <span>*</span> <span>4</span><span>;</span> <span>pdrop</span><span>=</span><span>pdrop</span><span>)</span>
    <span>],</span>
    <span>Dense</span><span>(</span><span>dim_embedding</span><span>,</span> <span>1</span><span>),</span> 
    <span>FlattenLayer</span><span>(),</span>
    <span>Dense</span><span>(</span><span>max_length</span><span>,</span> <span>nlabels</span><span>)</span>
    <span>)</span></code></pre></figure>

<p>Finally, we have a working transformer!</p>

<h2 id="use-case-amazon-reviews">Use case: Amazon Reviews</h2>

<p>Presented here is a subset of the results from scripts and notebooks at <a href="https://github.com/LiorSinai/TransformersLite.jl/tree/main/examples">github.com/LiorSinai/TransformersLite.jl/tree/main/examples</a>.</p>

<h3 id="data-exploration">Data exploration</h3>

<p>The <a href="https://huggingface.co/datasets/amazon_reviews_multi">Amazon Reviews</a> English dataset consists of 200,000 test samples and 5,000 training samples.
The reviews are equally divided into 5 stars where 1 is a low score and 5 is best.
There are eight features:</p>
<ol>
  <li>review_id</li>
  <li>product_id</li>
  <li>reviewer_id</li>
  <li>stars</li>
  <li>review_body</li>
  <li>review_title</li>
  <li>language</li>
  <li>product_category</li>
</ol>

<p>The models were only trained on “stars” and “review_body”.</p>

<p>A small sample of reviews (original spelling and punctuation):</p>
<table>
  <tbody><tr>
    <th>Star</th>
    <th>Review</th>
  </tr>
  <tr>
    <td>5</td>
    <td>I like everything abut them they are perfect!</td>
  </tr>
  <tr>
    <td>4</td>
    <td>This is not a bad chair for the price. I had some problems with the wheels but they were promptly addressed by very helpful customer service. So overall I can give the 4 stars to this product.</td>
  </tr>
  <tr>
    <td>3</td>
    <td>As expected and average product</td>
  </tr>
  <tr>
    <td>2</td>
    <td>Overall quality is good on this product although they are smaller in size than anticipated.</td>
  </tr>
  <tr>
    <td>1</td>
    <td>Dissapointing, bad quality dont buy</td>
  </tr>
</tbody></table>

<p>The reviews can go up to 4,000 characters, but most are much shorter than that:</p>
<figure>
<img src="https://liorsinai.github.io/assets/posts/transformers/content_lengths.png" alt="content lengths"/>
<figcaption></figcaption>
</figure>
<p>Of these reviews, 80% have less than 260 characters and/or less than 50 words.
This justifies setting a maximum token count of 50.</p>

<h3 id="pipeline">Pipeline</h3>

<p>Here is a pipeline for training a model, from tokenizing the input to saving the output data.
This pipeline implements a rudimentary development workflow with:</p>
<ul>
  <li>an output directory named after the date-time in “yyyymmdd-HHMM” format.</li>
  <li>training history saved in JSON format.</li>
  <li>hyperparameters that are used to control flow and are saved in JSON format.</li>
</ul>

<p>This includes a file I wrote called <a href="https://github.com/LiorSinai/TransformersLite.jl/blob/main/examples/training.jl">training.jl</a>. 
It defines the following functions: <code>split_validation</code>, <code>accuracy</code>, <code>batch_matrix</code>, <code>batched_metric</code>, and <code>train!</code>.
Please download or copy these functions to follow along.
The <code>train!</code> function is based off <code>Flux.train!</code> except it returns a history and uses the batch functions.
These reduce the maximum memory requirement at any one time. 
In practice I found this made the process faster even though the total memory consumed throughout is about the same.</p>

<p>The embedding dimension <code>dim_embedding</code> should be at least 8 for the Amazon Review task.
You might want to change <code>nhead</code> to 1 if you do use this value.
With a vocabulary of 6,000 words this results in a model with around 54,000 parameters.
It takes about 10 minutes to train on an Intel i7 CPU with 1.80 GHz processing power and 8GB of RAM.
Otherwise the default is an embedding dimension of 32 and 4 heads, which results in about 250,000 parameters.
This takes about 1 hour to train.
Results between the smaller and bigger models were almost identical, except the bigger model converged slightly faster.</p>

<p>Initialization:</p>

<figure><pre><code data-lang="julia"><span>using</span> <span>DataFrames</span>
<span>using</span> <span>Arrow</span>
<span>using</span> <span>Printf</span>
<span>using</span> <span>BSON</span><span>,</span> <span>JSON</span>
<span>using</span> <span>Flux</span>
<span>using</span> <span>Unicode</span>
<span>using</span> <span>Dates</span>

<span>using</span> <span>TokenizersLite</span>
<span>using</span> <span>TransformersLite</span>

<span>include</span><span>(</span><span>&#34;training.jl&#34;</span><span>)</span>

<span>path</span> <span>=</span> <span>&#34;path</span><span>\\</span><span>to</span><span>\\</span><span>amazon_reviews_multi</span><span>\\</span><span>en</span><span>\\</span><span>1.0.0</span><span>\\</span><span>&#34;</span>
<span>filename</span> <span>=</span> <span>&#34;amazon_reviews_multi-train.arrow&#34;</span>

<span>checksum</span> <span>=</span> <span>readdir</span><span>(</span><span>path</span><span>)[</span><span>1</span><span>]</span>
<span>filepath</span> <span>=</span> <span>joinpath</span><span>(</span><span>path</span><span>,</span> <span>checksum</span><span>,</span> <span>filename</span><span>)</span>

<span>df</span> <span>=</span> <span>DataFrame</span><span>(</span><span>Arrow</span><span>.</span><span>Table</span><span>(</span><span>filepath</span><span>))</span>
<span>display</span><span>(</span><span>df</span><span>)</span>

<span>hyperparameters</span> <span>=</span> <span>Dict</span><span>(</span>
    <span>&#34;seed&#34;</span> <span>=&gt;</span> <span>2718</span><span>,</span>
    <span>&#34;tokenizer&#34;</span> <span>=&gt;</span> <span>&#34;none&#34;</span><span>,</span>
    <span>&#34;nlabels&#34;</span> <span>=&gt;</span> <span>5</span><span>,</span>
    <span>&#34;model&#34;</span> <span>=&gt;</span> <span>&#34;TransformerClassifier&#34;</span><span>,</span>
    <span>&#34;pdrop&#34;</span> <span>=&gt;</span> <span>0.1</span><span>,</span>
    <span>&#34;dim_embedding&#34;</span> <span>=&gt;</span> <span>32</span>
<span>)</span>
<span>nlabels</span> <span>=</span> <span>hyperparameters</span><span>[</span><span>&#34;nlabels&#34;</span><span>]</span></code></pre></figure>

<p>Tokenizers:</p>

<figure><pre><code data-lang="julia"><span>if</span> <span>hyperparameters</span><span>[</span><span>&#34;tokenizer&#34;</span><span>]</span> <span>==</span> <span>&#34;bpe&#34;</span>
    <span>directory</span> <span>=</span> <span>&#34;vocab</span><span>\\</span><span>bpe&#34;</span>
    <span>path_rules</span> <span>=</span> <span>joinpath</span><span>(</span><span>directory</span><span>,</span> <span>&#34;amazon_reviews_train_en_rules.txt&#34;</span><span>)</span>
    <span>path_vocab</span> <span>=</span> <span>joinpath</span><span>(</span><span>directory</span><span>,</span> <span>&#34;amazon_reviews_train_en_vocab.txt&#34;</span><span>)</span>
    <span>tokenizer</span> <span>=</span> <span>load_bpe</span><span>(</span><span>path_rules</span><span>,</span> <span>startsym</span><span>=</span><span>&#34;⋅&#34;</span><span>)</span>
<span>elseif</span> <span>hyperparameters</span><span>[</span><span>&#34;tokenizer&#34;</span><span>]</span> <span>==</span> <span>&#34;affixes&#34;</span>
    <span>directory</span> <span>=</span> <span>&#34;vocab</span><span>\\</span><span>affixes&#34;</span>
    <span>path_vocab</span> <span>=</span> <span>joinpath</span><span>(</span><span>directory</span><span>,</span> <span>&#34;amazon_reviews_train_en_vocab.txt&#34;</span><span>)</span>
    <span>tokenizer</span> <span>=</span> <span>load_affix_tokenizer</span><span>(</span><span>path_vocab</span><span>)</span>
<span>elseif</span> <span>hyperparameters</span><span>[</span><span>&#34;tokenizer&#34;</span><span>]</span> <span>==</span> <span>&#34;none&#34;</span>
    <span>path_vocab</span> <span>=</span> <span>joinpath</span><span>(</span><span>&#34;vocab&#34;</span><span>,</span> <span>&#34;amazon_reviews_train_en.txt&#34;</span><span>)</span>
    <span>tokenizer</span> <span>=</span> <span>identity</span>
<span>end</span>
<span>vocab</span> <span>=</span> <span>load_vocab</span><span>(</span><span>path_vocab</span><span>)</span>
<span>indexer</span> <span>=</span> <span>IndexTokenizer</span><span>(</span><span>vocab</span><span>,</span> <span>&#34;[UNK]&#34;</span><span>)</span>
<span>display</span><span>(</span><span>tokenizer</span><span>)</span>
<span>display</span><span>(</span><span>indexer</span><span>)</span></code></pre></figure>

<p>Tokens pipeline:</p>

<figure><pre><code data-lang="julia"><span>function</span><span> clean</span><span>(</span><span>s</span><span>::</span><span>AbstractString</span><span>)</span>
    <span>s</span> <span>=</span> <span>lowercase</span><span>(</span><span>s</span><span>)</span>
    <span>s</span> <span>=</span> <span>Unicode</span><span>.</span><span>normalize</span><span>(</span><span>s</span><span>,</span> <span>:</span><span>NFD</span><span>)</span>
    <span>s</span> <span>=</span> <span>replace</span><span>(</span><span>s</span><span>,</span> <span>r</span><span>&#34;[&#39;`’</span><span>\u200d</span><span>\p{M}]&#34;</span> <span>=&gt;</span> <span>&#34;&#34;</span><span>)</span> <span># contractions, zero width joiner and marks from normalization</span>
    <span>s</span> <span>=</span> <span>replace</span><span>(</span><span>s</span><span>,</span> <span>r</span><span>&#34;</span><span>\n</span><span>&#34;</span> <span>=&gt;</span> <span>&#34; &#34;</span><span>)</span>
<span>end</span>

<span>function</span><span> preprocess</span><span>(</span><span>document</span><span>,</span> <span>tokenizer</span><span>;</span> <span>pattern</span> <span>=</span> <span>r</span><span>&#34;[A-Za-z][A-Za-z]+</span><span>\b</span><span>&#34;</span><span>,</span> <span>max_length</span><span>::</span><span>Union</span><span>{</span><span>Nothing</span><span>,</span> <span>Int</span><span>}</span><span>=</span><span>nothing</span><span>)</span>
    <span>document</span> <span>=</span> <span>clean</span><span>(</span><span>document</span><span>)</span>
    <span>words</span> <span>=</span> <span>map</span><span>(</span><span>m</span><span>-&gt;</span><span>string</span><span>(</span><span>m</span><span>.</span><span>match</span><span>),</span> <span>eachmatch</span><span>(</span><span>pattern</span><span>,</span> <span>document</span><span>))</span>
    <span>tokens</span> <span>=</span> <span>tokenizer</span><span>(</span><span>words</span><span>)</span>
    <span>if</span> <span>!</span><span>isnothing</span><span>(</span><span>max_length</span><span>)</span>
        <span>if</span> <span>length</span><span>(</span><span>tokens</span><span>)</span> <span>&gt;</span> <span>max_length</span>
            <span>tokens</span> <span>=</span> <span>tokens</span><span>[</span><span>1</span><span>:</span><span>max_length</span><span>]</span>
        <span>end</span>
    <span>end</span>
    <span>tokens</span>
<span>end</span>

<span>documents</span> <span>=</span> <span>df</span><span>[</span><span>!</span><span>,</span> <span>:</span><span>review_body</span><span>]</span>
<span>labels</span> <span>=</span> <span>df</span><span>[</span><span>!</span><span>,</span> <span>:</span><span>stars</span><span>]</span>
<span>max_length</span> <span>=</span> <span>50</span>
<span>@time</span> <span>tokens</span> <span>=</span> <span>map</span><span>(</span><span>d</span><span>-&gt;</span><span>preprocess</span><span>(</span><span>d</span><span>,</span> <span>tokenizer</span><span>,</span> <span>max_length</span><span>=</span><span>max_length</span><span>),</span> <span>documents</span><span>)</span> 
<span>@time</span> <span>indices</span> <span>=</span> <span>indexer</span><span>(</span><span>tokens</span><span>)</span> 

<span>y_train</span> <span>=</span> <span>copy</span><span>(</span><span>labels</span><span>)</span>
<span>if</span> <span>nlabels</span> <span>==</span> <span>1</span>
    <span>y_train</span><span>[</span><span>labels</span> <span>.≤</span> <span>2</span><span>]</span> <span>.=</span> <span>0</span>
    <span>y_train</span><span>[</span><span>labels</span> <span>.≥</span> <span>4</span><span>]</span> <span>.=</span> <span>1</span>
    <span>idxs</span> <span>=</span> <span>labels</span> <span>.!=</span> <span>3</span>
    <span>y_train</span> <span>=</span> <span>reshape</span><span>(</span><span>y_train</span><span>,</span> <span>1</span><span>,</span> <span>:</span><span>)</span>
<span>else</span>
    <span>idxs</span> <span>=</span> <span>Base</span><span>.</span><span>OneTo</span><span>(</span><span>length</span><span>(</span><span>labels</span><span>))</span>
    <span>y_train</span> <span>=</span> <span>Flux</span><span>.</span><span>onehotbatch</span><span>(</span><span>y_train</span><span>,</span> <span>1</span><span>:</span><span>nlabels</span><span>)</span>
<span>end</span>

<span>X_train</span><span>,</span> <span>y_train</span> <span>=</span> <span>indices</span><span>[</span><span>:</span><span>,</span> <span>idxs</span><span>],</span> <span>y_train</span><span>[</span><span>:</span><span>,</span> <span>idxs</span><span>];</span>
<span>train_data</span><span>,</span> <span>val_data</span> <span>=</span> <span>split_validation</span><span>(</span><span>X_train</span><span>,</span> <span>y_train</span><span>;</span> <span>rng</span><span>=</span><span>MersenneTwister</span><span>(</span><span>hyperparameters</span><span>[</span><span>&#34;seed&#34;</span><span>]))</span>

<span>println</span><span>(</span><span>&#34;train samples:      &#34;</span><span>,</span> <span>size</span><span>(</span><span>train_data</span><span>[</span><span>1</span><span>]),</span> <span>&#34; &#34;</span><span>,</span> <span>size</span><span>(</span><span>train_data</span><span>[</span><span>2</span><span>]))</span>
<span>println</span><span>(</span><span>&#34;validation samples: &#34;</span><span>,</span> <span>size</span><span>(</span><span>val_data</span><span>[</span><span>1</span><span>]),</span> <span>&#34; &#34;</span><span>,</span> <span>size</span><span>(</span><span>val_data</span><span>[</span><span>2</span><span>]))</span></code></pre></figure>

<p>Model:</p>

<figure><pre><code data-lang="julia"><span>dim_embedding</span> <span>=</span> <span>hyperparameters</span><span>[</span><span>&#34;dim_embedding&#34;</span><span>]</span>
<span>pdrop</span> <span>=</span> <span>hyperparameters</span><span>[</span><span>&#34;pdrop&#34;</span><span>]</span>
<span>model</span> <span>=</span> <span>TransformersLite</span><span>.</span><span>TransformerClassifier</span><span>(</span>
    <span>Embed</span><span>(</span><span>dim_embedding</span><span>,</span> <span>length</span><span>(</span><span>indexer</span><span>)),</span> 
    <span>PositionEncoding</span><span>(</span><span>dim_embedding</span><span>),</span> 
    <span>Dropout</span><span>(</span><span>pdrop</span><span>),</span>
    <span>TransformerEncoderBlock</span><span>[</span><span>TransformerEncoderBlock</span><span>(</span><span>4</span><span>,</span> <span>dim_embedding</span><span>,</span> <span>dim_embedding</span> <span>*</span> <span>4</span><span>;</span> <span>pdrop</span><span>=</span><span>pdrop</span><span>)],</span>
    <span>Dense</span><span>(</span><span>dim_embedding</span><span>,</span> <span>1</span><span>),</span> 
    <span>FlattenLayer</span><span>(),</span>
    <span>Dense</span><span>(</span><span>max_length</span><span>,</span> <span>nlabels</span><span>)</span>
    <span>)</span>
<span>display</span><span>(</span><span>model</span><span>)</span>
<span>hyperparameters</span><span>[</span><span>&#34;trainable parameters&#34;</span><span>]</span> <span>=</span> <span>sum</span><span>(</span><span>length</span><span>,</span> <span>Flux</span><span>.</span><span>params</span><span>(</span><span>model</span><span>));</span></code></pre></figure>

<p>Training:</p>

<figure><pre><code data-lang="julia"><span>if</span> <span>nlabels</span> <span>==</span> <span>1</span>
    <span>loss</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>)</span> <span>=</span> <span>Flux</span><span>.</span><span>logitbinarycrossentropy</span><span>(</span><span>model</span><span>(</span><span>x</span><span>),</span> <span>y</span><span>)</span>
<span>else</span>
    <span>loss</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>)</span> <span>=</span> <span>Flux</span><span>.</span><span>logitcrossentropy</span><span>(</span><span>model</span><span>(</span><span>x</span><span>),</span> <span>y</span><span>)</span>
<span>end</span>
<span>loss</span><span>(</span><span>x</span><span>::</span><span>Tuple</span><span>)</span> <span>=</span> <span>loss</span><span>(</span><span>x</span><span>[</span><span>1</span><span>],</span> <span>x</span><span>[</span><span>2</span><span>])</span>

<span>opt</span> <span>=</span> <span>ADAM</span><span>()</span>

<span>val_acc</span> <span>=</span> <span>batched_metric</span><span>(</span><span>accuracy</span><span>,</span> <span>val_data</span><span>[</span><span>1</span><span>],</span> <span>val_data</span><span>[</span><span>2</span><span>];</span> <span>g</span><span>=</span><span>model</span><span>)</span>
<span>val_loss</span> <span>=</span> <span>batched_metric</span><span>(</span><span>loss</span><span>,</span> <span>val_data</span><span>[</span><span>1</span><span>],</span> <span>val_data</span><span>[</span><span>2</span><span>])</span>

<span>@printf</span> <span>&#34;val_acc=%.4f ; &#34;</span> <span>val_acc</span> <span>*</span> <span>100</span>
<span>@printf</span> <span>&#34;val_loss=%.4f </span><span>\n</span><span>&#34;</span> <span>val_loss</span>

<span>start_time</span> <span>=</span> <span>time_ns</span><span>()</span>
<span>history</span> <span>=</span> <span>train!</span><span>(</span><span>loss</span><span>,</span> <span>Flux</span><span>.</span><span>params</span><span>(</span><span>model</span><span>),</span> <span>train_data</span><span>,</span> <span>opt</span><span>,</span> <span>val_data</span><span>;</span> <span>n_epochs</span><span>=</span><span>10</span><span>,</span> <span>batch_size</span><span>=</span><span>128</span><span>)</span>
<span>end_time</span> <span>=</span> <span>time_ns</span><span>()</span> <span>-</span> <span>start_time</span>
<span>println</span><span>(</span><span>&#34;done training&#34;</span><span>)</span>
<span>@printf</span> <span>&#34;time taken: %.2fs</span><span>\n</span><span>&#34;</span> <span>end_time</span><span>/</span><span>1e9</span></code></pre></figure>

<p>Save:</p>

<figure><pre><code data-lang="julia"><span>directory</span> <span>=</span> <span>&#34;outputs</span><span>\\</span><span>&#34;</span> <span>*</span> <span>Dates</span><span>.</span><span>format</span><span>(</span><span>now</span><span>(),</span> <span>&#34;yyyymmdd_HHMM&#34;</span><span>)</span>
<span>mkdir</span><span>(</span><span>directory</span><span>)</span>

<span>output_path</span> <span>=</span> <span>joinpath</span><span>(</span><span>directory</span><span>,</span> <span>&#34;model.bson&#34;</span><span>)</span>
<span>history_path</span> <span>=</span> <span>joinpath</span><span>(</span><span>directory</span><span>,</span> <span>&#34;history.json&#34;</span><span>)</span>
<span>hyperparameter_path</span> <span>=</span> <span>joinpath</span><span>(</span><span>directory</span><span>,</span> <span>&#34;hyperparameters.json&#34;</span><span>)</span>

<span>BSON</span><span>.</span><span>@save</span> <span>output_path</span> <span>model</span>

<span>open</span><span>(</span><span>history_path</span><span>,</span><span>&#34;w&#34;</span><span>)</span> <span>do</span> <span>f</span>
  <span>JSON</span><span>.</span><span>print</span><span>(</span><span>f</span><span>,</span> <span>history</span><span>)</span>
<span>end</span>

<span>open</span><span>(</span><span>hyperparameter_path</span><span>,</span> <span>&#34;w&#34;</span><span>)</span> <span>do</span> <span>f</span>
    <span>JSON</span><span>.</span><span>print</span><span>(</span><span>f</span><span>,</span> <span>hyperparameters</span><span>)</span>
<span>end</span></code></pre></figure>

<h3 id="evaluation">Evaluation</h3>

<p>The accuracy achieved was 87.4% for the binary task and 49.9% for the 5 star classification task.
This is up from a baseline of 50% for the binary task and 20% for the 5 star classification task.</p>

<p>The confusion matrix shows that the binary model does indeed mostly predict the correct class:</p>
<figure>
<img src="https://liorsinai.github.io/assets/posts/transformers/confusion_matrix_regression.png" alt="confusion matrix"/>
<figcaption></figcaption>
</figure>

<p>A useful cross-section of the confusion matrix is the probabilities per each ground truth class:</p>
<figure>
<img src="https://liorsinai.github.io/assets/posts/transformers/probabilities_ground_truth.png" alt="bar chart probabilities vs ground truth"/>
<figcaption></figcaption>
</figure>
<p>These distributions are mostly skewed in the correct way, with 1 star ratings being mostly negative and 5 star ratings mostly positive. The model was not trained on 3 star reviews so here the distribution is almost uniform (random) with a slight negative skew. However this may also be a reflection of the underlying data with humans not being consistent with their ratings for 3 stars.</p>

<p>Changing focus to the 5 star case:</p>
<figure>
<img src="https://liorsinai.github.io/assets/posts/transformers/confusion_matrix_classification5.png" alt="confusion matrix"/>
<figcaption></figcaption>
</figure>
<p>Looking at the confusion matrix we can see that the model struggles with the middle ratings of 2-4 but was mostly accurate with the extreme ratings of 1 and 5. Again this is hypothesized to be partially because of the underlying data.</p>

<figure>
<img src="https://liorsinai.github.io/assets/posts/transformers/predictions_classification5.png" alt="bar chart predication vs ground truth"/>
<figcaption></figcaption>
</figure>
<p>Seeing in another view as a bar chart, for each star the most likely prediction is the star itself.
However the distributions do have a spread and leave significant overlap for confusion.</p>

<p>Although these results are not 100% perfect, it is a big achievement to have a model that can automatically attach sentiment to text.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Thank you for following through this very long post.
I hope this has giving insight into transformers and how they work.</p>

<hr/>



        <hr/>
        
      </div></div>
  </body>
</html>
