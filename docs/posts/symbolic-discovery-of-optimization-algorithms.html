<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2302.06675">Original</a>
    <h1>Symbolic Discovery of Optimization Algorithms</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen%2C+X">Xiangning Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liang%2C+C">Chen Liang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Huang%2C+D">Da Huang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Real%2C+E">Esteban Real</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+K">Kaiyuan Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yao Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pham%2C+H">Hieu Pham</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dong%2C+X">Xuanyi Dong</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Luong%2C+T">Thang Luong</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hsieh%2C+C">Cho-Jui Hsieh</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lu%2C+Y">Yifeng Lu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Le%2C+Q+V">Quoc V. Le</a></p></div>
      
    
  
    <p><a href="https://arxiv.org/pdf/2302.06675">Download PDF</a></p><blockquote>
      <span>Abstract:</span>  We present a method to formulate algorithm discovery as program search, and
apply it to discover optimization algorithms for deep neural network training.
We leverage efficient search techniques to explore an infinite and sparse
program space. To bridge the large generalization gap between proxy and target
tasks, we also introduce program selection and simplification strategies. Our
method discovers a simple and effective optimization algorithm, $\textbf{Lion}$
($\textit{Evo$\textbf{L}$ved S$\textbf{i}$gn M$\textbf{o}$me$\textbf{n}$tum}$).
It is more memory-efficient than Adam as it only keeps track of the momentum.
Different from adaptive optimizers, its update has the same magnitude for each
parameter calculated through the sign operation. We compare Lion with widely
used optimizers, such as Adam and Adafactor, for training a variety of models
on different tasks. On image classification, Lion boosts the accuracy of ViT by
up to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On
vision-language contrastive learning, we achieve 88.3% $\textit{zero-shot}$ and
91.1% $\textit{fine-tuning}$ accuracy on ImageNet, surpassing the previous best
results by 2% and 0.1%, respectively. On diffusion models, Lion outperforms
Adam by achieving a better FID score and reducing the training compute by up to
2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion
exhibits a similar or better performance compared to Adam. Our analysis of Lion
reveals that its performance gain grows with the training batch size. It also
requires a smaller learning rate than Adam due to the larger norm of the update
produced by the sign function. Additionally, we examine the limitations of Lion
and identify scenarios where its improvements are small or not statistically
significant. The implementation of Lion is publicly available.

    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Xiangning Chen [<a href="https://arxiv.org/show-email/4a010d55/2302.06675">view email</a>]
      </p></div></div>
  </body>
</html>
