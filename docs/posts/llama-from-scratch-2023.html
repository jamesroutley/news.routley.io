<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.briankitano.com/llama-from-scratch/">Original</a>
    <h1>Llama from scratch (2023)</h1>
    
    <div id="readability-page-1" class="page"><div>
    

    
        
    

    
        

        <p>
            <i>
                <time datetime="2023-08-09T06:34Z">
                    09 Aug, 2023
                </time>
            </i>
        </p>
    

    <p>I want to provide some tips from my experience implementing a paper. I&#39;m going to cover my tips so far from implementing a dramatically scaled-down version of <a href="https://arxiv.org/pdf/2302.13971.pdf">Llama</a> for training <a href="https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt">TinyShakespeare</a>. This post is heavily inspired by Karpathy&#39;s <a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">Makemore series</a>, which I highly recommend.</p>
<p>I&#39;m only going to <em>loosely</em> follow the layout of their paper; while the formatting and order of sections makes sense for publication, we&#39;re going to be implementing the paper. I&#39;ll also be skipping over some of the more obvious steps, like setting up a virtual environment and installing dependencies.</p>
<p>A preview of what we&#39;re going to end up with:</p>
<div><pre><span></span><span>print</span><span>(</span><span>generate</span><span>(</span><span>llama</span><span>,</span> <span>MASTER_CONFIG</span><span>,</span> <span>500</span><span>)[</span><span>0</span><span>])</span>

<span>ZELBETH</span><span>:</span>
<span>Sey</span> <span>solmenter</span><span>!</span> <span>&#39;tis tonguerered if berryishdd, and What his stabe, you, and, but all I pilJefals, mode with,</span>
<span>Vurint</span> <span>as</span> <span>steolated</span> <span>have</span> <span>loven</span> <span>OlD</span> <span>the</span> <span>queen</span><span>&#39;d refore</span>
<span>Are</span> <span>been</span><span>,</span> <span>good</span> <span>plmp</span><span>:</span>

<span>Proforne</span><span>,</span> <span>wift</span><span>&#39;es swleen, was no bunderes&#39;</span><span>d</span> <span>a</span> <span>a</span> <span>quain</span> <span>beath</span><span>!</span>
<span>Tybell</span> <span>is</span> <span>my</span> <span>gateer</span> <span>stalk</span> <span>smen</span><span>&#39;d as be matious dazest brink thou</span>
<span>lord</span>
<span>Enves</span> <span>were</span> <span>cIUll</span><span>,</span> <span>afe</span> <span>and</span> <span>whwas</span> <span>seath</span> <span>This</span> <span>a</span> <span>is</span><span>,</span> <span>an</span> <span>tale</span> <span>hoice</span> <span>his</span> <span>his</span> <span>onety</span> <span>Meall</span><span>-</span><span>tearn</span> <span>not</span> <span>murkawn</span><span>,</span> <span>fase</span> <span>bettizen</span><span>&#39;d her,</span>
<span>To</span> <span>belacquesterer</span><span>?</span> <span>baxewed</span> <span>wupl</span> <span>usweggs</span> <span>yet</span> <span>tall</span>
<span>An</span>
</pre></div>
<h2 id="always-work-iteratively-start-small-stay-certain-and-build-up">Always work iteratively: start small, stay certain, and build up.</h2><p>My approach for implementing papers is:</p>
<ol>
<li>Make all of the helper functions required to test your model quantitatively (data splits, training, plotting the loss).</li>
<li>Before you even look at the paper, pick a small, simple, and fast model that you&#39;ve done in the past. Then make a helper function to evaluate the model qualitatively.</li>
<li>Start by picking apart different components of the paper, and then implementing them one-by-one, training and evaluating as you go.</li>
</ol>
<h2 id="make-sure-your-layers-do-what-you-think">Make sure your layers do what you think.</h2><ol>
<li>Use <code>.shape</code> religiously. <code>assert</code> and <code>plt.imshow</code> are also your friends.</li>
<li>Work out the results without matrix multiplication first, and then use the <code>torch</code> functions to make it efficient after.</li>
<li>Have a test to see that your layer is right. For example, the RoPE embeddings have a specific property that you can test for. For the Transformer, you can test that the attention is working by looking at the attention map.</li>
<li>Test your layers on various batch, sequence, and embedding sizes. Even if it works for one size, it might not work for others, which will cause problems at inference time.</li>
</ol>
<h2 id="about-llama">About Llama</h2><p>Llama is a transformer-based model for language modeling. Meta AI <a href="https://github.com/facebookresearch/llama">open-sourced</a> Llama this summer, and it&#39;s gained a lot of attention (pun intended). When you&#39;re reading the introduction, they clearly indicate their goal: make a model that&#39;s cheaper for running inference, rather than optimizing training costs.</p>
<p>At this point, we&#39;ll just load our libraries and get started.</p>
<div><pre><span></span><span>import</span><span> </span><span>torch</span>
<span>from</span><span> </span><span>torch</span><span> </span><span>import</span> <span>nn</span>
<span>from</span><span> </span><span>torch.nn</span><span> </span><span>import</span> <span>functional</span> <span>as</span> <span>F</span>
<span>import</span><span> </span><span>numpy</span><span> </span><span>as</span><span> </span><span>np</span>
<span>from</span><span> </span><span>matplotlib</span><span> </span><span>import</span> <span>pyplot</span> <span>as</span> <span>plt</span>
<span>import</span><span> </span><span>time</span>
<span>import</span><span> </span><span>pandas</span><span> </span><span>as</span><span> </span><span>pd</span>
</pre></div>
<h2 id="setting-up-our-dataset">Setting up our dataset</h2><p>While in Llama they train on 1.4T tokens, our dataset TinyShakespeare, the collection of all of Shakespeare&#39;s works, is about 1M characters.</p>
<div><pre><span></span><span>lines</span> <span>=</span> <span>open</span><span>(</span><span>&#39;./input.txt&#39;</span><span>,</span> <span>&#39;r&#39;</span><span>)</span><span>.</span><span>read</span><span>()</span>

<span>vocab</span> <span>=</span> <span>sorted</span><span>(</span><span>list</span><span>(</span><span>set</span><span>(</span><span>lines</span><span>)))</span>
<span>itos</span> <span>=</span> <span>{</span><span>i</span><span>:</span><span>ch</span> <span>for</span> <span>i</span><span>,</span> <span>ch</span> <span>in</span> <span>enumerate</span><span>(</span><span>vocab</span><span>)}</span>
<span>stoi</span> <span>=</span> <span>{</span><span>ch</span><span>:</span><span>i</span> <span>for</span> <span>i</span><span>,</span> <span>ch</span> <span>in</span> <span>enumerate</span><span>(</span><span>vocab</span><span>)}</span>

<span>print</span><span>(</span><span>lines</span><span>[:</span><span>30</span><span>])</span>
</pre></div>
<div><pre><span></span>First Citizen:
Before we proce
</pre></div>
<p>They use the <a href="https://github.com/google/sentencepiece">SentencePiece</a> byte-pair encoding tokenizer, but we&#39;re going to just use a simple character-level tokenizer.</p>
<div><pre><span></span><span># simple tokenization by characters</span>
<span>def</span><span> </span><span>encode</span><span>(</span><span>s</span><span>):</span>
    <span>return</span> <span>[</span><span>stoi</span><span>[</span><span>ch</span><span>]</span> <span>for</span> <span>ch</span> <span>in</span> <span>s</span><span>]</span>

<span>def</span><span> </span><span>decode</span><span>(</span><span>l</span><span>):</span>
    <span>return</span> <span>&#39;&#39;</span><span>.</span><span>join</span><span>([</span><span>itos</span><span>[</span><span>i</span><span>]</span> <span>for</span> <span>i</span> <span>in</span> <span>l</span><span>])</span>

<span>print</span><span>(</span><span>&#39;vocab size:&#39;</span><span>,</span> <span>len</span><span>(</span><span>vocab</span><span>))</span>
<span>decode</span><span>(</span><span>encode</span><span>(</span><span>&#34;hello&#34;</span><span>))</span>
</pre></div>

<p>Since our dataset is small enough, we don&#39;t need to worry about how we store it in memory etc.</p>
<p>First tip: I&#39;m creating a <code>config</code> object that stores some basic model params. It makes our code way more readable and removes constants and magic numbers from the code. I&#39;m not going to use types, as  I want to keep things flexible for now and be able to add more parameters later on.</p>
<div><pre><span></span><span>MASTER_CONFIG</span> <span>=</span> <span>{</span>
    <span>&#34;vocab_size&#34;</span><span>:</span> <span>len</span><span>(</span><span>vocab</span><span>),</span>
<span>}</span>
</pre></div>
<div><pre><span></span><span>dataset</span> <span>=</span> <span>torch</span><span>.</span><span>tensor</span><span>(</span><span>encode</span><span>(</span><span>lines</span><span>),</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>int8</span><span>)</span>
<span>dataset</span><span>.</span><span>shape</span>
</pre></div>

<p>Let&#39;s create a method to generate our training data and labels for batches. We&#39;ll use the same method for validation and test data. Note that I like to test my functions in the same block that I define them, just to make sure they work as expected before moving on.</p>
<div><pre><span></span><span>def</span><span> </span><span>get_batches</span><span>(</span><span>data</span><span>,</span> <span>split</span><span>,</span> <span>batch_size</span><span>,</span> <span>context_window</span><span>,</span> <span>config</span><span>=</span><span>MASTER_CONFIG</span><span>):</span>
    <span>train</span> <span>=</span> <span>data</span><span>[:</span><span>int</span><span>(</span><span>.8</span> <span>*</span> <span>len</span><span>(</span><span>data</span><span>))]</span>
    <span>val</span> <span>=</span> <span>data</span><span>[</span><span>int</span><span>(</span><span>.8</span> <span>*</span> <span>len</span><span>(</span><span>data</span><span>)):</span> <span>int</span><span>(</span><span>.9</span> <span>*</span> <span>len</span><span>(</span><span>data</span><span>))]</span>
    <span>test</span> <span>=</span> <span>data</span><span>[</span><span>int</span><span>(</span><span>.9</span> <span>*</span> <span>len</span><span>(</span><span>data</span><span>)):]</span>
    
    <span>batch_data</span> <span>=</span> <span>train</span>
    <span>if</span> <span>split</span> <span>==</span> <span>&#39;val&#39;</span><span>:</span>
        <span>batch_data</span> <span>=</span> <span>val</span>

    <span>if</span> <span>split</span> <span>==</span> <span>&#39;test&#39;</span><span>:</span>
        <span>batch_data</span> <span>=</span> <span>test</span>
    
    <span># pick random starting points</span>
    <span>ix</span> <span>=</span> <span>torch</span><span>.</span><span>randint</span><span>(</span><span>0</span><span>,</span> <span>batch_data</span><span>.</span><span>size</span><span>(</span><span>0</span><span>)</span> <span>-</span> <span>context_window</span> <span>-</span> <span>1</span><span>,</span> <span>(</span><span>batch_size</span><span>,))</span>
    <span>x</span> <span>=</span> <span>torch</span><span>.</span><span>stack</span><span>([</span><span>batch_data</span><span>[</span><span>i</span><span>:</span><span>i</span><span>+</span><span>context_window</span><span>]</span> <span>for</span> <span>i</span> <span>in</span> <span>ix</span><span>])</span><span>.</span><span>long</span><span>()</span>
    <span>y</span> <span>=</span> <span>torch</span><span>.</span><span>stack</span><span>([</span><span>batch_data</span><span>[</span><span>i</span><span>+</span><span>1</span><span>:</span><span>i</span><span>+</span><span>context_window</span><span>+</span><span>1</span><span>]</span> <span>for</span> <span>i</span> <span>in</span> <span>ix</span><span>])</span><span>.</span><span>long</span><span>()</span>
    <span>return</span> <span>x</span><span>,</span> <span>y</span>

<span>MASTER_CONFIG</span><span>.</span><span>update</span><span>({</span>
    <span>&#39;batch_size&#39;</span><span>:</span> <span>8</span><span>,</span>
    <span>&#39;context_window&#39;</span><span>:</span> <span>16</span>
<span>})</span>

<span>xs</span><span>,</span> <span>ys</span> <span>=</span> <span>get_batches</span><span>(</span><span>dataset</span><span>,</span> <span>&#39;train&#39;</span><span>,</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;context_window&#39;</span><span>])</span>

<span>[(</span><span>decode</span><span>(</span><span>xs</span><span>[</span><span>i</span><span>]</span><span>.</span><span>tolist</span><span>()),</span> <span>decode</span><span>(</span><span>ys</span><span>[</span><span>i</span><span>]</span><span>.</span><span>tolist</span><span>()))</span> <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>len</span><span>(</span><span>xs</span><span>))]</span>
</pre></div>
<div><pre><span></span>[(&#39;, or banishment,&#39;, &#39; or banishment, &#39;),
 (&#39;do what hands do&#39;, &#39;o what hands do;&#39;),
 (&#34;? If thou&#39;lt see&#34;, &#34; If thou&#39;lt see &#34;),
 (&#39;and could put br&#39;, &#39;nd could put bre&#39;),
 (&#34;hath deliver&#39;d.\n&#34;, &#34;ath deliver&#39;d.\nI&#34;),
 (&#39;ing by: whereof &#39;, &#39;ng by: whereof I&#39;),
 (&#39; blows! Despisin&#39;, &#39;blows! Despising&#39;),
 (&#39;ng of blood, who&#39;, &#39;g of blood, whos&#39;)]
</pre></div>
<p>What&#39;s interesting about implementing papers is that there are two aspects to the model <em>working</em>: compilation (do your tensors all match up from layer to layer), and training (does the loss go down). Figuring out how to ensure that each of your compoenents is working is key to developing your model in a predictable, engineering-minded way.</p>
<p>That&#39;s why we&#39;re also going to define the method for how we&#39;re going to evaluate the model. We want to do this before we even define the model, because we want to be able to use it to evaluate the model as we&#39;re training it.</p>
<div><pre><span></span><span>@torch</span><span>.</span><span>no_grad</span><span>()</span>  <span># don&#39;t compute gradients for this function</span>
<span>def</span><span> </span><span>evaluate_loss</span><span>(</span><span>model</span><span>,</span> <span>config</span><span>=</span><span>MASTER_CONFIG</span><span>):</span>
    <span>out</span> <span>=</span> <span>{}</span>
    <span>model</span><span>.</span><span>eval</span><span>()</span>
    <span>for</span> <span>split</span> <span>in</span> <span>[</span><span>&#34;train&#34;</span><span>,</span> <span>&#34;val&#34;</span><span>]:</span>
        <span>losses</span> <span>=</span> <span>[]</span>
        <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>10</span><span>):</span>
            <span>xb</span><span>,</span> <span>yb</span> <span>=</span> <span>get_batches</span><span>(</span><span>dataset</span><span>,</span> <span>split</span><span>,</span> <span>config</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>])</span>
            <span>_</span><span>,</span> <span>loss</span> <span>=</span> <span>model</span><span>(</span><span>xb</span><span>,</span> <span>yb</span><span>)</span>
            <span>losses</span><span>.</span><span>append</span><span>(</span><span>loss</span><span>.</span><span>item</span><span>())</span>
        <span>out</span><span>[</span><span>split</span><span>]</span> <span>=</span> <span>np</span><span>.</span><span>mean</span><span>(</span><span>losses</span><span>)</span>
    <span>model</span><span>.</span><span>train</span><span>()</span>
    <span>return</span> <span>out</span>
</pre></div>
<h2 id="setting-up-a-working-base-model">Setting up a working base model</h2><p>Here&#39;s a basic feed-forward neural network with embeddings. It&#39;s the base model we&#39;re going to start with, and then swap out parts of it as we go along until we eventually end up with the model as described in Llama.</p>
<div><pre><span></span><span>class</span><span> </span><span>SimpleBrokenModel</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span><span> </span><span>__init__</span><span>(</span><span>self</span><span>,</span> <span>config</span><span>=</span><span>MASTER_CONFIG</span><span>):</span>
        <span>super</span><span>()</span><span>.</span><span>__init__</span><span>()</span>
        <span>self</span><span>.</span><span>config</span> <span>=</span> <span>config</span>

        <span>self</span><span>.</span><span>embedding</span> <span>=</span> <span>nn</span><span>.</span><span>Embedding</span><span>(</span><span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>])</span>
        <span>self</span><span>.</span><span>linear</span> <span>=</span> <span>nn</span><span>.</span><span>Sequential</span><span>(</span>
            <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]),</span>
            <span>nn</span><span>.</span><span>ReLU</span><span>(),</span>
            <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>]),</span>
        <span>)</span>

        <span>print</span><span>(</span><span>&#34;model params:&#34;</span><span>,</span> <span>sum</span><span>([</span><span>m</span><span>.</span><span>numel</span><span>()</span> <span>for</span> <span>m</span> <span>in</span> <span>self</span><span>.</span><span>parameters</span><span>()]))</span>

    <span>def</span><span> </span><span>forward</span><span>(</span><span>self</span><span>,</span> <span>idx</span><span>,</span> <span>targets</span><span>=</span><span>None</span><span>):</span>
        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>embedding</span><span>(</span><span>idx</span><span>)</span>
        <span>a</span> <span>=</span> <span>self</span><span>.</span><span>linear</span><span>(</span><span>x</span><span>)</span>
        <span>logits</span> <span>=</span> <span>F</span><span>.</span><span>softmax</span><span>(</span><span>a</span><span>,</span> <span>dim</span><span>=-</span><span>1</span><span>)</span>

        <span>if</span> <span>targets</span> <span>is</span> <span>not</span> <span>None</span><span>:</span>
            <span>loss</span> <span>=</span> <span>F</span><span>.</span><span>cross_entropy</span><span>(</span><span>logits</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>,</span> <span>self</span><span>.</span><span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>]),</span> <span>targets</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>))</span>
            <span>return</span> <span>logits</span><span>,</span> <span>loss</span>

        <span>else</span><span>:</span>
            <span>return</span> <span>logits</span>

<span>MASTER_CONFIG</span><span>.</span><span>update</span><span>({</span>
    <span>&#39;d_model&#39;</span><span>:</span> <span>128</span><span>,</span>
<span>})</span>
<span>model</span> <span>=</span> <span>SimpleBrokenModel</span><span>(</span><span>MASTER_CONFIG</span><span>)</span>
<span>xs</span><span>,</span> <span>ys</span> <span>=</span> <span>get_batches</span><span>(</span><span>dataset</span><span>,</span> <span>&#39;train&#39;</span><span>,</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;context_window&#39;</span><span>])</span>

<span>logits</span><span>,</span> <span>loss</span> <span>=</span> <span>model</span><span>(</span><span>xs</span><span>,</span> <span>ys</span><span>)</span>
</pre></div>

<p>It&#39;s at this point that we have to start worrying about the shape of our tensors and making indices match. Check out this line of our model definition:</p>
<div><pre><span></span><span>loss</span> <span>=</span> <span>F</span><span>.</span><span>cross_entropy</span><span>(</span><span>logits</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>,</span> <span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>]),</span> <span>targets</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>))</span>
</pre></div>
<p>We have to reshape the <code>logits</code> and <code>targets</code> tensors so that their dimensions match when we compare. We do this with the <code>view</code> method. The <code>-1</code> argument means &#34;infer this dimension from the others&#34;. So, in this case, we&#39;re saying &#34;reshape <code>logits</code> and <code>targets</code> to have the same number of rows, and however many columns are needed to make that happen&#34;. This is a common pattern when you&#39;re working with batches of data.</p>
<p>Alright, let&#39;s train our <code>SimpleBrokenModel</code> to make sure gradients flow. After we confirm that, we can swap out parts of it to match Llama, train again, and track our progress. It&#39;s at this point that I start keeping a <em>log</em> of my training runs, so that I can easily just go back to a previous run in the event that I mess something up.</p>
<div><pre><span></span><span>MASTER_CONFIG</span><span>.</span><span>update</span><span>({</span>
    <span>&#39;epochs&#39;</span><span>:</span> <span>1000</span><span>,</span>
    <span>&#39;log_interval&#39;</span><span>:</span> <span>10</span><span>,</span>
    <span>&#39;batch_size&#39;</span><span>:</span> <span>32</span><span>,</span>
<span>})</span>
<span>model</span> <span>=</span> <span>SimpleBrokenModel</span><span>(</span><span>MASTER_CONFIG</span><span>)</span>

<span>optimizer</span> <span>=</span> <span>torch</span><span>.</span><span>optim</span><span>.</span><span>Adam</span><span>(</span>
    <span>model</span><span>.</span><span>parameters</span><span>(),</span> 
<span>)</span>

<span>def</span><span> </span><span>train</span><span>(</span><span>model</span><span>,</span> <span>optimizer</span><span>,</span> <span>scheduler</span><span>=</span><span>None</span><span>,</span> <span>config</span><span>=</span><span>MASTER_CONFIG</span><span>,</span> <span>print_logs</span><span>=</span><span>False</span><span>):</span>
    <span>losses</span> <span>=</span> <span>[]</span>
    <span>start_time</span> <span>=</span> <span>time</span><span>.</span><span>time</span><span>()</span>
    <span>for</span> <span>epoch</span> <span>in</span> <span>range</span><span>(</span><span>config</span><span>[</span><span>&#39;epochs&#39;</span><span>]):</span>
        <span>optimizer</span><span>.</span><span>zero_grad</span><span>()</span>
        
        <span>xs</span><span>,</span> <span>ys</span> <span>=</span> <span>get_batches</span><span>(</span><span>dataset</span><span>,</span> <span>&#39;train&#39;</span><span>,</span> <span>config</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>])</span>
        <span>logits</span><span>,</span> <span>loss</span> <span>=</span> <span>model</span><span>(</span><span>xs</span><span>,</span> <span>targets</span><span>=</span><span>ys</span><span>)</span>
        <span>loss</span><span>.</span><span>backward</span><span>()</span>
        <span>optimizer</span><span>.</span><span>step</span><span>()</span>
        
        <span>if</span> <span>scheduler</span><span>:</span>
            <span>scheduler</span><span>.</span><span>step</span><span>()</span>
        
        <span>if</span> <span>epoch</span> <span>%</span> <span>config</span><span>[</span><span>&#39;log_interval&#39;</span><span>]</span> <span>==</span> <span>0</span><span>:</span>
            <span>batch_time</span> <span>=</span> <span>time</span><span>.</span><span>time</span><span>()</span> <span>-</span> <span>start_time</span>
            <span>x</span> <span>=</span> <span>evaluate_loss</span><span>(</span><span>model</span><span>)</span>
            <span>losses</span> <span>+=</span> <span>[</span><span>x</span><span>]</span>
            <span>if</span> <span>print_logs</span><span>:</span>
                <span>print</span><span>(</span><span>f</span><span>&#34;Epoch </span><span>{</span><span>epoch</span><span>}</span><span> | val loss </span><span>{</span><span>x</span><span>[</span><span>&#39;val&#39;</span><span>]</span><span>:</span><span>.3f</span><span>}</span><span> | Time </span><span>{</span><span>batch_time</span><span>:</span><span>.3f</span><span>}</span><span> | ETA in seconds </span><span>{</span><span>batch_time</span><span> </span><span>*</span><span> </span><span>(</span><span>config</span><span>[</span><span>&#39;epochs&#39;</span><span>]</span><span> </span><span>-</span><span> </span><span>epoch</span><span>)</span><span>/</span><span>config</span><span>[</span><span>&#39;log_interval&#39;</span><span>]</span><span> </span><span>:</span><span>.3f</span><span>}</span><span>&#34;</span><span>)</span>
            <span>start_time</span> <span>=</span> <span>time</span><span>.</span><span>time</span><span>()</span>

            <span>if</span> <span>scheduler</span><span>:</span>
                <span>print</span><span>(</span><span>&#34;lr: &#34;</span><span>,</span> <span>scheduler</span><span>.</span><span>get_lr</span><span>())</span>

    <span>print</span><span>(</span><span>&#34;validation loss: &#34;</span><span>,</span> <span>losses</span><span>[</span><span>-</span><span>1</span><span>][</span><span>&#39;val&#39;</span><span>])</span>
    <span>return</span> <span>pd</span><span>.</span><span>DataFrame</span><span>(</span><span>losses</span><span>)</span><span>.</span><span>plot</span><span>()</span>

<span>train</span><span>(</span><span>model</span><span>,</span> <span>optimizer</span><span>)</span>
</pre></div>
<div><pre><span></span>model params: 33217
validation loss:  3.942167806625366





&lt;Axes: &gt;
</pre></div>
<p><img src="https://github.com/bkitano/llama-from-scratch/blob/main/llama_files/llama_23_2.png?raw=true" alt="png"/></p>
<p>Notice how we get a training curve that goes down, but barely by anything. How do we know it&#39;s barely training? We have to use first principles. The cross-entropy loss before training is 4.17, and after 1000 epochs is 3.93. How can we make sense of it intuitively?</p>
<p>Cross-entropy in this context is referring to how likely we are to pick the wrong word. So here,</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>T</mi><mo>,</mo><mi>q</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>N</mi></mrow></msubsup><mfrac><mrow><mn>1</mn></mrow><mrow><mi>N</mi></mrow></mfrac><mi>log</mi><mi>q</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></math><p>where <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>q</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></math> is the probability of picking the right word, as estimated by the model. If <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>q</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></math> is close to 1, then <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>log</mi><mi>q</mi></mrow></math> is close to 0; similarly, if <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>q</mi></mrow></math> is small, then <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>log</mi><mi>q</mi></mrow></math> is a large negative number, so <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo>−</mo><mi>log</mi><mi>q</mi></mrow></math> will be a large positive number. Now to build the intuition: to start, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo>−</mo><mi>log</mi><mi>q</mi><mo>=</mo><mn>4.17</mn></mrow></math>, so <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>q</mi><mo>=</mo><mn>0.015</mn></mrow></math>, or around <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mfrac><mrow><mn>1</mn></mrow><mrow><mn>64.715</mn></mrow></mfrac></mrow></math>. Recall that the vocabulary size <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo stretchy="false">|</mo><mi>V</mi><mo stretchy="false">|</mo><mo>=</mo><mn>65</mn></mrow></math>, so what we&#39;re basically saying here is that the model is as good at choosing the next letter as randomly picking from our vocabulary. After training, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo>−</mo><mi>log</mi><mi>q</mi><mo>=</mo><mn>3.93</mn></mrow></math>, so we&#39;re now basically choosing between 50 letters. This is a very small improvement, so something is probably wrong.</p>
<p>To get an intuition for how the loss relates to the model&#39;s performance, think about the model choosing among <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mover><mi>V</mi><mo stretchy="false">~</mo></mover></mrow></math> tokens; when <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mover><mi>V</mi><mo stretchy="false">~</mo></mover></mrow></math> is small, the model is more likely to guess right. In addition, we know <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo>max</mo><mover><mi>V</mi><mo stretchy="false">~</mo></mover><mo>=</mo><mi>V</mi></mrow></math>, which can help us understand if our model is learning at all.</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mover><mi>V</mi><mo stretchy="false">~</mo></mover><mo>=</mo><mi>exp</mi><mo stretchy="false">(</mo><mi>L</mi><mo stretchy="false">)</mo></mrow></math></p>
<p>Let&#39;s try to debug what&#39;s going on. Notice that in our model we&#39;re using a softmax layer on our logits, which is a function that takes a vector of numbers and squashes them into a probability distribution. But for using the built in <code>F.cross_entropy</code> function, we need to pass in the <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html">unnormalized logits directly</a>. So let&#39;s remove that from our model and try again.</p>
<div><pre><span></span><span>class</span><span> </span><span>SimpleModel</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span><span> </span><span>__init__</span><span>(</span><span>self</span><span>,</span> <span>config</span><span>):</span>
        <span>super</span><span>()</span><span>.</span><span>__init__</span><span>()</span>
        <span>self</span><span>.</span><span>config</span> <span>=</span> <span>config</span>

        <span>self</span><span>.</span><span>embedding</span> <span>=</span> <span>nn</span><span>.</span><span>Embedding</span><span>(</span><span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>])</span>
        <span>self</span><span>.</span><span>linear</span> <span>=</span> <span>nn</span><span>.</span><span>Sequential</span><span>(</span>
            <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]),</span>
            <span>nn</span><span>.</span><span>ReLU</span><span>(),</span>
            <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>]),</span>
        <span>)</span>

        <span>print</span><span>(</span><span>&#34;model params:&#34;</span><span>,</span> <span>sum</span><span>([</span><span>m</span><span>.</span><span>numel</span><span>()</span> <span>for</span> <span>m</span> <span>in</span> <span>self</span><span>.</span><span>parameters</span><span>()]))</span>

    <span>def</span><span> </span><span>forward</span><span>(</span><span>self</span><span>,</span> <span>idx</span><span>,</span> <span>targets</span><span>=</span><span>None</span><span>):</span>
        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>embedding</span><span>(</span><span>idx</span><span>)</span>
        <span>logits</span> <span>=</span> <span>self</span><span>.</span><span>linear</span><span>(</span><span>x</span><span>)</span>

        <span>if</span> <span>targets</span> <span>is</span> <span>not</span> <span>None</span><span>:</span>
            <span>loss</span> <span>=</span> <span>F</span><span>.</span><span>cross_entropy</span><span>(</span><span>logits</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>,</span> <span>self</span><span>.</span><span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>]),</span> <span>targets</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>))</span>
            <span>return</span> <span>logits</span><span>,</span> <span>loss</span>

        <span>else</span><span>:</span>
            <span>return</span> <span>logits</span>

<span>model</span> <span>=</span> <span>SimpleModel</span><span>(</span><span>MASTER_CONFIG</span><span>)</span>
<span>xs</span><span>,</span> <span>ys</span> <span>=</span> <span>get_batches</span><span>(</span><span>dataset</span><span>,</span> <span>&#39;train&#39;</span><span>,</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;context_window&#39;</span><span>])</span>

<span>logits</span><span>,</span> <span>loss</span> <span>=</span> <span>model</span><span>(</span><span>xs</span><span>,</span> <span>ys</span><span>)</span>
<span>optimizer</span> <span>=</span> <span>torch</span><span>.</span><span>optim</span><span>.</span><span>Adam</span><span>(</span><span>model</span><span>.</span><span>parameters</span><span>())</span>
<span>train</span><span>(</span><span>model</span><span>,</span> <span>optimizer</span><span>)</span>
</pre></div>
<div><pre><span></span>model params: 33217
validation loss:  2.5058052778244018





&lt;Axes: &gt;
</pre></div>
<p><img src="https://github.com/bkitano/llama-from-scratch/blob/main/llama_files/llama_25_2.png?raw=true" alt="png"/></p>
<p>Great, now our loss is <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>2.54</mn></mrow></math>, so we&#39;re choosing from <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>12.67</mn></mrow></math> characters. That&#39;s way better than the 65 we started with. Let&#39;s add a generate method to our model so we visually see the results of our model.</p>
<div><pre><span></span><span>def</span><span> </span><span>generate</span><span>(</span><span>model</span><span>,</span> <span>config</span><span>=</span><span>MASTER_CONFIG</span><span>,</span> <span>max_new_tokens</span><span>=</span><span>30</span><span>):</span>
    <span>idx</span> <span>=</span> <span>torch</span><span>.</span><span>zeros</span><span>(</span><span>5</span><span>,</span> <span>1</span><span>)</span><span>.</span><span>long</span><span>()</span>
    <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>max_new_tokens</span><span>):</span>
        <span># call the model</span>
        <span>logits</span> <span>=</span> <span>model</span><span>(</span><span>idx</span><span>[:,</span> <span>-</span><span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>]:])</span>
        <span>last_time_step_logits</span> <span>=</span> <span>logits</span><span>[</span>
            <span>:,</span> <span>-</span><span>1</span><span>,</span> <span>:</span>
        <span>]</span>  <span># all the batches (1), last time step, all the logits</span>
        <span>p</span> <span>=</span> <span>F</span><span>.</span><span>softmax</span><span>(</span><span>last_time_step_logits</span><span>,</span> <span>dim</span><span>=-</span><span>1</span><span>)</span>  <span># softmax to get probabilities</span>
        <span>idx_next</span> <span>=</span> <span>torch</span><span>.</span><span>multinomial</span><span>(</span>
            <span>p</span><span>,</span> <span>num_samples</span><span>=</span><span>1</span>
        <span>)</span>  <span># sample from the distribution to get the next token</span>
        <span>idx</span> <span>=</span> <span>torch</span><span>.</span><span>cat</span><span>([</span><span>idx</span><span>,</span> <span>idx_next</span><span>],</span> <span>dim</span><span>=-</span><span>1</span><span>)</span>  <span># append to the sequence</span>
    <span>return</span> <span>[</span><span>decode</span><span>(</span><span>x</span><span>)</span> <span>for</span> <span>x</span> <span>in</span> <span>idx</span><span>.</span><span>tolist</span><span>()]</span>

<span>generate</span><span>(</span><span>model</span><span>)</span>
</pre></div>
<div><pre><span></span>[&#39;\nFind!\nD:\nAr t,\nLis sthte o t l&#39;,
 &#39;\nAnd ronnot ar\nBE:\nKINRDYOrspr;&#39;,
 &#39;\nI t athe momyengthend thanswal&#39;,
 &#39;\nFis t bp he\nLacarn.\nA:\nYOMI wi&#39;,
 &#39;\nWh ly sck\nB-de pll t\nHERIns ou&#39;]
</pre></div>
<p>It&#39;s not half bad, but also not half good. But now we have a working model that is training to a validation loss. So here we&#39;ll iterate on our model to make it closer to Llama.</p>
<h2 id="llama-specifics">Llama specifics</h2><p>Llama describes three architectural modifications to the original Transformer:</p>
<ol>
<li>RMSNorm for pre-normalization</li>
<li>Rotary embeddings</li>
<li>SwiGLU activation function</li>
</ol>
<p>We&#39;re going to add each one, one at a time to our base model, and iterate.</p>
<h3 id="rmsnorm">RMSNorm</h3><p>In Vaswani 2017, the original transformer uses BatchNormalization. In Llama, the authors use RMSNorm, which is where you scale the bector by the variance without centering it. In addition, while Vaswani applies normalization to the output of the attention layer (post-normalization), Llama applies it to the inputs before (pre-normalization).</p>
<div><pre><span></span><span>class</span><span> </span><span>RMSNorm</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span><span> </span><span>__init__</span><span>(</span><span>self</span><span>,</span> <span>layer_shape</span><span>,</span> <span>eps</span><span>=</span><span>1e-8</span><span>,</span> <span>bias</span><span>=</span><span>False</span><span>):</span>
        <span>super</span><span>(</span><span>RMSNorm</span><span>,</span> <span>self</span><span>)</span><span>.</span><span>__init__</span><span>()</span>
        <span>self</span><span>.</span><span>register_parameter</span><span>(</span><span>&#34;scale&#34;</span><span>,</span> <span>nn</span><span>.</span><span>Parameter</span><span>(</span><span>torch</span><span>.</span><span>ones</span><span>(</span><span>layer_shape</span><span>)))</span>

    <span>def</span><span> </span><span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>):</span>
<span>        </span><span>&#34;&#34;&#34;</span>
<span>        assumes shape is (batch, seq_len, d_model)</span>
<span>        &#34;&#34;&#34;</span>
        <span># frob norm is not the same as RMS. RMS = 1/sqrt(N) * frob norm</span>
        <span>ff_rms</span> <span>=</span> <span>torch</span><span>.</span><span>linalg</span><span>.</span><span>norm</span><span>(</span><span>x</span><span>,</span> <span>dim</span><span>=</span><span>(</span><span>1</span><span>,</span><span>2</span><span>))</span> <span>*</span> <span>x</span><span>[</span><span>0</span><span>]</span><span>.</span><span>numel</span><span>()</span> <span>**</span> <span>-</span><span>.5</span>
        <span>raw</span> <span>=</span> <span>x</span> <span>/</span> <span>ff_rms</span><span>.</span><span>unsqueeze</span><span>(</span><span>-</span><span>1</span><span>)</span><span>.</span><span>unsqueeze</span><span>(</span><span>-</span><span>1</span><span>)</span>
        <span>return</span> <span>self</span><span>.</span><span>scale</span><span>[:</span><span>x</span><span>.</span><span>shape</span><span>[</span><span>1</span><span>],</span> <span>:]</span><span>.</span><span>unsqueeze</span><span>(</span><span>0</span><span>)</span> <span>*</span> <span>raw</span>

<span>config</span> <span>=</span> <span>{</span>
    <span>&#39;batch_size&#39;</span><span>:</span> <span>5</span><span>,</span>
    <span>&#39;context_window&#39;</span><span>:</span> <span>11</span><span>,</span>
    <span>&#39;d_model&#39;</span><span>:</span> <span>13</span><span>,</span>
<span>}</span>
<span>batch</span> <span>=</span> <span>torch</span><span>.</span><span>randn</span><span>((</span><span>config</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]))</span>
<span>m</span> <span>=</span> <span>RMSNorm</span><span>((</span><span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]))</span>
<span>g</span> <span>=</span> <span>m</span><span>(</span><span>batch</span><span>)</span>
<span>print</span><span>(</span><span>g</span><span>.</span><span>shape</span><span>)</span>
</pre></div>

<p>We want to test to ensure that the RMSNorm is doing what we think it should. We can do this the old-fashioned way: row-wise comparisons. The RMSNorm has the property where the norm of the layer will be the square root of the number of elements in the layer, so we can check that for every layer.</p>
<div><pre><span></span><span>rms</span> <span>=</span> <span>torch</span><span>.</span><span>linalg</span><span>.</span><span>norm</span><span>(</span><span>batch</span><span>,</span> <span>dim</span><span>=</span><span>(</span><span>1</span><span>,</span><span>2</span><span>))</span> <span>*</span> <span>(</span><span>batch</span><span>[</span><span>0</span><span>]</span><span>.</span><span>numel</span><span>()</span> <span>**</span> <span>-</span><span>.5</span><span>)</span>

<span># scaled_batch.var(dim=(1,2))</span>
<span>assert</span> <span>torch</span><span>.</span><span>linalg</span><span>.</span><span>norm</span><span>(</span> <span>torch</span><span>.</span><span>arange</span><span>(</span><span>5</span><span>)</span><span>.</span><span>float</span><span>()</span> <span>)</span> <span>==</span> <span>(</span><span>torch</span><span>.</span><span>arange</span><span>(</span><span>5</span><span>)</span><span>.</span><span>float</span><span>()</span> <span>**</span> <span>2</span> <span>)</span><span>.</span><span>sum</span><span>()</span> <span>**</span> <span>.5</span>
<span>rms</span> <span>=</span> <span>torch</span><span>.</span><span>linalg</span><span>.</span><span>norm</span><span>(</span> <span>torch</span><span>.</span><span>arange</span><span>(</span><span>5</span><span>)</span><span>.</span><span>float</span><span>()</span> <span>)</span> <span>*</span> <span>(</span><span>torch</span><span>.</span><span>arange</span><span>(</span><span>5</span><span>)</span><span>.</span><span>numel</span><span>()</span> <span>**</span> <span>-</span><span>.5</span><span>)</span>
<span>assert</span> <span>torch</span><span>.</span><span>allclose</span><span>(</span><span>torch</span><span>.</span><span>linalg</span><span>.</span><span>norm</span><span>(</span><span>torch</span><span>.</span><span>arange</span><span>(</span><span>5</span><span>)</span><span>.</span><span>float</span><span>()</span> <span>/</span> <span>rms</span><span>),</span> <span>torch</span><span>.</span><span>tensor</span><span>(</span><span>5</span> <span>**</span> <span>.5</span><span>))</span>
<span>ff_rms</span> <span>=</span> <span>torch</span><span>.</span><span>linalg</span><span>.</span><span>norm</span><span>(</span><span>batch</span><span>,</span> <span>dim</span><span>=</span><span>(</span><span>1</span><span>,</span><span>2</span><span>))</span> <span>*</span> <span>batch</span><span>.</span><span>shape</span><span>[</span><span>1</span><span>:]</span><span>.</span><span>numel</span><span>()</span> <span>**</span> <span>-</span><span>.5</span>

<span># RMS for sure</span>
<span>ffx</span> <span>=</span> <span>torch</span><span>.</span><span>zeros_like</span><span>(</span><span>batch</span><span>)</span>
<span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>batch</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]):</span>
    <span>ffx</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>batch</span><span>[</span><span>i</span><span>]</span> <span>/</span> <span>ff_rms</span><span>[</span><span>i</span><span>]</span>
<span>assert</span> <span>torch</span><span>.</span><span>allclose</span><span>(</span><span>torch</span><span>.</span><span>linalg</span><span>.</span><span>norm</span><span>(</span><span>ffx</span><span>,</span> <span>dim</span><span>=</span><span>(</span><span>1</span><span>,</span><span>2</span><span>))</span> <span>**</span> <span>2</span><span>,</span> <span>torch</span><span>.</span><span>tensor</span><span>(</span><span>143</span><span>)</span><span>.</span><span>float</span><span>())</span>
<span>assert</span> <span>torch</span><span>.</span><span>allclose</span><span>(</span><span>ffx</span><span>,</span> <span>g</span><span>)</span>
</pre></div>
<p>Alright, so that&#39;s RMSNorm, and it seems like it&#39;s working. Again, let&#39;s test it out.</p>
<div><pre><span></span><span>class</span><span> </span><span>SimpleModel_RMS</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span><span> </span><span>__init__</span><span>(</span><span>self</span><span>,</span> <span>config</span><span>):</span>
        <span>super</span><span>()</span><span>.</span><span>__init__</span><span>()</span>
        <span>self</span><span>.</span><span>config</span> <span>=</span> <span>config</span>

        <span>self</span><span>.</span><span>embedding</span> <span>=</span> <span>nn</span><span>.</span><span>Embedding</span><span>(</span><span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>])</span>
        <span>self</span><span>.</span><span>rms</span> <span>=</span> <span>RMSNorm</span><span>((</span><span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]))</span>
        <span>self</span><span>.</span><span>linear</span> <span>=</span> <span>nn</span><span>.</span><span>Sequential</span><span>(</span>
            <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]),</span>
            <span>nn</span><span>.</span><span>ReLU</span><span>(),</span>
            <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>]),</span>
        <span>)</span>

        <span>print</span><span>(</span><span>&#34;model params:&#34;</span><span>,</span> <span>sum</span><span>([</span><span>m</span><span>.</span><span>numel</span><span>()</span> <span>for</span> <span>m</span> <span>in</span> <span>self</span><span>.</span><span>parameters</span><span>()]))</span>

    <span>def</span><span> </span><span>forward</span><span>(</span><span>self</span><span>,</span> <span>idx</span><span>,</span> <span>targets</span><span>=</span><span>None</span><span>):</span>
        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>embedding</span><span>(</span><span>idx</span><span>)</span>
        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>rms</span><span>(</span><span>x</span><span>)</span> <span># rms pre-normalization</span>
        <span>logits</span> <span>=</span> <span>self</span><span>.</span><span>linear</span><span>(</span><span>x</span><span>)</span>

        <span>if</span> <span>targets</span> <span>is</span> <span>not</span> <span>None</span><span>:</span>
            <span>loss</span> <span>=</span> <span>F</span><span>.</span><span>cross_entropy</span><span>(</span><span>logits</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>,</span> <span>self</span><span>.</span><span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>]),</span> <span>targets</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>))</span>
            <span>return</span> <span>logits</span><span>,</span> <span>loss</span>

        <span>else</span><span>:</span>
            <span>return</span> <span>logits</span>

<span>model</span> <span>=</span> <span>SimpleModel_RMS</span><span>(</span><span>MASTER_CONFIG</span><span>)</span>
<span>xs</span><span>,</span> <span>ys</span> <span>=</span> <span>get_batches</span><span>(</span><span>dataset</span><span>,</span> <span>&#39;train&#39;</span><span>,</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;context_window&#39;</span><span>])</span>

<span>logits</span><span>,</span> <span>loss</span> <span>=</span> <span>model</span><span>(</span><span>xs</span><span>,</span> <span>ys</span><span>)</span>
<span>optimizer</span> <span>=</span> <span>torch</span><span>.</span><span>optim</span><span>.</span><span>Adam</span><span>(</span><span>model</span><span>.</span><span>parameters</span><span>())</span>
<span>train</span><span>(</span><span>model</span><span>,</span> <span>optimizer</span><span>)</span>
</pre></div>
<div><pre><span></span>model params: 35265
validation loss:  2.5015316724777223





&lt;Axes: &gt;
</pre></div>
<p><img src="https://github.com/bkitano/llama-from-scratch/blob/main/llama_files/llama_35_2.png?raw=true" alt="png"/></p>
<p>So RMSNorm works, and it got our loss down by a small amount.</p>
<h3 id="rotary-embeddings">Rotary Embeddings</h3><p><a href="https://arxiv.org/pdf/2104.09864.pdf">RoPE</a> is a kind of positional encoding for transformers. In Attention is All You Need, the authors propose two kinds of positional encodings, learned and fixed. In RoPE, the authors propose embedding the position of a token in a sequence by rotating the embedding, with a different rotation at each position.</p>
<div><pre><span></span><span>def</span><span> </span><span>get_rotary_matrix</span><span>(</span><span>context_window</span><span>,</span> <span>embedding_dim</span><span>):</span>
    <span>R</span> <span>=</span> <span>torch</span><span>.</span><span>zeros</span><span>((</span><span>context_window</span><span>,</span> <span>embedding_dim</span><span>,</span> <span>embedding_dim</span><span>),</span> <span>requires_grad</span><span>=</span><span>False</span><span>)</span>
    <span>for</span> <span>position</span> <span>in</span> <span>range</span><span>(</span><span>context_window</span><span>):</span>
        <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>embedding_dim</span><span>//</span><span>2</span><span>):</span>
            <span>theta</span> <span>=</span> <span>10000.</span> <span>**</span> <span>(</span><span>-</span><span>2.</span><span>*</span><span>(</span><span>i</span> <span>-</span> <span>1</span><span>)</span> <span>/</span> <span>embedding_dim</span><span>)</span>
            <span>m_theta</span> <span>=</span> <span>position</span> <span>*</span> <span>theta</span>
            <span>R</span><span>[</span><span>position</span><span>,</span> <span>2</span><span>*</span><span>i</span><span>,</span><span>2</span><span>*</span><span>i</span><span>]</span> <span>=</span> <span>np</span><span>.</span><span>cos</span><span>(</span><span>m_theta</span><span>)</span>
            <span>R</span><span>[</span><span>position</span><span>,</span> <span>2</span><span>*</span><span>i</span><span>,</span><span>2</span><span>*</span><span>i</span><span>+</span><span>1</span><span>]</span> <span>=</span> <span>-</span> <span>np</span><span>.</span><span>sin</span><span>(</span><span>m_theta</span><span>)</span>
            <span>R</span><span>[</span><span>position</span><span>,</span> <span>2</span><span>*</span><span>i</span><span>+</span><span>1</span><span>,</span><span>2</span><span>*</span><span>i</span><span>]</span> <span>=</span> <span>np</span><span>.</span><span>sin</span><span>(</span><span>m_theta</span><span>)</span>
            <span>R</span><span>[</span><span>position</span><span>,</span> <span>2</span><span>*</span><span>i</span><span>+</span><span>1</span><span>,</span><span>2</span><span>*</span><span>i</span><span>+</span><span>1</span><span>]</span> <span>=</span> <span>np</span><span>.</span><span>cos</span><span>(</span><span>m_theta</span><span>)</span>
    <span>return</span> <span>R</span>
</pre></div>
<div><pre><span></span><span>K</span> <span>=</span> <span>3</span>
<span>config</span> <span>=</span> <span>{</span>
    <span>&#39;batch_size&#39;</span><span>:</span> <span>10</span><span>,</span>
    <span>&#39;d_model&#39;</span><span>:</span> <span>32</span><span>,</span>
    <span>&#39;n_heads&#39;</span><span>:</span> <span>8</span><span>,</span>
    <span>&#39;context_window&#39;</span><span>:</span> <span>K</span><span>**</span><span>2</span><span>,</span>
<span>}</span>
<span>batch</span> <span>=</span> <span>torch</span><span>.</span><span>randn</span><span>(</span><span>1</span><span>,</span> <span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>])</span>
<span>R</span> <span>=</span> <span>get_rotary_matrix</span><span>(</span><span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>])</span>
<span>fig</span><span>,</span> <span>ax</span> <span>=</span> <span>plt</span><span>.</span><span>subplots</span><span>(</span><span>K</span><span>,</span> <span>K</span><span>,</span> <span>figsize</span><span>=</span><span>(</span><span>K</span> <span>*</span> <span>3</span><span>,</span> <span>K</span> <span>*</span> <span>4</span><span>))</span>

<span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>K</span><span>):</span>
    <span>for</span> <span>j</span> <span>in</span> <span>range</span><span>(</span><span>K</span><span>):</span>
        <span>ax</span><span>[</span><span>i</span><span>,</span> <span>j</span><span>]</span><span>.</span><span>imshow</span><span>(</span><span>R</span><span>[</span><span>i</span> <span>*</span> <span>K</span> <span>+</span> <span>j</span><span>,</span> <span>:,</span> <span>:]</span><span>.</span><span>detach</span><span>()</span><span>.</span><span>numpy</span><span>())</span>
        <span>ax</span><span>[</span><span>i</span><span>,</span> <span>j</span><span>]</span><span>.</span><span>set_title</span><span>(</span><span>f</span><span>&#39;rotation at </span><span>{</span><span>i</span><span> </span><span>*</span><span> </span><span>K</span><span> </span><span>+</span><span> </span><span>j</span><span>}</span><span>&#39;</span><span>)</span>
</pre></div>
<p><img src="https://github.com/bkitano/llama-from-scratch/blob/main/llama_files/llama_39_0.png?raw=true" alt="png"/></p>
<p>Let&#39;s make sure these work. They should exhibit the quality that</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msubsup><mi>q</mi><mi>m</mi><mi>T</mi></msubsup><msub><mi>k</mi><mi>n</mi></msub><mo>=</mo><mo stretchy="false">(</mo><msubsup><mi>R</mi><mrow><mi>Θ</mi><mo>,</mo><mi>m</mi></mrow><mi>d</mi></msubsup><msub><mi>W</mi><mi>q</mi></msub><msub><mi>x</mi><mi>m</mi></msub><msup><mo stretchy="false">)</mo><mi>T</mi></msup><mo stretchy="false">(</mo><msubsup><mi>R</mi><mrow><mi>Θ</mi><mo>,</mo><mi>n</mi></mrow><mi>d</mi></msubsup><msub><mi>W</mi><mi>k</mi></msub><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msup><mi>x</mi><mi>T</mi></msup><msub><mi>W</mi><mi>q</mi></msub><msubsup><mi>R</mi><mrow><mi>Θ</mi><mo>,</mo><mi>n</mi><mo>−</mo><mi>m</mi></mrow><mi>d</mi></msubsup><msub><mi>W</mi><mi>k</mi></msub><msub><mi>x</mi><mi>n</mi></msub><mo>.</mo></mrow></math><div><pre><span></span><span>config</span> <span>=</span> <span>{</span>
    <span>&#39;d_model&#39;</span><span>:</span> <span>128</span><span>,</span>
    <span>&#39;context_window&#39;</span><span>:</span> <span>16</span><span>,</span>
<span>}</span>

<span>R</span> <span>=</span> <span>get_rotary_matrix</span><span>(</span><span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>])</span>
<span>x</span> <span>=</span> <span>torch</span><span>.</span><span>randn</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>])</span>
<span>y</span> <span>=</span> <span>torch</span><span>.</span><span>randn</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>])</span>

<span>m</span> <span>=</span> <span>3</span>
<span>n</span> <span>=</span> <span>13</span>

<span>x_m</span> <span>=</span> <span>R</span><span>[</span><span>m</span><span>,:,:]</span> <span>@</span> <span>x</span>
<span>x_n</span> <span>=</span> <span>R</span><span>[</span><span>n</span><span>,:,:]</span> <span>@</span> <span>y</span>

<span>assert</span> <span>torch</span><span>.</span><span>isclose</span><span>(</span><span>x_m</span> <span>@</span> <span>x_n</span><span>,</span> <span>x</span> <span>@</span> <span>R</span><span>[</span><span>n</span><span>-</span><span>m</span><span>,:,:]</span> <span>@</span> <span>y</span><span>)</span>
</pre></div>
<p>So the RoPE rotations work as expected.</p>
<div><pre><span></span><span>config</span> <span>=</span> <span>{</span>
    <span>&#39;batch_size&#39;</span><span>:</span> <span>10</span><span>,</span>
    <span>&#39;d_model&#39;</span><span>:</span> <span>512</span><span>,</span>
    <span>&#39;n_heads&#39;</span><span>:</span> <span>8</span><span>,</span>
    <span>&#39;context_window&#39;</span><span>:</span> <span>16</span><span>,</span>
<span>}</span>

<span>class</span><span> </span><span>RoPEAttentionHead</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span><span> </span><span>__init__</span><span>(</span><span>self</span><span>,</span> <span>config</span><span>):</span>
        <span>super</span><span>()</span><span>.</span><span>__init__</span><span>()</span>
        <span>self</span><span>.</span><span>config</span> <span>=</span> <span>config</span>
        <span>self</span><span>.</span><span>w_q</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>bias</span><span>=</span><span>False</span><span>)</span>
        <span>self</span><span>.</span><span>w_k</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>bias</span><span>=</span><span>False</span><span>)</span>
        <span>self</span><span>.</span><span>w_v</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>bias</span><span>=</span><span>False</span><span>)</span>

        <span>self</span><span>.</span><span>R</span> <span>=</span> <span>get_rotary_matrix</span><span>(</span><span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>])</span>

    <span>def</span><span> </span><span>get_rotary_matrix</span><span>(</span><span>context_window</span><span>,</span> <span>embedding_dim</span><span>):</span>
        <span>R</span> <span>=</span> <span>torch</span><span>.</span><span>zeros</span><span>((</span><span>context_window</span><span>,</span> <span>embedding_dim</span><span>,</span> <span>embedding_dim</span><span>),</span> <span>requires_grad</span><span>=</span><span>False</span><span>)</span>
        <span>for</span> <span>position</span> <span>in</span> <span>range</span><span>(</span><span>context_window</span><span>):</span>
            <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>embedding_dim</span><span>//</span><span>2</span><span>):</span>
                <span>theta</span> <span>=</span> <span>10000.</span> <span>**</span> <span>(</span><span>-</span><span>2.</span><span>*</span><span>(</span><span>i</span> <span>-</span> <span>1</span><span>)</span> <span>/</span> <span>embedding_dim</span><span>)</span>
                <span>m_theta</span> <span>=</span> <span>position</span> <span>*</span> <span>theta</span>
                <span>R</span><span>[</span><span>position</span><span>,</span> <span>2</span><span>*</span><span>i</span><span>,</span><span>2</span><span>*</span><span>i</span><span>]</span> <span>=</span> <span>np</span><span>.</span><span>cos</span><span>(</span><span>m_theta</span><span>)</span>
                <span>R</span><span>[</span><span>position</span><span>,</span> <span>2</span><span>*</span><span>i</span><span>,</span><span>2</span><span>*</span><span>i</span><span>+</span><span>1</span><span>]</span> <span>=</span> <span>-</span> <span>np</span><span>.</span><span>sin</span><span>(</span><span>m_theta</span><span>)</span>
                <span>R</span><span>[</span><span>position</span><span>,</span> <span>2</span><span>*</span><span>i</span><span>+</span><span>1</span><span>,</span><span>2</span><span>*</span><span>i</span><span>]</span> <span>=</span> <span>np</span><span>.</span><span>sin</span><span>(</span><span>m_theta</span><span>)</span>
                <span>R</span><span>[</span><span>position</span><span>,</span> <span>2</span><span>*</span><span>i</span><span>+</span><span>1</span><span>,</span><span>2</span><span>*</span><span>i</span><span>+</span><span>1</span><span>]</span> <span>=</span> <span>np</span><span>.</span><span>cos</span><span>(</span><span>m_theta</span><span>)</span>
        <span>return</span> <span>R</span>
    
    <span>def</span><span> </span><span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>,</span> <span>return_attn_weights</span><span>=</span><span>False</span><span>):</span>
        <span>b</span><span>,</span><span>m</span><span>,</span><span>d</span> <span>=</span> <span>x</span><span>.</span><span>shape</span>
        
        <span>q</span> <span>=</span> <span>self</span><span>.</span><span>w_q</span><span>(</span><span>x</span><span>)</span>
        <span>k</span> <span>=</span> <span>self</span><span>.</span><span>w_k</span><span>(</span><span>x</span><span>)</span>
        <span>v</span> <span>=</span> <span>self</span><span>.</span><span>w_v</span><span>(</span><span>x</span><span>)</span>

        <span>q_rotated</span> <span>=</span> <span>(</span><span>torch</span><span>.</span><span>bmm</span><span>(</span><span>q</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>),</span> <span>self</span><span>.</span><span>R</span><span>[:</span><span>m</span><span>]))</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>)</span>
        <span>k_rotated</span> <span>=</span> <span>(</span><span>torch</span><span>.</span><span>bmm</span><span>(</span><span>k</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>),</span> <span>self</span><span>.</span><span>R</span><span>[:</span><span>m</span><span>]))</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>)</span>

        <span>activations</span> <span>=</span> <span>F</span><span>.</span><span>scaled_dot_product_attention</span><span>(</span>
            <span>q_rotated</span><span>,</span><span>k_rotated</span><span>,</span><span>v</span><span>,</span><span>dropout_p</span> <span>=</span><span>.1</span>
        <span>)</span>

        <span>if</span> <span>return_attn_weights</span><span>:</span>
            <span>attn_weights</span> <span>=</span> <span>torch</span><span>.</span><span>bmm</span><span>(</span><span>q_rotated</span><span>,</span> <span>k_rotated</span><span>.</span><span>transpose</span><span>(</span><span>1</span><span>,</span><span>2</span><span>))</span> <span>/</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>d</span><span>)</span>
            <span>attn_weights</span> <span>=</span> <span>F</span><span>.</span><span>softmax</span><span>(</span><span>attn_weights</span><span>,</span> <span>dim</span><span>=-</span><span>1</span><span>)</span>
            <span>return</span> <span>activations</span><span>,</span> <span>attn_weights</span>
        <span>return</span> <span>activations</span>

<span>layer</span> <span>=</span> <span>RoPEAttentionHead</span><span>(</span><span>config</span><span>)</span>
<span>batch</span> <span>=</span> <span>torch</span><span>.</span><span>randn</span><span>((</span><span>config</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]))</span>
<span>output</span><span>,</span> <span>attn_weights</span> <span>=</span> <span>layer</span><span>(</span><span>batch</span><span>,</span> <span>return_attn_weights</span><span>=</span><span>True</span><span>)</span>
</pre></div>
<blockquote>
<p>Tip here: know the difference between tensor dimensions at train time vs tensor dimensions at inference time.</p>
</blockquote>
<p>Although at train time, you can expect your tensor dimensions to match your model parameters closely, eg <code>batch.shape = (config[&#39;batch_size&#39;], config[&#39;context_window&#39;], config[&#39;d_model&#39;])</code>, at inference time, you may have to deal with a single example, eg <code>batch.shape = (1, 1, config[&#39;d_model&#39;])</code>. For this reason, you need to make sure that when you&#39;re indexing in the <code>forward</code> pass, you&#39;re indexing using shapes derived from the input, not necessarily the model parameters.</p>
<p>Let&#39;s make sure it does what we think it does. For this layer, we&#39;re going to want to test three things:</p>
<ol>
<li>that it rotates embeddings the way we think it does</li>
<li>that the attention mask used for causal attention is working properly.</li>
</ol>
<div><pre><span></span><span>x</span> <span>=</span> <span>torch</span><span>.</span><span>randn</span><span>((</span><span>config</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]))</span>

<span>q</span> <span>=</span> <span>layer</span><span>.</span><span>w_q</span><span>(</span><span>x</span><span>)</span>
<span>k</span> <span>=</span> <span>layer</span><span>.</span><span>w_k</span><span>(</span><span>x</span><span>)</span>
<span>v</span> <span>=</span> <span>layer</span><span>.</span><span>w_v</span><span>(</span><span>x</span><span>)</span>

<span>q_rotated</span> <span>=</span> <span>torch</span><span>.</span><span>zeros_like</span><span>(</span><span>x</span><span>)</span>
<span>k_rotated</span> <span>=</span> <span>torch</span><span>.</span><span>zeros_like</span><span>(</span><span>x</span><span>)</span>
<span>v_rotated</span> <span>=</span> <span>torch</span><span>.</span><span>zeros_like</span><span>(</span><span>x</span><span>)</span>

<span>for</span> <span>position</span> <span>in</span> <span>range</span><span>(</span><span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>]):</span>
    <span>q_rotated</span><span>[:,</span><span>position</span><span>,:]</span> <span>=</span> <span>torch</span><span>.</span><span>matmul</span><span>(</span><span>q</span><span>[:,</span><span>position</span><span>,:],</span> <span>layer</span><span>.</span><span>R</span><span>[</span><span>position</span><span>,:,:])</span>
    <span>k_rotated</span><span>[:,</span><span>position</span><span>,:]</span> <span>=</span> <span>torch</span><span>.</span><span>matmul</span><span>(</span><span>k</span><span>[:,</span><span>position</span><span>,:],</span> <span>layer</span><span>.</span><span>R</span><span>[</span><span>position</span><span>,:,:])</span>
    <span>v_rotated</span><span>[:,</span><span>position</span><span>,:]</span> <span>=</span> <span>torch</span><span>.</span><span>matmul</span><span>(</span><span>v</span><span>[:,</span><span>position</span><span>,:],</span> <span>layer</span><span>.</span><span>R</span><span>[</span><span>position</span><span>,:,:])</span>

<span>q_rotated</span> <span>=</span> <span>(</span><span>torch</span><span>.</span><span>bmm</span><span>(</span><span>q</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>),</span> <span>layer</span><span>.</span><span>R</span><span>))</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>)</span>
<span>k_rotated</span> <span>=</span> <span>(</span><span>torch</span><span>.</span><span>bmm</span><span>(</span><span>k</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>),</span> <span>layer</span><span>.</span><span>R</span><span>))</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>)</span>
<span>v_out</span> <span>=</span> <span>(</span><span>torch</span><span>.</span><span>bmm</span><span>(</span><span>v</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>),</span> <span>layer</span><span>.</span><span>R</span><span>))</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>)</span>

<span>assert</span> <span>torch</span><span>.</span><span>allclose</span><span>(</span><span>q</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>)[</span><span>0</span><span>],</span> <span>q</span><span>[:,</span><span>0</span><span>,:])</span>
<span>assert</span> <span>torch</span><span>.</span><span>allclose</span><span>(</span><span>q</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>)[</span><span>0</span><span>]</span> <span>@</span> <span>layer</span><span>.</span><span>R</span><span>[</span><span>0</span><span>],</span> <span>q</span><span>[:,</span><span>0</span><span>,:]</span> <span>@</span> <span>layer</span><span>.</span><span>R</span><span>[</span><span>0</span><span>])</span>
<span>assert</span> <span>torch</span><span>.</span><span>allclose</span><span>(</span><span>q_rotated</span><span>,</span> <span>q_rotated</span><span>)</span>
</pre></div>
<div><pre><span></span><span>config</span> <span>=</span> <span>{</span>
    <span>&#39;batch_size&#39;</span><span>:</span> <span>1</span><span>,</span>
    <span>&#39;d_model&#39;</span><span>:</span> <span>2</span><span>,</span>
    <span>&#39;n_heads&#39;</span><span>:</span> <span>2</span><span>,</span>
    <span>&#39;context_window&#39;</span><span>:</span> <span>3</span><span>,</span>
<span>}</span>

<span>layer</span> <span>=</span> <span>RoPEAttentionHead</span><span>(</span><span>config</span><span>)</span>
<span>batch</span> <span>=</span> <span>torch</span><span>.</span><span>ones</span><span>((</span><span>config</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]))</span>
<span>output</span><span>,</span> <span>attn_weights</span> <span>=</span> <span>layer</span><span>(</span><span>batch</span><span>,</span> <span>return_attn_weights</span><span>=</span><span>True</span><span>)</span>

<span>m</span> <span>=</span> <span>0</span>
<span>x_q</span> <span>=</span> <span>batch</span><span>[</span><span>0</span><span>,</span> <span>m</span><span>]</span>
<span>q</span> <span>=</span> <span>layer</span><span>.</span><span>R</span><span>[</span><span>m</span><span>,:,:]</span> <span>@</span> <span>layer</span><span>.</span><span>w_q</span><span>(</span><span>x_q</span><span>)</span>

<span>assert</span> <span>torch</span><span>.</span><span>allclose</span><span>(</span><span>layer</span><span>.</span><span>w_q</span><span>(</span><span>x_q</span><span>),</span> <span>layer</span><span>.</span><span>w_q</span><span>.</span><span>weight</span> <span>@</span> <span>x_q</span><span>)</span>
<span>assert</span> <span>torch</span><span>.</span><span>allclose</span><span>(</span><span>q</span><span>,</span> <span>layer</span><span>.</span><span>R</span><span>[</span><span>m</span><span>,</span> <span>:,</span> <span>:]</span> <span>@</span> <span>layer</span><span>.</span><span>w_q</span><span>.</span><span>weight</span> <span>@</span> <span>x_q</span><span>)</span>

<span>n</span> <span>=</span> <span>2</span>
<span>x_k</span> <span>=</span> <span>batch</span><span>[</span><span>0</span><span>,</span> <span>n</span><span>]</span>
<span>k</span> <span>=</span> <span>layer</span><span>.</span><span>R</span><span>[</span><span>n</span><span>,:,:]</span> <span>@</span> <span>layer</span><span>.</span><span>w_k</span><span>(</span><span>x_k</span><span>)</span>

<span>assert</span> <span>torch</span><span>.</span><span>allclose</span><span>(</span><span>layer</span><span>.</span><span>w_k</span><span>(</span><span>x_k</span><span>),</span> <span>layer</span><span>.</span><span>w_k</span><span>.</span><span>weight</span> <span>@</span> <span>x_k</span><span>)</span>
<span>assert</span> <span>torch</span><span>.</span><span>allclose</span><span>(</span><span>k</span><span>,</span> <span>layer</span><span>.</span><span>R</span><span>[</span><span>n</span><span>,</span> <span>:,</span> <span>:]</span> <span>@</span> <span>layer</span><span>.</span><span>w_k</span><span>.</span><span>weight</span> <span>@</span> <span>x_k</span><span>)</span>

<span>assert</span> <span>q</span><span>.</span><span>T</span> <span>@</span> <span>k</span> <span>==</span> <span>q</span> <span>@</span> <span>k</span> <span># transpose is redundant</span>
<span>assert</span> <span>torch</span><span>.</span><span>allclose</span><span>(</span><span>q</span> <span>@</span> <span>k</span><span>,</span> <span>x_k</span><span>.</span><span>T</span> <span>@</span> <span>layer</span><span>.</span><span>w_k</span><span>.</span><span>weight</span><span>.</span><span>T</span> <span>@</span> <span>layer</span><span>.</span><span>R</span><span>[</span><span>n</span><span>,</span> <span>:,</span> <span>:]</span><span>.</span><span>T</span> <span>@</span> <span>layer</span><span>.</span><span>R</span><span>[</span><span>m</span><span>,</span> <span>:,</span> <span>:]</span> <span>@</span> <span>layer</span><span>.</span><span>w_q</span><span>.</span><span>weight</span> <span>@</span> <span>x_q</span><span>)</span>
<span>assert</span> <span>torch</span><span>.</span><span>allclose</span><span>(</span><span>q</span> <span>@</span> <span>k</span><span>,</span> <span>x_k</span><span>.</span><span>T</span> <span>@</span> <span>layer</span><span>.</span><span>w_k</span><span>.</span><span>weight</span><span>.</span><span>T</span> <span>@</span> <span>layer</span><span>.</span><span>R</span><span>[</span><span>n</span><span>-</span><span>m</span><span>,</span> <span>:,</span> <span>:]</span><span>.</span><span>T</span> <span>@</span> <span>layer</span><span>.</span><span>w_q</span><span>.</span><span>weight</span> <span>@</span> <span>x_q</span><span>)</span>
</pre></div>
<div><pre><span></span>/var/folders/w4/2j887mvs097bkhhjpgfzjlyr0000gn/T/ipykernel_52564/2062321511.py:26: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3575.)
  assert q.T @ k == q @ k # transpose is redundant
</pre></div>
<p>Let&#39;s setup a multi-headed attention layer for this singular attention head and see what happens when we train.</p>
<div><pre><span></span><span># definitely there&#39;s an optimization we could make where we cache the rotation matrices, but skip.</span>
<span>class</span><span> </span><span>RoPEMultiheadAttention</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span><span> </span><span>__init__</span><span>(</span><span>self</span><span>,</span> <span>config</span><span>):</span>
        <span>super</span><span>()</span><span>.</span><span>__init__</span><span>()</span>
        <span>self</span><span>.</span><span>config</span> <span>=</span> <span>config</span>
        <span>self</span><span>.</span><span>heads</span> <span>=</span> <span>nn</span><span>.</span><span>ModuleList</span><span>([</span>
            <span>RoPEAttentionHead</span><span>(</span><span>config</span><span>)</span> <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>config</span><span>[</span><span>&#39;n_heads&#39;</span><span>])</span>
        <span>])</span>
        <span>self</span><span>.</span><span>linear</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;n_heads&#39;</span><span>]</span> <span>*</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>])</span>
        <span>self</span><span>.</span><span>dropout</span> <span>=</span> <span>nn</span><span>.</span><span>Dropout</span><span>(</span><span>.1</span><span>)</span>

    <span>def</span><span> </span><span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>):</span>
        <span>heads</span> <span>=</span> <span>[</span><span>h</span><span>(</span><span>x</span><span>)</span> <span>for</span> <span>h</span> <span>in</span> <span>self</span><span>.</span><span>heads</span><span>]</span>
        <span>x</span> <span>=</span> <span>torch</span><span>.</span><span>cat</span><span>(</span><span>heads</span><span>,</span> <span>dim</span><span>=-</span><span>1</span><span>)</span>
        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>linear</span><span>(</span><span>x</span><span>)</span>
        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>dropout</span><span>(</span><span>x</span><span>)</span>
        <span>return</span> <span>x</span>
    
<span>MASTER_CONFIG</span><span>.</span><span>update</span><span>({</span>
    <span>&#39;n_heads&#39;</span><span>:</span> <span>8</span><span>,</span>
<span>})</span>
<span>layer</span> <span>=</span> <span>RoPEMultiheadAttention</span><span>(</span><span>MASTER_CONFIG</span><span>)</span>
<span>batch</span> <span>=</span> <span>torch</span><span>.</span><span>ones</span><span>((</span><span>MASTER_CONFIG</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;d_model&#39;</span><span>]))</span>
<span>output</span> <span>=</span> <span>layer</span><span>(</span><span>batch</span><span>)</span>
<span>output</span><span>.</span><span>shape</span>
</pre></div>
<div><pre><span></span>torch.Size([32, 16, 128])
</pre></div>
<div><pre><span></span><span>class</span><span> </span><span>RopeModel</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span><span> </span><span>__init__</span><span>(</span><span>self</span><span>,</span> <span>config</span><span>):</span>
        <span>super</span><span>()</span><span>.</span><span>__init__</span><span>()</span>
        <span>self</span><span>.</span><span>config</span> <span>=</span> <span>config</span>

        <span>self</span><span>.</span><span>embedding</span> <span>=</span> <span>nn</span><span>.</span><span>Embedding</span><span>(</span><span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>])</span>
        <span>self</span><span>.</span><span>rms</span> <span>=</span> <span>RMSNorm</span><span>((</span><span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]))</span>
        <span>self</span><span>.</span><span>rope_attention</span> <span>=</span> <span>RoPEMultiheadAttention</span><span>(</span><span>config</span><span>)</span>

        <span>self</span><span>.</span><span>linear</span> <span>=</span> <span>nn</span><span>.</span><span>Sequential</span><span>(</span>
            <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]),</span>
            <span>nn</span><span>.</span><span>ReLU</span><span>(),</span>
        <span>)</span>

        <span>self</span><span>.</span><span>last_linear</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>])</span>
        
        <span>print</span><span>(</span><span>&#34;model params:&#34;</span><span>,</span> <span>sum</span><span>([</span><span>m</span><span>.</span><span>numel</span><span>()</span> <span>for</span> <span>m</span> <span>in</span> <span>self</span><span>.</span><span>parameters</span><span>()]))</span>

    <span>def</span><span> </span><span>forward</span><span>(</span><span>self</span><span>,</span> <span>idx</span><span>,</span> <span>targets</span><span>=</span><span>None</span><span>):</span>
        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>embedding</span><span>(</span><span>idx</span><span>)</span>
        
        <span># one block of attention</span>
        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>rms</span><span>(</span><span>x</span><span>)</span> <span># rms pre-normalization</span>
        <span>x</span> <span>=</span> <span>x</span> <span>+</span> <span>self</span><span>.</span><span>rope_attention</span><span>(</span><span>x</span><span>)</span>

        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>rms</span><span>(</span><span>x</span><span>)</span> <span># rms pre-normalization</span>
        <span>x</span> <span>=</span> <span>x</span> <span>+</span> <span>self</span><span>.</span><span>linear</span><span>(</span><span>x</span><span>)</span>

        <span>logits</span> <span>=</span> <span>self</span><span>.</span><span>last_linear</span><span>(</span><span>x</span><span>)</span>

        <span>if</span> <span>targets</span> <span>is</span> <span>not</span> <span>None</span><span>:</span>
            <span>loss</span> <span>=</span> <span>F</span><span>.</span><span>cross_entropy</span><span>(</span><span>logits</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>,</span> <span>self</span><span>.</span><span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>]),</span> <span>targets</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>))</span>
            <span>return</span> <span>logits</span><span>,</span> <span>loss</span>

        <span>else</span><span>:</span>
            <span>return</span> <span>logits</span>

<span>model</span> <span>=</span> <span>RopeModel</span><span>(</span><span>MASTER_CONFIG</span><span>)</span>
<span>xs</span><span>,</span> <span>ys</span> <span>=</span> <span>get_batches</span><span>(</span><span>dataset</span><span>,</span> <span>&#39;train&#39;</span><span>,</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;context_window&#39;</span><span>])</span>

<span>logits</span><span>,</span> <span>loss</span> <span>=</span> <span>model</span><span>(</span><span>xs</span><span>,</span> <span>ys</span><span>)</span>
<span>optimizer</span> <span>=</span> <span>torch</span><span>.</span><span>optim</span><span>.</span><span>Adam</span><span>(</span><span>model</span><span>.</span><span>parameters</span><span>())</span>
<span>train</span><span>(</span><span>model</span><span>,</span> <span>optimizer</span><span>)</span>
</pre></div>
<div><pre><span></span>model params: 559681
validation loss:  0.1623048834502697





&lt;Axes: &gt;
</pre></div>
<p><img src="https://github.com/bkitano/llama-from-scratch/blob/main/llama_files/llama_50_2.png?raw=true" alt="png"/></p>
<p>Wow, would you look at that, our validation loss is down to .16. That is so low...it&#39;s almost too low. Consider the check we used from above: <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>exp</mi><mo stretchy="false">(</mo><mi>.16</mi><mo stretchy="false">)</mo><mo>=</mo><mn>1.17</mn></mrow></math>, so it&#39;s as if the model is choosing basically next character correctly every time. Let&#39;s see what happens when we generate.</p>
<div><pre><span></span><span>generate</span><span>(</span><span>model</span><span>,</span> <span>config</span><span>=</span><span>MASTER_CONFIG</span><span>)</span>
</pre></div>
<div><pre><span></span>[&#39;\n\n\n\n\n\n\n\nI\n\nOOOOOOOOOFOOtOOOOOOO&#39;,
 &#39;\nIIIIII IIIIIIIIIIIIIIIIIIIIIII&#39;,
 &#39;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&#39;,
 &#39;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\naaame&#39;,
 &#39;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&#39;]
</pre></div>
<p>So it looks terrible. What is happening here? Let&#39;s start debugging this by looking at the attention.</p>
<div><pre><span></span><span>MASTER_CONFIG</span><span>.</span><span>update</span><span>({</span>
    <span>&#39;n_heads&#39;</span><span>:</span> <span>8</span><span>,</span>
<span>})</span>
<span>layer</span> <span>=</span> <span>RoPEAttentionHead</span><span>(</span><span>MASTER_CONFIG</span><span>)</span>
<span>batch</span> <span>=</span> <span>torch</span><span>.</span><span>ones</span><span>((</span><span>MASTER_CONFIG</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;d_model&#39;</span><span>]))</span>
<span>output</span><span>,</span> <span>attn_weights</span> <span>=</span> <span>layer</span><span>(</span><span>batch</span><span>,</span> <span>return_attn_weights</span><span>=</span><span>True</span><span>)</span>

<span>plt</span><span>.</span><span>imshow</span><span>(</span><span>attn_weights</span><span>[</span><span>0</span><span>]</span><span>.</span><span>detach</span><span>()</span><span>.</span><span>numpy</span><span>(),</span> <span>interpolation</span><span>=</span><span>&#39;nearest&#39;</span><span>)</span>
<span>plt</span><span>.</span><span>colorbar</span><span>()</span>
</pre></div>
<div><pre><span></span>&lt;matplotlib.colorbar.Colorbar at 0x2872d0c10&gt;
</pre></div>
<p><img src="https://github.com/bkitano/llama-from-scratch/blob/main/llama_files/llama_54_1.png?raw=true" alt="png"/></p>
<p>So here we see that all the attention is lit up, meaning that characters in any position are attending to characters in any other position. What&#39;s bad about this? We are trying to predict the next token <em>solely</em> on the tokens that came before it, but here we&#39;re seeing that the model is attending to tokens that come after it. In other words, the model is cheating, or leaking information from the future. This is a problem, and it&#39;s why we need to use a causal mask.</p>
<div><pre><span></span><span>config</span> <span>=</span> <span>{</span>
    <span>&#39;batch_size&#39;</span><span>:</span> <span>10</span><span>,</span>
    <span>&#39;d_model&#39;</span><span>:</span> <span>512</span><span>,</span>
    <span>&#39;n_heads&#39;</span><span>:</span> <span>8</span><span>,</span>
    <span>&#39;context_window&#39;</span><span>:</span> <span>16</span><span>,</span>
<span>}</span>

<span>class</span><span> </span><span>RoPEMaskedAttentionHead</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span><span> </span><span>__init__</span><span>(</span><span>self</span><span>,</span> <span>config</span><span>):</span>
        <span>super</span><span>()</span><span>.</span><span>__init__</span><span>()</span>
        <span>self</span><span>.</span><span>config</span> <span>=</span> <span>config</span>
        <span>self</span><span>.</span><span>w_q</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>bias</span><span>=</span><span>False</span><span>)</span>
        <span>self</span><span>.</span><span>w_k</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>bias</span><span>=</span><span>False</span><span>)</span>
        <span>self</span><span>.</span><span>w_v</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>bias</span><span>=</span><span>False</span><span>)</span>

        <span>self</span><span>.</span><span>R</span> <span>=</span> <span>get_rotary_matrix</span><span>(</span><span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>])</span>

    <span>def</span><span> </span><span>get_rotary_matrix</span><span>(</span><span>context_window</span><span>,</span> <span>embedding_dim</span><span>):</span>
        <span>R</span> <span>=</span> <span>torch</span><span>.</span><span>zeros</span><span>((</span><span>context_window</span><span>,</span> <span>embedding_dim</span><span>,</span> <span>embedding_dim</span><span>),</span> <span>requires_grad</span><span>=</span><span>False</span><span>)</span>
        <span>for</span> <span>position</span> <span>in</span> <span>range</span><span>(</span><span>context_window</span><span>):</span>
            <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>embedding_dim</span><span>//</span><span>2</span><span>):</span>
                <span>theta</span> <span>=</span> <span>10000.</span> <span>**</span> <span>(</span><span>-</span><span>2.</span><span>*</span><span>(</span><span>i</span> <span>-</span> <span>1</span><span>)</span> <span>/</span> <span>embedding_dim</span><span>)</span>
                <span>m_theta</span> <span>=</span> <span>position</span> <span>*</span> <span>theta</span>
                <span>R</span><span>[</span><span>position</span><span>,</span> <span>2</span><span>*</span><span>i</span><span>,</span><span>2</span><span>*</span><span>i</span><span>]</span> <span>=</span> <span>np</span><span>.</span><span>cos</span><span>(</span><span>m_theta</span><span>)</span>
                <span>R</span><span>[</span><span>position</span><span>,</span> <span>2</span><span>*</span><span>i</span><span>,</span><span>2</span><span>*</span><span>i</span><span>+</span><span>1</span><span>]</span> <span>=</span> <span>-</span> <span>np</span><span>.</span><span>sin</span><span>(</span><span>m_theta</span><span>)</span>
                <span>R</span><span>[</span><span>position</span><span>,</span> <span>2</span><span>*</span><span>i</span><span>+</span><span>1</span><span>,</span><span>2</span><span>*</span><span>i</span><span>]</span> <span>=</span> <span>np</span><span>.</span><span>sin</span><span>(</span><span>m_theta</span><span>)</span>
                <span>R</span><span>[</span><span>position</span><span>,</span> <span>2</span><span>*</span><span>i</span><span>+</span><span>1</span><span>,</span><span>2</span><span>*</span><span>i</span><span>+</span><span>1</span><span>]</span> <span>=</span> <span>np</span><span>.</span><span>cos</span><span>(</span><span>m_theta</span><span>)</span>
        <span>return</span> <span>R</span>
    
    <span>def</span><span> </span><span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>,</span> <span>return_attn_weights</span><span>=</span><span>False</span><span>):</span>
        <span>b</span><span>,</span><span>m</span><span>,</span><span>d</span> <span>=</span> <span>x</span><span>.</span><span>shape</span>
        
        <span>q</span> <span>=</span> <span>self</span><span>.</span><span>w_q</span><span>(</span><span>x</span><span>)</span>
        <span>k</span> <span>=</span> <span>self</span><span>.</span><span>w_k</span><span>(</span><span>x</span><span>)</span>
        <span>v</span> <span>=</span> <span>self</span><span>.</span><span>w_v</span><span>(</span><span>x</span><span>)</span>

        <span>q_rotated</span> <span>=</span> <span>(</span><span>torch</span><span>.</span><span>bmm</span><span>(</span><span>q</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>),</span> <span>self</span><span>.</span><span>R</span><span>[:</span><span>m</span><span>]))</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>)</span>
        <span>k_rotated</span> <span>=</span> <span>(</span><span>torch</span><span>.</span><span>bmm</span><span>(</span><span>k</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>),</span> <span>self</span><span>.</span><span>R</span><span>[:</span><span>m</span><span>]))</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>)</span>

        <span>activations</span> <span>=</span> <span>F</span><span>.</span><span>scaled_dot_product_attention</span><span>(</span>
            <span>q_rotated</span><span>,</span><span>k_rotated</span><span>,</span><span>v</span><span>,</span><span>dropout_p</span> <span>=</span><span>.1</span><span>,</span> <span>is_causal</span><span>=</span><span>True</span>
        <span>)</span>

        <span>if</span> <span>return_attn_weights</span><span>:</span>
            <span>attn_mask</span> <span>=</span> <span>torch</span><span>.</span><span>tril</span><span>(</span><span>torch</span><span>.</span><span>ones</span><span>((</span><span>m</span><span>,</span><span>m</span><span>)),</span> <span>diagonal</span><span>=</span><span>0</span><span>)</span>
            <span>attn_weights</span> <span>=</span> <span>torch</span><span>.</span><span>bmm</span><span>(</span><span>q_rotated</span><span>,</span> <span>k_rotated</span><span>.</span><span>transpose</span><span>(</span><span>1</span><span>,</span><span>2</span><span>))</span> <span>/</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>d</span><span>)</span> <span>+</span> <span>attn_mask</span>
            <span>attn_weights</span> <span>=</span> <span>F</span><span>.</span><span>softmax</span><span>(</span><span>attn_weights</span><span>,</span> <span>dim</span><span>=-</span><span>1</span><span>)</span>
            <span>return</span> <span>activations</span><span>,</span> <span>attn_weights</span>
        <span>return</span> <span>activations</span>

<span>layer</span> <span>=</span> <span>RoPEMaskedAttentionHead</span><span>(</span><span>config</span><span>)</span>
<span>batch</span> <span>=</span> <span>torch</span><span>.</span><span>randn</span><span>((</span><span>config</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]))</span>
<span>output</span><span>,</span> <span>attn_weights</span> <span>=</span> <span>layer</span><span>(</span><span>batch</span><span>,</span> <span>return_attn_weights</span><span>=</span><span>True</span><span>)</span>
</pre></div>
<div><pre><span></span><span>layer</span> <span>=</span> <span>RoPEMaskedAttentionHead</span><span>(</span><span>MASTER_CONFIG</span><span>)</span>
<span>batch</span> <span>=</span> <span>torch</span><span>.</span><span>ones</span><span>((</span><span>MASTER_CONFIG</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;d_model&#39;</span><span>]))</span>
<span>output</span><span>,</span> <span>attn_weights</span> <span>=</span> <span>layer</span><span>(</span><span>batch</span><span>,</span> <span>return_attn_weights</span><span>=</span><span>True</span><span>)</span>

<span>plt</span><span>.</span><span>imshow</span><span>(</span><span>attn_weights</span><span>[</span><span>0</span><span>]</span><span>.</span><span>detach</span><span>()</span><span>.</span><span>numpy</span><span>())</span>
<span>plt</span><span>.</span><span>colorbar</span><span>()</span>
</pre></div>
<div><pre><span></span>&lt;matplotlib.colorbar.Colorbar at 0x2873f30d0&gt;
</pre></div>
<p><img src="https://github.com/bkitano/llama-from-scratch/blob/main/llama_files/llama_57_1.png?raw=true" alt="png"/></p>
<p>Now, we can see that the upper triangular of our attention activations (the part that corresponds to the future) is nearly zeroed out. Let&#39;s see what happens when we train.</p>
<div><pre><span></span><span># definitely there&#39;s an optimization we could make where we cache the rotation matrices, but skip.</span>
<span>class</span><span> </span><span>RoPEMaskedMultiheadAttention</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span><span> </span><span>__init__</span><span>(</span><span>self</span><span>,</span> <span>config</span><span>):</span>
        <span>super</span><span>()</span><span>.</span><span>__init__</span><span>()</span>
        <span>self</span><span>.</span><span>config</span> <span>=</span> <span>config</span>
        <span>self</span><span>.</span><span>heads</span> <span>=</span> <span>nn</span><span>.</span><span>ModuleList</span><span>([</span>
            <span>RoPEMaskedAttentionHead</span><span>(</span><span>config</span><span>)</span> <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>config</span><span>[</span><span>&#39;n_heads&#39;</span><span>])</span>
        <span>])</span>
        <span>self</span><span>.</span><span>linear</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;n_heads&#39;</span><span>]</span> <span>*</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>])</span>
        <span>self</span><span>.</span><span>dropout</span> <span>=</span> <span>nn</span><span>.</span><span>Dropout</span><span>(</span><span>.1</span><span>)</span>

    <span>def</span><span> </span><span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>):</span>
        <span>heads</span> <span>=</span> <span>[</span><span>h</span><span>(</span><span>x</span><span>)</span> <span>for</span> <span>h</span> <span>in</span> <span>self</span><span>.</span><span>heads</span><span>]</span>
        <span>x</span> <span>=</span> <span>torch</span><span>.</span><span>cat</span><span>(</span><span>heads</span><span>,</span> <span>dim</span><span>=-</span><span>1</span><span>)</span>
        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>linear</span><span>(</span><span>x</span><span>)</span>
        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>dropout</span><span>(</span><span>x</span><span>)</span>
        <span>return</span> <span>x</span>
    
<span>MASTER_CONFIG</span><span>.</span><span>update</span><span>({</span>
    <span>&#39;n_heads&#39;</span><span>:</span> <span>8</span><span>,</span>
<span>})</span>
<span>layer</span> <span>=</span> <span>RoPEMultiheadAttention</span><span>(</span><span>MASTER_CONFIG</span><span>)</span>
<span>batch</span> <span>=</span> <span>torch</span><span>.</span><span>ones</span><span>((</span><span>MASTER_CONFIG</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;d_model&#39;</span><span>]))</span>
<span>output</span> <span>=</span> <span>layer</span><span>(</span><span>batch</span><span>)</span>
<span>output</span><span>.</span><span>shape</span>
</pre></div>
<div><pre><span></span>torch.Size([32, 16, 128])
</pre></div>
<div><pre><span></span><span>class</span><span> </span><span>RopeModel</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span><span> </span><span>__init__</span><span>(</span><span>self</span><span>,</span> <span>config</span><span>):</span>
        <span>super</span><span>()</span><span>.</span><span>__init__</span><span>()</span>
        <span>self</span><span>.</span><span>config</span> <span>=</span> <span>config</span>

        <span>self</span><span>.</span><span>embedding</span> <span>=</span> <span>nn</span><span>.</span><span>Embedding</span><span>(</span><span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>])</span>
        <span>self</span><span>.</span><span>rms</span> <span>=</span> <span>RMSNorm</span><span>((</span><span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]))</span>
        <span>self</span><span>.</span><span>rope_attention</span> <span>=</span> <span>RoPEMaskedMultiheadAttention</span><span>(</span><span>config</span><span>)</span>

        <span>self</span><span>.</span><span>linear</span> <span>=</span> <span>nn</span><span>.</span><span>Sequential</span><span>(</span>
            <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]),</span>
            <span>nn</span><span>.</span><span>ReLU</span><span>(),</span>
        <span>)</span>

        <span>self</span><span>.</span><span>last_linear</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>])</span>
        
        <span>print</span><span>(</span><span>&#34;model params:&#34;</span><span>,</span> <span>sum</span><span>([</span><span>m</span><span>.</span><span>numel</span><span>()</span> <span>for</span> <span>m</span> <span>in</span> <span>self</span><span>.</span><span>parameters</span><span>()]))</span>

    <span>def</span><span> </span><span>forward</span><span>(</span><span>self</span><span>,</span> <span>idx</span><span>,</span> <span>targets</span><span>=</span><span>None</span><span>):</span>
        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>embedding</span><span>(</span><span>idx</span><span>)</span>
        
        <span># one block of attention</span>
        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>rms</span><span>(</span><span>x</span><span>)</span> <span># rms pre-normalization</span>
        <span>x</span> <span>=</span> <span>x</span> <span>+</span> <span>self</span><span>.</span><span>rope_attention</span><span>(</span><span>x</span><span>)</span>

        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>rms</span><span>(</span><span>x</span><span>)</span> <span># rms pre-normalization</span>
        <span>x</span> <span>=</span> <span>x</span> <span>+</span> <span>self</span><span>.</span><span>linear</span><span>(</span><span>x</span><span>)</span>

        <span>logits</span> <span>=</span> <span>self</span><span>.</span><span>last_linear</span><span>(</span><span>x</span><span>)</span>

        <span>if</span> <span>targets</span> <span>is</span> <span>not</span> <span>None</span><span>:</span>
            <span>loss</span> <span>=</span> <span>F</span><span>.</span><span>cross_entropy</span><span>(</span><span>logits</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>,</span> <span>self</span><span>.</span><span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>]),</span> <span>targets</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>))</span>
            <span>return</span> <span>logits</span><span>,</span> <span>loss</span>

        <span>else</span><span>:</span>
            <span>return</span> <span>logits</span>

<span>model</span> <span>=</span> <span>RopeModel</span><span>(</span><span>MASTER_CONFIG</span><span>)</span>
<span>xs</span><span>,</span> <span>ys</span> <span>=</span> <span>get_batches</span><span>(</span><span>dataset</span><span>,</span> <span>&#39;train&#39;</span><span>,</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;context_window&#39;</span><span>])</span>

<span>logits</span><span>,</span> <span>loss</span> <span>=</span> <span>model</span><span>(</span><span>xs</span><span>,</span> <span>ys</span><span>)</span>
<span>optimizer</span> <span>=</span> <span>torch</span><span>.</span><span>optim</span><span>.</span><span>Adam</span><span>(</span><span>model</span><span>.</span><span>parameters</span><span>())</span>
<span>train</span><span>(</span><span>model</span><span>,</span> <span>optimizer</span><span>)</span>
</pre></div>
<div><pre><span></span>model params: 559681
validation loss:  2.0815173864364622





&lt;Axes: &gt;
</pre></div>
<p><img src="https://github.com/bkitano/llama-from-scratch/blob/main/llama_files/llama_60_2.png?raw=true" alt="png"/></p>
<p>Much better, our loss is now not merely dropping to near-zero. It looks like we can drive our loss down even lower. Let&#39;s do that by updating master config.</p>
<div><pre><span></span><span>MASTER_CONFIG</span><span>.</span><span>update</span><span>({</span>
    <span>&#34;epochs&#34;</span><span>:</span> <span>5000</span><span>,</span>
    <span>&#34;log_interval&#34;</span><span>:</span> <span>10</span><span>,</span>
<span>})</span>
<span>train</span><span>(</span><span>model</span><span>,</span> <span>optimizer</span><span>)</span>
</pre></div>
<div><pre><span></span>validation loss:  1.8985356330871581





&lt;Axes: &gt;
</pre></div>
<p><img src="https://github.com/bkitano/llama-from-scratch/blob/main/llama_files/llama_62_2.png?raw=true" alt="png"/></p>
<h3 id="swiglu">SwiGLU</h3><p>As it says in the paper, &#34;We replace the ReLU non-linearity by the SwiGLU activation function...we use a dimension of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mfrac><mrow><mn>2</mn></mrow><mrow><mn>3</mn></mrow></mfrac><mn>4</mn><mi>d</mi></mrow></math> isntead of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>4</mn><mi>d</mi></mrow></math> as in PaLM.&#34; SwiGLU is defined as:</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mtext>SwiGLU</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mtext>Swish</mtext><mi>β</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mi>W</mi><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo><mo>⊗</mo><mo stretchy="false">(</mo><mi>x</mi><mi>V</mi><mo>+</mo><mi>c</mi><mo stretchy="false">)</mo></mrow></math><p>where <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo>⊗</mo></mrow></math> is a component-wise product. The Swish function is defined as:</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mtext>Swish</mtext><mi>β</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>x</mi><mi>σ</mi><mo stretchy="false">(</mo><mi>β</mi><mi>x</mi><mo stretchy="false">)</mo></mrow></math><p>where <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>β</mi></mrow></math> is a learnable parameter.</p>
<div><pre><span></span><span>class</span><span> </span><span>SwiGLU</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
<span>    </span><span>&#34;&#34;&#34;</span>
<span>    Swish-Gated Linear Unit</span>
<span>    https://arxiv.org/pdf/2002.05202v1.pdf</span>
<span>    &#34;&#34;&#34;</span>
    <span>def</span><span> </span><span>__init__</span><span>(</span><span>self</span><span>,</span> <span>size</span><span>):</span>
        <span>super</span><span>()</span><span>.</span><span>__init__</span><span>()</span>
        <span>self</span><span>.</span><span>config</span> <span>=</span> <span>config</span>
        <span>self</span><span>.</span><span>linear_gate</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>size</span><span>,</span> <span>size</span><span>)</span>
        <span>self</span><span>.</span><span>linear</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>size</span><span>,</span> <span>size</span><span>)</span>
        <span>self</span><span>.</span><span>beta</span> <span>=</span> <span>torch</span><span>.</span><span>randn</span><span>(</span><span>1</span><span>,</span> <span>requires_grad</span><span>=</span><span>True</span><span>)</span>

        <span>self</span><span>.</span><span>beta</span> <span>=</span> <span>nn</span><span>.</span><span>Parameter</span><span>(</span><span>torch</span><span>.</span><span>ones</span><span>(</span><span>1</span><span>))</span>
        <span>self</span><span>.</span><span>register_parameter</span><span>(</span><span>&#34;beta&#34;</span><span>,</span> <span>self</span><span>.</span><span>beta</span><span>)</span>

    <span>def</span><span> </span><span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>):</span> 
        <span>swish_gate</span> <span>=</span> <span>self</span><span>.</span><span>linear_gate</span><span>(</span><span>x</span><span>)</span> <span>*</span> <span>torch</span><span>.</span><span>sigmoid</span><span>(</span><span>self</span><span>.</span><span>beta</span> <span>*</span> <span>self</span><span>.</span><span>linear_gate</span><span>(</span><span>x</span><span>))</span>
        <span>out</span> <span>=</span> <span>swish_gate</span> <span>*</span> <span>self</span><span>.</span><span>linear</span><span>(</span><span>x</span><span>)</span>
        <span>return</span> <span>out</span>
</pre></div>
<div><pre><span></span><span>class</span><span> </span><span>RopeModel</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span><span> </span><span>__init__</span><span>(</span><span>self</span><span>,</span> <span>config</span><span>):</span>
        <span>super</span><span>()</span><span>.</span><span>__init__</span><span>()</span>
        <span>self</span><span>.</span><span>config</span> <span>=</span> <span>config</span>

        <span>self</span><span>.</span><span>embedding</span> <span>=</span> <span>nn</span><span>.</span><span>Embedding</span><span>(</span><span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>])</span>
        <span>self</span><span>.</span><span>rms</span> <span>=</span> <span>RMSNorm</span><span>((</span><span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]))</span>
        <span>self</span><span>.</span><span>rope_attention</span> <span>=</span> <span>RoPEMaskedMultiheadAttention</span><span>(</span><span>config</span><span>)</span>

        <span>self</span><span>.</span><span>linear</span> <span>=</span> <span>nn</span><span>.</span><span>Sequential</span><span>(</span>
            <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]),</span>
            <span>SwiGLU</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]),</span>
        <span>)</span>

        <span>self</span><span>.</span><span>last_linear</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>])</span>
        
        <span>print</span><span>(</span><span>&#34;model params:&#34;</span><span>,</span> <span>sum</span><span>([</span><span>m</span><span>.</span><span>numel</span><span>()</span> <span>for</span> <span>m</span> <span>in</span> <span>self</span><span>.</span><span>parameters</span><span>()]))</span>

    <span>def</span><span> </span><span>forward</span><span>(</span><span>self</span><span>,</span> <span>idx</span><span>,</span> <span>targets</span><span>=</span><span>None</span><span>):</span>
        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>embedding</span><span>(</span><span>idx</span><span>)</span>
        
        <span># one block of attention</span>
        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>rms</span><span>(</span><span>x</span><span>)</span> <span># rms pre-normalization</span>
        <span>x</span> <span>=</span> <span>x</span> <span>+</span> <span>self</span><span>.</span><span>rope_attention</span><span>(</span><span>x</span><span>)</span>

        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>rms</span><span>(</span><span>x</span><span>)</span> <span># rms pre-normalization</span>
        <span>x</span> <span>=</span> <span>x</span> <span>+</span> <span>self</span><span>.</span><span>linear</span><span>(</span><span>x</span><span>)</span>

        <span>logits</span> <span>=</span> <span>self</span><span>.</span><span>last_linear</span><span>(</span><span>x</span><span>)</span>

        <span>if</span> <span>targets</span> <span>is</span> <span>not</span> <span>None</span><span>:</span>
            <span>loss</span> <span>=</span> <span>F</span><span>.</span><span>cross_entropy</span><span>(</span><span>logits</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>,</span> <span>self</span><span>.</span><span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>]),</span> <span>targets</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>))</span>
            <span>return</span> <span>logits</span><span>,</span> <span>loss</span>

        <span>else</span><span>:</span>
            <span>return</span> <span>logits</span>

<span>model</span> <span>=</span> <span>RopeModel</span><span>(</span><span>MASTER_CONFIG</span><span>)</span>
<span>xs</span><span>,</span> <span>ys</span> <span>=</span> <span>get_batches</span><span>(</span><span>dataset</span><span>,</span> <span>&#39;train&#39;</span><span>,</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;context_window&#39;</span><span>])</span>

<span>logits</span><span>,</span> <span>loss</span> <span>=</span> <span>model</span><span>(</span><span>xs</span><span>,</span> <span>ys</span><span>)</span>
<span>optimizer</span> <span>=</span> <span>torch</span><span>.</span><span>optim</span><span>.</span><span>Adam</span><span>(</span><span>model</span><span>.</span><span>parameters</span><span>())</span>
<span>train</span><span>(</span><span>model</span><span>,</span> <span>optimizer</span><span>)</span>
</pre></div>
<div><pre><span></span>model params: 592706
validation loss:  1.8963455319404603





&lt;Axes: &gt;
</pre></div>
<p><img src="https://github.com/bkitano/llama-from-scratch/blob/main/llama_files/llama_65_2.png?raw=true" alt="png"/></p>
<p>Now, let&#39;s add multiple layers of RopeAttention by creating blocks.</p>
<div><pre><span></span><span># add RMSNorm and residual conncection</span>
<span>class</span><span> </span><span>LlamaBlock</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span><span> </span><span>__init__</span><span>(</span><span>self</span><span>,</span> <span>config</span><span>):</span>
        <span>super</span><span>()</span><span>.</span><span>__init__</span><span>()</span>
        <span>self</span><span>.</span><span>config</span> <span>=</span> <span>config</span>

        <span>self</span><span>.</span><span>rms</span> <span>=</span> <span>RMSNorm</span><span>((</span><span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]))</span>
        
        <span>self</span><span>.</span><span>attention</span> <span>=</span> <span>RoPEMaskedMultiheadAttention</span><span>(</span><span>config</span><span>)</span>
        <span>self</span><span>.</span><span>feedforward</span> <span>=</span> <span>nn</span><span>.</span><span>Sequential</span><span>(</span>
            <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]),</span>
            <span>SwiGLU</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]),</span>
        <span>)</span>

    <span>def</span><span> </span><span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>):</span>
        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>rms</span><span>(</span><span>x</span><span>)</span> <span># rms pre-normalization</span>
        <span>x</span> <span>=</span> <span>x</span> <span>+</span> <span>self</span><span>.</span><span>attention</span><span>(</span><span>x</span><span>)</span>

        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>rms</span><span>(</span><span>x</span><span>)</span> <span># rms pre-normalization</span>
        <span>x</span> <span>=</span> <span>x</span> <span>+</span> <span>self</span><span>.</span><span>feedforward</span><span>(</span><span>x</span><span>)</span>
        <span>return</span> <span>x</span>
</pre></div>
<div><pre><span></span><span>block</span> <span>=</span> <span>LlamaBlock</span><span>(</span><span>MASTER_CONFIG</span><span>)</span>
<span>block</span><span>(</span><span>torch</span><span>.</span><span>randn</span><span>(</span><span>MASTER_CONFIG</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;d_model&#39;</span><span>]));</span>
</pre></div>
<div><pre><span></span><span>from</span><span> </span><span>collections</span><span> </span><span>import</span> <span>OrderedDict</span>

<span>MASTER_CONFIG</span><span>.</span><span>update</span><span>({</span>
    <span>&#39;n_layers&#39;</span><span>:</span> <span>4</span><span>,</span>
<span>})</span>
<span>class</span><span> </span><span>Llama</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span><span> </span><span>__init__</span><span>(</span><span>self</span><span>,</span> <span>config</span><span>):</span>
        <span>super</span><span>()</span><span>.</span><span>__init__</span><span>()</span>
        <span>self</span><span>.</span><span>config</span> <span>=</span> <span>config</span>
        <span>self</span><span>.</span><span>embeddings</span> <span>=</span> <span>nn</span><span>.</span><span>Embedding</span><span>(</span><span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>])</span>
        <span>self</span><span>.</span><span>llama_blocks</span> <span>=</span> <span>nn</span><span>.</span><span>Sequential</span><span>(</span>
            <span>OrderedDict</span><span>([(</span><span>f</span><span>&#34;llama_</span><span>{</span><span>i</span><span>}</span><span>&#34;</span><span>,</span> <span>LlamaBlock</span><span>(</span><span>config</span><span>))</span> <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>config</span><span>[</span><span>&#39;n_layers&#39;</span><span>])])</span>
        <span>)</span>

        <span>self</span><span>.</span><span>ffn</span> <span>=</span> <span>nn</span><span>.</span><span>Sequential</span><span>(</span>
            <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]),</span>
            <span>SwiGLU</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]),</span>
            <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>]),</span>
        <span>)</span>

        <span>print</span><span>(</span><span>&#34;model params:&#34;</span><span>,</span> <span>sum</span><span>([</span><span>m</span><span>.</span><span>numel</span><span>()</span> <span>for</span> <span>m</span> <span>in</span> <span>self</span><span>.</span><span>parameters</span><span>()]))</span>

    <span>def</span><span> </span><span>forward</span><span>(</span><span>self</span><span>,</span> <span>idx</span><span>,</span> <span>targets</span><span>=</span><span>None</span><span>):</span>
        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>embeddings</span><span>(</span><span>idx</span><span>)</span>
        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>llama_blocks</span><span>(</span><span>x</span><span>)</span>
        <span>logits</span> <span>=</span> <span>self</span><span>.</span><span>ffn</span><span>(</span><span>x</span><span>)</span>

        <span>if</span> <span>targets</span> <span>is</span> <span>None</span><span>:</span>
            <span>return</span> <span>logits</span>
        
        <span>else</span><span>:</span>
            <span>loss</span> <span>=</span> <span>F</span><span>.</span><span>cross_entropy</span><span>(</span><span>logits</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>,</span> <span>self</span><span>.</span><span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>]),</span> <span>targets</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>))</span>
            <span>return</span> <span>logits</span><span>,</span> <span>loss</span>

<span>llama</span> <span>=</span> <span>Llama</span><span>(</span><span>MASTER_CONFIG</span><span>)</span>
<span>optimizer</span> <span>=</span> <span>torch</span><span>.</span><span>optim</span><span>.</span><span>Adam</span><span>(</span><span>llama</span><span>.</span><span>parameters</span><span>())</span>
<span>train</span><span>(</span><span>llama</span><span>,</span> <span>optimizer</span><span>,</span> <span>config</span><span>=</span><span>MASTER_CONFIG</span><span>)</span>
</pre></div>
<div><pre><span></span>model params: 2370246
validation loss:  1.5532499313354493





&lt;Axes: &gt;
</pre></div>
<p><img src="https://github.com/bkitano/llama-from-scratch/blob/main/llama_files/llama_69_2.png?raw=true" alt="png"/></p>
<p>It looks like we can drive the loss down even more, and although we&#39;re overfitting a little, I think we can still do better. Let&#39;s train longer.</p>
<div><pre><span></span><span>MASTER_CONFIG</span><span>.</span><span>update</span><span>({</span>
    <span>&#39;epochs&#39;</span><span>:</span> <span>10000</span><span>,</span>
<span>})</span>
<span>train</span><span>(</span><span>llama</span><span>,</span> <span>optimizer</span><span>,</span> <span>scheduler</span><span>=</span><span>None</span><span>,</span> <span>config</span><span>=</span><span>MASTER_CONFIG</span><span>)</span>
</pre></div>
<div><pre><span></span>validation loss:  1.1478946447372436





&lt;Axes: &gt;
</pre></div>
<p><img src="https://github.com/bkitano/llama-from-scratch/blob/main/llama_files/llama_71_2.png?raw=true" alt="png"/></p>
<p>It seems we can go even lower, still without serious overfitting. Either there is a leak, or it&#39;s actually doing well. The loss here is 1.08, which is equivalent to choosing between 2.9 tokens randomly.</p>
<div><pre><span></span><span>train</span><span>(</span><span>llama</span><span>,</span> <span>optimizer</span><span>,</span> <span>config</span><span>=</span><span>MASTER_CONFIG</span><span>)</span>
</pre></div>
<div><pre><span></span>validation loss:  0.9996801257133484





&lt;Axes: &gt;
</pre></div>
<p><img src="https://github.com/bkitano/llama-from-scratch/blob/main/llama_files/llama_73_2.png?raw=true" alt="png"/></p>
<div><pre><span></span><span>print</span><span>(</span><span>generate</span><span>(</span><span>llama</span><span>,</span> <span>MASTER_CONFIG</span><span>,</span> <span>500</span><span>)[</span><span>0</span><span>])</span>
</pre></div>
<div><pre><span></span>ZELBETH:
Sey solmenter! &#39;tis tonguerered if berryishdd, and What his stabe, you, and, but all I pilJefals, mode with,
Vurint as steolated have loven OlD the queen&#39;d refore
Are been, good plmp:

Proforne, wift&#39;es swleen, was no bunderes&#39;d a a quain beath!
Tybell is my gateer stalk smen&#39;d as be matious dazest brink thou
lord
Enves were cIUll, afe and whwas seath This a is, an tale hoice his his onety Meall-tearn not murkawn, fase bettizen&#39;d her,
To belacquesterer? baxewed wupl usweggs yet tall
An
</pre></div>
<p>At this point, we&#39;ve hit the bottom with our training. Let&#39;s test on the test set.</p>
<div><pre><span></span><span>xs</span><span>,</span> <span>ys</span> <span>=</span> <span>get_batches</span><span>(</span><span>dataset</span><span>,</span> <span>&#39;test&#39;</span><span>,</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;context_window&#39;</span><span>])</span>

<span>logits</span><span>,</span> <span>loss</span> <span>=</span> <span>llama</span><span>(</span><span>xs</span><span>,</span> <span>ys</span><span>)</span>

<span>print</span><span>(</span><span>loss</span><span>)</span>
</pre></div>
<div><pre><span></span>tensor(1.2358, grad_fn=&lt;NllLossBackward0&gt;)
</pre></div>
<h2 id="check-for-gradient-flows">Check for Gradient Flows</h2><p>Let&#39;s inspect the gradients, we want to see how they&#39;re flowing. If there are too many gradients where the value is close to 0, that&#39;s a problem.</p>
<div><pre><span></span><span># print the percentage that are near 0</span>
<span>def</span><span> </span><span>show_grads</span><span>(</span><span>model</span><span>,</span> <span>tol</span><span>=</span><span>1e-2</span><span>):</span>
    <span>return</span> <span>sorted</span><span>([(</span><span>name</span><span>,</span> <span>100.0</span> <span>*</span> <span>float</span><span>(</span><span>torch</span><span>.</span><span>sum</span><span>(</span><span>torch</span><span>.</span><span>abs</span><span>(</span><span>param</span><span>)</span> <span>&lt;=</span> <span>tol</span><span>))</span> <span>/</span> <span>float</span><span>(</span><span>param</span><span>.</span><span>nelement</span><span>()))</span> <span>for</span> <span>name</span><span>,</span> <span>param</span> <span>in</span> <span>model</span><span>.</span><span>named_parameters</span><span>()</span> <span>if</span> <span>param</span><span>.</span><span>requires_grad</span><span>],</span> <span>key</span><span>=</span><span>lambda</span> <span>t</span><span>:</span> <span>t</span><span>[</span><span>1</span><span>],</span> <span>reverse</span><span>=</span><span>True</span><span>)</span>

<span>show_grads</span><span>(</span><span>llama</span><span>)</span>
</pre></div>
<p>Here, for all of our parameter gradients, the vast majority are non-zero, which is great. If we start to see this number peak higher, then our gradients would not be flowing.</p>
<h2 id="experiment-with-hyperparams-aka-change-the-oven-settings">Experiment with hyperparams, aka &#34;change the oven settings&#34;</h2><p>In the original Llama paper, the authors use Cosine Annealing learning schedule. We didn&#39;t do that here, because I experimented and saw that it was worse.</p>
<div><pre><span></span><span>MASTER_CONFIG</span><span>.</span><span>update</span><span>({</span>
    <span>&#34;epochs&#34;</span><span>:</span> <span>1000</span>
<span>})</span>
<span>llama_with_cosine</span> <span>=</span> <span>Llama</span><span>(</span><span>MASTER_CONFIG</span><span>)</span>
<span>llama_optimizer</span> <span>=</span> <span>torch</span><span>.</span><span>optim</span><span>.</span><span>Adam</span><span>(</span>
    <span>llama</span><span>.</span><span>parameters</span><span>(),</span> 
    <span>betas</span><span>=</span><span>(</span><span>.9</span><span>,</span> <span>.95</span><span>),</span> 
    <span>weight_decay</span><span>=</span><span>.1</span><span>,</span> 
    <span>eps</span><span>=</span><span>1e-9</span><span>,</span> 
    <span>lr</span><span>=</span><span>1e-3</span>
<span>)</span>
<span>scheduler</span> <span>=</span> <span>torch</span><span>.</span><span>optim</span><span>.</span><span>lr_scheduler</span><span>.</span><span>CosineAnnealingLR</span><span>(</span><span>llama_optimizer</span><span>,</span> <span>300</span><span>,</span> <span>eta_min</span><span>=</span><span>1e-5</span><span>)</span>
<span>train</span><span>(</span><span>llama_with_cosine</span><span>,</span> <span>llama_optimizer</span><span>,</span> <span>scheduler</span><span>=</span><span>scheduler</span><span>)</span>
</pre></div>
<div><pre><span></span><span>show_grads</span><span>(</span><span>llama_with_cosine</span><span>,</span> <span>1e-5</span><span>)</span>
</pre></div>
<p>Even at an extremely low tolerance, the attention biases are not getting any signal. I&#39;m not sure why the learning schedule from the paper doesn&#39;t work, but the lesson here is simple: start simple.</p>


    

    
        

        
            


        
    


  </div></div>
  </body>
</html>
