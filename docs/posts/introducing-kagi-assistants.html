<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.kagi.com/kagi-assistants">Original</a>
    <h1>Introducing Kagi Assistants</h1>
    
    <div id="readability-page-1" class="page"><div>
            
            <p><img src="https://kagifeedback.org/assets/files/2025-11-20/1763664692-827799-kicover.png" alt="Kagi Assistants graphic showing four assistant options with circular wave icons - “Quick” and “Research” are clearly visible, while two additional assistants on the right are blurred out"/></p>

<p><strong>TL;DR</strong></p>

<p>Today we’re releasing two research assistants: Quick Assistant and Research Assistant (previously named Ki during beta).</p>

<p>Kagi’s Research Assistant happened to top a popular benchmark (SimpleQA) when we ran it in August 2025. This was a happy accident. We’re building our research assistants to be useful products, not maximize benchmark scores.</p>



<p>Kagi Quick Assistant and Research Assistant (<a href="https://help.kagi.com/kagi/ai/kagi-research.html">documentation here</a>) are Kagi’s flagship research assistants. We’re building our research assistants with our <a href="https://blog.kagi.com/kagi-ai-search#philosophy">philosophy on using AI in our products</a> in mind: *<strong>Humans should be at the center of the experience</strong>,* and <strong><em>AI should enhance, not replace</em></strong> the search experience. We know that <a href="https://blog.kagi.com/llms">LLMs are prone to bullshitting</a>, but they’re incredibly useful tools when built into a product with their failings in mind.</p>

<p>Our assistants use different base models for specific tasks. We <a href="https://help.kagi.com/kagi/ai/llm-benchmark.html">continuously benchmark top-performing models</a> and select the best one for each job, so you don’t have to.</p>

<p>Their main strength is research: identifying what to search for, executing multiple simultaneous searches (in different languages, if needed), and synthesising the findings into high-quality answers.</p>

<p>The <strong>Quick Assistant (available on all plans) optimises for speed</strong>, providing direct and concise answers. The <strong>Research Assistant focuses on depth and diversity</strong>, conducting exhaustive analysis for thorough results.</p>

<p>We’re working on tools like research assistants because we find them useful. We hope you find them useful too. We’re not planning to force AI onto our users or products. We try to build tools because we think they’ll empower the humans that use them.</p>

<h3>Accessible from any search bar</h3>

<p>You can access the Quick Assistant and Research Assistant (ultimate tier only) from the <a href="https://kagi.com/assistant">Kagi Assistant webapp</a>.</p>

<p>But they are also accessible from <a href="https://help.kagi.com/kagi/features/bangs.html">bangs</a>, directly in your search bar:</p>

<ul>
<li><p><code>?</code> calls <a href="https://help.kagi.com/kagi/ai/quick-answer.html">quick answer</a>. <code>Best current football team?</code></p></li>

<li><p><code>!quick</code> will call Quick Assistant. The query would look like <code>Best current football team !quick</code></p></li>

<li><p><code>!research</code> calls Research Assistant. You would use <code>Best current football team !research</code></p></li>
</ul>

<p>Quick Assistant is expected to answer in less than 5 seconds and its cost will be negligible. Research Assistant can be expected to take over 20 seconds of research and have a higher cost against our <a href="https://kagi.com/pricing">fair use policy</a>.</p>

<h3>Assistants in action</h3>

<p>The research assistant should massively reduce the time it takes to find information. Here it is in action:</p>

<p><img src="https://kagifeedback.org/assets/files/2025-11-20/1763664287-689802-1.png" alt="Screenshot showing Kagi search results for audiophile cable research, displaying search queries and sources including Reddit discussions and scientific studies about expensive cables."/></p>

<p>The research assistant calls various tools as it researches the answer. The tools called are in the purple dropdown boxes in the screenshot, which you can open up to look into the search results:</p>

<p><img src="https://kagifeedback.org/assets/files/2025-11-20/1763664349-992775-2.png" alt="Screenshot of Kagi Assistant research process for “$5000 audiophile cables worth it” showing planned research steps, searches conducted, and sources gathered including blind test studies"/></p>

<p>Our full research assistant comfortably holds its own against competing “deep research” agents in accuracy, but it’s best qualified as a <a href="https://jina.ai/news/a-practical-guide-to-implementing-deepsearch-deepresearch/">“Deep Search”</a> agent. We found that <a href="https://openai.com/index/introducing-deep-research/">since the popularization</a> of deep research tools, they have been based around a long, report style output format.</p>

<p>Long reports are not the best format to answer most questions. This is true even of ones that require a lot of research.</p>

<p>What we do focus on, however, is verifiability of the generated answer. Answers in Kagi’s research assistants are expected to be sourced and referenced. We even add attribution of citations relevance to the final answer:</p>

<p><img src="https://kagifeedback.org/assets/files/2025-11-20/1763664411-15777-3.png" alt="Screenshot of Kagi Assistant answer stating expensive audiophile cables are not worth it, with bottom line conclusion and references to scientific evidence from blind testing"/></p>

<p>If we want to enhance the human search experience with LLM based tools, <strong>the experience should not stop with blindly trusting text generated by an LLM</strong>. Our design should aim to encourage humans to look further into the answer, to accelerate their research process.</p>

<p>The design <strong>should not replace the research process</strong> by encouraging humans to disengage from thinking about the question at hand.</p>

<h3>Other tools</h3>

<p>The research assistant has <a href="https://help.kagi.com/kagi/ai/kagi-research.html">access to many other tools beyond web search and retrieval</a>, like running code to check calculations, image generation, and calling specific APIs like Wolfram Alpha, news or location-specific searches.</p>

<p>Those should happen naturally as part of the answering process.</p>



<p>We’re in late 2025, it’s easy to be cynical about AI benchmarking. Some days it feels like most benchmark claims look something like this:</p>

<p><img src="https://kagifeedback.org/assets/files/2025-11-20/1763664490-83554-4.png" alt="Misleading bar chart comparing “our stuff” at 97.4% to “their stuff” at 97.35% and 12.1%, with annotation “Look how good we are” highlighting manipulated visualization"/></p>

<p>That said, benchmarking is necessary to build good quality products that use machine learning. Machine learning development differs from traditional software development: there is a smooth gradient of failure along the “quality” axis. The way to solve this is to continuously measure the quality!</p>

<p>We’ve always taken benchmarking seriously at Kagi; we’ve <a href="https://help.kagi.com/kagi/ai/llm-benchmark.html">maintained unpolluted private LLM benchmarks</a> for a long time. This lets us independently measure new models separately from their claimed performance on public benchmarks, right as they come out.</p>

<p>We also believe that <strong>benchmarks must be living targets</strong>. As the landscape of the internet and model capability changes, the way we measure them needs to adapt over time.</p>

<p>With that said, it’s good to sometimes compare ourselves on big public benchmarks. We run experiments on factual retrieval datasets like SimpleQA because they let us compare against others. Benchmarks like SimpleQA also easily measure how Kagi Search performs as a search backend against other search engines at returning factual answers.</p>

<h2>Kagi Tops SimpleQA, then gives up</h2>

<p>When we measured it in August 2025, Kagi Research achieved a <strong>95.5% score on the SimpleQA benchmark</strong>. As far as we could tell it was the #1 SimpleQA score at the time we ran it.</p>

<p>We’re not aiming to further improve our SimpleQA score. Aiming to score high on SimpleQA will make us “overfit” to the particularities of the SimpleQA dataset, which would make the Kagi Assistant worse overall for our users.</p>

<p>Since we ran it, it seems that DeepSeek v3 Terminus has since beaten the score:</p>

<p><img src="https://kagifeedback.org/assets/files/2025-11-20/1763664544-836301-5.png" alt="Horizontal bar chart showing SimpleQA Failed Task percentages for various AI models, with Kagi Research highlighted in yellow at 4.5%, ranking second best after Deepseek Terminus at 3.2%"/></p>

<h2>Some notes on SimpleQA</h2>

<p>SimpleQA wasn’t built with the intention of measuring search engine quality. It was <a href="https://arxiv.org/pdf/2411.04368">built to test whether models “know what they know”</a> or blindly hallucinate answers to questions like <em>“What is the name of the former Prime Minister of Iceland who worked as a cabin crew member until 1971?”</em></p>

<p>The SimpleQA results since its release seem to tell an interesting story: LLMs do not seem to be improving much at recalling simple facts without hallucinating. OpenAI’s GPT-5 (August 2025) <a href="https://cdn.openai.com/gpt-5-system-card.pdf">scored 55% on SimpleQA</a> (without search), whereas the comparatively weak O1 (September 2024) scored 47%.</p>

<p>However, “grounding” an LLM on factual data at the time of the query – a much smaller model like gemini 2.0 flash <a href="https://liner.com/learn/liner-accurate-ai-search">will score 83%</a> if it can use Google Search. We find the same result – it’s common for single models to score highly if they have access to web search. We find model scoring in the area of 85% (GPT 4o-mini + kagi search) to 91% (Claude-4-sonnet-thinking +  kagi search).</p>

<p>Lastly, we found that <strong>Kagi’s search engine seems to perform better at SimpleQA simply because our results are less noisy</strong>. We found many, many examples of benchmark tasks where the same model using Kagi Search as a backend outperformed other search engines, simply because Kagi Search either returned the relevant Wikipedia page higher, or because the other results were <strong>not polluting the model’s context window with more irrelevant data.</strong></p>

<p>This benchmark unwittingly showed us that Kagi Search is a better backend for LLM-based search than Google/Bing because we filter out the noise that confuses other models.</p>

<h2>Why we’re not aiming for high scores on public benchmarks</h2>

<p>There’s a large difference between a 91% score and a 95.4% score: the second is making half as many errors.</p>

<p>With that said, we analyzed the remaining SimpleQA tasks and found patterns we were uninterested in pursuing.</p>

<p>Some tasks have <strong>contemporaneous results from official sources that disagree with the benchmark answer.</strong> Some examples:</p>

<p>- The question <em>“How many degrees was the Rattler’s maximum vertical angle at Six Flags Fiesta Texas?”</em> has an answer of <em>“61 degrees”</em>, which is <a href="https://coasterpedia.net/wiki/Rattler_(Six_Flags_Fiesta_Texas)">what is found in coasterpedia</a> but <a href="https://www.sixflags.com/fiestatexas/attractions/iron-rattler">Six Flag’s own page reports 81 degrees</a>.</p>

<p>- <em>“What number is the painting The Meeting at Křížky in The Slav Epic?”</em> has the answer <em>“9”</em> which <a href="https://en.wikipedia.org/wiki/The_Slav_Epic">agrees with wikipedia</a> but the <a href="https://www.muchafoundation.org/en/gallery/browse-works/object/221">gallery hosting the epic disagrees - it’s #10</a></p>

<p>- <em>“What month and year did Canon launch the EOS R50?”</em> has an answer of <em>“April, 2023”</em> which <a href="https://en.wikipedia.org/wiki/Canon_EOS_R50">agrees with Wikipedia</a>  but <a href="https://global.canon/en/c-museum/product/dslr906.html">disagrees with the product page on Canon’s website</a>.</p>

<p>Some other examples would <strong>require bending ethical design principles to perform well on.</strong> Let’s take one example: the question <em>“What day, month, and year was the municipality of San Juan de Urabá, Antioquia, Colombia, founded?”</em> Has a stated answer of <em>“24 June 1896”.</em></p>

<p>At time of writing, this answer can only be found by models on the <a href="https://es.wikipedia.org/wiki/San_Juan_de_Urab%C3%A1">spanish language wikipedia page</a>. However, information on this page is conflicting:</p>

<p><img src="https://kagifeedback.org/assets/files/2025-11-20/1763664586-538582-6.png" alt="Spanish Wikipedia page for San Juan del Urabá showing conflicting founding dates - June 24, 1886 in main text versus June 24, 1896 in the information box, highlighted with red arrows"/></p>

<p>The correct answer could be found by crawling the <a href="https://web.archive.org/web/20151203205241/http://sanjuandeuraba-antioquia.gov.co/informacion_general.shtml">Internet Archive’s Wayback Machine page that is referenced</a>, but we doubt that the Internet Archive’s team would be enthused at the idea of LLMs being configured to aggressively crawl their archive.</p>

<p>Lastly, it’s important to remember that SimpleQA was made by specific researchers for one purpose. It is inherently infused with their personal biases, even if the initial researchers wrote it with the greatest care.</p>

<p>By trying to achieve a 100% score on this benchmark, we guarantee that our model would effectively shape itself to those biases. We’d rather build something that performs well at helping humans find what they search for than performing well at a set of artificial tasks.</p>

        </div></div>
  </body>
</html>
