<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://gabrielsimmer.com/blog/archiveteam-warrior-kubernetes">Original</a>
    <h1>Running ArchiveTeam&#39;s Warrior in Kubernetes</h1>
    
    <div id="readability-page-1" class="page"><div><section><p>The &#34;officially endorsed&#34; way of running the ArchiveTeam Warrior project is using one of the available appliance virtual machine images, which keeps itself up to date and &#34;just works&#34;. But virtual machines aren&#39;t (typically) my preferred method of running applications unless they have some special requirements (like persistant state) - instead I throw containers into my homelab Kubernetes cluster and see what happens. For a while, I ran the Warrior in a Proxmox virtual machine, following roughly <a href="https://i12bretro.github.io/tutorials/0387.html">this guide</a> to get it working. This did limit my ability to &#34;turn up&#34; my contributions to time-critical jobs though. When I was helping the archival effort for cohost, I could only run 6 concurrent jobs as that&#39;s the max a single Warrior instance can do. But once the cohost archival wrapped up, I shut it down and forgot about it.</p><p>Until recently, when they launched an effort to <a href="https://wiki.archiveteam.org/index.php/US_Government">archive US government related websites and resources</a>. I decided to pick it back up and see if I could run it in my cluster instead, and lo-and-behold, there was <a href="https://github.com/ArchiveTeam/warrior-dockerfile">a fairly decent starting point</a> available in one of their repositories. There&#39;s nothing particularly &#34;wrong&#34; with the Kubernetes manifests, although it does statically allocate a NodePort since the assumption you only run one instance. But that wouldn&#39;t do!</p><p>With some tinkering, I came up with <a href="https://git.gmem.ca/arch/infra/src/branch/trunk/kubernetes/archiveteam/Deployment-warrior.yaml">my own manifest</a>. There&#39;s a few key things - first, everything is configured using environment variables. Also, I explicitly mount an memory-based <code>emptyDir</code> for the data storage as without it the pods would frequently be evicted due to disk space usage. I have fairly small disks for my Kubernetes nodes (~16GiB), so opted instead to give it a dedicated volume. To go along with that there is a memory limit just above the <code>emptyDir</code> size limit, so if the volume fills up it&#39;ll just be OOMKilled and replaced. This does also technically make the Warrior faster since it&#39;s only writing to memory rather than reading and writing to and from a disk. Another key thing is the inclusion of an explicit nodeSelector. I wasn&#39;t able to build their container image for arm64 nicely (their lowest custom base image coredumps when cross-building) so opted instead to ensure it schedules only to my amd64 nodes.</p><p>You may note another deployment manifest in that directory. I wanted to get a quick overview of how the Warriors were progressing, and slapped together a quick Python script using the generated Kubernetes client. This is becoming one of my favourite ways of doing automated work with Kubernetes, rather than reaching for Golang and building a more fleshed out operator or binary (I initially experimented with it with an <a href="https://git.gmem.ca/arch/infra/src/branch/trunk/haproxy-updater">HAProxy &lt;-&gt; Kubernetes node sync script</a>). It just surfaces the pod info and metrics so I can get an idea of how it&#39;s performing memory-wise, but I do have an itch to try and pull in socket data from the ArchiveTeam&#39;s tracking pages/websocket.</p><p><img src="https://cdn.gabrielsimmer.com/images/Screenshot_20250204_171258.png" alt="https://cdn.gabrielsimmer.com/images/Screenshot_20250204_171258.png"/></p><p>It&#39;s very much a quick-and-dirty &#34;holy cow my hyperfocus has taken over&#34; job, but it does work, I promise!</p><p>If you use this as a starting point for helping with archival efforts, awesome! <a href="https://floofy.tech/@arch">Let me know, or send feedback</a>!</p><p><b>An update 2025-02-04 22:30UTC (ish)</b></p><p>After chatting with katia in the Kubernetes IRC around OOMKill behaviour and the in-memory cache volume, it looks like Kubernetes doesn&#39;t actually clear the in-memory volume when the OOMKill happens. This is contrary to my assumption! And very odd. But while chatting, a few things were pointed out. First, for each ArchiveTeam Warrior job, there is actually <a href="https://github.com/ArchiveTeam/usgovernment-grab">a =*-grab= container image</a>! I missed this initially, but it greatly lowers the time it takes to actually start archiving, and removes the web UI in favour of logging to stdout. katia also has <a href="https://github.com/iakat/kubernetes-archiveteam/">her own Kubernetes manifests for this</a> which are worth taking a look, splitting out each job into its own Kustomization.</p><p>Swapping to the <code>*-grab</code> images doesn&#39;t make my end goal of building a little operator/management UI around this much more difficult - if anything it makes it easier since it logs in a way that makes it easy to grab from the Kubernetes API.</p><p>I&#39;ve pushed <a href="https://git.gmem.ca/arch/infra/commit/7cd3d07db49bf85ffc926d89b939a90551db4eef">this commit</a> to my repository with these changes!</p></section></div></div>
  </body>
</html>
