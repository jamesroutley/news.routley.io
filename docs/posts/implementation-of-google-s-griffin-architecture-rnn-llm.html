<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/google-deepmind/recurrentgemma">Original</a>
    <h1>Implementation of Google&#39;s Griffin Architecture – RNN LLM</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">RecurrentGemma is a family of open-weights Language Models by <a href="https://deepmind.google/" rel="nofollow">Google DeepMind</a>, based on the novel <a href="https://arxiv.org/abs/2402.19427" rel="nofollow">Griffin architecture</a>. This architecture achieves fast inference when generating long sequences by replacing global attention with a mixture of local attention and linear recurrences.</p>
<p dir="auto">This repository contains the model implementation and examples for sampling and fine-tuning. We recommend most users adopt the <a href="https://github.com/google/flax">Flax</a> implementation, which is highly optimized. We also provide an un-optimized <a href="https://github.com/pytorch/pytorch">PyTorch</a> implementation for reference.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Learn more about RecurrentGemma</h3><a id="user-content-learn-more-about-recurrentgemma" aria-label="Permalink: Learn more about RecurrentGemma" href="#learn-more-about-recurrentgemma"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>The RecurrentGemma <a href="https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf" rel="nofollow">technical report</a> gives specific details on the training and evaluation of RecurrentGemma.</li>
<li>The <a href="https://arxiv.org/abs/2402.19427" rel="nofollow">Griffin paper</a> describes the underlying model architecture.</li>
</ul>



<p dir="auto">RecurrentGemma uses <a href="https://python-poetry.org/docs/" rel="nofollow">Poetry</a> for dependency
management.</p>
<p dir="auto">To install dependencies for the full project:</p>
<ul dir="auto">
<li>Checkout the code.</li>
<li><code>poetry install -E full</code> to create a virtual environment with all dependencies.</li>
<li><code>poetry shell</code> to activate the created virtual environment.</li>
</ul>
<p dir="auto">If you only need to install a subset of dependencies use one of the alternative
library-specific commands below.</p>

<p dir="auto">If you want to use <code>pip</code> instead of Poetry,
then create a virtual environment (run <code>python -m venv recurrentgemma-demo</code> and <code>. recurrentgemma-demo/bin/activate</code>) and:</p>
<ul dir="auto">
<li>Checkout the code.</li>
<li><code>pip install .[full]</code></li>
</ul>
<div dir="auto"><h4 tabindex="-1" dir="auto">Installing library-specific packages</h4><a id="user-content-installing-library-specific-packages" aria-label="Permalink: Installing library-specific packages" href="#installing-library-specific-packages"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>

<p dir="auto">To install dependencies only for the JAX pathway use:
<code>poetry install -E jax</code> or (<code>pip install .[jax]</code>).</p>

<p dir="auto">To install dependencies only for the PyTorch pathway use:
<code>poetry install -E torch</code> (or <code>pip install .[torch]</code>).</p>

<p dir="auto">To install dependencies required for running unit tests use:
<code>poetry install -E test</code> (or <code>pip install .[test]</code>)</p>

<p dir="auto">The model checkpoints are available through Kaggle at
<a href="http://kaggle.com/models/google/recurrentgemma" rel="nofollow">http://kaggle.com/models/google/recurrentgemma</a>.
Select either the <strong>Flax</strong> or <strong>PyTorch</strong> model variations, click the ⤓ button
to download the model archive, then extract the contents to a local directory.</p>
<p dir="auto">In both cases, the archive contains both the model weights and
the tokenizer.</p>

<p dir="auto">To run the tests, install the optional <code>[test]</code> dependencies (e.g. using <code>pip install .[test]</code>) from the root of the source tree, then:</p>


<p dir="auto">To run the example sampling script, pass the paths to the weights directory and tokenizer:</p>
<div data-snippet-clipboard-copy-content="python examples/sampling_jax.py \
  --path_checkpoint=/path/to/archive/contents/2b/ \
  --path_tokenizer=/path/to/archive/contents/tokenizer.model"><pre><code>python examples/sampling_jax.py \
  --path_checkpoint=/path/to/archive/contents/2b/ \
  --path_tokenizer=/path/to/archive/contents/tokenizer.model
</code></pre></div>

<ul dir="auto">
<li>
<p dir="auto"><a href="https://colab.sandbox.google.com/github/google-deepmind/recurrentgemma/blob/main/colabs/sampling_tutorial_jax.ipynb" rel="nofollow"><code>colabs/sampling_tutorial_jax.ipynb</code></a>
contains a <a href="http://colab.google" rel="nofollow">Colab</a> notebook with a sampling example using JAX.</p>
</li>
<li>
<p dir="auto"><a href="https://colab.sandbox.google.com/github/google-deepmind/recurrentgemma/blob/main/colabs/sampling_tutorial_pytorch.ipynb" rel="nofollow"><code>colabs/sampling_tutorial_pytorch.ipynb</code></a>
contains a <a href="http://colab.google" rel="nofollow">Colab</a> notebook with a sampling example using PyTorch.</p>
</li>
<li>
<p dir="auto"><a href="https://colab.sandbox.google.com/github/google-deepmind/recurrentgemma/blob/main/colabs/fine_tuning_tutorial_jax.ipynb" rel="nofollow"><code>colabs/fine_tuning_tutorial_jax.ipynb</code></a>
contains a <a href="http://colab.google" rel="nofollow">Colab</a> with a basic tutorial on how to
fine-tune RecurrentGemma for a task, such as English to French translation, using JAX.</p>
</li>
</ul>
<p dir="auto">To run these notebooks you will need to have a Kaggle account and first read and accept
the Gemma license terms and conditions from the <a href="http://kaggle.com/models/google/recurrentgemma" rel="nofollow">RecurrentGemma page</a>.
After this you can run the notebooks, which will automatically download the weights and tokenizer from there.</p>
<p dir="auto">Currently different notebooks are supported under the following hardware:</p>
<table>
<thead>
<tr>
<th>Hardware</th>
<th>T4</th>
<th>P100</th>
<th>V100</th>
<th>A100</th>
<th>TPUv2</th>
<th>TPUv3+</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sampling in Jax</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>Sampling in PyTorch</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>Finetuning in Jax</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>❌</td>
<td>✅</td>
</tr>
</tbody>
</table>

<p dir="auto">RecurrentGemma code can run on CPU, GPU or TPU.
The code has been optimized for running on TPU using the Flax implementation,
which contains a low level <a href="https://jax.readthedocs.io/en/latest/pallas/index.html" rel="nofollow">Pallas</a> kernel to perform the linear scan in the recurrent layers.</p>

<p dir="auto">We are open to bug reports and issues. Please see
<a href="https://github.com/google-deepmind/recurrentgemma/blob/main/CONTRIBUTING.md">CONTRIBUTING.md</a> for details on PRs.</p>

<p dir="auto">Copyright 2024 DeepMind Technologies Limited</p>
<p dir="auto">This code is licensed under the Apache License, Version 2.0 (the &#34;License&#34;);
you may not use this file except in compliance with the License. You may obtain
a copy of the License at <a href="http://www.apache.org/licenses/LICENSE-2.0" rel="nofollow">http://www.apache.org/licenses/LICENSE-2.0</a>.</p>
<p dir="auto">Unless required by applicable law or agreed to in writing, software distributed
under the License is distributed on an AS IS BASIS, WITHOUT WARRANTIES OR
CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.</p>

<p dir="auto">This is not an official Google product.</p>
</article></div></div>
  </body>
</html>
