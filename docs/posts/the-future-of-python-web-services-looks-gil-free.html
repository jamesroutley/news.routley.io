<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.baro.dev/p/the-future-of-python-web-services-looks-gil-free">Original</a>
    <h1>The future of Python web services looks GIL-free</h1>
    
    <div id="readability-page-1" class="page"><div>
        <p>Python 3.14 was released at the beginning of the month. This release was particularly interesting to me because of the improvements on the &#34;free-threaded&#34; variant of the interpreter.</p>

<p>Specifically, the two major changes when compared to the free-threaded variant of Python 3.13 are:</p>

<ul>
<li>Free-threaded support now reached <em>phase II</em>, meaning it&#39;s no longer considered experimental</li>
<li>The implementation is now completed, meaning that the <em>workarounds</em> introduced in Python 3.13 to make code sound without the GIL are now gone, and the free-threaded implementation now uses the <a href="https://peps.python.org/pep-0659/">adaptive interpreter</a> as the GIL enabled variant. These facts, plus additional optimizations make the performance penalty now way better, moving from a 35% penalty to a 5-10% difference.</li>
</ul>

<p>Miguel Grinberg put together <a href="https://blog.miguelgrinberg.com/post/python-3-14-is-here-how-fast-is-it">a nice article</a> about Python 3.14 performance, with sections dedicated to the free-threaded variant compared to the GIL one. His results show a compelling improvement in performance for Python 3.14t compared to 3.13t, which is compelling!</p>

<p>While his benchmarks focus on CPU-bound work, calculating the Fibonacci sequence and using the bubble sort algorithm, a huge part of my experience with Python is centered on web development – the main OSS projects I maintain are a web framework and a web server for Python, after all – and thus I wanted to make a proper comparison of free-threaded and GIL Python interpreters on web applications: even if 99.9999% of web services out there are I/O bound – interacting with a database or making requests to other services – concurrency is a key factor here, and we spent decades doing weird stuff with the <code>multiprocessing</code> module to do <em>more work</em> in parallel. Is this the time where we can finally stop wasting gigabytes of memory just to serve more than 1 request at time?</p>

<h2 id="benchmarks-are-hard">Benchmarks are hard</h2>

<p>Let&#39;s face it. We had hard times understanding benchmarks – especially when those benchmarks are around web technologies. The internet is full of discussions around them, with people arguing about every aspect of them: the methodology, the code, the environment. The most popular reactions to benchmarks are like &#34;but why you didn&#39;t tested also X&#34; or &#34;my app using that library doesn&#39;t scale like this, you&#39;re lying&#34;. I hear already these kind of comments on this article.</p>

<p>But this is because we tend to <em>generalise</em> over benchmarks, and we really shouldn&#39;t. To my perspective, a good benchmarks is very self-contained, testing a very small thing out of the actual – and much wider – context. And why is that? Because a good benchmark should reduce <em>noise</em> as much as possible. I&#39;m definitely not interested into the <em>framework X is faster than Y</em> war – also because those statements usually lack the <em>on what</em> part – nor I really care on having a very wide matrix of test cases.</p>

<p>I really just want to see if, taking <strong>one single</strong> ASGI web application and <strong>one single</strong> WSGI application, doing the same thing, we can spot differences between the standard 3.14 Python and its free-threaded variant, and make considerations based on those results. Please keep this in mind when looking at the numbers below.</p>

<h2 id="the-methodology">The methodology</h2>

<p>As mentioned before, the idea is to test the two main application protocols in Python – ASGI and WSGI – on Python 3.14 with the GIL enabled and disabled, keeping everything else fixed: the server, the code, the concurrency, the event loop.</p>

<p>Thus, I created an ASGI application using FastAPI and a WSGI application using Flask – why these frameworks? Just because they&#39;re the most popular – with two endpoints: a silly JSON response generator, and a fake I/O bound endpoint. Here is the code for the FastAPI version:</p>

<div>
<pre><span></span><code><span>import</span><span> </span><span>asyncio</span>
<span>from</span><span> </span><span>fastapi</span><span> </span><span>import</span> <span>FastAPI</span>
<span>from</span><span> </span><span>fastapi.responses</span><span> </span><span>import</span> <span>PlainTextResponse</span><span>,</span> <span>JSONResponse</span>

<span>app</span> <span>=</span> <span>FastAPI</span><span>()</span>

<span>@app</span><span>.</span><span>get</span><span>(</span><span>&#34;/json&#34;</span><span>)</span>
<span>async</span> <span>def</span><span> </span><span>json_data</span><span>():</span>
    <span>return</span> <span>JSONResponse</span><span>({</span><span>&#34;message&#34;</span><span>:</span> <span>&#34;Hello, world!&#34;</span><span>})</span>

<span>@app</span><span>.</span><span>get</span><span>(</span><span>&#34;/io&#34;</span><span>)</span>
<span>async</span> <span>def</span><span> </span><span>io_fake</span><span>():</span>
    <span>await</span> <span>asyncio</span><span>.</span><span>sleep</span><span>(</span><span>0.01</span><span>)</span>
    <span>return</span> <span>PlainTextResponse</span><span>(</span><span>b</span><span>&#34;Hello, waited 10ms&#34;</span><span>)</span>
</code></pre>
</div>

<p>and here&#39;s the code for the Flask version:</p>

<div>
<pre><span></span><code><span>import</span><span> </span><span>json</span>
<span>import</span><span> </span><span>time</span>
<span>import</span><span> </span><span>flask</span>

<span>app</span> <span>=</span> <span>flask</span><span>.</span><span>Flask</span><span>(</span><span>__name__</span><span>)</span>
<span>app</span><span>.</span><span>config</span><span>[</span><span>&#34;JSONIFY_PRETTYPRINT_REGULAR&#34;</span><span>]</span> <span>=</span> <span>False</span>

<span>@app</span><span>.</span><span>route</span><span>(</span><span>&#34;/json&#34;</span><span>)</span>
<span>def</span><span> </span><span>json_data</span><span>():</span>
    <span>return</span> <span>flask</span><span>.</span><span>jsonify</span><span>(</span><span>message</span><span>=</span><span>&#34;Hello, world!&#34;</span><span>)</span>

<span>@app</span><span>.</span><span>route</span><span>(</span><span>&#34;/io&#34;</span><span>)</span>
<span>def</span><span> </span><span>io_fake</span><span>():</span>
    <span>time</span><span>.</span><span>sleep</span><span>(</span><span>0.01</span><span>)</span>
    <span>response</span> <span>=</span> <span>flask</span><span>.</span><span>make_response</span><span>(</span><span>b</span><span>&#34;Hello, waited 10ms&#34;</span><span>)</span>
    <span>response</span><span>.</span><span>content_type</span> <span>=</span> <span>&#34;text/plain&#34;</span>
    <span>return</span> <span>response</span>
</code></pre>
</div>

<p>As you can see, the <em>fake I/O</em> endpoint wait for 10ms, as the idea is to simulate something like waiting for the database to return a query result. Yes, I know, I&#39;m ignoring all the serialization/deserialization part of communicating with the database here, and the JSON endpoint is not something you will have in an actual application, but – again – that&#39;s not the point here.</p>

<p>We then serve these applications using <a href="https://github.com/emmett-framework/granian">Granian</a> and spawn a bunch of requests using <code>rewrk</code>. Why Granian? Well, first I maintain the project, but also – and more importantly – is the only server I&#39;m aware of which uses threads in place of processes for running workers on free-threaded Python.</p>

<p>Everything is run on a single machine with the following specs:</p>

<ul>
<li>Gentoo Linux 6.12.47</li>
<li>AMD Ryzen 7 5700X</li>
<li>CPython 3.14 and 3.14t installed through <code>uv</code></li>
</ul>

<h2 id="asgi-benchmarks">ASGI benchmarks</h2>

<p>We run the FastAPI application both with 1 worker and 2 workers, with a concurrency of 128 in the first case and 256 in the latter. Here are the Granian and rewrk commands:</p>

<ul>
<li><code>granian --interface asgi --loop asyncio --workers {N} impl_fastapi:app</code></li>
<li><code>rewrk -d 30s -c {CONCURRENCY} --host http://127.0.0.1:8000/{ENDPOINT}</code></li>
</ul>

<h3 id="json-endpoint">JSON endpoint</h3>

<table>
<thead>
<tr>
  <th>Python</th>
  <th>workers</th>
  <th>RPS</th>
  <th>Latency avg</th>
  <th>Latency max</th>
  <th>CPU</th>
  <th>RAM</th>
</tr>
</thead>
<tbody>
<tr>
  <td>3.14</td>
  <td>1</td>
  <td>30415</td>
  <td>4.20ms</td>
  <td>45.29ms</td>
  <td>0.42</td>
  <td>90MB</td>
</tr>
<tr>
  <td>3.14t</td>
  <td>1</td>
  <td>24218</td>
  <td>5.27ms</td>
  <td>59.25ms</td>
  <td>0.80</td>
  <td>80MB</td>
</tr>
<tr>
  <td>3.14</td>
  <td>2</td>
  <td>59219</td>
  <td>4.32ms</td>
  <td>70.71ms</td>
  <td>1.47</td>
  <td>147MB</td>
</tr>
<tr>
  <td>3.14t</td>
  <td>2</td>
  <td>48446</td>
  <td>5.28ms</td>
  <td>68.17ms</td>
  <td>1.73</td>
  <td>90MB</td>
</tr>
</tbody>
</table>

<p>As we can see from the numbers, the free-threaded implementation is ~20% slower, but with the advantage of reduced memory usage.</p>

<h3 id="io-endpoint">I/O endpoint</h3>

<table>
<thead>
<tr>
  <th>Python</th>
  <th>workers</th>
  <th>RPS</th>
  <th>Latency avg</th>
  <th>Latency max</th>
  <th>CPU</th>
  <th>RAM</th>
</tr>
</thead>
<tbody>
<tr>
  <td>3.14</td>
  <td>1</td>
  <td>11333</td>
  <td>11.28ms</td>
  <td>40.72ms</td>
  <td>0.41</td>
  <td>90MB</td>
</tr>
<tr>
  <td>3.14t</td>
  <td>1</td>
  <td>11351</td>
  <td>11.26ms</td>
  <td>35.18ms</td>
  <td>0.38</td>
  <td>81MB</td>
</tr>
<tr>
  <td>3.14</td>
  <td>2</td>
  <td>22775</td>
  <td>11.22ms</td>
  <td>114.82ms</td>
  <td>0.69</td>
  <td>148MB</td>
</tr>
<tr>
  <td>3.14t</td>
  <td>2</td>
  <td>23473</td>
  <td>10.89ms</td>
  <td>60.29ms</td>
  <td>1.10</td>
  <td>91MB</td>
</tr>
</tbody>
</table>

<p>Here the two implementation are very similar in terms of throughput, with the free-threaded one slightly better. Once again, the free-threaded implementation has the advantage of consuming less memory.</p>

<h2 id="wsgi-benchmarks">WSGI benchmarks</h2>

<p>Running a WSGI application containing both a – very low – CPU bound endpoint and an I/O bound endpoint with the same configuration is much more complicated. Why? Because – on the GIL interpreter – while for CPU bound endpoints we want to avoid as much as possible GIL contention, and thus have as less threads as possible, for I/O bound workloads we want to have a decent amount of threads to do work while another request is waiting on I/O.</p>

<p>To clarify this point, let&#39;s see what happens to both endpoints on the GIL Python 3.14 when we use a single worker but different numbers of threads in Granian:</p>

<table>
<thead>
<tr>
  <th>endpoint</th>
  <th>threads</th>
  <th>RPS</th>
  <th>Latency avg</th>
  <th>Latency max</th>
</tr>
</thead>
<tbody>
<tr>
  <td>JSON</td>
  <td>1</td>
  <td>19377</td>
  <td>6.60ms</td>
  <td>28.35ms</td>
</tr>
<tr>
  <td>JSON</td>
  <td>8</td>
  <td>18704</td>
  <td>6.76ms</td>
  <td>25.82ms</td>
</tr>
<tr>
  <td>JSON</td>
  <td>32</td>
  <td>18639</td>
  <td>6.68ms</td>
  <td>33.91ms</td>
</tr>
<tr>
  <td>JSON</td>
  <td>128</td>
  <td>15547</td>
  <td>8.17ms</td>
  <td>3949.40ms</td>
</tr>
<tr>
  <td>I/O</td>
  <td>1</td>
  <td>94</td>
  <td>1263.59ms</td>
  <td>1357.80ms</td>
</tr>
<tr>
  <td>I/O</td>
  <td>8</td>
  <td>781</td>
  <td>161.99ms</td>
  <td>197.73ms</td>
</tr>
<tr>
  <td>I/O</td>
  <td>32</td>
  <td>3115</td>
  <td>40.82ms</td>
  <td>120.61ms</td>
</tr>
<tr>
  <td>I/O</td>
  <td>128</td>
  <td>11271</td>
  <td>11.28ms</td>
  <td>59.58ms</td>
</tr>
</tbody>
</table>

<p>As you can see, the more threads we add, the more close we get to the expected result on the I/O endpoint, but, at the same time, the JSON endpoint gets more and more worse. When deploying WSGI applications, a lot of time is usually put into finding the balance between GIL contention and proper parallelism. This is also why for the last 20 years people discussed a lot about the numbers of threads to use with WSGI applications – with several instances where we actually use completely empirical values like <code>2*CPU+1</code> – and also why gevent was a thing before asyncio.</p>

<p>On the free-threaded side of things, we can stop worry about this, as each thread can actually run code in parallel without waiting for the GIL, but we have a different thing to consider: should we increase the workers or the threads? At the end of the day workers are also threads, right? Let&#39;s experiment a bit with the JSON endpoint:</p>

<table>
<thead>
<tr>
  <th>workers</th>
  <th>threads</th>
  <th>RPS</th>
  <th>Latency avg</th>
  <th>Latency max</th>
</tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td>2</td>
  <td>28898</td>
  <td>4.42ms</td>
  <td>86.96ms</td>
</tr>
<tr>
  <td>2</td>
  <td>1</td>
  <td>28424</td>
  <td>4.49ms</td>
  <td>75.80ms</td>
</tr>
<tr>
  <td>1</td>
  <td>4</td>
  <td>54669</td>
  <td>2.33ms</td>
  <td>112.06ms</td>
</tr>
<tr>
  <td>4</td>
  <td>1</td>
  <td>53532</td>
  <td>2.38ms</td>
  <td>121.91ms</td>
</tr>
<tr>
  <td>2</td>
  <td>2</td>
  <td>55426</td>
  <td>2.30ms</td>
  <td>124.16ms</td>
</tr>
</tbody>
</table>

<p>It seems that increasing workers add some overhead – which makes sense – and the <em>sweet spot</em> is balancing the two depending on the workload. Given we still need a high thread count to support the I/O endpoint – in the same way Granian cannot understand when the application is waiting for I/O in the GIL implementation, it can&#39;t also in the free-threaded implementation – this doesn&#39;t really matter, but it was fun to observe.</p>

<p>Given all of the above, my approach is to run the Flask application both with a single worker and two of them, while keeping the amount of threads (per worker) fixed to 64. As for the ASGI benchmark, we use a concurrency of 128 in the first case and 256 in the second. Here are the commands:</p>

<ul>
<li><code>granian --interface wsgi --workers {N} --blocking-threads 64 impl_flask:app</code></li>
<li><code>rewrk -d 30s -c {CONCURRENCY} --host http://127.0.0.1:8000/{ENDPOINT}</code></li>
</ul>

<h3 id="json-endpoint-2">JSON endpoint</h3>

<table>
<thead>
<tr>
  <th>Python</th>
  <th>workers</th>
  <th>RPS</th>
  <th>Latency avg</th>
  <th>Latency max</th>
  <th>CPU</th>
  <th>RAM</th>
</tr>
</thead>
<tbody>
<tr>
  <td>3.14</td>
  <td>1</td>
  <td>18773</td>
  <td>6.11ms</td>
  <td>27446.19ms</td>
  <td>0.53</td>
  <td>101MB</td>
</tr>
<tr>
  <td>3.14t</td>
  <td>1</td>
  <td>70626</td>
  <td>1.81ms</td>
  <td>311.76ms</td>
  <td>6.50</td>
  <td>356MB</td>
</tr>
<tr>
  <td>3.14</td>
  <td>2</td>
  <td>36173</td>
  <td>5.73ms</td>
  <td>27692.21ms</td>
  <td>1.31</td>
  <td>188MB</td>
</tr>
<tr>
  <td>3.14t</td>
  <td>2</td>
  <td>60138</td>
  <td>4.25ms</td>
  <td>294.55ms</td>
  <td>6.56</td>
  <td>413MB</td>
</tr>
</tbody>
</table>

<p>For CPU-bound workloads, we cleary see the advantage of the free-threaded version in terms of throughput: it shouldn&#39;t surprise us given it can utilize a lot more cpu. The memory usage is way higher on the free-threaded version though; it&#39;s not clear from this benchmarks if it&#39;s just because of the higher concurrency, or the Python garbage collector operates less efficiently in this context.</p>

<h3 id="io-endpoint-2">I/O endpoint</h3>

<table>
<thead>
<tr>
  <th>Python</th>
  <th>workers</th>
  <th>RPS</th>
  <th>Latency avg</th>
  <th>Latency max</th>
  <th>CPU</th>
  <th>RAM</th>
</tr>
</thead>
<tbody>
<tr>
  <td>3.14</td>
  <td>1</td>
  <td>6282</td>
  <td>20.34ms</td>
  <td>62.28ms</td>
  <td>0.40</td>
  <td>105MB</td>
</tr>
<tr>
  <td>3.14t</td>
  <td>1</td>
  <td>6244</td>
  <td>20.47ms</td>
  <td>164.59ms</td>
  <td>0.42</td>
  <td>216MB</td>
</tr>
<tr>
  <td>3.14</td>
  <td>2</td>
  <td>12566</td>
  <td>20.33ms</td>
  <td>88.34ms</td>
  <td>0.65</td>
  <td>180MB</td>
</tr>
<tr>
  <td>3.14t</td>
  <td>2</td>
  <td>12444</td>
  <td>20.55ms</td>
  <td>124.06ms</td>
  <td>1.18</td>
  <td>286MB</td>
</tr>
</tbody>
</table>

<p>For I/O bound workloads, the two implementations are very similar. Once again, the memory usage on the free-threaded implementation is way higher than its counter-part.</p>

<h2 id="final-thoughts">Final thoughts</h2>

<p>While pure-Python code execution appears to be up to 20% slower on free-threaded Python 3.14, we can spot several advantages on the <em>free-threaded side of things</em>.</p>

<p>On asynchronous protocols like ASGI, despite the fact the concurrency model doesn&#39;t change that much – we shift from one event loop per process, to one event loop per thread – just the fact we no longer need to scale memory allocations just to use more CPU is a <em>massive improvement</em>. Even considering memory is cheap compared to CPUs, with modern hardware this can make a huge difference in cost both on large deployments and projects running on a single VM. You can now allocate <em>more stuff</em> on a single machine and scale once the CPU is overwhelmed, instead of worrying about how much RAM you need.</p>

<p>On the throughput difference, it might be also worth noting that all the above benchmarks used the stdlib asyncio event loop implementation, and projects like <a href="https://github.com/MagicStack/uvloop">uvloop</a> or <a href="https://github.com/gi0baro/rloop">rloop</a> might play a role in improving the throughput and latency down the road. But also: the latency in the above benchmarks is actually better for I/O bound workloads on the free-threaded implementation, and given, to quote DHH, we&#39;re all <em>CRUD monkeys</em>, and thus the vast majority of time our applications are just waiting for the database, it&#39;s also possible the free-threaded Python implementation is already better to use on ASGI applications, today.</p>

<p>On synchronous protocol like WSGI, you might have mixed feelings due to the memory usage, but it&#39;s absolutely possible at this stage that Granian just need some changes to improve the garbage collection on the Python side of things. If that&#39;s the case, WSGI is now fun again, and we can stop worrying about balancing threads, stop monkeypatching our applications to use gevent, stop planning that <em>let&#39;s migrate to asyncio</em> rewrite and just rely on the fact we don&#39;t need to worry anymore about blocking operations.</p>

<p>For people like me, managing thousands of ASGI and WSGI Python containers on a vast infrastructure – yes, I work at Sentry, in case you missed – free-threaded Python has the potential to be a huge <em>quality of life improvement</em>. But also for everybody out there coding a web application in Python: simplifying the concurrency paradigms and the deployment process of such applications is <em>a good thing</em>.</p>

<p>I&#39;m pretty sure the whole <em>gilectomy</em> thing wasn&#39;t planned with web applications in mind, and it might also take a while to get there, but to me the future of Python web services looks GIL-free.</p>

    </div></div>
  </body>
</html>
