<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://little-book-of.github.io/linear-algebra/books/en-US/lab.html">Original</a>
    <h1>Show HN: The Little Notebook for Learning Linear Algebra with Python</h1>
    
    <div id="readability-page-1" class="page"><div id="quarto-document-content">




<section id="chapter-1.-vectors-scalars-and-geometry">
<h2 data-anchor-id="chapter-1.-vectors-scalars-and-geometry">Chapter 1. Vectors, scalars, and geometry</h2>
<section id="scalars-vectors-and-coordinate-systems">
<h3 data-anchor-id="scalars-vectors-and-coordinate-systems">1. Scalars, Vectors, and Coordinate Systems</h3>
<p>Let’s get our hands dirty! This lab is about playing with the <em>building blocks</em> of linear algebra: scalars and vectors. Think of a scalar as just a plain number, like <code>3</code> or <code>-1.5</code>. A vector is a small list of numbers, which you can picture as an arrow in space.</p>
<p>We’ll use Python (with NumPy) to explore them. Don’t worry if this is your first time with NumPy - we’ll go slowly.</p>
<section id="set-up-your-lab">
<h4 data-anchor-id="set-up-your-lab">Set Up Your Lab</h4>

<p>That’s it - we’re ready! NumPy is the main tool we’ll use for linear algebra.</p>
</section>
<section id="step-by-step-code-walkthrough">
<h4 data-anchor-id="step-by-step-code-walkthrough">Step-by-Step Code Walkthrough</h4>
<p>Scalars are just numbers.</p>
<div id="8def0671" data-execution_count="2">
<div><div id="cb2"><pre><code><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>a <span>=</span> <span>5</span>       <span># a scalar</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>b <span>=</span> <span>-</span><span>2.5</span>    <span># another scalar</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(a <span>+</span> b)   <span># add them</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(a <span>*</span> b)   <span># multiply them</span></span></code></pre></div></div>

</div>
<p>Vectors are lists of numbers.</p>
<div id="846e01a0" data-execution_count="3">
<div><div id="cb4"><pre><code><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.array([<span>2</span>, <span>3</span>])      <span># a vector in 2D</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>w <span>=</span> np.array([<span>1</span>, <span>-</span><span>1</span>, <span>4</span>])  <span># a vector in 3D</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(v)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(w)</span></code></pre></div></div>

</div>
<p>Coordinates tell us where we are. Think of <code>[2, 3]</code> as “go 2 steps in the x-direction, 3 steps in the y-direction.”</p>
<p>We can even <em>draw</em> it:</p>
<div id="87e544fb" data-execution_count="4">
<div><div id="cb6"><pre><code><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span># plot vector v</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>, <span>0</span>, v[<span>0</span>], v[<span>1</span>], angles<span>=</span><span>&#39;xy&#39;</span>, scale_units<span>=</span><span>&#39;xy&#39;</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>&#39;r&#39;</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span>0</span>, <span>4</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span>0</span>, <span>4</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-5-output-1.png" width="581" height="416"/></p>
</figure>
</div>
</div>
</div>
<p>This makes a little arrow from the origin <code>(0,0)</code> to <code>(2,3)</code>.</p>
</section>
<section id="try-it-yourself">
<h4 data-anchor-id="try-it-yourself">Try It Yourself</h4>
<ol type="1">
<li>Change the vector <code>v</code> to <code>[4, 1]</code>. Where does the arrow point now?</li>
<li>Try making a 3D vector with 4 numbers, like <code>[1, 2, 3, 4]</code>. What happens?</li>
<li>Replace <code>np.array([2,3])</code> with <code>np.array([0,0])</code>. What does the arrow look like?</li>
</ol>
</section>
</section>
<section id="vector-notation-components-and-arrows">
<h3 data-anchor-id="vector-notation-components-and-arrows">2. Vector Notation, Components, and Arrows</h3>
<p>In this lab, we’ll practice reading, writing, and visualizing vectors in different ways. A vector can look simple at first - just a list of numbers - but how we <em>write</em> it and how we <em>interpret</em> it really matters. This is where notation and components come into play.</p>
<p>A vector has:</p>
<ul>
<li>A symbol (we might call it <code>v</code>, <code>w</code>, or even <code>→AB</code> in geometry).</li>
<li>Components (the individual numbers, like <code>2</code> and <code>3</code> in <code>[2, 3]</code>).</li>
<li>An arrow picture (a geometric way to see the vector as a directed line segment).</li>
</ul>
<p>Let’s see all three in action with Python.</p>
<section id="set-up-your-lab-1">
<h4 data-anchor-id="set-up-your-lab-1">Set Up Your Lab</h4>
<div id="ed61dc56" data-execution_count="5">
<div><div id="cb7"><pre><code><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-1">
<h4 data-anchor-id="step-by-step-code-walkthrough-1">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Writing vectors in Python</li>
</ol>
<div id="ac675ca4" data-execution_count="6">
<div><div id="cb8"><pre><code><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span># Two-dimensional vector</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.array([<span>2</span>, <span>3</span>])</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span># Three-dimensional vector</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>w <span>=</span> np.array([<span>1</span>, <span>-</span><span>1</span>, <span>4</span>])</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;v =&#34;</span>, v)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;w =&#34;</span>, w)</span></code></pre></div></div>

</div>
<p>Here <code>v</code> has components <code>(2, 3)</code> and <code>w</code> has components <code>(1, -1, 4)</code>.</p>
<ol start="2" type="1">
<li>Accessing components Each number in the vector is a <em>component</em>. We can pick them out using indexing.</li>
</ol>
<div id="4dfb373e" data-execution_count="7">
<div><div id="cb10"><pre><code><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;First component of v:&#34;</span>, v[<span>0</span>])</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Second component of v:&#34;</span>, v[<span>1</span>])</span></code></pre></div></div>
<div>
<pre><code>First component of v: 2
Second component of v: 3</code></pre>
</div>
</div>
<p>Notice: in Python, indices start at <code>0</code>, so <code>v[0]</code> is the <em>first</em> component.</p>
<ol start="3" type="1">
<li>Visualizing vectors as arrows In 2D, it’s easy to draw a vector from the origin <code>(0,0)</code> to its endpoint <code>(x,y)</code>.</li>
</ol>
<div id="925b4653" data-execution_count="8">
<div><div id="cb12"><pre><code><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>, <span>0</span>, v[<span>0</span>], v[<span>1</span>], angles<span>=</span><span>&#39;xy&#39;</span>, scale_units<span>=</span><span>&#39;xy&#39;</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>&#39;r&#39;</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span>-</span><span>1</span>, <span>4</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span>-</span><span>2</span>, <span>4</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span>0</span>, color<span>=</span><span>&#39;black&#39;</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span>0</span>, color<span>=</span><span>&#39;black&#39;</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-9-output-1.png" width="573" height="416"/></p>
</figure>
</div>
</div>
</div>
<p>This shows vector v as a red arrow from <code>(0,0)</code> to <code>(2,3)</code>.</p>
<ol start="4" type="1">
<li>Drawing multiple vectors We can plot several arrows at once to compare them.</li>
</ol>
<div id="7ce280f5" data-execution_count="9">
<div><div id="cb13"><pre><code><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>u <span>=</span> np.array([<span>3</span>, <span>1</span>])</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>z <span>=</span> np.array([<span>-</span><span>1</span>, <span>2</span>])</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span># Draw v, u, z in different colors</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>, <span>0</span>, v[<span>0</span>], v[<span>1</span>], angles<span>=</span><span>&#39;xy&#39;</span>, scale_units<span>=</span><span>&#39;xy&#39;</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>&#39;r&#39;</span>, label<span>=</span><span>&#39;v&#39;</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>, <span>0</span>, u[<span>0</span>], u[<span>1</span>], angles<span>=</span><span>&#39;xy&#39;</span>, scale_units<span>=</span><span>&#39;xy&#39;</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>&#39;b&#39;</span>, label<span>=</span><span>&#39;u&#39;</span>)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>, <span>0</span>, z[<span>0</span>], z[<span>1</span>], angles<span>=</span><span>&#39;xy&#39;</span>, scale_units<span>=</span><span>&#39;xy&#39;</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>&#39;g&#39;</span>, label<span>=</span><span>&#39;z&#39;</span>)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span>-</span><span>2</span>, <span>4</span>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span>-</span><span>2</span>, <span>4</span>)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span>0</span>, color<span>=</span><span>&#39;black&#39;</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span>0</span>, color<span>=</span><span>&#39;black&#39;</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-10-output-1.png" width="573" height="416"/></p>
</figure>
</div>
</div>
</div>
<p>Now you’ll see three arrows starting at the same point, each pointing in a different direction.</p>
</section>
<section id="try-it-yourself-1">
<h4 data-anchor-id="try-it-yourself-1">Try It Yourself</h4>
<ol type="1">
<li>Change <code>v</code> to <code>[5, 0]</code>. What does the arrow look like now?</li>
<li>Try a vector like <code>[0, -3]</code>. Which axis does it line up with?</li>
<li>Make a new vector <code>q = np.array([2, 0, 0])</code>. What happens if you try to plot it with <code>plt.quiver</code> in 2D?</li>
</ol>
</section>
</section>
<section id="vector-addition-and-scalar-multiplication">
<h3 data-anchor-id="vector-addition-and-scalar-multiplication">3. Vector Addition and Scalar Multiplication</h3>
<p>In this lab, we’ll explore the two most fundamental operations you can perform with vectors: adding them together and scaling them by a number (a scalar). These operations form the basis of everything else in linear algebra, from geometry to machine learning. Understanding how they work, both in code and visually, is key to building intuition.</p>
<section id="set-up-your-lab-2">
<h4 data-anchor-id="set-up-your-lab-2">Set Up Your Lab</h4>
<div id="1d4a49d6" data-execution_count="10">
<div><div id="cb14"><pre><code><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-2">
<h4 data-anchor-id="step-by-step-code-walkthrough-2">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Vector addition When you add two vectors, you simply add their components one by one.</li>
</ol>
<div id="dc42f49c" data-execution_count="11">
<div><div id="cb15"><pre><code><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.array([<span>2</span>, <span>3</span>])</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>u <span>=</span> np.array([<span>1</span>, <span>-</span><span>1</span>])</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>sum_vector <span>=</span> v <span>+</span> u</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;v + u =&#34;</span>, sum_vector)</span></code></pre></div></div>

</div>
<p>Here, <code>(2,3) + (1,-1) = (3,2)</code>.</p>
<ol start="2" type="1">
<li>Visualizing vector addition (tip-to-tail method) Graphically, vector addition means placing the tail of one vector at the head of the other. The resulting vector goes from the start of the first to the end of the second.</li>
</ol>
<div id="bad2574c" data-execution_count="12">
<div><div id="cb17"><pre><code><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>, <span>0</span>, v[<span>0</span>], v[<span>1</span>], angles<span>=</span><span>&#39;xy&#39;</span>, scale_units<span>=</span><span>&#39;xy&#39;</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>&#39;r&#39;</span>, label<span>=</span><span>&#39;v&#39;</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>plt.quiver(v[<span>0</span>], v[<span>1</span>], u[<span>0</span>], u[<span>1</span>], angles<span>=</span><span>&#39;xy&#39;</span>, scale_units<span>=</span><span>&#39;xy&#39;</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>&#39;b&#39;</span>, label<span>=</span><span>&#39;u placed at end of v&#39;</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>, <span>0</span>, sum_vector[<span>0</span>], sum_vector[<span>1</span>], angles<span>=</span><span>&#39;xy&#39;</span>, scale_units<span>=</span><span>&#39;xy&#39;</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>&#39;g&#39;</span>, label<span>=</span><span>&#39;v + u&#39;</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span>-</span><span>1</span>, <span>5</span>)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span>-</span><span>2</span>, <span>5</span>)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span>0</span>, color<span>=</span><span>&#39;black&#39;</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span>0</span>, color<span>=</span><span>&#39;black&#39;</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-13-output-1.png" width="573" height="416"/></p>
</figure>
</div>
</div>
</div>
<p>The green arrow is the result of adding <code>v</code> and <code>u</code>.</p>
<ol start="3" type="1">
<li>Scalar multiplication Multiplying a vector by a scalar stretches or shrinks it. If the scalar is negative, the vector flips direction.</li>
</ol>
<div id="785c4156" data-execution_count="13">
<div><div id="cb18"><pre><code><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>c <span>=</span> <span>2</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>scaled_v <span>=</span> c <span>*</span> v</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;2 * v =&#34;</span>, scaled_v)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>d <span>=</span> <span>-</span><span>1</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>scaled_v_neg <span>=</span> d <span>*</span> v</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;-1 * v =&#34;</span>, scaled_v_neg)</span></code></pre></div></div>
<div>
<pre><code>2 * v = [4 6]
-1 * v = [-2 -3]</code></pre>
</div>
</div>
<p>So <code>2 * (2,3) = (4,6)</code> and <code>-1 * (2,3) = (-2,-3)</code>.</p>
<ol start="4" type="1">
<li>Visualizing scalar multiplication</li>
</ol>
<div id="3a2ffac7" data-execution_count="14">
<div><div id="cb20"><pre><code><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>, <span>0</span>, v[<span>0</span>], v[<span>1</span>], angles<span>=</span><span>&#39;xy&#39;</span>, scale_units<span>=</span><span>&#39;xy&#39;</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>&#39;r&#39;</span>, label<span>=</span><span>&#39;v&#39;</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>, <span>0</span>, scaled_v[<span>0</span>], scaled_v[<span>1</span>], angles<span>=</span><span>&#39;xy&#39;</span>, scale_units<span>=</span><span>&#39;xy&#39;</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>&#39;b&#39;</span>, label<span>=</span><span>&#39;2 * v&#39;</span>)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>, <span>0</span>, scaled_v_neg[<span>0</span>], scaled_v_neg[<span>1</span>], angles<span>=</span><span>&#39;xy&#39;</span>, scale_units<span>=</span><span>&#39;xy&#39;</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>&#39;g&#39;</span>, label<span>=</span><span>&#39;-1 * v&#39;</span>)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span>-</span><span>5</span>, <span>5</span>)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span>-</span><span>5</span>, <span>7</span>)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span>0</span>, color<span>=</span><span>&#39;black&#39;</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span>0</span>, color<span>=</span><span>&#39;black&#39;</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-15-output-1.png" width="569" height="411"/></p>
</figure>
</div>
</div>
</div>
<p>Here, the blue arrow is twice as long as the red arrow, while the green arrow points in the opposite direction.</p>
<ol start="5" type="1">
<li>Combining both operations We can scale vectors and then add them. This is called a linear combination (and it’s the foundation for the next section).</li>
</ol>
<div id="dbfa1cd3" data-execution_count="15">
<div><div id="cb21"><pre><code><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>combo <span>=</span> <span>3</span><span>*</span>v <span>+</span> (<span>-</span><span>2</span>)<span>*</span>u</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;3*v - 2*u =&#34;</span>, combo)</span></code></pre></div></div>

</div>
</section>
<section id="try-it-yourself-2">
<h4 data-anchor-id="try-it-yourself-2">Try It Yourself</h4>
<ol type="1">
<li>Replace <code>c = 2</code> with <code>c = 0.5</code>. What happens to the vector?</li>
<li>Try adding three vectors: <code>v + u + np.array([-1,2])</code>. Can you predict the result before printing?</li>
<li>Visualize <code>3*v + 2*u</code> using arrows. How does it compare to just <code>v + u</code>?</li>
</ol>
</section>
</section>
<section id="linear-combinations-and-span">
<h3 data-anchor-id="linear-combinations-and-span">4. Linear Combinations and Span</h3>
<p>Now that we know how to add vectors and scale them, we can combine these two moves to create linear combinations. A linear combination is just a recipe: multiply vectors by scalars, then add them together. The set of all possible results you can get from such recipes is called the span.</p>
<p>This idea is powerful because span tells us what directions and regions of space we can reach using given vectors.</p>
<section id="set-up-your-lab-3">
<h4 data-anchor-id="set-up-your-lab-3">Set Up Your Lab</h4>
<div id="42faf404" data-execution_count="16">
<div><div id="cb23"><pre><code><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-3">
<h4 data-anchor-id="step-by-step-code-walkthrough-3">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Linear combinations in Python</li>
</ol>
<div id="3cbd9e33" data-execution_count="17">
<div><div id="cb24"><pre><code><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.array([<span>2</span>, <span>1</span>])</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>u <span>=</span> np.array([<span>1</span>, <span>3</span>])</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>combo1 <span>=</span> <span>2</span><span>*</span>v <span>+</span> <span>3</span><span>*</span>u</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>combo2 <span>=</span> <span>-</span><span>1</span><span>*</span>v <span>+</span> <span>4</span><span>*</span>u</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;2*v + 3*u =&#34;</span>, combo1)</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;-v + 4*u =&#34;</span>, combo2)</span></code></pre></div></div>
<div>
<pre><code>2*v + 3*u = [ 7 11]
-v + 4*u = [ 2 11]</code></pre>
</div>
</div>
<p>Here, we multiplied and added vectors using scalars. Each result is a new vector.</p>
<ol start="2" type="1">
<li>Visualizing linear combinations Let’s plot <code>v</code>, <code>u</code>, and their combinations.</li>
</ol>
<div id="1f4b0ecb" data-execution_count="18">
<div><div id="cb26"><pre><code><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>, <span>0</span>, v[<span>0</span>], v[<span>1</span>], angles<span>=</span><span>&#39;xy&#39;</span>, scale_units<span>=</span><span>&#39;xy&#39;</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>&#39;r&#39;</span>, label<span>=</span><span>&#39;v&#39;</span>)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>, <span>0</span>, u[<span>0</span>], u[<span>1</span>], angles<span>=</span><span>&#39;xy&#39;</span>, scale_units<span>=</span><span>&#39;xy&#39;</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>&#39;b&#39;</span>, label<span>=</span><span>&#39;u&#39;</span>)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>, <span>0</span>, combo1[<span>0</span>], combo1[<span>1</span>], angles<span>=</span><span>&#39;xy&#39;</span>, scale_units<span>=</span><span>&#39;xy&#39;</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>&#39;g&#39;</span>, label<span>=</span><span>&#39;2v + 3u&#39;</span>)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>, <span>0</span>, combo2[<span>0</span>], combo2[<span>1</span>], angles<span>=</span><span>&#39;xy&#39;</span>, scale_units<span>=</span><span>&#39;xy&#39;</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>&#39;m&#39;</span>, label<span>=</span><span>&#39;-v + 4u&#39;</span>)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span>-</span><span>5</span>, <span>10</span>)</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span>-</span><span>5</span>, <span>10</span>)</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span>0</span>, color<span>=</span><span>&#39;black&#39;</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span>0</span>, color<span>=</span><span>&#39;black&#39;</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-19-output-1.png" width="577" height="416"/></p>
</figure>
</div>
</div>
</div>
<p>This shows how new arrows can be generated from scaling and adding the original ones.</p>
<ol start="3" type="1">
<li>Exploring the span The span of two 2D vectors is either:</li>
</ol>
<ul>
<li>A line (if one is a multiple of the other).</li>
<li>The whole 2D plane (if they are independent).</li>
</ul>
<div id="63bfdd3f" data-execution_count="19">
<div><div id="cb27"><pre><code><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span># Generate many combinations</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>coeffs <span>=</span> <span>range</span>(<span>-</span><span>5</span>, <span>6</span>)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>points <span>=</span> []</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span>for</span> a <span>in</span> coeffs:</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    <span>for</span> b <span>in</span> coeffs:</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>        point <span>=</span> a<span>*</span>v <span>+</span> b<span>*</span>u</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>        points.append(point)</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>points <span>=</span> np.array(points)</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>plt.scatter(points[:,<span>0</span>], points[:,<span>1</span>], s<span>=</span><span>10</span>, color<span>=</span><span>&#39;gray&#39;</span>)</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>, <span>0</span>, v[<span>0</span>], v[<span>1</span>], angles<span>=</span><span>&#39;xy&#39;</span>, scale_units<span>=</span><span>&#39;xy&#39;</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>&#39;r&#39;</span>)</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>, <span>0</span>, u[<span>0</span>], u[<span>1</span>], angles<span>=</span><span>&#39;xy&#39;</span>, scale_units<span>=</span><span>&#39;xy&#39;</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>&#39;b&#39;</span>)</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span>-</span><span>10</span>, <span>10</span>)</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span>-</span><span>10</span>, <span>10</span>)</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span>0</span>, color<span>=</span><span>&#39;black&#39;</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span>0</span>, color<span>=</span><span>&#39;black&#39;</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-20-output-1.png" width="605" height="416"/></p>
</figure>
</div>
</div>
</div>
<p>The gray dots show all reachable points with combinations of <code>v</code> and <code>u</code>.</p>
<ol start="4" type="1">
<li>Special case: dependent vectors</li>
</ol>
<div id="ecbce6ba" data-execution_count="20">
<div><div id="cb28"><pre><code><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>w <span>=</span> np.array([<span>4</span>, <span>2</span>])  <span># notice w = 2*v</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>coeffs <span>=</span> <span>range</span>(<span>-</span><span>5</span>, <span>6</span>)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>points <span>=</span> []</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span>for</span> a <span>in</span> coeffs:</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>    <span>for</span> b <span>in</span> coeffs:</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>        points.append(a<span>*</span>v <span>+</span> b<span>*</span>w)</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>points <span>=</span> np.array(points)</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>plt.scatter(points[:,<span>0</span>], points[:,<span>1</span>], s<span>=</span><span>10</span>, color<span>=</span><span>&#39;gray&#39;</span>)</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>, <span>0</span>, v[<span>0</span>], v[<span>1</span>], angles<span>=</span><span>&#39;xy&#39;</span>, scale_units<span>=</span><span>&#39;xy&#39;</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>&#39;r&#39;</span>)</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>, <span>0</span>, w[<span>0</span>], w[<span>1</span>], angles<span>=</span><span>&#39;xy&#39;</span>, scale_units<span>=</span><span>&#39;xy&#39;</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>&#39;b&#39;</span>)</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span>-</span><span>10</span>, <span>10</span>)</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span>-</span><span>10</span>, <span>10</span>)</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span>0</span>, color<span>=</span><span>&#39;black&#39;</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span>0</span>, color<span>=</span><span>&#39;black&#39;</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-21-output-1.png" width="605" height="416"/></p>
</figure>
</div>
</div>
</div>
<p>Here, the span collapses to a line because <code>w</code> is just a scaled copy of <code>v</code>.</p>
</section>
<section id="try-it-yourself-3">
<h4 data-anchor-id="try-it-yourself-3">Try It Yourself</h4>
<ol type="1">
<li>Replace <code>u = [1,3]</code> with <code>u = [-1,2]</code>. What does the span look like?</li>
<li>Try three vectors in 2D (e.g., <code>v, u, w</code>). Do you get more than the whole plane?</li>
<li>Experiment with 3D vectors. Use <code>np.array([x,y,z])</code> and check whether different vectors span a plane or all of space.</li>
</ol>
</section>
</section>
<section id="length-norm-and-distance">
<h3 data-anchor-id="length-norm-and-distance">5. Length (Norm) and Distance</h3>
<p>In this lab, we’ll measure how big a vector is (its length, also called its norm) and how far apart two vectors are (their distance). These ideas connect algebra to geometry: when we compute a norm, we’re measuring the size of an arrow; when we compute a distance, we’re measuring the gap between two points in space.</p>
<section id="set-up-your-lab-4">
<h4 data-anchor-id="set-up-your-lab-4">Set Up Your Lab</h4>
<div id="3b0770ab" data-execution_count="21">
<div><div id="cb29"><pre><code><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-4">
<h4 data-anchor-id="step-by-step-code-walkthrough-4">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Vector length (norm) in 2D The length of a vector is computed using the Pythagorean theorem. For a vector <code>(x, y)</code>, the length is <code>sqrt(x² + y²)</code>.</li>
</ol>
<div id="5fcb476e" data-execution_count="22">
<div><div id="cb30"><pre><code><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.array([<span>3</span>, <span>4</span>])</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>length <span>=</span> np.linalg.norm(v)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Length of v =&#34;</span>, length)</span></code></pre></div></div>

</div>
<p>This prints <code>5.0</code>, because <code>(3,4)</code> forms a right triangle with sides 3 and 4, and <code>sqrt(3²+4²)=5</code>.</p>
<ol start="2" type="1">
<li>Manual calculation vs NumPy</li>
</ol>
<div id="be0dc49c" data-execution_count="23">
<div><div id="cb32"><pre><code><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>manual_length <span>=</span> (v[<span>0</span>]<span>**</span><span>2</span> <span>+</span> v[<span>1</span>]<span>**</span><span>2</span>)<span>**</span><span>0.5</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Manual length =&#34;</span>, manual_length)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;NumPy length =&#34;</span>, np.linalg.norm(v))</span></code></pre></div></div>
<div>
<pre><code>Manual length = 5.0
NumPy length = 5.0</code></pre>
</div>
</div>
<p>Both give the same result.</p>
<ol start="3" type="1">
<li>Visualizing vector length</li>
</ol>
<div id="62751078" data-execution_count="24">
<div><div id="cb34"><pre><code><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>, <span>0</span>, v[<span>0</span>], v[<span>1</span>], angles<span>=</span><span>&#39;xy&#39;</span>, scale_units<span>=</span><span>&#39;xy&#39;</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>&#39;r&#39;</span>)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span>0</span>, <span>5</span>)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span>0</span>, <span>5</span>)</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span>0</span>, color<span>=</span><span>&#39;black&#39;</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span>0</span>, color<span>=</span><span>&#39;black&#39;</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>plt.text(v[<span>0</span>]<span>/</span><span>2</span>, v[<span>1</span>]<span>/</span><span>2</span>, <span>f&#34;Length=</span><span>{</span>length<span>}</span><span>&#34;</span>, fontsize<span>=</span><span>10</span>, color<span>=</span><span>&#39;blue&#39;</span>)</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-25-output-1.png" width="562" height="416"/></p>
</figure>
</div>
</div>
</div>
<p>You’ll see the arrow <code>(3,4)</code> with its length labeled.</p>
<ol start="4" type="1">
<li>Distance between two vectors The distance between <code>v</code> and another vector <code>u</code> is the length of their difference: <code>‖v - u‖</code>.</li>
</ol>
<div id="2a74f2c7" data-execution_count="25">
<div><div id="cb35"><pre><code><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>u <span>=</span> np.array([<span>0</span>, <span>0</span>])   <span># the origin</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>dist <span>=</span> np.linalg.norm(v <span>-</span> u)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Distance between v and u =&#34;</span>, dist)</span></code></pre></div></div>
<div>
<pre><code>Distance between v and u = 5.0</code></pre>
</div>
</div>
<p>Since <code>u</code> is the origin, this is just the length of <code>v</code>.</p>
<ol start="5" type="1">
<li>A more interesting distance</li>
</ol>
<div id="4d218420" data-execution_count="26">
<div><div id="cb37"><pre><code><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>u <span>=</span> np.array([<span>1</span>, <span>1</span>])</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>dist <span>=</span> np.linalg.norm(v <span>-</span> u)</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Distance between v and u =&#34;</span>, dist)</span></code></pre></div></div>
<div>
<pre><code>Distance between v and u = 3.605551275463989</code></pre>
</div>
</div>
<p>This measures how far <code>(3,4)</code> is from <code>(1,1)</code>.</p>
<ol start="6" type="1">
<li>Visualizing distance between points</li>
</ol>
<div id="d3c95c45" data-execution_count="27">
<div><div id="cb39"><pre><code><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>plt.scatter([v[<span>0</span>], u[<span>0</span>]], [v[<span>1</span>], u[<span>1</span>]], color<span>=</span>[<span>&#39;red&#39;</span>,<span>&#39;blue&#39;</span>])</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>plt.plot([v[<span>0</span>], u[<span>0</span>]], [v[<span>1</span>], u[<span>1</span>]], <span>&#39;k--&#39;</span>)</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>plt.text(v[<span>0</span>], v[<span>1</span>], <span>&#39;v&#39;</span>, fontsize<span>=</span><span>12</span>, color<span>=</span><span>&#39;red&#39;</span>)</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>plt.text(u[<span>0</span>], u[<span>1</span>], <span>&#39;u&#39;</span>, fontsize<span>=</span><span>12</span>, color<span>=</span><span>&#39;blue&#39;</span>)</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-28-output-1.png" width="571" height="411"/></p>
</figure>
</div>
</div>
</div>
<p>The dashed line shows the distance between the two points.</p>
<ol start="7" type="1">
<li>Higher-dimensional vectors Norms and distances work the same way in any dimension:</li>
</ol>
<div id="0a3ac301" data-execution_count="28">
<div><div id="cb40"><pre><code><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>a <span>=</span> np.array([<span>1</span>,<span>2</span>,<span>3</span>])</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>b <span>=</span> np.array([<span>4</span>,<span>0</span>,<span>8</span>])</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;‖a‖ =&#34;</span>, np.linalg.norm(a))</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;‖b‖ =&#34;</span>, np.linalg.norm(b))</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Distance between a and b =&#34;</span>, np.linalg.norm(a<span>-</span>b))</span></code></pre></div></div>
<div>
<pre><code>‖a‖ = 3.7416573867739413
‖b‖ = 8.94427190999916
Distance between a and b = 6.164414002968976</code></pre>
</div>
</div>
<p>Even though we can’t draw 3D easily on paper, the formulas still apply.</p>
</section>
<section id="try-it-yourself-4">
<h4 data-anchor-id="try-it-yourself-4">Try It Yourself</h4>
<ol type="1">
<li>Compute the length of <code>np.array([5,12])</code>. What do you expect?</li>
<li>Find the distance between <code>(2,3)</code> and <code>(7,7)</code>. Can you sketch it by hand and check?</li>
<li>In 3D, try vectors <code>(1,1,1)</code> and <code>(2,2,2)</code>. Why is the distance exactly <code>sqrt(3)</code>?</li>
</ol>
</section>
</section>
<section id="dot-product">
<h3 data-anchor-id="dot-product">6. Dot Product</h3>
<p>The dot product is one of the most important operations in linear algebra. It takes two vectors and gives you a single number. That number combines both the lengths of the vectors and how much they point in the same direction. In this lab, we’ll calculate dot products in several ways, see how they relate to geometry, and visualize their meaning.</p>
<section id="set-up-your-lab-5">
<h4 data-anchor-id="set-up-your-lab-5">Set Up Your Lab</h4>
<div id="4a818a9a" data-execution_count="29">
<div><div id="cb42"><pre><code><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-5">
<h4 data-anchor-id="step-by-step-code-walkthrough-5">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Algebraic definition The dot product of two vectors is the sum of the products of their components:</li>
</ol>
<div id="e5c2b122" data-execution_count="30">
<div><div id="cb43"><pre><code><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.array([<span>2</span>, <span>3</span>])</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>u <span>=</span> np.array([<span>4</span>, <span>-</span><span>1</span>])</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>dot_manual <span>=</span> v[<span>0</span>]<span>*</span>u[<span>0</span>] <span>+</span> v[<span>1</span>]<span>*</span>u[<span>1</span>]</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>dot_numpy <span>=</span> np.dot(v, u)</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Manual dot product:&#34;</span>, dot_manual)</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;NumPy dot product:&#34;</span>, dot_numpy)</span></code></pre></div></div>
<div>
<pre><code>Manual dot product: 5
NumPy dot product: 5</code></pre>
</div>
</div>
<p>Here, <code>(2*4) + (3*-1) = 8 - 3 = 5</code>.</p>
<ol start="2" type="1">
<li>Geometric definition The dot product also equals the product of the lengths of the vectors times the cosine of the angle between them:</li>
</ol>
<p><span>\[
v \cdot u = \|v\| \|u\| \cos \theta
\]</span></p>
<p>We can compute the angle:</p>
<div id="0df8c69b" data-execution_count="31">
<div><div id="cb45"><pre><code><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>norm_v <span>=</span> np.linalg.norm(v)</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>norm_u <span>=</span> np.linalg.norm(u)</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>cos_theta <span>=</span> np.dot(v, u) <span>/</span> (norm_v <span>*</span> norm_u)</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>theta <span>=</span> np.arccos(cos_theta)</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;cos(theta) =&#34;</span>, cos_theta)</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;theta (in radians) =&#34;</span>, theta)</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;theta (in degrees) =&#34;</span>, np.degrees(theta))</span></code></pre></div></div>
<div>
<pre><code>cos(theta) = 0.33633639699815626
theta (in radians) = 1.2277723863741932
theta (in degrees) = 70.3461759419467</code></pre>
</div>
</div>
<p>This gives the angle between <code>v</code> and <code>u</code>.</p>
<ol start="3" type="1">
<li>Visualizing the dot product Let’s draw the two vectors:</li>
</ol>
<div id="034e1e64" data-execution_count="32">
<div><div id="cb47"><pre><code><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>,<span>0</span>,v[<span>0</span>],v[<span>1</span>],angles<span>=</span><span>&#39;xy&#39;</span>,scale_units<span>=</span><span>&#39;xy&#39;</span>,scale<span>=</span><span>1</span>,color<span>=</span><span>&#39;r&#39;</span>,label<span>=</span><span>&#39;v&#39;</span>)</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>,<span>0</span>,u[<span>0</span>],u[<span>1</span>],angles<span>=</span><span>&#39;xy&#39;</span>,scale_units<span>=</span><span>&#39;xy&#39;</span>,scale<span>=</span><span>1</span>,color<span>=</span><span>&#39;b&#39;</span>,label<span>=</span><span>&#39;u&#39;</span>)</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span>-</span><span>1</span>,<span>5</span>)</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span>-</span><span>2</span>,<span>4</span>)</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span>0</span>,color<span>=</span><span>&#39;black&#39;</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span>0</span>,color<span>=</span><span>&#39;black&#39;</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-33-output-1.png" width="573" height="416"/></p>
</figure>
</div>
</div>
</div>
<p>The dot product is positive if the angle is less than 90°, negative if greater than 90°, and zero if the vectors are perpendicular.</p>
<ol start="4" type="1">
<li>Projections and dot product The dot product lets us compute how much of one vector lies in the direction of another.</li>
</ol>
<div id="d7b5d4c7" data-execution_count="33">
<div><div id="cb48"><pre><code><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>proj_length <span>=</span> np.dot(v, u) <span>/</span> np.linalg.norm(u)</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Projection length of v onto u:&#34;</span>, proj_length)</span></code></pre></div></div>
<div>
<pre><code>Projection length of v onto u: 1.212678125181665</code></pre>
</div>
</div>
<p>This is the length of the shadow of <code>v</code> onto <code>u</code>.</p>
<ol start="5" type="1">
<li>Special cases</li>
</ol>
<ul>
<li>If vectors point in the same direction, the dot product is large and positive.</li>
<li>If vectors are perpendicular, the dot product is zero.</li>
<li>If vectors point in opposite directions, the dot product is negative.</li>
</ul>
<div id="f8ad307c" data-execution_count="34">
<div><div id="cb50"><pre><code><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>a <span>=</span> np.array([<span>1</span>,<span>0</span>])</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>b <span>=</span> np.array([<span>0</span>,<span>1</span>])</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>c <span>=</span> np.array([<span>-</span><span>1</span>,<span>0</span>])</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;a · b =&#34;</span>, np.dot(a,b))   <span># perpendicular</span></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;a · a =&#34;</span>, np.dot(a,a))   <span># length squared</span></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;a · c =&#34;</span>, np.dot(a,c))   <span># opposite</span></span></code></pre></div></div>
<div>
<pre><code>a · b = 0
a · a = 1
a · c = -1</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-5">
<h4 data-anchor-id="try-it-yourself-5">Try It Yourself</h4>
<ol type="1">
<li>Compute the dot product of <code>(3,4)</code> with <code>(4,3)</code>. Is the result larger or smaller than the product of their lengths?</li>
<li>Try <code>(1,2,3) · (4,5,6)</code>. Does the geometric meaning still work in 3D?</li>
<li>Create two perpendicular vectors (e.g. <code>(2,0)</code> and <code>(0,5)</code>). Verify the dot product is zero.</li>
</ol>
</section>
</section>
<section id="angles-between-vectors-and-cosine">
<h3 data-anchor-id="angles-between-vectors-and-cosine">7. Angles Between Vectors and Cosine</h3>
<p>In this lab, we’ll go deeper into the connection between vectors and geometry by calculating angles. Angles tell us how much two vectors “point in the same direction.” The bridge between algebra and geometry here is the cosine formula, which comes directly from the dot product.</p>
<section id="set-up-your-lab-6">
<h4 data-anchor-id="set-up-your-lab-6">Set Up Your Lab</h4>
<div id="f229ec15" data-execution_count="35">
<div><div id="cb52"><pre><code><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-6">
<h4 data-anchor-id="step-by-step-code-walkthrough-6">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Formula for the angle The angle <span>\(\theta\)</span> between two vectors <span>\(v\)</span> and <span>\(u\)</span> is given by:</li>
</ol>
<p><span>\[
\cos \theta = \frac{v \cdot u}{\|v\| \, \|u\|}
\]</span></p>
<p>This means:</p>
<ul>
<li>If <span>\(\cos \theta = 1\)</span>, the vectors point in exactly the same direction.</li>
<li>If <span>\(\cos \theta = 0\)</span>, they are perpendicular.</li>
<li>If <span>\(\cos \theta = -1\)</span>, they point in opposite directions.</li>
</ul>
<ol start="2" type="1">
<li>Computing the angle in Python</li>
</ol>
<div id="c36587e8" data-execution_count="36">
<div><div id="cb53"><pre><code><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.array([<span>2</span>, <span>3</span>])</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>u <span>=</span> np.array([<span>3</span>, <span>-</span><span>1</span>])</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>dot <span>=</span> np.dot(v, u)</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>norm_v <span>=</span> np.linalg.norm(v)</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>norm_u <span>=</span> np.linalg.norm(u)</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>cos_theta <span>=</span> dot <span>/</span> (norm_v <span>*</span> norm_u)</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>theta <span>=</span> np.arccos(cos_theta)</span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;cos(theta) =&#34;</span>, cos_theta)</span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;theta in radians =&#34;</span>, theta)</span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;theta in degrees =&#34;</span>, np.degrees(theta))</span></code></pre></div></div>
<div>
<pre><code>cos(theta) = 0.2631174057921088
theta in radians = 1.3045442776439713
theta in degrees = 74.74488129694222</code></pre>
</div>
</div>
<p>This gives both the cosine value and the actual angle.</p>
<ol start="3" type="1">
<li>Visualizing the vectors</li>
</ol>
<div id="fa3d4270" data-execution_count="37">
<div><div id="cb55"><pre><code><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>,<span>0</span>,v[<span>0</span>],v[<span>1</span>],angles<span>=</span><span>&#39;xy&#39;</span>,scale_units<span>=</span><span>&#39;xy&#39;</span>,scale<span>=</span><span>1</span>,color<span>=</span><span>&#39;r&#39;</span>,label<span>=</span><span>&#39;v&#39;</span>)</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>,<span>0</span>,u[<span>0</span>],u[<span>1</span>],angles<span>=</span><span>&#39;xy&#39;</span>,scale_units<span>=</span><span>&#39;xy&#39;</span>,scale<span>=</span><span>1</span>,color<span>=</span><span>&#39;b&#39;</span>,label<span>=</span><span>&#39;u&#39;</span>)</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span>-</span><span>1</span>,<span>4</span>)</span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span>-</span><span>2</span>,<span>4</span>)</span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span>0</span>,color<span>=</span><span>&#39;black&#39;</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span>0</span>,color<span>=</span><span>&#39;black&#39;</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-38-output-1.png" width="573" height="416"/></p>
</figure>
</div>
</div>
</div>
<p>You can see the angle between <code>v</code> and <code>u</code> as the gap between the red and blue arrows.</p>
<ol start="4" type="1">
<li>Checking special cases</li>
</ol>
<div id="96246b33" data-execution_count="38">
<div><div id="cb56"><pre><code><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>a <span>=</span> np.array([<span>1</span>,<span>0</span>])</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>b <span>=</span> np.array([<span>0</span>,<span>1</span>])</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>c <span>=</span> np.array([<span>-</span><span>1</span>,<span>0</span>])</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Angle between a and b =&#34;</span>, np.degrees(np.arccos(np.dot(a,b)<span>/</span>(np.linalg.norm(a)<span>*</span>np.linalg.norm(b)))))</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Angle between a and c =&#34;</span>, np.degrees(np.arccos(np.dot(a,c)<span>/</span>(np.linalg.norm(a)<span>*</span>np.linalg.norm(c)))))</span></code></pre></div></div>
<div>
<pre><code>Angle between a and b = 90.0
Angle between a and c = 180.0</code></pre>
</div>
</div>
<ul>
<li>Angle between <code>(1,0)</code> and <code>(0,1)</code> is 90°.</li>
<li>Angle between <code>(1,0)</code> and <code>(-1,0)</code> is 180°.</li>
</ul>
<ol start="5" type="1">
<li>Using cosine as a similarity measure In data science and machine learning, people often use cosine similarity instead of raw angles. It’s just the cosine value itself:</li>
</ol>
<div id="16faf51b" data-execution_count="39">
<div><div id="cb58"><pre><code><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>cosine_similarity <span>=</span> np.dot(v,u)<span>/</span>(np.linalg.norm(v)<span>*</span>np.linalg.norm(u))</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Cosine similarity =&#34;</span>, cosine_similarity)</span></code></pre></div></div>
<div>
<pre><code>Cosine similarity = 0.2631174057921088</code></pre>
</div>
</div>
<p>Values close to <code>1</code> mean vectors are aligned, values near <code>0</code> mean unrelated, and values near <code>-1</code> mean opposite.</p>
</section>
<section id="try-it-yourself-6">
<h4 data-anchor-id="try-it-yourself-6">Try It Yourself</h4>
<ol type="1">
<li>Create two random vectors with <code>np.random.randn(3)</code> and compute the angle between them.</li>
<li>Verify that swapping the vectors gives the same angle (symmetry).</li>
<li>Find two vectors where cosine similarity is exactly <code>0</code>. Can you come up with an example in 2D?</li>
</ol>
</section>
</section>
<section id="projections-and-decompositions">
<h3 data-anchor-id="projections-and-decompositions">8. Projections and Decompositions</h3>
<p>In this lab, we’ll learn how to split one vector into parts: one part that lies <em>along</em> another vector, and one part that is <em>perpendicular</em>. This process is called projection and decomposition. Projections let us measure “how much of a vector points in a given direction,” and decompositions give us a way to break vectors into useful components.</p>
<section id="set-up-your-lab-7">
<h4 data-anchor-id="set-up-your-lab-7">Set Up Your Lab</h4>
<div id="9bce7458" data-execution_count="40">
<div><div id="cb60"><pre><code><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-7">
<h4 data-anchor-id="step-by-step-code-walkthrough-7">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Projection formula The projection of vector <span>\(v\)</span> onto vector <span>\(u\)</span> is:</li>
</ol>
<p><span>\[
\text{proj}_u(v) = \frac{v \cdot u}{u \cdot u} \, u
\]</span></p>
<p>This gives the component of <span>\(v\)</span> that points in the direction of <span>\(u\)</span>.</p>
<ol start="2" type="1">
<li>Computing projection in Python</li>
</ol>
<div id="7852d8c3" data-execution_count="41">
<div><div id="cb61"><pre><code><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.array([<span>3</span>, <span>2</span>])</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>u <span>=</span> np.array([<span>2</span>, <span>0</span>])</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>proj_u_v <span>=</span> (np.dot(v, u) <span>/</span> np.dot(u, u)) <span>*</span> u</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Projection of v onto u:&#34;</span>, proj_u_v)</span></code></pre></div></div>
<div>
<pre><code>Projection of v onto u: [3. 0.]</code></pre>
</div>
</div>
<p>Here, <span>\(v = (3,2)\)</span> and <span>\(u = (2,0)\)</span>. The projection of <code>v</code> onto <code>u</code> is a vector pointing along the x-axis.</p>
<ol start="3" type="1">
<li>Decomposing into parallel and perpendicular parts</li>
</ol>
<p>We can write:</p>
<p><span>\[
v = \text{proj}_u(v) + (v - \text{proj}_u(v))
\]</span></p>
<p>The first part is parallel to <code>u</code>, the second part is perpendicular.</p>
<div id="6f00c0ff" data-execution_count="42">
<div><div id="cb63"><pre><code><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>perp <span>=</span> v <span>-</span> proj_u_v</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Parallel part:&#34;</span>, proj_u_v)</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Perpendicular part:&#34;</span>, perp)</span></code></pre></div></div>
<div>
<pre><code>Parallel part: [3. 0.]
Perpendicular part: [0. 2.]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Visualizing projection and decomposition</li>
</ol>
<div id="ca9d206f" data-execution_count="43">
<div><div id="cb65"><pre><code><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>, <span>0</span>, v[<span>0</span>], v[<span>1</span>], angles<span>=</span><span>&#39;xy&#39;</span>, scale_units<span>=</span><span>&#39;xy&#39;</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>&#39;r&#39;</span>, label<span>=</span><span>&#39;v&#39;</span>)</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>, <span>0</span>, u[<span>0</span>], u[<span>1</span>], angles<span>=</span><span>&#39;xy&#39;</span>, scale_units<span>=</span><span>&#39;xy&#39;</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>&#39;b&#39;</span>, label<span>=</span><span>&#39;u&#39;</span>)</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>, <span>0</span>, proj_u_v[<span>0</span>], proj_u_v[<span>1</span>], angles<span>=</span><span>&#39;xy&#39;</span>, scale_units<span>=</span><span>&#39;xy&#39;</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>&#39;g&#39;</span>, label<span>=</span><span>&#39;proj_u(v)&#39;</span>)</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>plt.quiver(proj_u_v[<span>0</span>], proj_u_v[<span>1</span>], perp[<span>0</span>], perp[<span>1</span>], angles<span>=</span><span>&#39;xy&#39;</span>, scale_units<span>=</span><span>&#39;xy&#39;</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>&#39;m&#39;</span>, label<span>=</span><span>&#39;perpendicular&#39;</span>)</span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span>-</span><span>1</span>, <span>5</span>)</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span>-</span><span>1</span>, <span>4</span>)</span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span>0</span>, color<span>=</span><span>&#39;black&#39;</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span>0</span>, color<span>=</span><span>&#39;black&#39;</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb65-11"><a href="#cb65-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-44-output-1.png" width="573" height="416"/></p>
</figure>
</div>
</div>
</div>
<p>You’ll see <code>v</code> (red), <code>u</code> (blue), the projection (green), and the perpendicular remainder (magenta).</p>
<ol start="5" type="1">
<li>Projection in higher dimensions</li>
</ol>
<p>This formula works in any dimension:</p>
<div id="033ca9c4" data-execution_count="44">
<div><div id="cb66"><pre><code><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>a <span>=</span> np.array([<span>1</span>,<span>2</span>,<span>3</span>])</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>b <span>=</span> np.array([<span>0</span>,<span>1</span>,<span>0</span>])</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>proj <span>=</span> (np.dot(a,b)<span>/</span>np.dot(b,b)) <span>*</span> b</span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>perp <span>=</span> a <span>-</span> proj</span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Projection of a onto b:&#34;</span>, proj)</span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Perpendicular component:&#34;</span>, perp)</span></code></pre></div></div>
<div>
<pre><code>Projection of a onto b: [0. 2. 0.]
Perpendicular component: [1. 0. 3.]</code></pre>
</div>
</div>
<p>Even in 3D or higher, projections are about splitting into “along” and “across.”</p>
</section>
<section id="try-it-yourself-7">
<h4 data-anchor-id="try-it-yourself-7">Try It Yourself</h4>
<ol type="1">
<li>Try projecting <code>(2,3)</code> onto <code>(0,5)</code>. Where does it land?</li>
<li>Take a 3D vector like <code>(4,2,6)</code> and project it onto <code>(1,0,0)</code>. What does this give you?</li>
<li>Change the base vector <code>u</code> to something not aligned with the axes, like <code>(1,1)</code>. Does the projection still work?</li>
</ol>
</section>
</section>
<section id="cauchyschwarz-and-triangle-inequalities">
<h3 data-anchor-id="cauchyschwarz-and-triangle-inequalities">9. Cauchy–Schwarz and Triangle Inequalities</h3>
<p>This lab introduces two fundamental inequalities in linear algebra. They may look abstract at first, but they provide guarantees that always hold true for vectors. We’ll explore them with small examples in Python to see why they matter.</p>
<section id="set-up-your-lab-8">
<h4 data-anchor-id="set-up-your-lab-8">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-8">
<h4 data-anchor-id="step-by-step-code-walkthrough-8">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Cauchy–Schwarz inequality</li>
</ol>
<p>The inequality states:</p>
<p><span>\[
|v \cdot u| \leq \|v\| \, \|u\|
\]</span></p>
<p>It means the dot product is never “bigger” than the product of the vector lengths. Equality happens only if the two vectors are pointing in exactly the same (or opposite) direction.</p>
<div id="1431feab" data-execution_count="46">
<div><div id="cb69"><pre><code><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.array([<span>3</span>, <span>4</span>])</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>u <span>=</span> np.array([<span>1</span>, <span>2</span>])</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>lhs <span>=</span> <span>abs</span>(np.dot(v, u))</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>rhs <span>=</span> np.linalg.norm(v) <span>*</span> np.linalg.norm(u)</span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Left-hand side (|v·u|):&#34;</span>, lhs)</span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Right-hand side (‖v‖‖u‖):&#34;</span>, rhs)</span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Inequality holds?&#34;</span>, lhs <span>&lt;=</span> rhs)</span></code></pre></div></div>
<div>
<pre><code>Left-hand side (|v·u|): 11
Right-hand side (‖v‖‖u‖): 11.180339887498949
Inequality holds? True</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Testing Cauchy–Schwarz with different vectors</li>
</ol>
<div id="2c7a1089" data-execution_count="47">
<div><div id="cb71"><pre><code><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>pairs <span>=</span> [</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>    (np.array([<span>1</span>,<span>0</span>]), np.array([<span>0</span>,<span>1</span>])),  <span># perpendicular</span></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>    (np.array([<span>2</span>,<span>3</span>]), np.array([<span>4</span>,<span>6</span>])),  <span># multiples</span></span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>    (np.array([<span>-</span><span>1</span>,<span>2</span>]), np.array([<span>3</span>,<span>-</span><span>6</span>])) <span># opposite multiples</span></span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a><span>for</span> v,u <span>in</span> pairs:</span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>    lhs <span>=</span> <span>abs</span>(np.dot(v, u))</span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>    rhs <span>=</span> np.linalg.norm(v) <span>*</span> np.linalg.norm(u)</span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a>    <span>print</span>(<span>f&#34;v=</span><span>{</span>v<span>}</span><span>, u=</span><span>{</span>u<span>}</span><span> -&gt; |v·u|=</span><span>{</span>lhs<span>}</span><span>, ‖v‖‖u‖=</span><span>{</span>rhs<span>}</span><span>, holds=</span><span>{</span>lhs<span>&lt;=</span>rhs<span>}</span><span>&#34;</span>)</span></code></pre></div></div>
<div>
<pre><code>v=[1 0], u=[0 1] -&gt; |v·u|=0, ‖v‖‖u‖=1.0, holds=True
v=[2 3], u=[4 6] -&gt; |v·u|=26, ‖v‖‖u‖=25.999999999999996, holds=False
v=[-1  2], u=[ 3 -6] -&gt; |v·u|=15, ‖v‖‖u‖=15.000000000000002, holds=True</code></pre>
</div>
</div>
<ul>
<li>Perpendicular vectors give <code>|v·u| = 0</code>, far less than the product of norms.</li>
<li>Multiples give equality (<code>lhs = rhs</code>).</li>
</ul>
<ol start="3" type="1">
<li>Triangle inequality</li>
</ol>
<p>The triangle inequality states:</p>
<p><span>\[
\|v + u\| \leq \|v\| + \|u\|
\]</span></p>
<p>Geometrically, the length of one side of a triangle can never be longer than the sum of the other two sides.</p>
<div id="9a8dfb9d" data-execution_count="48">
<div><div id="cb73"><pre><code><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.array([<span>3</span>, <span>4</span>])</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>u <span>=</span> np.array([<span>1</span>, <span>2</span>])</span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>lhs <span>=</span> np.linalg.norm(v <span>+</span> u)</span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>rhs <span>=</span> np.linalg.norm(v) <span>+</span> np.linalg.norm(u)</span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;‖v+u‖ =&#34;</span>, lhs)</span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;‖v‖ + ‖u‖ =&#34;</span>, rhs)</span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Inequality holds?&#34;</span>, lhs <span>&lt;=</span> rhs)</span></code></pre></div></div>
<div>
<pre><code>‖v+u‖ = 7.211102550927978
‖v‖ + ‖u‖ = 7.23606797749979
Inequality holds? True</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Visual demonstration with a triangle</li>
</ol>
<div id="68dd049c" data-execution_count="49">
<div><div id="cb75"><pre><code><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>origin <span>=</span> np.array([<span>0</span>,<span>0</span>])</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>points <span>=</span> np.array([origin, v, v<span>+</span>u, origin])</span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a>plt.plot(points[:,<span>0</span>], points[:,<span>1</span>], <span>&#39;ro-&#39;</span>)  <span># triangle outline</span></span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a>plt.text(v[<span>0</span>], v[<span>1</span>], <span>&#39;v&#39;</span>)</span>
<span id="cb75-8"><a href="#cb75-8" aria-hidden="true" tabindex="-1"></a>plt.text(v[<span>0</span>]<span>+</span>u[<span>0</span>], v[<span>1</span>]<span>+</span>u[<span>1</span>], <span>&#39;v+u&#39;</span>)</span>
<span id="cb75-9"><a href="#cb75-9" aria-hidden="true" tabindex="-1"></a>plt.text(u[<span>0</span>], u[<span>1</span>], <span>&#39;u&#39;</span>)</span>
<span id="cb75-10"><a href="#cb75-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-11"><a href="#cb75-11" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb75-12"><a href="#cb75-12" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span>0</span>,color<span>=</span><span>&#39;black&#39;</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb75-13"><a href="#cb75-13" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span>0</span>,color<span>=</span><span>&#39;black&#39;</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb75-14"><a href="#cb75-14" aria-hidden="true" tabindex="-1"></a>plt.axis(<span>&#39;equal&#39;</span>)</span>
<span id="cb75-15"><a href="#cb75-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-50-output-1.png" width="558" height="411"/></p>
</figure>
</div>
</div>
</div>
<p>This triangle shows why the inequality is called the “triangle” inequality.</p>
<ol start="5" type="1">
<li>Testing triangle inequality with random vectors</li>
</ol>
<div id="0cfd0ecb" data-execution_count="50">
<div><div id="cb76"><pre><code><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span>for</span> _ <span>in</span> <span>range</span>(<span>5</span>):</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>    v <span>=</span> np.random.randn(<span>2</span>)</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>    u <span>=</span> np.random.randn(<span>2</span>)</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>    lhs <span>=</span> np.linalg.norm(v<span>+</span>u)</span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>    rhs <span>=</span> np.linalg.norm(v) <span>+</span> np.linalg.norm(u)</span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a>    <span>print</span>(<span>f&#34;‖v+u‖=</span><span>{</span>lhs<span>:.3f}</span><span>, ‖v‖+‖u‖=</span><span>{</span>rhs<span>:.3f}</span><span>, holds=</span><span>{</span>lhs <span>&lt;=</span> rhs<span>}</span><span>&#34;</span>)</span></code></pre></div></div>
<div>
<pre><code>‖v+u‖=0.778, ‖v‖+‖u‖=2.112, holds=True
‖v+u‖=1.040, ‖v‖+‖u‖=2.621, holds=True
‖v+u‖=1.632, ‖v‖+‖u‖=2.482, holds=True
‖v+u‖=1.493, ‖v‖+‖u‖=2.250, holds=True
‖v+u‖=2.653, ‖v‖+‖u‖=2.692, holds=True</code></pre>
</div>
</div>
<p>No matter what vectors you try, the inequality always holds.</p>
</section>
<section id="the-takeaway">
<h4 data-anchor-id="the-takeaway">The Takeaway</h4>
<ul>
<li>Cauchy–Schwarz: The dot product is always bounded by the product of vector lengths.</li>
<li>Triangle inequality: The length of one side of a triangle can’t exceed the sum of the other two.</li>
<li>These inequalities form the backbone of geometry, analysis, and many proofs in linear algebra.</li>
</ul>
</section>
</section>
<section id="orthonormal-sets-in-ℝ²ℝ³">
<h3 data-anchor-id="orthonormal-sets-in-ℝ²ℝ³">10. Orthonormal Sets in ℝ²/ℝ³</h3>
<p>In this lab, we’ll explore orthonormal sets - collections of vectors that are both orthogonal (perpendicular) and normalized (length = 1). These sets are the “nicest” possible bases for vector spaces. In 2D and 3D, they correspond to the coordinate axes we already know, but we can also construct and test new ones.</p>
<section id="set-up-your-lab-9">
<h4 data-anchor-id="set-up-your-lab-9">Set Up Your Lab</h4>
<div id="b47d5e69" data-execution_count="51">
<div><div id="cb78"><pre><code><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-9">
<h4 data-anchor-id="step-by-step-code-walkthrough-9">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Orthogonal vectors Two vectors are orthogonal if their dot product is zero.</li>
</ol>
<div id="a6fb3cd0" data-execution_count="52">
<div><div id="cb79"><pre><code><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>x_axis <span>=</span> np.array([<span>1</span>, <span>0</span>])</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>y_axis <span>=</span> np.array([<span>0</span>, <span>1</span>])</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;x_axis · y_axis =&#34;</span>, np.dot(x_axis, y_axis))  <span># should be 0</span></span></code></pre></div></div>

</div>
<p>So the standard axes are orthogonal.</p>
<ol start="2" type="1">
<li>Normalizing vectors Normalization means dividing a vector by its length to make its norm equal to 1.</li>
</ol>
<div id="09bf0d78" data-execution_count="53">
<div><div id="cb81"><pre><code><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.array([<span>3</span>, <span>4</span>])</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>v_normalized <span>=</span> v <span>/</span> np.linalg.norm(v)</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Original v:&#34;</span>, v)</span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Normalized v:&#34;</span>, v_normalized)</span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Length of normalized v:&#34;</span>, np.linalg.norm(v_normalized))</span></code></pre></div></div>
<div>
<pre><code>Original v: [3 4]
Normalized v: [0.6 0.8]
Length of normalized v: 1.0</code></pre>
</div>
</div>
<p>Now <code>v_normalized</code> points in the same direction as <code>v</code> but has unit length.</p>
<ol start="3" type="1">
<li>Building an orthonormal set in 2D</li>
</ol>
<div id="741fb64a" data-execution_count="54">
<div><div id="cb83"><pre><code><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>u1 <span>=</span> np.array([<span>1</span>, <span>0</span>])</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>u2 <span>=</span> np.array([<span>0</span>, <span>1</span>])</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;u1 length:&#34;</span>, np.linalg.norm(u1))</span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;u2 length:&#34;</span>, np.linalg.norm(u2))</span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;u1 · u2 =&#34;</span>, np.dot(u1,u2))</span></code></pre></div></div>
<div>
<pre><code>u1 length: 1.0
u2 length: 1.0
u1 · u2 = 0</code></pre>
</div>
</div>
<p>Both have length 1, and their dot product is 0. That makes <code>{u1, u2}</code> an orthonormal set in 2D.</p>
<ol start="4" type="1">
<li>Visualizing 2D orthonormal vectors</li>
</ol>
<div id="1866c318" data-execution_count="55">
<div><div id="cb85"><pre><code><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>,<span>0</span>,u1[<span>0</span>],u1[<span>1</span>],angles<span>=</span><span>&#39;xy&#39;</span>,scale_units<span>=</span><span>&#39;xy&#39;</span>,scale<span>=</span><span>1</span>,color<span>=</span><span>&#39;r&#39;</span>)</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>,<span>0</span>,u2[<span>0</span>],u2[<span>1</span>],angles<span>=</span><span>&#39;xy&#39;</span>,scale_units<span>=</span><span>&#39;xy&#39;</span>,scale<span>=</span><span>1</span>,color<span>=</span><span>&#39;b&#39;</span>)</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span>-</span><span>1.5</span>,<span>1.5</span>)</span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span>-</span><span>1.5</span>,<span>1.5</span>)</span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span>0</span>,color<span>=</span><span>&#39;black&#39;</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span>0</span>,color<span>=</span><span>&#39;black&#39;</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb85-8"><a href="#cb85-8" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb85-9"><a href="#cb85-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-56-output-1.png" width="592" height="416"/></p>
</figure>
</div>
</div>
</div>
<p>You’ll see the red and blue arrows at right angles, each of length 1.</p>
<ol start="5" type="1">
<li>Orthonormal set in 3D In 3D, the standard basis vectors are:</li>
</ol>
<div id="e1af4bea" data-execution_count="56">
<div><div id="cb86"><pre><code><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>i <span>=</span> np.array([<span>1</span>,<span>0</span>,<span>0</span>])</span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>j <span>=</span> np.array([<span>0</span>,<span>1</span>,<span>0</span>])</span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>k <span>=</span> np.array([<span>0</span>,<span>0</span>,<span>1</span>])</span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;‖i‖ =&#34;</span>, np.linalg.norm(i))</span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;‖j‖ =&#34;</span>, np.linalg.norm(j))</span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;‖k‖ =&#34;</span>, np.linalg.norm(k))</span>
<span id="cb86-8"><a href="#cb86-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;i·j =&#34;</span>, np.dot(i,j))</span>
<span id="cb86-9"><a href="#cb86-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;j·k =&#34;</span>, np.dot(j,k))</span>
<span id="cb86-10"><a href="#cb86-10" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;i·k =&#34;</span>, np.dot(i,k))</span></code></pre></div></div>
<div>
<pre><code>‖i‖ = 1.0
‖j‖ = 1.0
‖k‖ = 1.0
i·j = 0
j·k = 0
i·k = 0</code></pre>
</div>
</div>
<p>Lengths are all 1, and dot products are 0. So <code>{i, j, k}</code> is an orthonormal set in ℝ³.</p>
<ol start="6" type="1">
<li>Testing if a set is orthonormal We can write a helper function:</li>
</ol>
<div id="0ff7a610" data-execution_count="57">
<div><div id="cb88"><pre><code><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span>def</span> is_orthonormal(vectors):</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>    <span>for</span> i <span>in</span> <span>range</span>(<span>len</span>(vectors)):</span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a>        <span>for</span> j <span>in</span> <span>range</span>(<span>len</span>(vectors)):</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a>            dot <span>=</span> np.dot(vectors[i], vectors[j])</span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a>            <span>if</span> i <span>==</span> j:</span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a>                <span>if</span> <span>not</span> np.isclose(dot, <span>1</span>): <span>return</span> <span>False</span></span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a>            <span>else</span>:</span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a>                <span>if</span> <span>not</span> np.isclose(dot, <span>0</span>): <span>return</span> <span>False</span></span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a>    <span>return</span> <span>True</span></span>
<span id="cb88-10"><a href="#cb88-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-11"><a href="#cb88-11" aria-hidden="true" tabindex="-1"></a><span>print</span>(is_orthonormal([i, j, k]))  <span># True</span></span></code></pre></div></div>

</div>
<ol start="7" type="1">
<li>Constructing a new orthonormal pair Not all orthonormal sets look like the axes.</li>
</ol>
<div id="f321baec" data-execution_count="58">
<div><div id="cb90"><pre><code><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>u1 <span>=</span> np.array([<span>1</span>,<span>1</span>]) <span>/</span> np.sqrt(<span>2</span>)</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>u2 <span>=</span> np.array([<span>-</span><span>1</span>,<span>1</span>]) <span>/</span> np.sqrt(<span>2</span>)</span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;u1·u2 =&#34;</span>, np.dot(u1,u2))</span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;‖u1‖ =&#34;</span>, np.linalg.norm(u1))</span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;‖u2‖ =&#34;</span>, np.linalg.norm(u2))</span></code></pre></div></div>
<div>
<pre><code>u1·u2 = 0.0
‖u1‖ = 0.9999999999999999
‖u2‖ = 0.9999999999999999</code></pre>
</div>
</div>
<p>This gives a rotated orthonormal basis in 2D.</p>
</section>
<section id="try-it-yourself-8">
<h4 data-anchor-id="try-it-yourself-8">Try It Yourself</h4>
<ol type="1">
<li>Normalize <code>(2,2,1)</code> to make it a unit vector.</li>
<li>Test whether the set <code>{[1,0,0], [0,2,0], [0,0,3]}</code> is orthonormal.</li>
<li>Construct two vectors in 2D that are not perpendicular. Normalize them and check if the dot product is still zero.</li>
</ol>
</section>
</section>
</section>
<section id="chapter-2.-matrices-and-basic-operations">
<h2 data-anchor-id="chapter-2.-matrices-and-basic-operations">Chapter 2. Matrices and basic operations</h2>
<section id="matrices-as-tables-and-as-machines">
<h3 data-anchor-id="matrices-as-tables-and-as-machines">11. Matrices as Tables and as Machines</h3>
<p>Matrices can feel mysterious at first, but there are two simple ways to think about them:</p>
<ol type="1">
<li>As tables of numbers - just a grid you can store and manipulate.</li>
<li>As machines - something that takes a vector in and spits a new vector out.</li>
</ol>
<p>In this lab, we’ll explore both views and see how they connect.</p>
<section id="set-up-your-lab-10">
<h4 data-anchor-id="set-up-your-lab-10">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-10">
<h4 data-anchor-id="step-by-step-code-walkthrough-10">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>A matrix as a table of numbers</li>
</ol>
<div id="a758b9ca" data-execution_count="60">
<div><div id="cb93"><pre><code><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>2</span>, <span>3</span>],</span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a>    [<span>4</span>, <span>5</span>, <span>6</span>]</span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-6"><a href="#cb93-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Matrix A:</span><span>\n</span><span>&#34;</span>, A)</span>
<span id="cb93-7"><a href="#cb93-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Shape of A:&#34;</span>, A.shape)</span></code></pre></div></div>
<div>
<pre><code>Matrix A:
 [[1 2 3]
 [4 5 6]]
Shape of A: (2, 3)</code></pre>
</div>
</div>
<p>Here, <code>A</code> is a 2×3 matrix (2 rows, 3 columns).</p>
<ul>
<li>Rows = horizontal slices → <code>[1,2,3]</code> and <code>[4,5,6]</code></li>
<li>Columns = vertical slices → <code>[1,4]</code>, <code>[2,5]</code>, <code>[3,6]</code></li>
</ul>
<ol start="2" type="1">
<li>Accessing rows and columns</li>
</ol>
<div id="390a96fe" data-execution_count="61">
<div><div id="cb95"><pre><code><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>first_row <span>=</span> A[<span>0</span>]        <span># row 0</span></span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>second_column <span>=</span> A[:,<span>1</span>]  <span># column 1</span></span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;First row:&#34;</span>, first_row)</span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Second column:&#34;</span>, second_column)</span></code></pre></div></div>
<div>
<pre><code>First row: [1 2 3]
Second column: [2 5]</code></pre>
</div>
</div>
<p>Rows are whole vectors too, and so are columns.</p>
<ol start="3" type="1">
<li>A matrix as a machine</li>
</ol>
<p>A matrix can “act” on a vector. If <code>x = [x1, x2, x3]</code>, then <code>A·x</code> is computed by taking linear combinations of the columns of <code>A</code>.</p>
<div id="59cfb580" data-execution_count="62">
<div><div id="cb97"><pre><code><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a>x <span>=</span> np.array([<span>1</span>, <span>0</span>, <span>-</span><span>1</span>])  <span># a 3D vector</span></span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a>result <span>=</span> A.dot(x)</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;A·x =&#34;</span>, result)</span></code></pre></div></div>

</div>
<p>Interpretation: multiply <code>A</code> by <code>x</code> = combine columns of <code>A</code> with weights from <code>x</code>.</p>
<p><span>\[
A \cdot x = 1 \cdot \text{(col 1)} + 0 \cdot \text{(col 2)} + (-1) \cdot \text{(col 3)}
\]</span></p>
<ol start="4" type="1">
<li>Verifying column combination view</li>
</ol>
<div id="a26fdaa9" data-execution_count="63">
<div><div id="cb99"><pre><code><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a>col1 <span>=</span> A[:,<span>0</span>]</span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a>col2 <span>=</span> A[:,<span>1</span>]</span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a>col3 <span>=</span> A[:,<span>2</span>]</span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-5"><a href="#cb99-5" aria-hidden="true" tabindex="-1"></a>manual <span>=</span> <span>1</span><span>*</span>col1 <span>+</span> <span>0</span><span>*</span>col2 <span>+</span> (<span>-</span><span>1</span>)<span>*</span>col3</span>
<span id="cb99-6"><a href="#cb99-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Manual combination:&#34;</span>, manual)</span>
<span id="cb99-7"><a href="#cb99-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;A·x result:&#34;</span>, result)</span></code></pre></div></div>
<div>
<pre><code>Manual combination: [-2 -2]
A·x result: [-2 -2]</code></pre>
</div>
</div>
<p>They match exactly. This shows the “machine” interpretation is just a shortcut for column combinations.</p>
<ol start="5" type="1">
<li>Geometric intuition (2D example)</li>
</ol>
<div id="c06ed7b1" data-execution_count="64">
<div><div id="cb101"><pre><code><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a>B <span>=</span> np.array([</span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>, <span>0</span>],</span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>, <span>1</span>]</span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-6"><a href="#cb101-6" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.array([<span>1</span>,<span>2</span>])</span>
<span id="cb101-7"><a href="#cb101-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;B·v =&#34;</span>, B.dot(v))</span></code></pre></div></div>

</div>
<p>Here, <code>B</code> scales the x-direction by 2 while leaving the y-direction alone. So <code>(1,2)</code> becomes <code>(2,2)</code>.</p>
</section>
<section id="try-it-yourself-9">
<h4 data-anchor-id="try-it-yourself-9">Try It Yourself</h4>
<ol type="1">
<li>Create a 3×3 identity matrix with <code>np.eye(3)</code> and multiply it by different vectors. What happens?</li>
<li>Build a matrix <code>[[0,-1],[1,0]]</code>. Try multiplying it by <code>(1,0)</code> and <code>(0,1)</code>. What transformation is this?</li>
<li>Create your own 2×2 matrix that flips vectors across the x-axis. Test it on <code>(1,2)</code> and <code>(−3,4)</code>.</li>
</ol>
</section>
<section id="the-takeaway-1">
<h4 data-anchor-id="the-takeaway-1">The Takeaway</h4>
<ul>
<li>A matrix is both a grid of numbers and a machine that transforms vectors.</li>
<li>Matrix–vector multiplication is the same as combining columns with given weights.</li>
<li>Thinking of matrices as machines helps build intuition for rotations, scalings, and other transformations later.</li>
</ul>
</section>
</section>
<section id="matrix-shapes-indexing-and-block-views">
<h3 data-anchor-id="matrix-shapes-indexing-and-block-views">12. Matrix Shapes, Indexing, and Block Views</h3>
<p>Matrices come in many shapes, and learning to read their structure is essential. Shape tells us how many rows and columns a matrix has. Indexing lets us grab specific entries, rows, or columns. Block views let us zoom in on submatrices, which is extremely useful for both theory and computation.</p>
<section id="set-up-your-lab-11">
<h4 data-anchor-id="set-up-your-lab-11">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-11">
<h4 data-anchor-id="step-by-step-code-walkthrough-11">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Matrix shapes</li>
</ol>
<p>The shape of a matrix is <code>(rows, columns)</code>.</p>
<div id="40e360b9" data-execution_count="66">
<div><div id="cb104"><pre><code><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([</span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>2</span>, <span>3</span>],</span>
<span id="cb104-3"><a href="#cb104-3" aria-hidden="true" tabindex="-1"></a>    [<span>4</span>, <span>5</span>, <span>6</span>],</span>
<span id="cb104-4"><a href="#cb104-4" aria-hidden="true" tabindex="-1"></a>    [<span>7</span>, <span>8</span>, <span>9</span>]</span>
<span id="cb104-5"><a href="#cb104-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb104-6"><a href="#cb104-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-7"><a href="#cb104-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Matrix A:</span><span>\n</span><span>&#34;</span>, A)</span>
<span id="cb104-8"><a href="#cb104-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Shape of A:&#34;</span>, A.shape)</span></code></pre></div></div>
<div>
<pre><code>Matrix A:
 [[1 2 3]
 [4 5 6]
 [7 8 9]]
Shape of A: (3, 3)</code></pre>
</div>
</div>
<p>Here, <code>A</code> is a 3×3 matrix.</p>
<ol start="2" type="1">
<li>Indexing elements</li>
</ol>
<p>In NumPy, rows and columns are 0-based. The first entry is <code>A[0,0]</code>.</p>
<div id="b0e93cd3" data-execution_count="67">
<div><div id="cb106"><pre><code><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;A[0,0] =&#34;</span>, A[<span>0</span>,<span>0</span>])  <span># top-left element</span></span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;A[1,2] =&#34;</span>, A[<span>1</span>,<span>2</span>])  <span># second row, third column</span></span></code></pre></div></div>

</div>
<ol start="3" type="1">
<li>Extracting rows and columns</li>
</ol>
<div id="ceed2300" data-execution_count="68">
<div><div id="cb108"><pre><code><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a>row1 <span>=</span> A[<span>0</span>]       <span># first row</span></span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a>col2 <span>=</span> A[:,<span>1</span>]     <span># second column</span></span>
<span id="cb108-3"><a href="#cb108-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-4"><a href="#cb108-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;First row:&#34;</span>, row1)</span>
<span id="cb108-5"><a href="#cb108-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Second column:&#34;</span>, col2)</span></code></pre></div></div>
<div>
<pre><code>First row: [1 2 3]
Second column: [2 5 8]</code></pre>
</div>
</div>
<p>Notice: <code>A[i]</code> gives a row, <code>A[:,j]</code> gives a column.</p>
<ol start="4" type="1">
<li>Slicing submatrices (block view)</li>
</ol>
<p>You can slice multiple rows and columns to form a smaller matrix.</p>
<div id="ae55a2f7" data-execution_count="69">
<div><div id="cb110"><pre><code><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a>block <span>=</span> A[<span>0</span>:<span>2</span>, <span>1</span>:<span>3</span>]  <span># rows 0–1, columns 1–2</span></span>
<span id="cb110-2"><a href="#cb110-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Block submatrix:</span><span>\n</span><span>&#34;</span>, block)</span></code></pre></div></div>
<div>
<pre><code>Block submatrix:
 [[2 3]
 [5 6]]</code></pre>
</div>
</div>
<p>This block is:</p>
<p><span>\[
\begin{bmatrix}
2 &amp; 3 \\
5 &amp; 6
\end{bmatrix}
\]</span></p>
<ol start="5" type="1">
<li>Modifying parts of a matrix</li>
</ol>
<div id="f7ec2ac0" data-execution_count="70">
<div><div id="cb112"><pre><code><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a>A[<span>0</span>,<span>0</span>] <span>=</span> <span>99</span></span>
<span id="cb112-2"><a href="#cb112-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Modified A:</span><span>\n</span><span>&#34;</span>, A)</span>
<span id="cb112-3"><a href="#cb112-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-4"><a href="#cb112-4" aria-hidden="true" tabindex="-1"></a>A[<span>1</span>,:] <span>=</span> [<span>10</span>, <span>11</span>, <span>12</span>]   <span># replace row 1</span></span>
<span id="cb112-5"><a href="#cb112-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;After replacing row 1:</span><span>\n</span><span>&#34;</span>, A)</span></code></pre></div></div>
<div>
<pre><code>Modified A:
 [[99  2  3]
 [ 4  5  6]
 [ 7  8  9]]
After replacing row 1:
 [[99  2  3]
 [10 11 12]
 [ 7  8  9]]</code></pre>
</div>
</div>
<ol start="6" type="1">
<li>Non-square matrices</li>
</ol>
<p>Not all matrices are square. Shapes can be rectangular, too.</p>
<div id="57287bcd" data-execution_count="71">
<div><div id="cb114"><pre><code><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a>B <span>=</span> np.array([</span>
<span id="cb114-2"><a href="#cb114-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>2</span>],</span>
<span id="cb114-3"><a href="#cb114-3" aria-hidden="true" tabindex="-1"></a>    [<span>3</span>, <span>4</span>],</span>
<span id="cb114-4"><a href="#cb114-4" aria-hidden="true" tabindex="-1"></a>    [<span>5</span>, <span>6</span>]</span>
<span id="cb114-5"><a href="#cb114-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb114-6"><a href="#cb114-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-7"><a href="#cb114-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Matrix B:</span><span>\n</span><span>&#34;</span>, B)</span>
<span id="cb114-8"><a href="#cb114-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Shape of B:&#34;</span>, B.shape)</span></code></pre></div></div>
<div>
<pre><code>Matrix B:
 [[1 2]
 [3 4]
 [5 6]]
Shape of B: (3, 2)</code></pre>
</div>
</div>
<p>Here, <code>B</code> is 3×2 (3 rows, 2 columns).</p>
<ol start="7" type="1">
<li>Block decomposition idea</li>
</ol>
<p>We can think of large matrices as made of smaller blocks. This is common in linear algebra proofs and algorithms.</p>
<div id="c4971b80" data-execution_count="72">
<div><div id="cb116"><pre><code><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a>C <span>=</span> np.array([</span>
<span id="cb116-2"><a href="#cb116-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>2</span>,<span>3</span>,<span>4</span>],</span>
<span id="cb116-3"><a href="#cb116-3" aria-hidden="true" tabindex="-1"></a>    [<span>5</span>,<span>6</span>,<span>7</span>,<span>8</span>],</span>
<span id="cb116-4"><a href="#cb116-4" aria-hidden="true" tabindex="-1"></a>    [<span>9</span>,<span>10</span>,<span>11</span>,<span>12</span>],</span>
<span id="cb116-5"><a href="#cb116-5" aria-hidden="true" tabindex="-1"></a>    [<span>13</span>,<span>14</span>,<span>15</span>,<span>16</span>]</span>
<span id="cb116-6"><a href="#cb116-6" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb116-7"><a href="#cb116-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb116-8"><a href="#cb116-8" aria-hidden="true" tabindex="-1"></a>top_left <span>=</span> C[<span>0</span>:<span>2</span>, <span>0</span>:<span>2</span>]</span>
<span id="cb116-9"><a href="#cb116-9" aria-hidden="true" tabindex="-1"></a>bottom_right <span>=</span> C[<span>2</span>:<span>4</span>, <span>2</span>:<span>4</span>]</span>
<span id="cb116-10"><a href="#cb116-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb116-11"><a href="#cb116-11" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Top-left block:</span><span>\n</span><span>&#34;</span>, top_left)</span>
<span id="cb116-12"><a href="#cb116-12" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Bottom-right block:</span><span>\n</span><span>&#34;</span>, bottom_right)</span></code></pre></div></div>
<div>
<pre><code>Top-left block:
 [[1 2]
 [5 6]]
Bottom-right block:
 [[11 12]
 [15 16]]</code></pre>
</div>
</div>
<p>This is the start of block matrix notation.</p>
</section>
<section id="try-it-yourself-10">
<h4 data-anchor-id="try-it-yourself-10">Try It Yourself</h4>
<ol type="1">
<li>Create a 4×5 matrix with values 1–20 using <code>np.arange(1,21).reshape(4,5)</code>. Find its shape.</li>
<li>Extract the middle row and last column.</li>
<li>Cut it into four 2×2 blocks. Can you reassemble them in a different order?</li>
</ol>
</section>
</section>
<section id="matrix-addition-and-scalar-multiplication">
<h3 data-anchor-id="matrix-addition-and-scalar-multiplication">13. Matrix Addition and Scalar Multiplication</h3>
<p>Now that we understand matrix shapes and indexing, let’s practice two of the simplest but most important operations: adding matrices and scaling them with numbers (scalars). These operations extend the rules we already know for vectors.</p>
<section id="set-up-your-lab-12">
<h4 data-anchor-id="set-up-your-lab-12">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-12">
<h4 data-anchor-id="step-by-step-code-walkthrough-12">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Adding two matrices You can add two matrices if (and only if) they have the same shape. Addition happens entry by entry.</li>
</ol>
<div id="7444c906" data-execution_count="74">
<div><div id="cb119"><pre><code><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([</span>
<span id="cb119-2"><a href="#cb119-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>2</span>],</span>
<span id="cb119-3"><a href="#cb119-3" aria-hidden="true" tabindex="-1"></a>    [<span>3</span>, <span>4</span>]</span>
<span id="cb119-4"><a href="#cb119-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb119-5"><a href="#cb119-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-6"><a href="#cb119-6" aria-hidden="true" tabindex="-1"></a>B <span>=</span> np.array([</span>
<span id="cb119-7"><a href="#cb119-7" aria-hidden="true" tabindex="-1"></a>    [<span>5</span>, <span>6</span>],</span>
<span id="cb119-8"><a href="#cb119-8" aria-hidden="true" tabindex="-1"></a>    [<span>7</span>, <span>8</span>]</span>
<span id="cb119-9"><a href="#cb119-9" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb119-10"><a href="#cb119-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-11"><a href="#cb119-11" aria-hidden="true" tabindex="-1"></a>C <span>=</span> A <span>+</span> B</span>
<span id="cb119-12"><a href="#cb119-12" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;A + B =</span><span>\n</span><span>&#34;</span>, C)</span></code></pre></div></div>

</div>
<p>Each element in <code>C</code> is the sum of corresponding elements in <code>A</code> and <code>B</code>.</p>
<ol start="2" type="1">
<li>Scalar multiplication Multiplying a matrix by a scalar multiplies every entry by that number.</li>
</ol>
<div id="babe7c4a" data-execution_count="75">
<div><div id="cb121"><pre><code><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a>k <span>=</span> <span>3</span></span>
<span id="cb121-2"><a href="#cb121-2" aria-hidden="true" tabindex="-1"></a>D <span>=</span> k <span>*</span> A</span>
<span id="cb121-3"><a href="#cb121-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;3 * A =</span><span>\n</span><span>&#34;</span>, D)</span></code></pre></div></div>

</div>
<p>Here, each element of <code>A</code> is tripled.</p>
<ol start="3" type="1">
<li>Combining both operations We can mix addition and scaling, just like with vectors.</li>
</ol>
<div id="6a9c40dc" data-execution_count="76">
<div><div id="cb123"><pre><code><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a>combo <span>=</span> <span>2</span><span>*</span>A <span>-</span> B</span>
<span id="cb123-2"><a href="#cb123-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;2A - B =</span><span>\n</span><span>&#34;</span>, combo)</span></code></pre></div></div>
<div>
<pre><code>2A - B =
 [[-3 -2]
 [-1  0]]</code></pre>
</div>
</div>
<p>This creates new matrices as linear combinations of others.</p>
<ol start="4" type="1">
<li>Zero matrix A matrix of all zeros acts like “nothing happens” for addition.</li>
</ol>
<div id="f79f2289" data-execution_count="77">
<div><div id="cb125"><pre><code><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a>zero <span>=</span> np.zeros((<span>2</span>,<span>2</span>))</span>
<span id="cb125-2"><a href="#cb125-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Zero matrix:</span><span>\n</span><span>&#34;</span>, zero)</span>
<span id="cb125-3"><a href="#cb125-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;A + Zero =</span><span>\n</span><span>&#34;</span>, A <span>+</span> zero)</span></code></pre></div></div>
<div>
<pre><code>Zero matrix:
 [[0. 0.]
 [0. 0.]]
A + Zero =
 [[1. 2.]
 [3. 4.]]</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Shape mismatch (what fails) If shapes don’t match, NumPy throws an error.</li>
</ol>
<div id="2ea1b396" data-execution_count="78">
<div><div id="cb127"><pre><code><span id="cb127-1"><a href="#cb127-1" aria-hidden="true" tabindex="-1"></a>X <span>=</span> np.array([</span>
<span id="cb127-2"><a href="#cb127-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>2</span>,<span>3</span>],</span>
<span id="cb127-3"><a href="#cb127-3" aria-hidden="true" tabindex="-1"></a>    [<span>4</span>,<span>5</span>,<span>6</span>]</span>
<span id="cb127-4"><a href="#cb127-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb127-5"><a href="#cb127-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb127-6"><a href="#cb127-6" aria-hidden="true" tabindex="-1"></a><span>try</span>:</span>
<span id="cb127-7"><a href="#cb127-7" aria-hidden="true" tabindex="-1"></a>    <span>print</span>(A <span>+</span> X)</span>
<span id="cb127-8"><a href="#cb127-8" aria-hidden="true" tabindex="-1"></a><span>except</span> <span>ValueError</span> <span>as</span> e:</span>
<span id="cb127-9"><a href="#cb127-9" aria-hidden="true" tabindex="-1"></a>    <span>print</span>(<span>&#34;Error:&#34;</span>, e)</span></code></pre></div></div>
<div>
<pre><code>Error: operands could not be broadcast together with shapes (2,2) (2,3) </code></pre>
</div>
</div>
<p>This shows why shape consistency matters.</p>
</section>
<section id="try-it-yourself-11">
<h4 data-anchor-id="try-it-yourself-11">Try It Yourself</h4>
<ol type="1">
<li>Create two random 3×3 matrices with <code>np.random.randint(0,10,(3,3))</code> and add them.</li>
<li>Multiply a 4×4 matrix by <code>-1</code>. What happens to its entries?</li>
<li>Compute <code>3A + 2B</code> with the matrices from above. Compare with doing each step manually.</li>
</ol>
</section>
</section>
<section id="matrixvector-product-linear-combinations-of-columns">
<h3 data-anchor-id="matrixvector-product-linear-combinations-of-columns">14. Matrix–Vector Product (Linear Combinations of Columns)</h3>
<p>This lab introduces the matrix–vector product, one of the most important operations in linear algebra. Multiplying a matrix by a vector doesn’t just crunch numbers - it produces a new vector by combining the matrix’s columns in a weighted way.</p>
<section id="set-up-your-lab-13">
<h4 data-anchor-id="set-up-your-lab-13">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-13">
<h4 data-anchor-id="step-by-step-code-walkthrough-13">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>A simple matrix and vector</li>
</ol>
<div id="818eaa3c" data-execution_count="80">
<div><div id="cb130"><pre><code><span id="cb130-1"><a href="#cb130-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([</span>
<span id="cb130-2"><a href="#cb130-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>2</span>],</span>
<span id="cb130-3"><a href="#cb130-3" aria-hidden="true" tabindex="-1"></a>    [<span>3</span>, <span>4</span>],</span>
<span id="cb130-4"><a href="#cb130-4" aria-hidden="true" tabindex="-1"></a>    [<span>5</span>, <span>6</span>]</span>
<span id="cb130-5"><a href="#cb130-5" aria-hidden="true" tabindex="-1"></a>])  <span># 3×2 matrix</span></span>
<span id="cb130-6"><a href="#cb130-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb130-7"><a href="#cb130-7" aria-hidden="true" tabindex="-1"></a>x <span>=</span> np.array([<span>2</span>, <span>-</span><span>1</span>])  <span># 2D vector</span></span></code></pre></div></div>
</div>
<p>Here, <code>A</code> has 2 columns, so we can multiply it by a 2D vector <code>x</code>.</p>
<ol start="2" type="1">
<li>Matrix–vector multiplication in NumPy</li>
</ol>
<div id="1b59b9d1" data-execution_count="81">
<div><div id="cb131"><pre><code><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a>y <span>=</span> A.dot(x)</span>
<span id="cb131-2"><a href="#cb131-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;A·x =&#34;</span>, y)</span></code></pre></div></div>

</div>
<p>Result: a 3D vector.</p>
<ol start="3" type="1">
<li>Interpreting the result as linear combinations</li>
</ol>
<p>Matrix <code>A</code> has two columns:</p>
<div id="393d622e" data-execution_count="82">
<div><div id="cb133"><pre><code><span id="cb133-1"><a href="#cb133-1" aria-hidden="true" tabindex="-1"></a>col1 <span>=</span> A[:,<span>0</span>]   <span># first column</span></span>
<span id="cb133-2"><a href="#cb133-2" aria-hidden="true" tabindex="-1"></a>col2 <span>=</span> A[:,<span>1</span>]   <span># second column</span></span>
<span id="cb133-3"><a href="#cb133-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-4"><a href="#cb133-4" aria-hidden="true" tabindex="-1"></a>manual <span>=</span> <span>2</span><span>*</span>col1 <span>+</span> (<span>-</span><span>1</span>)<span>*</span>col2</span>
<span id="cb133-5"><a href="#cb133-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Manual linear combination:&#34;</span>, manual)</span></code></pre></div></div>
<div>
<pre><code>Manual linear combination: [0 2 4]</code></pre>
</div>
</div>
<p>This matches <code>A·x</code>. In words: <em>multiply each column by the corresponding entry of <code>x</code> and then add them up</em>.</p>
<ol start="4" type="1">
<li>Another example (geometry)</li>
</ol>
<div id="c6149e36" data-execution_count="83">
<div><div id="cb135"><pre><code><span id="cb135-1"><a href="#cb135-1" aria-hidden="true" tabindex="-1"></a>B <span>=</span> np.array([</span>
<span id="cb135-2"><a href="#cb135-2" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>, <span>0</span>],</span>
<span id="cb135-3"><a href="#cb135-3" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>, <span>1</span>]</span>
<span id="cb135-4"><a href="#cb135-4" aria-hidden="true" tabindex="-1"></a>])  <span># stretches x-axis by 2</span></span>
<span id="cb135-5"><a href="#cb135-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb135-6"><a href="#cb135-6" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.array([<span>1</span>, <span>3</span>])</span>
<span id="cb135-7"><a href="#cb135-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;B·v =&#34;</span>, B.dot(v))</span></code></pre></div></div>

</div>
<p>Here, <code>(1,3)</code> becomes <code>(2,3)</code>. The x-component was doubled, while y stayed the same.</p>
<ol start="5" type="1">
<li>Visualization of matrix action</li>
</ol>
<div id="099f57e9" data-execution_count="84">
<div><div id="cb137"><pre><code><span id="cb137-1"><a href="#cb137-1" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span>
<span id="cb137-2"><a href="#cb137-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb137-3"><a href="#cb137-3" aria-hidden="true" tabindex="-1"></a><span># draw original vector</span></span>
<span id="cb137-4"><a href="#cb137-4" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>,<span>0</span>,v[<span>0</span>],v[<span>1</span>],angles<span>=</span><span>&#39;xy&#39;</span>,scale_units<span>=</span><span>&#39;xy&#39;</span>,scale<span>=</span><span>1</span>,color<span>=</span><span>&#39;r&#39;</span>,label<span>=</span><span>&#39;v&#39;</span>)</span>
<span id="cb137-5"><a href="#cb137-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb137-6"><a href="#cb137-6" aria-hidden="true" tabindex="-1"></a><span># draw transformed vector</span></span>
<span id="cb137-7"><a href="#cb137-7" aria-hidden="true" tabindex="-1"></a>v_transformed <span>=</span> B.dot(v)</span>
<span id="cb137-8"><a href="#cb137-8" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>,<span>0</span>,v_transformed[<span>0</span>],v_transformed[<span>1</span>],angles<span>=</span><span>&#39;xy&#39;</span>,scale_units<span>=</span><span>&#39;xy&#39;</span>,scale<span>=</span><span>1</span>,color<span>=</span><span>&#39;b&#39;</span>,label<span>=</span><span>&#39;B·v&#39;</span>)</span>
<span id="cb137-9"><a href="#cb137-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb137-10"><a href="#cb137-10" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span>-</span><span>1</span>,<span>4</span>)</span>
<span id="cb137-11"><a href="#cb137-11" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span>-</span><span>1</span>,<span>4</span>)</span>
<span id="cb137-12"><a href="#cb137-12" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span>0</span>,color<span>=</span><span>&#39;black&#39;</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb137-13"><a href="#cb137-13" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span>0</span>,color<span>=</span><span>&#39;black&#39;</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb137-14"><a href="#cb137-14" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb137-15"><a href="#cb137-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-85-output-1.png" width="573" height="416"/></p>
</figure>
</div>
</div>
</div>
<p>Red arrow = original vector, blue arrow = transformed vector.</p>
</section>
<section id="try-it-yourself-12">
<h4 data-anchor-id="try-it-yourself-12">Try It Yourself</h4>
<ol type="1">
<li><p>Multiply</p>
<p><span>\[
A = \begin{bmatrix}1 &amp; 0 \\ 0 &amp; 1 \\ -1 &amp; 2\end{bmatrix},\; x = [3,1]
\]</span></p>
<p>What’s the result?</p></li>
<li><p>Replace <code>B</code> with <code>[[0,-1],[1,0]]</code>. Multiply it by <code>(1,0)</code> and <code>(0,1)</code>. What geometric transformation does this represent?</p></li>
<li><p>For a 4×4 identity matrix (<code>np.eye(4)</code>), try multiplying by any 4D vector. What do you observe?</p></li>
</ol>
</section>
</section>
<section id="matrixmatrix-product-composition-of-linear-steps">
<h3 data-anchor-id="matrixmatrix-product-composition-of-linear-steps">15. Matrix–Matrix Product (Composition of Linear Steps)</h3>
<p>Matrix–matrix multiplication is how we combine two linear transformations into one. Instead of applying one transformation, then another, we can multiply their matrices and get a single matrix that does both at once.</p>
<section id="set-up-your-lab-14">
<h4 data-anchor-id="set-up-your-lab-14">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-14">
<h4 data-anchor-id="step-by-step-code-walkthrough-14">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Matrix–matrix multiplication in NumPy</li>
</ol>
<div id="920114d3" data-execution_count="86">
<div><div id="cb139"><pre><code><span id="cb139-1"><a href="#cb139-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([</span>
<span id="cb139-2"><a href="#cb139-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>2</span>],</span>
<span id="cb139-3"><a href="#cb139-3" aria-hidden="true" tabindex="-1"></a>    [<span>3</span>, <span>4</span>]</span>
<span id="cb139-4"><a href="#cb139-4" aria-hidden="true" tabindex="-1"></a>])  <span># 2×2</span></span>
<span id="cb139-5"><a href="#cb139-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb139-6"><a href="#cb139-6" aria-hidden="true" tabindex="-1"></a>B <span>=</span> np.array([</span>
<span id="cb139-7"><a href="#cb139-7" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>, <span>0</span>],</span>
<span id="cb139-8"><a href="#cb139-8" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>2</span>]</span>
<span id="cb139-9"><a href="#cb139-9" aria-hidden="true" tabindex="-1"></a>])  <span># 2×2</span></span>
<span id="cb139-10"><a href="#cb139-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb139-11"><a href="#cb139-11" aria-hidden="true" tabindex="-1"></a>C <span>=</span> A.dot(B)   <span># or A @ B</span></span>
<span id="cb139-12"><a href="#cb139-12" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;A·B =</span><span>\n</span><span>&#34;</span>, C)</span></code></pre></div></div>

</div>
<p>The result <code>C</code> is another 2×2 matrix.</p>
<ol start="2" type="1">
<li>Manual computation</li>
</ol>
<p>Each entry of <code>C</code> is computed as a row of A dotted with a column of B:</p>
<div id="91f8a6a2" data-execution_count="87">
<div><div id="cb141"><pre><code><span id="cb141-1"><a href="#cb141-1" aria-hidden="true" tabindex="-1"></a>c11 <span>=</span> A[<span>0</span>,:].dot(B[:,<span>0</span>])</span>
<span id="cb141-2"><a href="#cb141-2" aria-hidden="true" tabindex="-1"></a>c12 <span>=</span> A[<span>0</span>,:].dot(B[:,<span>1</span>])</span>
<span id="cb141-3"><a href="#cb141-3" aria-hidden="true" tabindex="-1"></a>c21 <span>=</span> A[<span>1</span>,:].dot(B[:,<span>0</span>])</span>
<span id="cb141-4"><a href="#cb141-4" aria-hidden="true" tabindex="-1"></a>c22 <span>=</span> A[<span>1</span>,:].dot(B[:,<span>1</span>])</span>
<span id="cb141-5"><a href="#cb141-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb141-6"><a href="#cb141-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Manual C =</span><span>\n</span><span>&#34;</span>, np.array([[c11,c12],[c21,c22]]))</span></code></pre></div></div>
<div>
<pre><code>Manual C =
 [[ 4  4]
 [10  8]]</code></pre>
</div>
</div>
<p>This should match <code>A·B</code>.</p>
<ol start="3" type="1">
<li>Geometric interpretation</li>
</ol>
<p>Let’s see how two transformations combine.</p>
<ul>
<li>Matrix <code>B</code> scales x by 2 and stretches y by 2.</li>
<li>Matrix <code>A</code> applies another linear transformation.</li>
</ul>
<p>Together, <code>C = A·B</code> does both in one step.</p>
<div id="0049a10b" data-execution_count="88">
<div><div id="cb143"><pre><code><span id="cb143-1"><a href="#cb143-1" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.array([<span>1</span>,<span>1</span>])</span>
<span id="cb143-2"><a href="#cb143-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-3"><a href="#cb143-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;First apply B:&#34;</span>, B.dot(v))</span>
<span id="cb143-4"><a href="#cb143-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Then apply A:&#34;</span>, A.dot(B.dot(v)))</span>
<span id="cb143-5"><a href="#cb143-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Directly with C:&#34;</span>, C.dot(v))</span></code></pre></div></div>
<div>
<pre><code>First apply B: [2 3]
Then apply A: [ 8 18]
Directly with C: [ 8 18]</code></pre>
</div>
</div>
<p>The result is the same: applying <code>B</code> then <code>A</code> is equivalent to applying <code>C</code>.</p>
<ol start="4" type="1">
<li>Non-square matrices</li>
</ol>
<p>Matrix multiplication also works for rectangular matrices, as long as the inner dimensions match.</p>
<div id="acba7cd2" data-execution_count="89">
<div><div id="cb145"><pre><code><span id="cb145-1"><a href="#cb145-1" aria-hidden="true" tabindex="-1"></a>M <span>=</span> np.array([</span>
<span id="cb145-2"><a href="#cb145-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>0</span>, <span>2</span>],</span>
<span id="cb145-3"><a href="#cb145-3" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>, <span>1</span>, <span>3</span>]</span>
<span id="cb145-4"><a href="#cb145-4" aria-hidden="true" tabindex="-1"></a>])  <span># 2×3</span></span>
<span id="cb145-5"><a href="#cb145-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-6"><a href="#cb145-6" aria-hidden="true" tabindex="-1"></a>N <span>=</span> np.array([</span>
<span id="cb145-7"><a href="#cb145-7" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>2</span>],</span>
<span id="cb145-8"><a href="#cb145-8" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>, <span>1</span>],</span>
<span id="cb145-9"><a href="#cb145-9" aria-hidden="true" tabindex="-1"></a>    [<span>4</span>, <span>0</span>]</span>
<span id="cb145-10"><a href="#cb145-10" aria-hidden="true" tabindex="-1"></a>])  <span># 3×2</span></span>
<span id="cb145-11"><a href="#cb145-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-12"><a href="#cb145-12" aria-hidden="true" tabindex="-1"></a>P <span>=</span> M.dot(N)  <span># result is 2×2</span></span>
<span id="cb145-13"><a href="#cb145-13" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;M·N =</span><span>\n</span><span>&#34;</span>, P)</span></code></pre></div></div>

</div>
<p>Shape rule: <code>(2×3)·(3×2) = (2×2)</code>.</p>
<ol start="5" type="1">
<li>Associativity (but not commutativity)</li>
</ol>
<p>Matrix multiplication is associative: <code>(A·B)·C = A·(B·C)</code>. But it’s not commutative: in general, <code>A·B ≠ B·A</code>.</p>
<div id="025d7f5b" data-execution_count="90">
<div><div id="cb147"><pre><code><span id="cb147-1"><a href="#cb147-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>1</span>,<span>2</span>],[<span>3</span>,<span>4</span>]])</span>
<span id="cb147-2"><a href="#cb147-2" aria-hidden="true" tabindex="-1"></a>B <span>=</span> np.array([[<span>0</span>,<span>1</span>],[<span>1</span>,<span>0</span>]])</span>
<span id="cb147-3"><a href="#cb147-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-4"><a href="#cb147-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;A·B =</span><span>\n</span><span>&#34;</span>, A.dot(B))</span>
<span id="cb147-5"><a href="#cb147-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;B·A =</span><span>\n</span><span>&#34;</span>, B.dot(A))</span></code></pre></div></div>
<div>
<pre><code>A·B =
 [[2 1]
 [4 3]]
B·A =
 [[3 4]
 [1 2]]</code></pre>
</div>
</div>
<p>The two results are different.</p>
</section>
<section id="try-it-yourself-13">
<h4 data-anchor-id="try-it-yourself-13">Try It Yourself</h4>
<ol type="1">
<li><p>Multiply</p>
<p><span>\[
A = \begin{bmatrix}1 &amp; 0 \\ 0 &amp; 1\end{bmatrix},\;
B = \begin{bmatrix}0 &amp; -1 \\ 1 &amp; 0\end{bmatrix}
\]</span></p>
<p>What transformation does <code>A·B</code> represent?</p></li>
<li><p>Create a random 3×2 matrix and a 2×4 matrix. Multiply them. What shape is the result?</p></li>
<li><p>Verify with Python that <code>(A·B)·C = A·(B·C)</code> for some 3×3 random matrices.</p></li>
</ol>
</section>
</section>
<section id="identity-inverse-and-transpose">
<h3 data-anchor-id="identity-inverse-and-transpose">16. Identity, Inverse, and Transpose</h3>
<p>In this lab, we’ll meet three special matrix operations and objects: the identity matrix, the inverse, and the transpose. These are the building blocks of matrix algebra, each with a simple meaning but deep importance.</p>
<section id="set-up-your-lab-15">
<h4 data-anchor-id="set-up-your-lab-15">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-15">
<h4 data-anchor-id="step-by-step-code-walkthrough-15">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Identity matrix The identity matrix is like the number <code>1</code> for matrices: multiplying by it changes nothing.</li>
</ol>
<div id="93cf229b" data-execution_count="92">
<div><div id="cb150"><pre><code><span id="cb150-1"><a href="#cb150-1" aria-hidden="true" tabindex="-1"></a>I <span>=</span> np.eye(<span>3</span>)  <span># 3×3 identity matrix</span></span>
<span id="cb150-2"><a href="#cb150-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Identity matrix:</span><span>\n</span><span>&#34;</span>, I)</span>
<span id="cb150-3"><a href="#cb150-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb150-4"><a href="#cb150-4" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([</span>
<span id="cb150-5"><a href="#cb150-5" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>, <span>1</span>, <span>0</span>],</span>
<span id="cb150-6"><a href="#cb150-6" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>, <span>1</span>, <span>3</span>],</span>
<span id="cb150-7"><a href="#cb150-7" aria-hidden="true" tabindex="-1"></a>    [<span>4</span>, <span>0</span>, <span>1</span>]</span>
<span id="cb150-8"><a href="#cb150-8" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb150-9"><a href="#cb150-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb150-10"><a href="#cb150-10" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;A·I =</span><span>\n</span><span>&#34;</span>, A.dot(I))</span>
<span id="cb150-11"><a href="#cb150-11" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;I·A =</span><span>\n</span><span>&#34;</span>, I.dot(A))</span></code></pre></div></div>
<div>
<pre><code>Identity matrix:
 [[1. 0. 0.]
 [0. 1. 0.]
 [0. 0. 1.]]
A·I =
 [[2. 1. 0.]
 [0. 1. 3.]
 [4. 0. 1.]]
I·A =
 [[2. 1. 0.]
 [0. 1. 3.]
 [4. 0. 1.]]</code></pre>
</div>
</div>
<p>Both equal <code>A</code>.</p>
<ol start="2" type="1">
<li>Transpose The transpose flips rows and columns.</li>
</ol>
<div id="63cbc75e" data-execution_count="93">
<div><div id="cb152"><pre><code><span id="cb152-1"><a href="#cb152-1" aria-hidden="true" tabindex="-1"></a>B <span>=</span> np.array([</span>
<span id="cb152-2"><a href="#cb152-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>2</span>, <span>3</span>],</span>
<span id="cb152-3"><a href="#cb152-3" aria-hidden="true" tabindex="-1"></a>    [<span>4</span>, <span>5</span>, <span>6</span>]</span>
<span id="cb152-4"><a href="#cb152-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb152-5"><a href="#cb152-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-6"><a href="#cb152-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;B:</span><span>\n</span><span>&#34;</span>, B)</span>
<span id="cb152-7"><a href="#cb152-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;B.T:</span><span>\n</span><span>&#34;</span>, B.T)</span></code></pre></div></div>
<div>
<pre><code>B:
 [[1 2 3]
 [4 5 6]]
B.T:
 [[1 4]
 [2 5]
 [3 6]]</code></pre>
</div>
</div>
<ul>
<li>Original: 2×3</li>
<li>Transpose: 3×2</li>
</ul>
<p>Geometrically, transpose swaps the axes when vectors are viewed in row/column form.</p>
<ol start="3" type="1">
<li>Inverse The inverse matrix is like dividing by a number: multiplying a matrix by its inverse gives the identity.</li>
</ol>
<div id="7d0b964e" data-execution_count="94">
<div><div id="cb154"><pre><code><span id="cb154-1"><a href="#cb154-1" aria-hidden="true" tabindex="-1"></a>C <span>=</span> np.array([</span>
<span id="cb154-2"><a href="#cb154-2" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>, <span>1</span>],</span>
<span id="cb154-3"><a href="#cb154-3" aria-hidden="true" tabindex="-1"></a>    [<span>5</span>, <span>3</span>]</span>
<span id="cb154-4"><a href="#cb154-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb154-5"><a href="#cb154-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb154-6"><a href="#cb154-6" aria-hidden="true" tabindex="-1"></a>C_inv <span>=</span> np.linalg.inv(C)</span>
<span id="cb154-7"><a href="#cb154-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Inverse of C:</span><span>\n</span><span>&#34;</span>, C_inv)</span>
<span id="cb154-8"><a href="#cb154-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb154-9"><a href="#cb154-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;C·C_inv =</span><span>\n</span><span>&#34;</span>, C.dot(C_inv))</span>
<span id="cb154-10"><a href="#cb154-10" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;C_inv·C =</span><span>\n</span><span>&#34;</span>, C_inv.dot(C))</span></code></pre></div></div>
<div>
<pre><code>Inverse of C:
 [[ 3. -1.]
 [-5.  2.]]
C·C_inv =
 [[ 1.00000000e+00  2.22044605e-16]
 [-8.88178420e-16  1.00000000e+00]]
C_inv·C =
 [[1.00000000e+00 3.33066907e-16]
 [0.00000000e+00 1.00000000e+00]]</code></pre>
</div>
</div>
<p>Both products are (approximately) the identity.</p>
<ol start="4" type="1">
<li>Matrices that don’t have inverses Not every matrix is invertible. If a matrix is singular (determinant = 0), it has no inverse.</li>
</ol>
<div id="a9b5364c" data-execution_count="95">
<div><div id="cb156"><pre><code><span id="cb156-1"><a href="#cb156-1" aria-hidden="true" tabindex="-1"></a>D <span>=</span> np.array([</span>
<span id="cb156-2"><a href="#cb156-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>2</span>],</span>
<span id="cb156-3"><a href="#cb156-3" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>, <span>4</span>]</span>
<span id="cb156-4"><a href="#cb156-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb156-5"><a href="#cb156-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb156-6"><a href="#cb156-6" aria-hidden="true" tabindex="-1"></a><span>try</span>:</span>
<span id="cb156-7"><a href="#cb156-7" aria-hidden="true" tabindex="-1"></a>    np.linalg.inv(D)</span>
<span id="cb156-8"><a href="#cb156-8" aria-hidden="true" tabindex="-1"></a><span>except</span> np.linalg.LinAlgError <span>as</span> e:</span>
<span id="cb156-9"><a href="#cb156-9" aria-hidden="true" tabindex="-1"></a>    <span>print</span>(<span>&#34;Error:&#34;</span>, e)</span></code></pre></div></div>

</div>
<p>Here, the second row is a multiple of the first, so <code>D</code> can’t be inverted.</p>
<ol start="5" type="1">
<li>Transpose and inverse together For invertible matrices,</li>
</ol>
<p><span>\[
(A^T)^{-1} = (A^{-1})^T
\]</span></p>
<p>We can check this numerically:</p>
<div id="c1a422ee" data-execution_count="96">
<div><div id="cb158"><pre><code><span id="cb158-1"><a href="#cb158-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([</span>
<span id="cb158-2"><a href="#cb158-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>2</span>],</span>
<span id="cb158-3"><a href="#cb158-3" aria-hidden="true" tabindex="-1"></a>    [<span>3</span>, <span>5</span>]</span>
<span id="cb158-4"><a href="#cb158-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb158-5"><a href="#cb158-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb158-6"><a href="#cb158-6" aria-hidden="true" tabindex="-1"></a>lhs <span>=</span> np.linalg.inv(A.T)</span>
<span id="cb158-7"><a href="#cb158-7" aria-hidden="true" tabindex="-1"></a>rhs <span>=</span> np.linalg.inv(A).T</span>
<span id="cb158-8"><a href="#cb158-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb158-9"><a href="#cb158-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Do they match?&#34;</span>, np.allclose(lhs, rhs))</span></code></pre></div></div>

</div>
</section>
<section id="try-it-yourself-14">
<h4 data-anchor-id="try-it-yourself-14">Try It Yourself</h4>
<ol type="1">
<li>Create a 4×4 identity matrix. Multiply it by any 4×1 vector. Does it change?</li>
<li>Take a random 2×2 matrix with <code>np.random.randint</code>. Compute its inverse and check if multiplying gives identity.</li>
<li>Pick a rectangular 3×2 matrix. What happens when you try <code>np.linalg.inv</code>? Why?</li>
<li>Compute <code>(A.T).T</code> for some matrix <code>A</code>. What do you notice?</li>
</ol>
</section>
</section>
<section id="symmetric-diagonal-triangular-and-permutation-matrices">
<h3 data-anchor-id="symmetric-diagonal-triangular-and-permutation-matrices">17. Symmetric, Diagonal, Triangular, and Permutation Matrices</h3>
<p>In this lab, we’ll meet four important families of special matrices. They have patterns that make them easier to understand, compute with, and use in algorithms.</p>
<section id="set-up-your-lab-16">
<h4 data-anchor-id="set-up-your-lab-16">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-16">
<h4 data-anchor-id="step-by-step-code-walkthrough-16">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Symmetric matrices A matrix is symmetric if it equals its transpose: <span>\(A = A^T\)</span>.</li>
</ol>
<div id="9742147a" data-execution_count="98">
<div><div id="cb161"><pre><code><span id="cb161-1"><a href="#cb161-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([</span>
<span id="cb161-2"><a href="#cb161-2" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>, <span>3</span>, <span>4</span>],</span>
<span id="cb161-3"><a href="#cb161-3" aria-hidden="true" tabindex="-1"></a>    [<span>3</span>, <span>5</span>, <span>6</span>],</span>
<span id="cb161-4"><a href="#cb161-4" aria-hidden="true" tabindex="-1"></a>    [<span>4</span>, <span>6</span>, <span>8</span>]</span>
<span id="cb161-5"><a href="#cb161-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb161-6"><a href="#cb161-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-7"><a href="#cb161-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;A:</span><span>\n</span><span>&#34;</span>, A)</span>
<span id="cb161-8"><a href="#cb161-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;A.T:</span><span>\n</span><span>&#34;</span>, A.T)</span>
<span id="cb161-9"><a href="#cb161-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Is symmetric?&#34;</span>, np.allclose(A, A.T))</span></code></pre></div></div>
<div>
<pre><code>A:
 [[2 3 4]
 [3 5 6]
 [4 6 8]]
A.T:
 [[2 3 4]
 [3 5 6]
 [4 6 8]]
Is symmetric? True</code></pre>
</div>
</div>
<p>Symmetric matrices appear in physics, optimization, and statistics (e.g., covariance matrices).</p>
<ol start="2" type="1">
<li>Diagonal matrices A diagonal matrix has nonzero entries only on the main diagonal.</li>
</ol>
<div id="518501f7" data-execution_count="99">
<div><div id="cb163"><pre><code><span id="cb163-1"><a href="#cb163-1" aria-hidden="true" tabindex="-1"></a>D <span>=</span> np.diag([<span>1</span>, <span>5</span>, <span>9</span>])</span>
<span id="cb163-2"><a href="#cb163-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Diagonal matrix:</span><span>\n</span><span>&#34;</span>, D)</span>
<span id="cb163-3"><a href="#cb163-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb163-4"><a href="#cb163-4" aria-hidden="true" tabindex="-1"></a>x <span>=</span> np.array([<span>2</span>, <span>3</span>, <span>4</span>])</span>
<span id="cb163-5"><a href="#cb163-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;D·x =&#34;</span>, D.dot(x))  <span># scales each component</span></span></code></pre></div></div>
<div>
<pre><code>Diagonal matrix:
 [[1 0 0]
 [0 5 0]
 [0 0 9]]
D·x = [ 2 15 36]</code></pre>
</div>
</div>
<p>Diagonal multiplication simply scales each coordinate separately.</p>
<ol start="3" type="1">
<li>Triangular matrices Upper triangular: all entries below the diagonal are zero. Lower triangular: all entries above the diagonal are zero.</li>
</ol>
<div id="d5505799" data-execution_count="100">
<div><div id="cb165"><pre><code><span id="cb165-1"><a href="#cb165-1" aria-hidden="true" tabindex="-1"></a>U <span>=</span> np.array([</span>
<span id="cb165-2"><a href="#cb165-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>2</span>, <span>3</span>],</span>
<span id="cb165-3"><a href="#cb165-3" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>, <span>4</span>, <span>5</span>],</span>
<span id="cb165-4"><a href="#cb165-4" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>, <span>0</span>, <span>6</span>]</span>
<span id="cb165-5"><a href="#cb165-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb165-6"><a href="#cb165-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb165-7"><a href="#cb165-7" aria-hidden="true" tabindex="-1"></a>L <span>=</span> np.array([</span>
<span id="cb165-8"><a href="#cb165-8" aria-hidden="true" tabindex="-1"></a>    [<span>7</span>, <span>0</span>, <span>0</span>],</span>
<span id="cb165-9"><a href="#cb165-9" aria-hidden="true" tabindex="-1"></a>    [<span>8</span>, <span>9</span>, <span>0</span>],</span>
<span id="cb165-10"><a href="#cb165-10" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>2</span>, <span>3</span>]</span>
<span id="cb165-11"><a href="#cb165-11" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb165-12"><a href="#cb165-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb165-13"><a href="#cb165-13" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Upper triangular U:</span><span>\n</span><span>&#34;</span>, U)</span>
<span id="cb165-14"><a href="#cb165-14" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Lower triangular L:</span><span>\n</span><span>&#34;</span>, L)</span></code></pre></div></div>
<div>
<pre><code>Upper triangular U:
 [[1 2 3]
 [0 4 5]
 [0 0 6]]
Lower triangular L:
 [[7 0 0]
 [8 9 0]
 [1 2 3]]</code></pre>
</div>
</div>
<p>These are important in solving linear systems (e.g., Gaussian elimination).</p>
<ol start="4" type="1">
<li>Permutation matrices A permutation matrix rearranges the order of coordinates. Each row and each column has exactly one <code>1</code>, everything else is <code>0</code>.</li>
</ol>
<div id="be2a43c4" data-execution_count="101">
<div><div id="cb167"><pre><code><span id="cb167-1"><a href="#cb167-1" aria-hidden="true" tabindex="-1"></a>P <span>=</span> np.array([</span>
<span id="cb167-2"><a href="#cb167-2" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>, <span>1</span>, <span>0</span>],</span>
<span id="cb167-3"><a href="#cb167-3" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>, <span>0</span>, <span>1</span>],</span>
<span id="cb167-4"><a href="#cb167-4" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>0</span>, <span>0</span>]</span>
<span id="cb167-5"><a href="#cb167-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb167-6"><a href="#cb167-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb167-7"><a href="#cb167-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Permutation matrix P:</span><span>\n</span><span>&#34;</span>, P)</span>
<span id="cb167-8"><a href="#cb167-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb167-9"><a href="#cb167-9" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.array([<span>10</span>, <span>20</span>, <span>30</span>])</span>
<span id="cb167-10"><a href="#cb167-10" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;P·v =&#34;</span>, P.dot(v))</span></code></pre></div></div>
<div>
<pre><code>Permutation matrix P:
 [[0 1 0]
 [0 0 1]
 [1 0 0]]
P·v = [20 30 10]</code></pre>
</div>
</div>
<p>Here, <code>P</code> cycles <code>(10,20,30)</code> into <code>(20,30,10)</code>.</p>
<ol start="5" type="1">
<li>Checking properties</li>
</ol>
<div id="f335c6be" data-execution_count="102">
<div><div id="cb169"><pre><code><span id="cb169-1"><a href="#cb169-1" aria-hidden="true" tabindex="-1"></a><span>def</span> is_symmetric(M): <span>return</span> np.allclose(M, M.T)</span>
<span id="cb169-2"><a href="#cb169-2" aria-hidden="true" tabindex="-1"></a><span>def</span> is_diagonal(M): <span>return</span> np.count_nonzero(M <span>-</span> np.diag(np.diag(M))) <span>==</span> <span>0</span></span>
<span id="cb169-3"><a href="#cb169-3" aria-hidden="true" tabindex="-1"></a><span>def</span> is_upper_triangular(M): <span>return</span> np.allclose(M, np.triu(M))</span>
<span id="cb169-4"><a href="#cb169-4" aria-hidden="true" tabindex="-1"></a><span>def</span> is_lower_triangular(M): <span>return</span> np.allclose(M, np.tril(M))</span>
<span id="cb169-5"><a href="#cb169-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb169-6"><a href="#cb169-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;A symmetric?&#34;</span>, is_symmetric(A))</span>
<span id="cb169-7"><a href="#cb169-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;D diagonal?&#34;</span>, is_diagonal(D))</span>
<span id="cb169-8"><a href="#cb169-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;U upper triangular?&#34;</span>, is_upper_triangular(U))</span>
<span id="cb169-9"><a href="#cb169-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;L lower triangular?&#34;</span>, is_lower_triangular(L))</span></code></pre></div></div>
<div>
<pre><code>A symmetric? True
D diagonal? True
U upper triangular? True
L lower triangular? True</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-15">
<h4 data-anchor-id="try-it-yourself-15">Try It Yourself</h4>
<ol type="1">
<li>Create a random symmetric matrix by generating any matrix <code>M</code> and computing <code>(M + M.T)/2</code>.</li>
<li>Build a 4×4 diagonal matrix with diagonal entries <code>[2,4,6,8]</code> and multiply it by <code>[1,1,1,1]</code>.</li>
<li>Make a permutation matrix that swaps the first and last components of a 3D vector.</li>
<li>Check whether the identity matrix is diagonal, symmetric, upper triangular, and lower triangular all at once.</li>
</ol>
</section>
</section>
<section id="trace-and-basic-matrix-properties">
<h3 data-anchor-id="trace-and-basic-matrix-properties">18. Trace and Basic Matrix Properties</h3>
<p>In this lab, we’ll introduce the trace of a matrix and a few quick properties that often appear in proofs, algorithms, and applications. The trace is simple to compute but surprisingly powerful.</p>
<section id="set-up-your-lab-17">
<h4 data-anchor-id="set-up-your-lab-17">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-17">
<h4 data-anchor-id="step-by-step-code-walkthrough-17">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>What is the trace? The trace of a square matrix is the sum of its diagonal entries:</li>
</ol>
<p><span>\[
\text{tr}(A) = \sum_i A_{ii}
\]</span></p>
<div id="0727a617" data-execution_count="104">
<div><div id="cb172"><pre><code><span id="cb172-1"><a href="#cb172-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([</span>
<span id="cb172-2"><a href="#cb172-2" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>, <span>1</span>, <span>3</span>],</span>
<span id="cb172-3"><a href="#cb172-3" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>, <span>4</span>, <span>5</span>],</span>
<span id="cb172-4"><a href="#cb172-4" aria-hidden="true" tabindex="-1"></a>    [<span>7</span>, <span>8</span>, <span>6</span>]</span>
<span id="cb172-5"><a href="#cb172-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb172-6"><a href="#cb172-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb172-7"><a href="#cb172-7" aria-hidden="true" tabindex="-1"></a>trace_A <span>=</span> np.trace(A)</span>
<span id="cb172-8"><a href="#cb172-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Matrix A:</span><span>\n</span><span>&#34;</span>, A)</span>
<span id="cb172-9"><a href="#cb172-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Trace of A =&#34;</span>, trace_A)</span></code></pre></div></div>
<div>
<pre><code>Matrix A:
 [[2 1 3]
 [0 4 5]
 [7 8 6]]
Trace of A = 12</code></pre>
</div>
</div>
<p>Here, trace = <span>\(2 + 4 + 6 = 12\)</span>.</p>
<ol start="2" type="1">
<li>Trace is linear For matrices <code>A</code> and <code>B</code>:</li>
</ol>
<p><span>\[
\text{tr}(A+B) = \text{tr}(A) + \text{tr}(B)
\]</span></p>
<p><span>\[
\text{tr}(cA) = c \cdot \text{tr}(A)
\]</span></p>
<div id="a6ff0d5a" data-execution_count="105">
<div><div id="cb174"><pre><code><span id="cb174-1"><a href="#cb174-1" aria-hidden="true" tabindex="-1"></a>B <span>=</span> np.array([</span>
<span id="cb174-2"><a href="#cb174-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>0</span>, <span>0</span>],</span>
<span id="cb174-3"><a href="#cb174-3" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>, <span>2</span>, <span>0</span>],</span>
<span id="cb174-4"><a href="#cb174-4" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>, <span>0</span>, <span>3</span>]</span>
<span id="cb174-5"><a href="#cb174-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb174-6"><a href="#cb174-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb174-7"><a href="#cb174-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;tr(A+B) =&#34;</span>, np.trace(A<span>+</span>B))</span>
<span id="cb174-8"><a href="#cb174-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;tr(A) + tr(B) =&#34;</span>, np.trace(A) <span>+</span> np.trace(B))</span>
<span id="cb174-9"><a href="#cb174-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb174-10"><a href="#cb174-10" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;tr(3A) =&#34;</span>, np.trace(<span>3</span><span>*</span>A))</span>
<span id="cb174-11"><a href="#cb174-11" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;3 * tr(A) =&#34;</span>, <span>3</span><span>*</span>np.trace(A))</span></code></pre></div></div>
<div>
<pre><code>tr(A+B) = 18
tr(A) + tr(B) = 18
tr(3A) = 36
3 * tr(A) = 36</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Trace of a product One important property is:</li>
</ol>
<p><span>\[
\text{tr}(AB) = \text{tr}(BA)
\]</span></p>
<div id="4958c8fd" data-execution_count="106">
<div><div id="cb176"><pre><code><span id="cb176-1"><a href="#cb176-1" aria-hidden="true" tabindex="-1"></a>C <span>=</span> np.array([</span>
<span id="cb176-2"><a href="#cb176-2" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>1</span>],</span>
<span id="cb176-3"><a href="#cb176-3" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>,<span>3</span>]</span>
<span id="cb176-4"><a href="#cb176-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb176-5"><a href="#cb176-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb176-6"><a href="#cb176-6" aria-hidden="true" tabindex="-1"></a>D <span>=</span> np.array([</span>
<span id="cb176-7"><a href="#cb176-7" aria-hidden="true" tabindex="-1"></a>    [<span>4</span>,<span>5</span>],</span>
<span id="cb176-8"><a href="#cb176-8" aria-hidden="true" tabindex="-1"></a>    [<span>6</span>,<span>7</span>]</span>
<span id="cb176-9"><a href="#cb176-9" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb176-10"><a href="#cb176-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb176-11"><a href="#cb176-11" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;tr(CD) =&#34;</span>, np.trace(C.dot(D)))</span>
<span id="cb176-12"><a href="#cb176-12" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;tr(DC) =&#34;</span>, np.trace(D.dot(C)))</span></code></pre></div></div>

</div>
<p>Both are equal, even though <code>CD</code> and <code>DC</code> are different matrices.</p>
<ol start="4" type="1">
<li>Trace and eigenvalues The trace equals the sum of eigenvalues of a matrix (counting multiplicities).</li>
</ol>
<div id="50502a18" data-execution_count="107">
<div><div id="cb178"><pre><code><span id="cb178-1"><a href="#cb178-1" aria-hidden="true" tabindex="-1"></a>vals, vecs <span>=</span> np.linalg.eig(A)</span>
<span id="cb178-2"><a href="#cb178-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvalues:&#34;</span>, vals)</span>
<span id="cb178-3"><a href="#cb178-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Sum of eigenvalues =&#34;</span>, np.<span>sum</span>(vals))</span>
<span id="cb178-4"><a href="#cb178-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Trace =&#34;</span>, np.trace(A))</span></code></pre></div></div>
<div>
<pre><code>Eigenvalues: [12.83286783  2.13019807 -2.9630659 ]
Sum of eigenvalues = 12.000000000000007
Trace = 12</code></pre>
</div>
</div>
<p>The results should match (within rounding error).</p>
<ol start="5" type="1">
<li>Quick invariants</li>
</ol>
<ul>
<li>Trace doesn’t change under transpose: <code>tr(A) = tr(A.T)</code></li>
<li>Trace doesn’t change under similarity transforms: <code>tr(P^-1 A P) = tr(A)</code></li>
</ul>
<div id="3eec7482" data-execution_count="108">
<div><div id="cb180"><pre><code><span id="cb180-1"><a href="#cb180-1" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;tr(A) =&#34;</span>, np.trace(A))</span>
<span id="cb180-2"><a href="#cb180-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;tr(A.T) =&#34;</span>, np.trace(A.T))</span></code></pre></div></div>

</div>
</section>
<section id="try-it-yourself-16">
<h4 data-anchor-id="try-it-yourself-16">Try It Yourself</h4>
<ol type="1">
<li><p>Create a 2×2 rotation matrix for 90°:</p>
<p><span>\[
R = \begin{bmatrix}0 &amp; -1 \\ 1 &amp; 0\end{bmatrix}
\]</span></p>
<p>What is its trace? What does that tell you about its eigenvalues?</p></li>
<li><p>Make a random 3×3 matrix and compare <code>tr(A)</code> with the sum of eigenvalues.</p></li>
<li><p>Test <code>tr(AB)</code> and <code>tr(BA)</code> with a rectangular matrix <code>A</code> (e.g. 2×3) and <code>B</code> (3×2). Do they still match?</p></li>
</ol>
</section>
</section>
<section id="affine-transforms-and-homogeneous-coordinates">
<h3 data-anchor-id="affine-transforms-and-homogeneous-coordinates">19. Affine Transforms and Homogeneous Coordinates</h3>
<p>Affine transformations let us do more than just linear operations - they include translations (shifting points), which ordinary matrices can’t handle alone. To unify rotations, scalings, reflections, and translations, we use homogeneous coordinates.</p>
<section id="set-up-your-lab-18">
<h4 data-anchor-id="set-up-your-lab-18">Set Up Your Lab</h4>
<div id="4421cd9a" data-execution_count="109">
<div><div id="cb182"><pre><code><span id="cb182-1"><a href="#cb182-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb182-2"><a href="#cb182-2" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-18">
<h4 data-anchor-id="step-by-step-code-walkthrough-18">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Linear transformations vs affine transformations</li>
</ol>
<ul>
<li>A linear transformation can rotate, scale, or shear, but always keeps the origin fixed.</li>
<li>An affine transformation allows translation as well.</li>
</ul>
<p>For example, shifting every point by <code>(2,3)</code> is affine but not linear.</p>
<ol start="2" type="1">
<li>Homogeneous coordinates idea We add an extra coordinate (usually <code>1</code>) to vectors.</li>
</ol>
<ul>
<li>A 2D point <code>(x,y)</code> becomes <code>(x,y,1)</code>.</li>
<li>A 3D point <code>(x,y,z)</code> becomes <code>(x,y,z,1)</code>.</li>
</ul>
<p>This trick lets us represent translations using matrix multiplication.</p>
<ol start="3" type="1">
<li>2D translation matrix</li>
</ol>
<p><span>\[
T = \begin{bmatrix}
1 &amp; 0 &amp; t_x \\
0 &amp; 1 &amp; t_y \\
0 &amp; 0 &amp; 1
\end{bmatrix}
\]</span></p>
<div id="e420eb7a" data-execution_count="110">
<div><div id="cb183"><pre><code><span id="cb183-1"><a href="#cb183-1" aria-hidden="true" tabindex="-1"></a>T <span>=</span> np.array([</span>
<span id="cb183-2"><a href="#cb183-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>0</span>, <span>2</span>],</span>
<span id="cb183-3"><a href="#cb183-3" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>, <span>1</span>, <span>3</span>],</span>
<span id="cb183-4"><a href="#cb183-4" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>, <span>0</span>, <span>1</span>]</span>
<span id="cb183-5"><a href="#cb183-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb183-6"><a href="#cb183-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb183-7"><a href="#cb183-7" aria-hidden="true" tabindex="-1"></a>p <span>=</span> np.array([<span>1</span>, <span>1</span>, <span>1</span>])  <span># point at (1,1)</span></span>
<span id="cb183-8"><a href="#cb183-8" aria-hidden="true" tabindex="-1"></a>p_translated <span>=</span> T.dot(p)</span>
<span id="cb183-9"><a href="#cb183-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb183-10"><a href="#cb183-10" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Original point:&#34;</span>, p)</span>
<span id="cb183-11"><a href="#cb183-11" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Translated point:&#34;</span>, p_translated)</span></code></pre></div></div>
<div>
<pre><code>Original point: [1 1 1]
Translated point: [3 4 1]</code></pre>
</div>
</div>
<p>This shifts <code>(1,1)</code> to <code>(3,4)</code>.</p>
<ol start="4" type="1">
<li>Combining rotation and translation</li>
</ol>
<p>A 90° rotation around the origin in 2D:</p>
<div id="0700f9de" data-execution_count="111">
<div><div id="cb185"><pre><code><span id="cb185-1"><a href="#cb185-1" aria-hidden="true" tabindex="-1"></a>R <span>=</span> np.array([</span>
<span id="cb185-2"><a href="#cb185-2" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>, <span>-</span><span>1</span>, <span>0</span>],</span>
<span id="cb185-3"><a href="#cb185-3" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,  <span>0</span>, <span>0</span>],</span>
<span id="cb185-4"><a href="#cb185-4" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,  <span>0</span>, <span>1</span>]</span>
<span id="cb185-5"><a href="#cb185-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb185-6"><a href="#cb185-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb185-7"><a href="#cb185-7" aria-hidden="true" tabindex="-1"></a>M <span>=</span> T.dot(R)  <span># rotate then translate</span></span>
<span id="cb185-8"><a href="#cb185-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Combined transform:</span><span>\n</span><span>&#34;</span>, M)</span>
<span id="cb185-9"><a href="#cb185-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb185-10"><a href="#cb185-10" aria-hidden="true" tabindex="-1"></a>p <span>=</span> np.array([<span>1</span>, <span>0</span>, <span>1</span>])</span>
<span id="cb185-11"><a href="#cb185-11" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rotated + translated point:&#34;</span>, M.dot(p))</span></code></pre></div></div>
<div>
<pre><code>Combined transform:
 [[ 0 -1  2]
 [ 1  0  3]
 [ 0  0  1]]
Rotated + translated point: [2 4 1]</code></pre>
</div>
</div>
<p>Now we can apply rotation and translation in one step.</p>
<ol start="5" type="1">
<li>Visualization of translation</li>
</ol>
<div id="e2fd8359" data-execution_count="112">
<div><div id="cb187"><pre><code><span id="cb187-1"><a href="#cb187-1" aria-hidden="true" tabindex="-1"></a>points <span>=</span> np.array([</span>
<span id="cb187-2"><a href="#cb187-2" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>0</span>,<span>1</span>],</span>
<span id="cb187-3"><a href="#cb187-3" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>0</span>,<span>1</span>],</span>
<span id="cb187-4"><a href="#cb187-4" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>1</span>,<span>1</span>],</span>
<span id="cb187-5"><a href="#cb187-5" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>1</span>,<span>1</span>]</span>
<span id="cb187-6"><a href="#cb187-6" aria-hidden="true" tabindex="-1"></a>])  <span># a unit square</span></span>
<span id="cb187-7"><a href="#cb187-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb187-8"><a href="#cb187-8" aria-hidden="true" tabindex="-1"></a>transformed <span>=</span> points.dot(T.T)</span>
<span id="cb187-9"><a href="#cb187-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb187-10"><a href="#cb187-10" aria-hidden="true" tabindex="-1"></a>plt.scatter(points[:,<span>0</span>], points[:,<span>1</span>], color<span>=</span><span>&#39;r&#39;</span>, label<span>=</span><span>&#39;original&#39;</span>)</span>
<span id="cb187-11"><a href="#cb187-11" aria-hidden="true" tabindex="-1"></a>plt.scatter(transformed[:,<span>0</span>], transformed[:,<span>1</span>], color<span>=</span><span>&#39;b&#39;</span>, label<span>=</span><span>&#39;translated&#39;</span>)</span>
<span id="cb187-12"><a href="#cb187-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb187-13"><a href="#cb187-13" aria-hidden="true" tabindex="-1"></a><span>for</span> i <span>in</span> <span>range</span>(<span>len</span>(points)):</span>
<span id="cb187-14"><a href="#cb187-14" aria-hidden="true" tabindex="-1"></a>    plt.arrow(points[i,<span>0</span>], points[i,<span>1</span>],</span>
<span id="cb187-15"><a href="#cb187-15" aria-hidden="true" tabindex="-1"></a>              transformed[i,<span>0</span>]<span>-</span>points[i,<span>0</span>],</span>
<span id="cb187-16"><a href="#cb187-16" aria-hidden="true" tabindex="-1"></a>              transformed[i,<span>1</span>]<span>-</span>points[i,<span>1</span>],</span>
<span id="cb187-17"><a href="#cb187-17" aria-hidden="true" tabindex="-1"></a>              head_width<span>=</span><span>0.05</span>, color<span>=</span><span>&#39;gray&#39;</span>)</span>
<span id="cb187-18"><a href="#cb187-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb187-19"><a href="#cb187-19" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb187-20"><a href="#cb187-20" aria-hidden="true" tabindex="-1"></a>plt.axis(<span>&#39;equal&#39;</span>)</span>
<span id="cb187-21"><a href="#cb187-21" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb187-22"><a href="#cb187-22" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-113-output-1.png" width="571" height="411"/></p>
</figure>
</div>
</div>
</div>
<p>You’ll see the red unit square moved to a blue unit square shifted by <code>(2,3)</code>.</p>
<ol start="6" type="1">
<li>Extending to 3D In 3D, homogeneous coordinates use 4×4 matrices. Translations, rotations, and scalings all fit the same framework.</li>
</ol>
<div id="5677d97b" data-execution_count="113">
<div><div id="cb188"><pre><code><span id="cb188-1"><a href="#cb188-1" aria-hidden="true" tabindex="-1"></a>T3 <span>=</span> np.array([</span>
<span id="cb188-2"><a href="#cb188-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>0</span>,<span>0</span>,<span>5</span>],</span>
<span id="cb188-3"><a href="#cb188-3" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>1</span>,<span>0</span>,<span>-</span><span>2</span>],</span>
<span id="cb188-4"><a href="#cb188-4" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>0</span>,<span>1</span>,<span>3</span>],</span>
<span id="cb188-5"><a href="#cb188-5" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>0</span>,<span>0</span>,<span>1</span>]</span>
<span id="cb188-6"><a href="#cb188-6" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb188-7"><a href="#cb188-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb188-8"><a href="#cb188-8" aria-hidden="true" tabindex="-1"></a>p3 <span>=</span> np.array([<span>1</span>,<span>2</span>,<span>3</span>,<span>1</span>])</span>
<span id="cb188-9"><a href="#cb188-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Translated 3D point:&#34;</span>, T3.dot(p3))</span></code></pre></div></div>
<div>
<pre><code>Translated 3D point: [6 0 6 1]</code></pre>
</div>
</div>
<p>This shifts <code>(1,2,3)</code> to <code>(6,0,6)</code>.</p>
</section>
<section id="try-it-yourself-17">
<h4 data-anchor-id="try-it-yourself-17">Try It Yourself</h4>
<ol type="1">
<li>Build a scaling matrix in homogeneous coordinates that doubles both x and y, and apply it to <code>(1,1)</code>.</li>
<li>Create a 2D transform that rotates by 90° and then shifts by <code>(−2,1)</code>. Apply it to <code>(0,2)</code>.</li>
<li>In 3D, translate <code>(0,0,0)</code> by <code>(10,10,10)</code>. What homogeneous matrix did you use?</li>
</ol>
</section>
</section>
<section id="computing-with-matrices-cost-counts-and-simple-speedups">
<h3 data-anchor-id="computing-with-matrices-cost-counts-and-simple-speedups">20. Computing with Matrices (Cost Counts and Simple Speedups)</h3>
<p>Working with matrices is not just about theory - in practice, we care about how much computation it takes to perform operations, and how we can make them faster. This lab introduces basic cost analysis (counting operations) and demonstrates simple NumPy optimizations.</p>
<section id="set-up-your-lab-19">
<h4 data-anchor-id="set-up-your-lab-19">Set Up Your Lab</h4>
<div id="103969ab" data-execution_count="114">
<div><div id="cb190"><pre><code><span id="cb190-1"><a href="#cb190-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb190-2"><a href="#cb190-2" aria-hidden="true" tabindex="-1"></a><span>import</span> time</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-19">
<h4 data-anchor-id="step-by-step-code-walkthrough-19">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Counting operations (matrix–vector multiply)</li>
</ol>
<p>If <code>A</code> is an <span>\(m \times n\)</span> matrix and <code>x</code> is an <span>\(n\)</span>-dimensional vector, computing <code>A·x</code> takes about <span>\(m \times n\)</span> multiplications and the same number of additions.</p>
<div id="e1461123" data-execution_count="115">
<div><div id="cb191"><pre><code><span id="cb191-1"><a href="#cb191-1" aria-hidden="true" tabindex="-1"></a>m, n <span>=</span> <span>3</span>, <span>4</span></span>
<span id="cb191-2"><a href="#cb191-2" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.random.randint(<span>1</span>,<span>10</span>,(m,n))</span>
<span id="cb191-3"><a href="#cb191-3" aria-hidden="true" tabindex="-1"></a>x <span>=</span> np.random.randint(<span>1</span>,<span>10</span>,n)</span>
<span id="cb191-4"><a href="#cb191-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb191-5"><a href="#cb191-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Matrix A:</span><span>\n</span><span>&#34;</span>, A)</span>
<span id="cb191-6"><a href="#cb191-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Vector x:&#34;</span>, x)</span>
<span id="cb191-7"><a href="#cb191-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;A·x =&#34;</span>, A.dot(x))</span></code></pre></div></div>
<div>
<pre><code>Matrix A:
 [[6 6 6 2]
 [1 1 1 1]
 [1 8 7 4]]
Vector x: [6 5 4 5]
A·x = [100  20  94]</code></pre>
</div>
</div>
<p>Here the cost is <span>\(3 \times 4 = 12\)</span> multiplications + 12 additions.</p>
<ol start="2" type="1">
<li>Counting operations (matrix–matrix multiply)</li>
</ol>
<p>For an <span>\(m \times n\)</span> times <span>\(n \times p\)</span> multiplication, the cost is about <span>\(m \times n \times p\)</span>.</p>
<div id="bf7e874f" data-execution_count="116">
<div><div id="cb193"><pre><code><span id="cb193-1"><a href="#cb193-1" aria-hidden="true" tabindex="-1"></a>m, n, p <span>=</span> <span>3</span>, <span>4</span>, <span>2</span></span>
<span id="cb193-2"><a href="#cb193-2" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.random.randint(<span>1</span>,<span>10</span>,(m,n))</span>
<span id="cb193-3"><a href="#cb193-3" aria-hidden="true" tabindex="-1"></a>B <span>=</span> np.random.randint(<span>1</span>,<span>10</span>,(n,p))</span>
<span id="cb193-4"><a href="#cb193-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-5"><a href="#cb193-5" aria-hidden="true" tabindex="-1"></a>C <span>=</span> A.dot(B)</span>
<span id="cb193-6"><a href="#cb193-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;A·B =</span><span>\n</span><span>&#34;</span>, C)</span></code></pre></div></div>
<div>
<pre><code>A·B =
 [[ 59  92]
 [ 43  81]
 [ 65 102]]</code></pre>
</div>
</div>
<p>Here the cost is <span>\(3 \times 4 \times 2 = 24\)</span> multiplications + 24 additions.</p>
<ol start="3" type="1">
<li>Timing with NumPy (vectorized vs loop)</li>
</ol>
<p>NumPy is optimized in C and Fortran under the hood. Let’s compare matrix multiplication with and without vectorization.</p>
<div id="f9bebf65" data-execution_count="117">
<div><div id="cb195"><pre><code><span id="cb195-1"><a href="#cb195-1" aria-hidden="true" tabindex="-1"></a>n <span>=</span> <span>50</span></span>
<span id="cb195-2"><a href="#cb195-2" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.random.randn(n,n)</span>
<span id="cb195-3"><a href="#cb195-3" aria-hidden="true" tabindex="-1"></a>B <span>=</span> np.random.randn(n,n)</span>
<span id="cb195-4"><a href="#cb195-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb195-5"><a href="#cb195-5" aria-hidden="true" tabindex="-1"></a><span># Vectorized</span></span>
<span id="cb195-6"><a href="#cb195-6" aria-hidden="true" tabindex="-1"></a>start <span>=</span> time.time()</span>
<span id="cb195-7"><a href="#cb195-7" aria-hidden="true" tabindex="-1"></a>C1 <span>=</span> A.dot(B)</span>
<span id="cb195-8"><a href="#cb195-8" aria-hidden="true" tabindex="-1"></a>end <span>=</span> time.time()</span>
<span id="cb195-9"><a href="#cb195-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Vectorized dot:&#34;</span>, <span>round</span>(end<span>-</span>start,<span>3</span>), <span>&#34;seconds&#34;</span>)</span>
<span id="cb195-10"><a href="#cb195-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb195-11"><a href="#cb195-11" aria-hidden="true" tabindex="-1"></a><span># Manual loops</span></span>
<span id="cb195-12"><a href="#cb195-12" aria-hidden="true" tabindex="-1"></a>C2 <span>=</span> np.zeros((n,n))</span>
<span id="cb195-13"><a href="#cb195-13" aria-hidden="true" tabindex="-1"></a>start <span>=</span> time.time()</span>
<span id="cb195-14"><a href="#cb195-14" aria-hidden="true" tabindex="-1"></a><span>for</span> i <span>in</span> <span>range</span>(n):</span>
<span id="cb195-15"><a href="#cb195-15" aria-hidden="true" tabindex="-1"></a>    <span>for</span> j <span>in</span> <span>range</span>(n):</span>
<span id="cb195-16"><a href="#cb195-16" aria-hidden="true" tabindex="-1"></a>        <span>for</span> k <span>in</span> <span>range</span>(n):</span>
<span id="cb195-17"><a href="#cb195-17" aria-hidden="true" tabindex="-1"></a>            C2[i,j] <span>+=</span> A[i,k]<span>*</span>B[k,j]</span>
<span id="cb195-18"><a href="#cb195-18" aria-hidden="true" tabindex="-1"></a>end <span>=</span> time.time()</span>
<span id="cb195-19"><a href="#cb195-19" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Triple loop:&#34;</span>, <span>round</span>(end<span>-</span>start,<span>3</span>), <span>&#34;seconds&#34;</span>)</span></code></pre></div></div>
<div>
<pre><code>Vectorized dot: 0.0 seconds
Triple loop: 0.026 seconds</code></pre>
</div>
</div>
<p>The vectorized version should be thousands of times faster.</p>
<ol start="4" type="1">
<li>Broadcasting tricks</li>
</ol>
<p>NumPy lets us avoid loops by broadcasting operations across entire rows or columns.</p>
<div id="0c94fbbf" data-execution_count="118">
<div><div id="cb197"><pre><code><span id="cb197-1"><a href="#cb197-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([</span>
<span id="cb197-2"><a href="#cb197-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>2</span>,<span>3</span>],</span>
<span id="cb197-3"><a href="#cb197-3" aria-hidden="true" tabindex="-1"></a>    [<span>4</span>,<span>5</span>,<span>6</span>]</span>
<span id="cb197-4"><a href="#cb197-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb197-5"><a href="#cb197-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb197-6"><a href="#cb197-6" aria-hidden="true" tabindex="-1"></a><span># Add 10 to every entry</span></span>
<span id="cb197-7"><a href="#cb197-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;A+10 =</span><span>\n</span><span>&#34;</span>, A<span>+</span><span>10</span>)</span>
<span id="cb197-8"><a href="#cb197-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb197-9"><a href="#cb197-9" aria-hidden="true" tabindex="-1"></a><span># Multiply each row by a different scalar</span></span>
<span id="cb197-10"><a href="#cb197-10" aria-hidden="true" tabindex="-1"></a>scales <span>=</span> np.array([<span>1</span>,<span>10</span>])[:,<span>None</span>]</span>
<span id="cb197-11"><a href="#cb197-11" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Row-scaled A =</span><span>\n</span><span>&#34;</span>, A<span>*</span>scales)</span></code></pre></div></div>
<div>
<pre><code>A+10 =
 [[11 12 13]
 [14 15 16]]
Row-scaled A =
 [[ 1  2  3]
 [40 50 60]]</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Memory and data types</li>
</ol>
<p>For large computations, data type matters.</p>
<div id="c61e5792" data-execution_count="119">
<div><div id="cb199"><pre><code><span id="cb199-1"><a href="#cb199-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.random.randn(<span>1000</span>,<span>1000</span>).astype(np.float32)  <span># 32-bit floats</span></span>
<span id="cb199-2"><a href="#cb199-2" aria-hidden="true" tabindex="-1"></a>B <span>=</span> np.random.randn(<span>1000</span>,<span>1000</span>).astype(np.float32)</span>
<span id="cb199-3"><a href="#cb199-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb199-4"><a href="#cb199-4" aria-hidden="true" tabindex="-1"></a>start <span>=</span> time.time()</span>
<span id="cb199-5"><a href="#cb199-5" aria-hidden="true" tabindex="-1"></a>C <span>=</span> A.dot(B)</span>
<span id="cb199-6"><a href="#cb199-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Result shape:&#34;</span>, C.shape, <span>&#34;dtype:&#34;</span>, C.dtype)</span>
<span id="cb199-7"><a href="#cb199-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Time:&#34;</span>, <span>round</span>(time.time()<span>-</span>start,<span>3</span>), <span>&#34;seconds&#34;</span>)</span></code></pre></div></div>
<div>
<pre><code>Result shape: (1000, 1000) dtype: float32
Time: 0.002 seconds</code></pre>
</div>
</div>
<p>Using <code>float32</code> instead of <code>float64</code> halves memory use and can speed up computation (at the cost of some precision).</p>
</section>
<section id="try-it-yourself-18">
<h4 data-anchor-id="try-it-yourself-18">Try It Yourself</h4>
<ol type="1">
<li>Compute the cost of multiplying a 200×500 matrix with a 500×1000 matrix. How many multiplications are needed?</li>
<li>Time matrix multiplication for sizes 100, 500, 1000 in NumPy. How does the time scale?</li>
<li>Experiment with <code>float32</code> vs <code>float64</code> in NumPy. How do speed and memory change?</li>
<li>Try broadcasting: multiply each column of a matrix by <code>[1,2,3,...]</code>.</li>
</ol>
</section>
<section id="the-takeaway-2">
<h4 data-anchor-id="the-takeaway-2">The Takeaway</h4>
<ul>
<li>Matrix operations have predictable computational costs: <code>A·x</code> ~ <span>\(m \times n\)</span>, <code>A·B</code> ~ <span>\(m \times n \times p\)</span>.</li>
<li>Vectorized NumPy operations are vastly faster than Python loops.</li>
<li>Broadcasting and choosing the right data type are simple speedups every beginner should learn.</li>
</ul>
</section>
</section>
</section>
<section id="chapter-3.-linear-systems-and-elimination">
<h2 data-anchor-id="chapter-3.-linear-systems-and-elimination">Chapter 3. Linear Systems and Elimination</h2>
<section id="from-equations-to-matrices-augmenting-and-encoding">
<h3 data-anchor-id="from-equations-to-matrices-augmenting-and-encoding">21. From Equations to Matrices (Augmenting and Encoding)</h3>
<p>Linear algebra often begins with solving systems of linear equations. For example:</p>
<p><span>\[
\begin{cases}  
x + 2y = 5 \\  
3x - y = 4  
\end{cases}
\]</span></p>
<p>Instead of juggling symbols, we can encode the entire system into a matrix. This is the key idea that lets computers handle thousands or millions of equations efficiently.</p>
<section id="set-up-your-lab-20">
<h4 data-anchor-id="set-up-your-lab-20">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-20">
<h4 data-anchor-id="step-by-step-code-walkthrough-20">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Write a system of equations</li>
</ol>
<p>We’ll use this small example:</p>
<p><span>\[
\begin{cases}  
2x + y = 8 \\  
-3x + 4y = -11  
\end{cases}
\]</span></p>
<ol start="2" type="1">
<li>Encode coefficients and constants</li>
</ol>
<ul>
<li>Coefficient matrix <span>\(A\)</span>: numbers multiplying variables.</li>
<li>Variable vector <span>\(x\)</span>: unknowns <code>[x, y]</code>.</li>
<li>Constant vector <span>\(b\)</span>: right-hand side.</li>
</ul>
<div id="6a6169f5" data-execution_count="121">
<div><div id="cb202"><pre><code><span id="cb202-1"><a href="#cb202-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([</span>
<span id="cb202-2"><a href="#cb202-2" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>, <span>1</span>],</span>
<span id="cb202-3"><a href="#cb202-3" aria-hidden="true" tabindex="-1"></a>    [<span>-</span><span>3</span>, <span>4</span>]</span>
<span id="cb202-4"><a href="#cb202-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb202-5"><a href="#cb202-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb202-6"><a href="#cb202-6" aria-hidden="true" tabindex="-1"></a>b <span>=</span> np.array([<span>8</span>, <span>-</span><span>11</span>])</span>
<span id="cb202-7"><a href="#cb202-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb202-8"><a href="#cb202-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Coefficient matrix A:</span><span>\n</span><span>&#34;</span>, A)</span>
<span id="cb202-9"><a href="#cb202-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Constants vector b:&#34;</span>, b)</span></code></pre></div></div>
<div>
<pre><code>Coefficient matrix A:
 [[ 2  1]
 [-3  4]]
Constants vector b: [  8 -11]</code></pre>
</div>
</div>
<p>So the system is <span>\(A·x = b\)</span>.</p>
<ol start="3" type="1">
<li>Augmented matrix</li>
</ol>
<p>We can bundle the system into one compact matrix:</p>
<p><span>\[
[A|b] = \begin{bmatrix}2 &amp; 1 &amp; | &amp; 8 \\ -3 &amp; 4 &amp; | &amp; -11 \end{bmatrix}
\]</span></p>
<div id="a55e8e75" data-execution_count="122">
<div><div id="cb204"><pre><code><span id="cb204-1"><a href="#cb204-1" aria-hidden="true" tabindex="-1"></a>augmented <span>=</span> np.column_stack((A, b))</span>
<span id="cb204-2"><a href="#cb204-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Augmented matrix:</span><span>\n</span><span>&#34;</span>, augmented)</span></code></pre></div></div>
<div>
<pre><code>Augmented matrix:
 [[  2   1   8]
 [ -3   4 -11]]</code></pre>
</div>
</div>
<p>This format is useful for elimination algorithms.</p>
<ol start="4" type="1">
<li>Solving directly with NumPy</li>
</ol>
<div id="84bd6cc5" data-execution_count="123">
<div><div id="cb206"><pre><code><span id="cb206-1"><a href="#cb206-1" aria-hidden="true" tabindex="-1"></a>solution <span>=</span> np.linalg.solve(A, b)</span>
<span id="cb206-2"><a href="#cb206-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Solution (x,y):&#34;</span>, solution)</span></code></pre></div></div>
<div>
<pre><code>Solution (x,y): [3.90909091 0.18181818]</code></pre>
</div>
</div>
<p>Here NumPy solves the system using efficient algorithms.</p>
<ol start="5" type="1">
<li>Checking the solution</li>
</ol>
<p>Always verify:</p>
<div id="8039438f" data-execution_count="124">
<div><div id="cb208"><pre><code><span id="cb208-1"><a href="#cb208-1" aria-hidden="true" tabindex="-1"></a>check <span>=</span> A.dot(solution)</span>
<span id="cb208-2"><a href="#cb208-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;A·x =&#34;</span>, check, <span>&#34;should equal b =&#34;</span>, b)</span></code></pre></div></div>
<div>
<pre><code>A·x = [  8. -11.] should equal b = [  8 -11]</code></pre>
</div>
</div>
<ol start="6" type="1">
<li>Another example (3 variables)</li>
</ol>
<p><span>\[
\begin{cases}  
x + y + z = 6 \\  
2x - y + z = 3 \\  
- x + 2y - z = 2  
\end{cases}
\]</span></p>
<div id="57d31f2e" data-execution_count="125">
<div><div id="cb210"><pre><code><span id="cb210-1"><a href="#cb210-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([</span>
<span id="cb210-2"><a href="#cb210-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>1</span>, <span>1</span>],</span>
<span id="cb210-3"><a href="#cb210-3" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>, <span>-</span><span>1</span>, <span>1</span>],</span>
<span id="cb210-4"><a href="#cb210-4" aria-hidden="true" tabindex="-1"></a>    [<span>-</span><span>1</span>, <span>2</span>, <span>-</span><span>1</span>]</span>
<span id="cb210-5"><a href="#cb210-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb210-6"><a href="#cb210-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb210-7"><a href="#cb210-7" aria-hidden="true" tabindex="-1"></a>b <span>=</span> np.array([<span>6</span>, <span>3</span>, <span>2</span>])</span>
<span id="cb210-8"><a href="#cb210-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb210-9"><a href="#cb210-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Augmented matrix:</span><span>\n</span><span>&#34;</span>, np.column_stack((A, b)))</span>
<span id="cb210-10"><a href="#cb210-10" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Solution:&#34;</span>, np.linalg.solve(A, b))</span></code></pre></div></div>
<div>
<pre><code>Augmented matrix:
 [[ 1  1  1  6]
 [ 2 -1  1  3]
 [-1  2 -1  2]]
Solution: [2.33333333 2.66666667 1.        ]</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-19">
<h4 data-anchor-id="try-it-yourself-19">Try It Yourself</h4>
<ol type="1">
<li><p>Encode the system:</p>
<p><span>\[
\begin{cases}  
2x - y = 1 \\  
x + 3y = 7  
\end{cases}
\]</span></p>
<p>Write <code>A</code> and <code>b</code>, then solve.</p></li>
<li><p>For a 3×3 system, try creating a random coefficient matrix with <code>np.random.randint(-5,5,(3,3))</code> and a random <code>b</code>. Use <code>np.linalg.solve</code>.</p></li>
<li><p>Modify the constants <code>b</code> slightly and see how the solution changes. This introduces the idea of sensitivity.</p></li>
</ol>
</section>
<section id="the-takeaway-3">
<h4 data-anchor-id="the-takeaway-3">The Takeaway</h4>
<ul>
<li>Systems of linear equations can be neatly written as <span>\(A·x = b\)</span>.</li>
<li>The augmented matrix <span>\([A|b]\)</span> is a compact way to set up elimination.</li>
<li>This matrix encoding transforms algebra problems into matrix problems - the gateway to all of linear algebra.</li>
</ul>
</section>
</section>
<section id="row-operations-legal-moves-that-keep-solutions">
<h3 data-anchor-id="row-operations-legal-moves-that-keep-solutions">22. Row Operations (Legal Moves That Keep Solutions)</h3>
<p>When solving linear systems, we don’t want to change the solutions - just simplify the system into an easier form. This is where row operations come in. They are the “legal moves” we can do on an augmented matrix <span>\([A|b]\)</span> without changing the solution set.</p>
<section id="set-up-your-lab-21">
<h4 data-anchor-id="set-up-your-lab-21">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-21">
<h4 data-anchor-id="step-by-step-code-walkthrough-21">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li><p>Three legal row operations</p></li>
<li><p>Swap two rows <span>\((R_i \leftrightarrow R_j)\)</span></p></li>
<li><p>Multiply a row by a nonzero scalar <span>\((R_i \to c·R_i)\)</span></p></li>
<li><p>Replace a row with itself plus a multiple of another row <span>\((R_i \to R_i + c·R_j)\)</span></p></li>
</ol>
<p>These preserve the solution set.</p>
<ol start="2" type="1">
<li>Start with an augmented matrix</li>
</ol>
<p>System:</p>
<p><span>\[
\begin{cases}  
x + 2y = 5 \\  
3x + 4y = 6  
\end{cases}
\]</span></p>
<div id="8c223c3c" data-execution_count="127">
<div><div id="cb213"><pre><code><span id="cb213-1"><a href="#cb213-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([</span>
<span id="cb213-2"><a href="#cb213-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>2</span>, <span>5</span>],</span>
<span id="cb213-3"><a href="#cb213-3" aria-hidden="true" tabindex="-1"></a>    [<span>3</span>, <span>4</span>, <span>6</span>]</span>
<span id="cb213-4"><a href="#cb213-4" aria-hidden="true" tabindex="-1"></a>], dtype<span>=</span><span>float</span>)</span>
<span id="cb213-5"><a href="#cb213-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb213-6"><a href="#cb213-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Initial augmented matrix:</span><span>\n</span><span>&#34;</span>, A)</span></code></pre></div></div>
<div>
<pre><code>Initial augmented matrix:
 [[1. 2. 5.]
 [3. 4. 6.]]</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Row swap</li>
</ol>
<p>Swap row 0 and row 1.</p>
<div id="86fba1f6" data-execution_count="128">
<div><div id="cb215"><pre><code><span id="cb215-1"><a href="#cb215-1" aria-hidden="true" tabindex="-1"></a>A[[<span>0</span>,<span>1</span>]] <span>=</span> A[[<span>1</span>,<span>0</span>]]</span>
<span id="cb215-2"><a href="#cb215-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;After swapping rows:</span><span>\n</span><span>&#34;</span>, A)</span></code></pre></div></div>
<div>
<pre><code>After swapping rows:
 [[3. 4. 6.]
 [1. 2. 5.]]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Multiply a row by a scalar</li>
</ol>
<p>Make the pivot in row 0 equal to 1.</p>
<div id="df5c841e" data-execution_count="129">
<div><div id="cb217"><pre><code><span id="cb217-1"><a href="#cb217-1" aria-hidden="true" tabindex="-1"></a>A[<span>0</span>] <span>=</span> A[<span>0</span>] <span>/</span> A[<span>0</span>,<span>0</span>]</span>
<span id="cb217-2"><a href="#cb217-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;After scaling first row:</span><span>\n</span><span>&#34;</span>, A)</span></code></pre></div></div>
<div>
<pre><code>After scaling first row:
 [[1.         1.33333333 2.        ]
 [1.         2.         5.        ]]</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Add a multiple of another row</li>
</ol>
<p>Eliminate the first column of row 1.</p>
<div id="d24792e2" data-execution_count="130">
<div><div id="cb219"><pre><code><span id="cb219-1"><a href="#cb219-1" aria-hidden="true" tabindex="-1"></a>A[<span>1</span>] <span>=</span> A[<span>1</span>] <span>-</span> <span>3</span><span>*</span>A[<span>0</span>]</span>
<span id="cb219-2"><a href="#cb219-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;After eliminating x from second row:</span><span>\n</span><span>&#34;</span>, A)</span></code></pre></div></div>
<div>
<pre><code>After eliminating x from second row:
 [[ 1.          1.33333333  2.        ]
 [-2.         -2.         -1.        ]]</code></pre>
</div>
</div>
<p>Now the system is simpler: second row has only <code>y</code>.</p>
<ol start="6" type="1">
<li>Solving from the new system</li>
</ol>
<div id="db898428" data-execution_count="131">
<div><div id="cb221"><pre><code><span id="cb221-1"><a href="#cb221-1" aria-hidden="true" tabindex="-1"></a>y <span>=</span> A[<span>1</span>,<span>2</span>] <span>/</span> A[<span>1</span>,<span>1</span>]</span>
<span id="cb221-2"><a href="#cb221-2" aria-hidden="true" tabindex="-1"></a>x <span>=</span> (A[<span>0</span>,<span>2</span>] <span>-</span> A[<span>0</span>,<span>1</span>]<span>*</span>y) <span>/</span> A[<span>0</span>,<span>0</span>]</span>
<span id="cb221-3"><a href="#cb221-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Solution: x =&#34;</span>, x, <span>&#34;, y =&#34;</span>, y)</span></code></pre></div></div>
<div>
<pre><code>Solution: x = 1.3333333333333335 , y = 0.5</code></pre>
</div>
</div>
<ol start="7" type="1">
<li>Using NumPy step-by-step vs solver</li>
</ol>
<div id="c69385f3" data-execution_count="132">
<div><div id="cb223"><pre><code><span id="cb223-1"><a href="#cb223-1" aria-hidden="true" tabindex="-1"></a>coeff <span>=</span> np.array([[<span>1</span>,<span>2</span>],[<span>3</span>,<span>4</span>]])</span>
<span id="cb223-2"><a href="#cb223-2" aria-hidden="true" tabindex="-1"></a>const <span>=</span> np.array([<span>5</span>,<span>6</span>])</span>
<span id="cb223-3"><a href="#cb223-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;np.linalg.solve result:&#34;</span>, np.linalg.solve(coeff,const))</span></code></pre></div></div>
<div>
<pre><code>np.linalg.solve result: [-4.   4.5]</code></pre>
</div>
</div>
<p>Both methods give the same solution.</p>
</section>
<section id="try-it-yourself-20">
<h4 data-anchor-id="try-it-yourself-20">Try It Yourself</h4>
<ol type="1">
<li><p>Take the system:</p>
<p><span>\[
\begin{cases}  
2x + y = 7 \\  
x - y = 1  
\end{cases}
\]</span></p>
<p>Write its augmented matrix, then:</p>
<ul>
<li>Swap rows.</li>
<li>Scale the first row.</li>
<li>Eliminate one variable.</li>
</ul></li>
<li><p>Create a random 3×3 system with integers between -5 and 5. Perform at least one of each row operation manually in code.</p></li>
<li><p>Experiment with multiplying a row by <code>0</code>. What happens, and why is this not allowed as a legal operation?</p></li>
</ol>
</section>
<section id="the-takeaway-4">
<h4 data-anchor-id="the-takeaway-4">The Takeaway</h4>
<ul>
<li>The three legal row operations are row swap, row scaling, and row replacement.</li>
<li>These steps preserve the solution set while moving toward a simpler form.</li>
<li>They are the foundation of Gaussian elimination, the standard algorithm for solving linear systems.</li>
</ul>
</section>
</section>
<section id="row-echelon-and-reduced-row-echelon-forms-target-shapes">
<h3 data-anchor-id="row-echelon-and-reduced-row-echelon-forms-target-shapes">23. Row-Echelon and Reduced Row-Echelon Forms (Target Shapes)</h3>
<p>When solving systems, our goal is to simplify the augmented matrix into a standard shape where the solutions are easy to read. These shapes are called row-echelon form (REF) and reduced row-echelon form (RREF).</p>
<section id="set-up-your-lab-22">
<h4 data-anchor-id="set-up-your-lab-22">Set Up Your Lab</h4>
<div id="24aacc77" data-execution_count="133">
<div><div id="cb225"><pre><code><span id="cb225-1"><a href="#cb225-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb225-2"><a href="#cb225-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
<p>We’ll use NumPy for basic work and SymPy for exact RREF (since NumPy doesn’t have it built-in).</p>
</section>
<section id="step-by-step-code-walkthrough-22">
<h4 data-anchor-id="step-by-step-code-walkthrough-22">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Row-Echelon Form (REF)</li>
</ol>
<ul>
<li>All nonzero rows are above any zero rows.</li>
<li>Each leading entry (pivot) is to the right of the pivot in the row above.</li>
<li>Pivots are usually scaled to 1, but not strictly required.</li>
</ul>
<p>Example system:</p>
<p><span>\[
\begin{cases}  
x + 2y + z = 7 \\  
2x + 4y + z = 12 \\  
3x + 6y + 2z = 17  
\end{cases}
\]</span></p>
<div id="c783cd66" data-execution_count="134">
<div><div id="cb226"><pre><code><span id="cb226-1"><a href="#cb226-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([</span>
<span id="cb226-2"><a href="#cb226-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>2</span>, <span>1</span>, <span>7</span>],</span>
<span id="cb226-3"><a href="#cb226-3" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>, <span>4</span>, <span>1</span>, <span>12</span>],</span>
<span id="cb226-4"><a href="#cb226-4" aria-hidden="true" tabindex="-1"></a>    [<span>3</span>, <span>6</span>, <span>2</span>, <span>17</span>]</span>
<span id="cb226-5"><a href="#cb226-5" aria-hidden="true" tabindex="-1"></a>], dtype<span>=</span><span>float</span>)</span>
<span id="cb226-6"><a href="#cb226-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb226-7"><a href="#cb226-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Augmented matrix:</span><span>\n</span><span>&#34;</span>, A)</span></code></pre></div></div>
<div>
<pre><code>Augmented matrix:
 [[ 1.  2.  1.  7.]
 [ 2.  4.  1. 12.]
 [ 3.  6.  2. 17.]]</code></pre>
</div>
</div>
<p>Perform elimination manually:</p>
<div id="554a715c" data-execution_count="135">
<div><div id="cb228"><pre><code><span id="cb228-1"><a href="#cb228-1" aria-hidden="true" tabindex="-1"></a><span># eliminate first column entries below pivot</span></span>
<span id="cb228-2"><a href="#cb228-2" aria-hidden="true" tabindex="-1"></a>A[<span>1</span>] <span>=</span> A[<span>1</span>] <span>-</span> <span>2</span><span>*</span>A[<span>0</span>]</span>
<span id="cb228-3"><a href="#cb228-3" aria-hidden="true" tabindex="-1"></a>A[<span>2</span>] <span>=</span> A[<span>2</span>] <span>-</span> <span>3</span><span>*</span>A[<span>0</span>]</span>
<span id="cb228-4"><a href="#cb228-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;After eliminating first column:</span><span>\n</span><span>&#34;</span>, A)</span></code></pre></div></div>
<div>
<pre><code>After eliminating first column:
 [[ 1.  2.  1.  7.]
 [ 0.  0. -1. -2.]
 [ 0.  0. -1. -4.]]</code></pre>
</div>
</div>
<p>Now the pivots move diagonally across the matrix - this is row-echelon form.</p>
<ol start="2" type="1">
<li>Reduced Row-Echelon Form (RREF) In RREF, we go further:</li>
</ol>
<ul>
<li>Every pivot = 1.</li>
<li>Every pivot is the only nonzero in its column.</li>
</ul>
<p>Instead of coding manually, we’ll let SymPy handle it:</p>
<div id="0fe6c71b" data-execution_count="136">
<div><div id="cb230"><pre><code><span id="cb230-1"><a href="#cb230-1" aria-hidden="true" tabindex="-1"></a>M <span>=</span> Matrix([</span>
<span id="cb230-2"><a href="#cb230-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>2</span>, <span>1</span>, <span>7</span>],</span>
<span id="cb230-3"><a href="#cb230-3" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>, <span>4</span>, <span>1</span>, <span>12</span>],</span>
<span id="cb230-4"><a href="#cb230-4" aria-hidden="true" tabindex="-1"></a>    [<span>3</span>, <span>6</span>, <span>2</span>, <span>17</span>]</span>
<span id="cb230-5"><a href="#cb230-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb230-6"><a href="#cb230-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb230-7"><a href="#cb230-7" aria-hidden="true" tabindex="-1"></a>M_rref <span>=</span> M.rref()</span>
<span id="cb230-8"><a href="#cb230-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;RREF form:</span><span>\n</span><span>&#34;</span>, M_rref[<span>0</span>])</span></code></pre></div></div>
<div>
<pre><code>RREF form:
 Matrix([[1, 2, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])</code></pre>
</div>
</div>
<p>SymPy shows the final canonical form.</p>
<ol start="3" type="1">
<li>Reading solutions from RREF</li>
</ol>
<p>If the RREF looks like:</p>
<p><span>\[
\begin{bmatrix}  
1 &amp; 0 &amp; a &amp; b \\  
0 &amp; 1 &amp; c &amp; d \\  
0 &amp; 0 &amp; 0 &amp; 0  
\end{bmatrix}
\]</span></p>
<p>It means:</p>
<ul>
<li>The first two variables are leading (pivots).</li>
<li>The third variable is free.</li>
<li>Solutions can be written in terms of the free variable.</li>
</ul>
<ol start="4" type="1">
<li>A quick example with free variables</li>
</ol>
<p>System:</p>
<p><span>\[
x + y + z = 3 \\  
2x + y - z = 0  
\]</span></p>
<div id="3a701896" data-execution_count="137">
<div><div id="cb232"><pre><code><span id="cb232-1"><a href="#cb232-1" aria-hidden="true" tabindex="-1"></a>M2 <span>=</span> Matrix([</span>
<span id="cb232-2"><a href="#cb232-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>1</span>,<span>1</span>,<span>3</span>],</span>
<span id="cb232-3"><a href="#cb232-3" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>,<span>1</span>,<span>-</span><span>1</span>,<span>0</span>]</span>
<span id="cb232-4"><a href="#cb232-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb232-5"><a href="#cb232-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-6"><a href="#cb232-6" aria-hidden="true" tabindex="-1"></a>M2_rref <span>=</span> M2.rref()</span>
<span id="cb232-7"><a href="#cb232-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;RREF form:</span><span>\n</span><span>&#34;</span>, M2_rref[<span>0</span>])</span></code></pre></div></div>
<div>
<pre><code>RREF form:
 Matrix([[1, 0, -2, -3], [0, 1, 3, 6]])</code></pre>
</div>
</div>
<p>Here, one column will not have a pivot → that variable is free.</p>
</section>
<section id="try-it-yourself-21">
<h4 data-anchor-id="try-it-yourself-21">Try It Yourself</h4>
<ol type="1">
<li><p>Take the system:</p>
<p><span>\[
2x + 3y = 6, \quad 4x + 6y = 12
\]</span></p>
<p>Write the augmented matrix and compute its RREF. What does it tell you about solutions?</p></li>
<li><p>Create a random 3×4 matrix in NumPy. Use SymPy’s <code>Matrix.rref()</code> to compute its reduced form. Identify the pivot columns.</p></li>
<li><p>For the system:</p>
<p><span>\[
x + 2y + 3z = 4, \quad 2x + 4y + 6z = 8
\]</span></p>
<p>Check if the equations are independent or multiples of each other by looking at the RREF.</p></li>
</ol>
</section>
<section id="the-takeaway-5">
<h4 data-anchor-id="the-takeaway-5">The Takeaway</h4>
<ul>
<li>REF organizes equations into a staircase shape.</li>
<li>RREF goes further, making each pivot the only nonzero in its column.</li>
<li>These canonical forms make it easy to identify pivot variables, free variables, and the solution set structure.</li>
</ul>
</section>
</section>
<section id="pivots-free-variables-and-leading-ones-reading-solutions">
<h3 data-anchor-id="pivots-free-variables-and-leading-ones-reading-solutions">24. Pivots, Free Variables, and Leading Ones (Reading Solutions)</h3>
<p>Once a matrix is in row-echelon or reduced row-echelon form, the solutions to the system become visible. The key is identifying pivots, leading ones, and free variables.</p>
<section id="set-up-your-lab-23">
<h4 data-anchor-id="set-up-your-lab-23">Set Up Your Lab</h4>
<div id="891e0aed" data-execution_count="138">
<div><div id="cb234"><pre><code><span id="cb234-1"><a href="#cb234-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb234-2"><a href="#cb234-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-23">
<h4 data-anchor-id="step-by-step-code-walkthrough-23">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>What are pivots?</li>
</ol>
<ul>
<li>A pivot is the first nonzero entry in a row (after elimination).</li>
<li>In RREF, pivots are scaled to <code>1</code> and are called leading ones.</li>
<li>Pivot columns correspond to basic variables.</li>
</ul>
<ol start="2" type="1">
<li>Example system</li>
</ol>
<p><span>\[
\begin{cases}  
x + y + z = 6 \\  
2x + 3y + z = 10  
\end{cases}
\]</span></p>
<div id="59c90403" data-execution_count="139">
<div><div id="cb235"><pre><code><span id="cb235-1"><a href="#cb235-1" aria-hidden="true" tabindex="-1"></a>M <span>=</span> Matrix([</span>
<span id="cb235-2"><a href="#cb235-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>1</span>,<span>1</span>,<span>6</span>],</span>
<span id="cb235-3"><a href="#cb235-3" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>,<span>3</span>,<span>1</span>,<span>10</span>]</span>
<span id="cb235-4"><a href="#cb235-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb235-5"><a href="#cb235-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb235-6"><a href="#cb235-6" aria-hidden="true" tabindex="-1"></a>M_rref <span>=</span> M.rref()</span>
<span id="cb235-7"><a href="#cb235-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;RREF form:</span><span>\n</span><span>&#34;</span>, M_rref[<span>0</span>])</span></code></pre></div></div>
<div>
<pre><code>RREF form:
 Matrix([[1, 0, 2, 8], [0, 1, -1, -2]])</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Interpreting the RREF</li>
</ol>
<p>Suppose the RREF comes out as:</p>
<p><span>\[
\begin{bmatrix}  
1 &amp; 0 &amp; -2 &amp; 4 \\  
0 &amp; 1 &amp; 1 &amp; 2  
\end{bmatrix}
\]</span></p>
<p>This means:</p>
<ul>
<li><p>Pivot columns: 1 and 2 → variables <span>\(x\)</span> and <span>\(y\)</span> are basic.</p></li>
<li><p>Free variable: <span>\(z\)</span>.</p></li>
<li><p>Equations:</p>
<p><span>\[
x - 2z = 4, \quad y + z = 2
\]</span></p></li>
<li><p>Solution in terms of <span>\(z\)</span>:</p>
<p><span>\[
x = 4 + 2z, \quad y = 2 - z, \quad z = z
\]</span></p></li>
</ul>
<ol start="4" type="1">
<li>Coding the solution extraction</li>
</ol>
<div id="f357876e" data-execution_count="140">
<div><div id="cb237"><pre><code><span id="cb237-1"><a href="#cb237-1" aria-hidden="true" tabindex="-1"></a>rref_matrix, pivots <span>=</span> M_rref</span>
<span id="cb237-2"><a href="#cb237-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Pivot columns:&#34;</span>, pivots)</span>
<span id="cb237-3"><a href="#cb237-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb237-4"><a href="#cb237-4" aria-hidden="true" tabindex="-1"></a><span># free variables are the columns not in pivots</span></span>
<span id="cb237-5"><a href="#cb237-5" aria-hidden="true" tabindex="-1"></a>all_vars <span>=</span> <span>set</span>(<span>range</span>(rref_matrix.shape[<span>1</span>]<span>-</span><span>1</span>))  <span># exclude last column (constants)</span></span>
<span id="cb237-6"><a href="#cb237-6" aria-hidden="true" tabindex="-1"></a>free_vars <span>=</span> all_vars <span>-</span> <span>set</span>(pivots)</span>
<span id="cb237-7"><a href="#cb237-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Free variable indices:&#34;</span>, free_vars)</span></code></pre></div></div>
<div>
<pre><code>Pivot columns: (0, 1)
Free variable indices: {2}</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Another example with infinitely many solutions</li>
</ol>
<p><span>\[
x + 2y + 3z = 4, \quad 2x + 4y + 6z = 8
\]</span></p>
<div id="06e82a94" data-execution_count="141">
<div><div id="cb239"><pre><code><span id="cb239-1"><a href="#cb239-1" aria-hidden="true" tabindex="-1"></a>M2 <span>=</span> Matrix([</span>
<span id="cb239-2"><a href="#cb239-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>2</span>,<span>3</span>,<span>4</span>],</span>
<span id="cb239-3"><a href="#cb239-3" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>,<span>4</span>,<span>6</span>,<span>8</span>]</span>
<span id="cb239-4"><a href="#cb239-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb239-5"><a href="#cb239-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb239-6"><a href="#cb239-6" aria-hidden="true" tabindex="-1"></a>M2_rref <span>=</span> M2.rref()</span>
<span id="cb239-7"><a href="#cb239-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;RREF form:</span><span>\n</span><span>&#34;</span>, M2_rref[<span>0</span>])</span></code></pre></div></div>
<div>
<pre><code>RREF form:
 Matrix([[1, 2, 3, 4], [0, 0, 0, 0]])</code></pre>
</div>
</div>
<p>The second row becomes all zeros, showing redundancy. Pivot in column 1, free variables in columns 2 and 3.</p>
<ol start="6" type="1">
<li>Solving underdetermined systems</li>
</ol>
<p>If you have more variables than equations, expect free variables. Example:</p>
<p><span>\[
x + y = 3
\]</span></p>
<div id="57b89a4a" data-execution_count="142">
<div><div id="cb241"><pre><code><span id="cb241-1"><a href="#cb241-1" aria-hidden="true" tabindex="-1"></a>M3 <span>=</span> Matrix([[<span>1</span>,<span>1</span>,<span>3</span>]])</span>
<span id="cb241-2"><a href="#cb241-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;RREF form:</span><span>\n</span><span>&#34;</span>, M3.rref()[<span>0</span>])</span></code></pre></div></div>
<div>
<pre><code>RREF form:
 Matrix([[1, 1, 3]])</code></pre>
</div>
</div>
<p>Here, <span>\(x = 3 - y\)</span>. Variable <span>\(y\)</span> is free.</p>
</section>
<section id="try-it-yourself-22">
<h4 data-anchor-id="try-it-yourself-22">Try It Yourself</h4>
<ol type="1">
<li><p>Take the system:</p>
<p><span>\[
x + y + z = 2, \quad y + z = 1
\]</span></p>
<p>Compute its RREF and identify pivot and free variables.</p></li>
<li><p>Create a random 3×4 system and compute its pivots. How many free variables do you get?</p></li>
<li><p>For the system:</p>
<p><span>\[
x - y = 0, \quad 2x - 2y = 0
\]</span></p>
<p>Verify that the system has infinitely many solutions and describe them in terms of a free variable.</p></li>
</ol>
</section>
<section id="the-takeaway-6">
<h4 data-anchor-id="the-takeaway-6">The Takeaway</h4>
<ul>
<li>Pivots / leading ones mark the basic variables.</li>
<li>Free variables correspond to non-pivot columns.</li>
<li>Solutions are written in terms of free variables, showing whether the system has a unique, infinite, or no solution.</li>
</ul>
</section>
</section>
<section id="solving-consistent-systems-unique-vs.-infinite-solutions">
<h3 data-anchor-id="solving-consistent-systems-unique-vs.-infinite-solutions">25. Solving Consistent Systems (Unique vs. Infinite Solutions)</h3>
<p>Now that we can spot pivots and free variables, we can classify systems of equations as having a unique solution or infinitely many solutions (assuming they’re consistent). In this lab, we’ll practice solving both types.</p>
<section id="set-up-your-lab-24">
<h4 data-anchor-id="set-up-your-lab-24">Set Up Your Lab</h4>
<div id="5ba48c72" data-execution_count="143">
<div><div id="cb243"><pre><code><span id="cb243-1"><a href="#cb243-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb243-2"><a href="#cb243-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-24">
<h4 data-anchor-id="step-by-step-code-walkthrough-24">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Unique solution example</li>
</ol>
<p>System:</p>
<p><span>\[
x + y = 3, \quad 2x - y = 0
\]</span></p>
<div id="7f049f3f" data-execution_count="144">
<div><div id="cb244"><pre><code><span id="cb244-1"><a href="#cb244-1" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span>
<span id="cb244-2"><a href="#cb244-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb244-3"><a href="#cb244-3" aria-hidden="true" tabindex="-1"></a>M <span>=</span> Matrix([</span>
<span id="cb244-4"><a href="#cb244-4" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>1</span>, <span>3</span>],</span>
<span id="cb244-5"><a href="#cb244-5" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>, <span>-</span><span>1</span>, <span>0</span>]</span>
<span id="cb244-6"><a href="#cb244-6" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb244-7"><a href="#cb244-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb244-8"><a href="#cb244-8" aria-hidden="true" tabindex="-1"></a>M_rref <span>=</span> M.rref()</span>
<span id="cb244-9"><a href="#cb244-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;RREF form:</span><span>\n</span><span>&#34;</span>, M_rref[<span>0</span>])</span>
<span id="cb244-10"><a href="#cb244-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb244-11"><a href="#cb244-11" aria-hidden="true" tabindex="-1"></a><span># Split into coefficient matrix A and right-hand side b</span></span>
<span id="cb244-12"><a href="#cb244-12" aria-hidden="true" tabindex="-1"></a>A <span>=</span> M[:, :<span>2</span>]</span>
<span id="cb244-13"><a href="#cb244-13" aria-hidden="true" tabindex="-1"></a>b <span>=</span> M[:, <span>2</span>]</span>
<span id="cb244-14"><a href="#cb244-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb244-15"><a href="#cb244-15" aria-hidden="true" tabindex="-1"></a>solution <span>=</span> A.solve_least_squares(b)</span>
<span id="cb244-16"><a href="#cb244-16" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Solution:&#34;</span>, solution)</span></code></pre></div></div>
<div>
<pre><code>RREF form:
 Matrix([[1, 0, 1], [0, 1, 2]])
Solution: Matrix([[1], [2]])</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Infinite solution example</li>
</ol>
<p>System:</p>
<p><span>\[
x + y + z = 2, \quad 2x + 2y + 2z = 4
\]</span></p>
<div id="5eeb9587" data-execution_count="145">
<div><div id="cb246"><pre><code><span id="cb246-1"><a href="#cb246-1" aria-hidden="true" tabindex="-1"></a>M2 <span>=</span> Matrix([</span>
<span id="cb246-2"><a href="#cb246-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>1</span>, <span>1</span>, <span>2</span>],</span>
<span id="cb246-3"><a href="#cb246-3" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>, <span>2</span>, <span>2</span>, <span>4</span>]</span>
<span id="cb246-4"><a href="#cb246-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb246-5"><a href="#cb246-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb246-6"><a href="#cb246-6" aria-hidden="true" tabindex="-1"></a>M2_rref <span>=</span> M2.rref()</span>
<span id="cb246-7"><a href="#cb246-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;RREF form:</span><span>\n</span><span>&#34;</span>, M2_rref[<span>0</span>])</span></code></pre></div></div>
<div>
<pre><code>RREF form:
 Matrix([[1, 1, 1, 2], [0, 0, 0, 0]])</code></pre>
</div>
</div>
<p>Only one pivot → two free variables.</p>
<p>Interpretation:</p>
<ul>
<li><span>\(x = 2 - y - z\)</span></li>
<li><span>\(y, z\)</span> are free</li>
<li>Infinite solutions described by parameters.</li>
</ul>
<ol start="3" type="1">
<li>Classifying consistency</li>
</ol>
<p>A system is consistent if the RREF does <em>not</em> have a row like:</p>
<p><span>\[
[0, 0, 0, c] \quad (c \neq 0)
\]</span></p>
<p>Example consistent system:</p>
<div id="955af846" data-execution_count="146">
<div><div id="cb248"><pre><code><span id="cb248-1"><a href="#cb248-1" aria-hidden="true" tabindex="-1"></a>M3 <span>=</span> Matrix([</span>
<span id="cb248-2"><a href="#cb248-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>2</span>, <span>3</span>],</span>
<span id="cb248-3"><a href="#cb248-3" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>, <span>1</span>, <span>4</span>]</span>
<span id="cb248-4"><a href="#cb248-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb248-5"><a href="#cb248-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;RREF:</span><span>\n</span><span>&#34;</span>, M3.rref()[<span>0</span>])</span></code></pre></div></div>
<div>
<pre><code>RREF:
 Matrix([[1, 0, -5], [0, 1, 4]])</code></pre>
</div>
</div>
<p>Example inconsistent system (no solution):</p>
<div id="7bbeaf13" data-execution_count="147">
<div><div id="cb250"><pre><code><span id="cb250-1"><a href="#cb250-1" aria-hidden="true" tabindex="-1"></a>M4 <span>=</span> Matrix([</span>
<span id="cb250-2"><a href="#cb250-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>1</span>, <span>2</span>],</span>
<span id="cb250-3"><a href="#cb250-3" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>, <span>2</span>, <span>5</span>]</span>
<span id="cb250-4"><a href="#cb250-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb250-5"><a href="#cb250-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;RREF:</span><span>\n</span><span>&#34;</span>, M4.rref()[<span>0</span>])</span></code></pre></div></div>
<div>
<pre><code>RREF:
 Matrix([[1, 1, 0], [0, 0, 1]])</code></pre>
</div>
</div>
<p>The second one ends with <code>[0,0,1]</code>, meaning contradiction (0 = 1).</p>
<ol start="4" type="1">
<li>Quick NumPy comparison</li>
</ol>
<p>For systems with unique solutions:</p>
<div id="81f5beeb" data-execution_count="148">
<div><div id="cb252"><pre><code><span id="cb252-1"><a href="#cb252-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>1</span>,<span>1</span>],[<span>2</span>,<span>-</span><span>1</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb252-2"><a href="#cb252-2" aria-hidden="true" tabindex="-1"></a>b <span>=</span> np.array([<span>3</span>,<span>0</span>], dtype<span>=</span><span>float</span>)</span>
<span id="cb252-3"><a href="#cb252-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Unique solution with np.linalg.solve:&#34;</span>, np.linalg.solve(A,b))</span></code></pre></div></div>
<div>
<pre><code>Unique solution with np.linalg.solve: [1. 2.]</code></pre>
</div>
</div>
<p>For systems with infinite solutions, <code>np.linalg.solve</code> will fail, but SymPy handles parametric solutions.</p>
</section>
<section id="try-it-yourself-23">
<h4 data-anchor-id="try-it-yourself-23">Try It Yourself</h4>
<ol type="1">
<li><p>Solve:</p>
<p><span>\[
x + y + z = 1, \quad 2x + 3y + z = 2
\]</span></p>
<p>Is the solution unique or infinite?</p></li>
<li><p>Check consistency of:</p>
<p><span>\[
x + 2y = 3, \quad 2x + 4y = 8
\]</span></p></li>
<li><p>Build a random 3×4 augmented matrix and compute its RREF. Identify:</p>
<ul>
<li>Does it have a unique solution, infinitely many, or none?</li>
</ul></li>
</ol>
</section>
<section id="the-takeaway-7">
<h4 data-anchor-id="the-takeaway-7">The Takeaway</h4>
<ul>
<li>Unique solution: pivot in every variable column.</li>
<li>Infinite solutions: free variables remain, system is still consistent.</li>
<li>No solution: an inconsistent row appears.</li>
</ul>
<p>Understanding pivots and free variables gives a complete picture of the solution set.</p>
</section>
</section>
<section id="detecting-inconsistency-when-no-solution-exists">
<h3 data-anchor-id="detecting-inconsistency-when-no-solution-exists">26. Detecting Inconsistency (When No Solution Exists)</h3>
<p>Not all systems of linear equations can be solved. Some are inconsistent, meaning the equations contradict each other. In this lab, we’ll learn how to recognize inconsistency using augmented matrices and RREF.</p>
<section id="set-up-your-lab-25">
<h4 data-anchor-id="set-up-your-lab-25">Set Up Your Lab</h4>
<div id="e0f4b5ae" data-execution_count="149">
<div><div id="cb254"><pre><code><span id="cb254-1"><a href="#cb254-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb254-2"><a href="#cb254-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-25">
<h4 data-anchor-id="step-by-step-code-walkthrough-25">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>An inconsistent system</li>
</ol>
<p><span>\[
x + y = 2, \quad 2x + 2y = 5
\]</span></p>
<p>Notice the second equation looks like a multiple of the first, but the constant doesn’t match - contradiction.</p>
<div id="d872ab30" data-execution_count="150">
<div><div id="cb255"><pre><code><span id="cb255-1"><a href="#cb255-1" aria-hidden="true" tabindex="-1"></a>M <span>=</span> Matrix([</span>
<span id="cb255-2"><a href="#cb255-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>1</span>, <span>2</span>],</span>
<span id="cb255-3"><a href="#cb255-3" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>, <span>2</span>, <span>5</span>]</span>
<span id="cb255-4"><a href="#cb255-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb255-5"><a href="#cb255-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb255-6"><a href="#cb255-6" aria-hidden="true" tabindex="-1"></a>M_rref <span>=</span> M.rref()</span>
<span id="cb255-7"><a href="#cb255-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;RREF:</span><span>\n</span><span>&#34;</span>, M_rref[<span>0</span>])</span></code></pre></div></div>
<div>
<pre><code>RREF:
 Matrix([[1, 1, 0], [0, 0, 1]])</code></pre>
</div>
</div>
<p>RREF gives:</p>
<p><span>\[
\begin{bmatrix}  
1 &amp; 1 &amp; 2 \\  
0 &amp; 0 &amp; 1  
\end{bmatrix}
\]</span></p>
<p>The last row means <span>\(0 = 1\)</span>, so no solution exists.</p>
<ol start="2" type="1">
<li>A consistent system (for contrast)</li>
</ol>
<p><span>\[
x + y = 2, \quad 2x + 2y = 4
\]</span></p>
<div id="83b2f8c5" data-execution_count="151">
<div><div id="cb257"><pre><code><span id="cb257-1"><a href="#cb257-1" aria-hidden="true" tabindex="-1"></a>M2 <span>=</span> Matrix([</span>
<span id="cb257-2"><a href="#cb257-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>1</span>, <span>2</span>],</span>
<span id="cb257-3"><a href="#cb257-3" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>, <span>2</span>, <span>4</span>]</span>
<span id="cb257-4"><a href="#cb257-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb257-5"><a href="#cb257-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb257-6"><a href="#cb257-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;RREF:</span><span>\n</span><span>&#34;</span>, M2.rref()[<span>0</span>])</span></code></pre></div></div>
<div>
<pre><code>RREF:
 Matrix([[1, 1, 2], [0, 0, 0]])</code></pre>
</div>
</div>
<p>This reduces to one equation and a redundant row of zeros → infinitely many solutions.</p>
<ol start="3" type="1">
<li>Visualizing inconsistency (2D case)</li>
</ol>
<p>System:</p>
<p><span>\[
x + y = 2 \quad \text{and} \quad x + y = 3
\]</span></p>
<p>These are parallel lines that never meet.</p>
<div id="1596c237" data-execution_count="152">
<div><div id="cb259"><pre><code><span id="cb259-1"><a href="#cb259-1" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span>
<span id="cb259-2"><a href="#cb259-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb259-3"><a href="#cb259-3" aria-hidden="true" tabindex="-1"></a>x_vals <span>=</span> np.linspace(<span>-</span><span>1</span>, <span>3</span>, <span>100</span>)</span>
<span id="cb259-4"><a href="#cb259-4" aria-hidden="true" tabindex="-1"></a>y1 <span>=</span> <span>2</span> <span>-</span> x_vals</span>
<span id="cb259-5"><a href="#cb259-5" aria-hidden="true" tabindex="-1"></a>y2 <span>=</span> <span>3</span> <span>-</span> x_vals</span>
<span id="cb259-6"><a href="#cb259-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb259-7"><a href="#cb259-7" aria-hidden="true" tabindex="-1"></a>plt.plot(x_vals, y1, label<span>=</span><span>&#34;x+y=2&#34;</span>)</span>
<span id="cb259-8"><a href="#cb259-8" aria-hidden="true" tabindex="-1"></a>plt.plot(x_vals, y2, label<span>=</span><span>&#34;x+y=3&#34;</span>)</span>
<span id="cb259-9"><a href="#cb259-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb259-10"><a href="#cb259-10" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb259-11"><a href="#cb259-11" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span>0</span>,color<span>=</span><span>&#39;black&#39;</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb259-12"><a href="#cb259-12" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span>0</span>,color<span>=</span><span>&#39;black&#39;</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb259-13"><a href="#cb259-13" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb259-14"><a href="#cb259-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-153-output-1.png" width="569" height="411"/></p>
</figure>
</div>
</div>
</div>
<p>The two lines are parallel → no solution.</p>
<ol start="4" type="1">
<li>Detecting inconsistency automatically</li>
</ol>
<p>We can scan the RREF for a row of the form <span>\([0, 0, …, c]\)</span> with <span>\(c \neq 0\)</span>.</p>
<div id="55c10e5c" data-execution_count="153">
<div><div id="cb260"><pre><code><span id="cb260-1"><a href="#cb260-1" aria-hidden="true" tabindex="-1"></a><span>def</span> is_inconsistent(M):</span>
<span id="cb260-2"><a href="#cb260-2" aria-hidden="true" tabindex="-1"></a>    rref_matrix, _ <span>=</span> M.rref()</span>
<span id="cb260-3"><a href="#cb260-3" aria-hidden="true" tabindex="-1"></a>    <span>for</span> row <span>in</span> rref_matrix.tolist():</span>
<span id="cb260-4"><a href="#cb260-4" aria-hidden="true" tabindex="-1"></a>        <span>if</span> <span>all</span>(v <span>==</span> <span>0</span> <span>for</span> v <span>in</span> row[:<span>-</span><span>1</span>]) <span>and</span> row[<span>-</span><span>1</span>] <span>!=</span> <span>0</span>:</span>
<span id="cb260-5"><a href="#cb260-5" aria-hidden="true" tabindex="-1"></a>            <span>return</span> <span>True</span></span>
<span id="cb260-6"><a href="#cb260-6" aria-hidden="true" tabindex="-1"></a>    <span>return</span> <span>False</span></span>
<span id="cb260-7"><a href="#cb260-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb260-8"><a href="#cb260-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;System 1 inconsistent?&#34;</span>, is_inconsistent(M))</span>
<span id="cb260-9"><a href="#cb260-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;System 2 inconsistent?&#34;</span>, is_inconsistent(M2))</span></code></pre></div></div>
<div>
<pre><code>System 1 inconsistent? True
System 2 inconsistent? False</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-24">
<h4 data-anchor-id="try-it-yourself-24">Try It Yourself</h4>
<ol type="1">
<li><p>Test the system:</p>
<p><span>\[
x + 2y = 4, \quad 2x + 4y = 10
\]</span></p>
<p>Write the augmented matrix and check if it’s inconsistent.</p></li>
<li><p>Build a random 2×3 augmented matrix with integer entries. Use <code>is_inconsistent</code> to check.</p></li>
<li><p>Plot two linear equations in 2D. Adjust constants to see when they intersect (consistent) vs when they are parallel (inconsistent).</p></li>
</ol>
</section>
<section id="the-takeaway-8">
<h4 data-anchor-id="the-takeaway-8">The Takeaway</h4>
<ul>
<li><p>A system is inconsistent if RREF contains a row like <span>\([0,0,…,c]\)</span> with <span>\(c \neq 0\)</span>.</p></li>
<li><p>Geometrically, this means the equations describe parallel lines (2D), parallel planes (3D), or higher-dimensional contradictions.</p></li>
<li><p>Recognizing inconsistency quickly saves time and avoids chasing impossible solutions.</p></li>
</ul>
</section>
</section>
<section id="gaussian-elimination-by-hand-a-disciplined-procedure">
<h3 data-anchor-id="gaussian-elimination-by-hand-a-disciplined-procedure">27. Gaussian Elimination by Hand (A Disciplined Procedure)</h3>
<p>Gaussian elimination is the systematic way to solve linear systems using row operations. It transforms the augmented matrix into row-echelon form (REF) and then uses back substitution to find solutions. In this lab, we’ll walk step by step through the process.</p>
<section id="set-up-your-lab-26">
<h4 data-anchor-id="set-up-your-lab-26">Set Up Your Lab</h4>
<div id="2a6906e3" data-execution_count="154">
<div><div id="cb262"><pre><code><span id="cb262-1"><a href="#cb262-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb262-2"><a href="#cb262-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-26">
<h4 data-anchor-id="step-by-step-code-walkthrough-26">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Example system</li>
</ol>
<p><span>\[
\begin{cases}  
x + y + z = 6 \\  
2x + 3y + z = 14 \\  
x + 2y + 3z = 14  
\end{cases}
\]</span></p>
<div id="6ec8fdf5" data-execution_count="155">
<div><div id="cb263"><pre><code><span id="cb263-1"><a href="#cb263-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([</span>
<span id="cb263-2"><a href="#cb263-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>1</span>, <span>1</span>, <span>6</span>],</span>
<span id="cb263-3"><a href="#cb263-3" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>, <span>3</span>, <span>1</span>, <span>14</span>],</span>
<span id="cb263-4"><a href="#cb263-4" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>2</span>, <span>3</span>, <span>14</span>]</span>
<span id="cb263-5"><a href="#cb263-5" aria-hidden="true" tabindex="-1"></a>], dtype<span>=</span><span>float</span>)</span>
<span id="cb263-6"><a href="#cb263-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb263-7"><a href="#cb263-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Initial augmented matrix:</span><span>\n</span><span>&#34;</span>, A)</span></code></pre></div></div>
<div>
<pre><code>Initial augmented matrix:
 [[ 1.  1.  1.  6.]
 [ 2.  3.  1. 14.]
 [ 1.  2.  3. 14.]]</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Step 1: Get a pivot in the first column</li>
</ol>
<p>Make the pivot at (0,0) into 1 (it already is). Now eliminate below it.</p>
<div id="8fb97f24" data-execution_count="156">
<div><div id="cb265"><pre><code><span id="cb265-1"><a href="#cb265-1" aria-hidden="true" tabindex="-1"></a>A[<span>1</span>] <span>=</span> A[<span>1</span>] <span>-</span> <span>2</span><span>*</span>A[<span>0</span>]   <span># Row2 → Row2 - 2*Row1</span></span>
<span id="cb265-2"><a href="#cb265-2" aria-hidden="true" tabindex="-1"></a>A[<span>2</span>] <span>=</span> A[<span>2</span>] <span>-</span> A[<span>0</span>]     <span># Row3 → Row3 - Row1</span></span>
<span id="cb265-3"><a href="#cb265-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;After eliminating first column:</span><span>\n</span><span>&#34;</span>, A)</span></code></pre></div></div>
<div>
<pre><code>After eliminating first column:
 [[ 1.  1.  1.  6.]
 [ 0.  1. -1.  2.]
 [ 0.  1.  2.  8.]]</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Step 2: Pivot in the second column</li>
</ol>
<p>Make the pivot in row 1, col 1 equal to 1.</p>
<div id="099a1149" data-execution_count="157">
<div><div id="cb267"><pre><code><span id="cb267-1"><a href="#cb267-1" aria-hidden="true" tabindex="-1"></a>A[<span>1</span>] <span>=</span> A[<span>1</span>] <span>/</span> A[<span>1</span>,<span>1</span>]</span>
<span id="cb267-2"><a href="#cb267-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;After scaling second row:</span><span>\n</span><span>&#34;</span>, A)</span></code></pre></div></div>
<div>
<pre><code>After scaling second row:
 [[ 1.  1.  1.  6.]
 [ 0.  1. -1.  2.]
 [ 0.  1.  2.  8.]]</code></pre>
</div>
</div>
<p>Now eliminate below:</p>
<div id="c069ba01" data-execution_count="158">
<div><div id="cb269"><pre><code><span id="cb269-1"><a href="#cb269-1" aria-hidden="true" tabindex="-1"></a>A[<span>2</span>] <span>=</span> A[<span>2</span>] <span>-</span> A[<span>2</span>,<span>1</span>]<span>*</span>A[<span>1</span>]</span>
<span id="cb269-2"><a href="#cb269-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;After eliminating second column:</span><span>\n</span><span>&#34;</span>, A)</span></code></pre></div></div>
<div>
<pre><code>After eliminating second column:
 [[ 1.  1.  1.  6.]
 [ 0.  1. -1.  2.]
 [ 0.  0.  3.  6.]]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Step 3: Pivot in the third column</li>
</ol>
<p>Make the bottom-right entry into 1.</p>
<div id="9a63fca4" data-execution_count="159">
<div><div id="cb271"><pre><code><span id="cb271-1"><a href="#cb271-1" aria-hidden="true" tabindex="-1"></a>A[<span>2</span>] <span>=</span> A[<span>2</span>] <span>/</span> A[<span>2</span>,<span>2</span>]</span>
<span id="cb271-2"><a href="#cb271-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;After scaling third row:</span><span>\n</span><span>&#34;</span>, A)</span></code></pre></div></div>
<div>
<pre><code>After scaling third row:
 [[ 1.  1.  1.  6.]
 [ 0.  1. -1.  2.]
 [ 0.  0.  1.  2.]]</code></pre>
</div>
</div>
<p>At this point, the matrix is in row-echelon form (REF).</p>
<ol start="5" type="1">
<li>Back substitution</li>
</ol>
<p>Now solve from the bottom up:</p>
<div id="f6622884" data-execution_count="160">
<div><div id="cb273"><pre><code><span id="cb273-1"><a href="#cb273-1" aria-hidden="true" tabindex="-1"></a>z <span>=</span> A[<span>2</span>,<span>3</span>]</span>
<span id="cb273-2"><a href="#cb273-2" aria-hidden="true" tabindex="-1"></a>y <span>=</span> A[<span>1</span>,<span>3</span>] <span>-</span> A[<span>1</span>,<span>2</span>]<span>*</span>z</span>
<span id="cb273-3"><a href="#cb273-3" aria-hidden="true" tabindex="-1"></a>x <span>=</span> A[<span>0</span>,<span>3</span>] <span>-</span> A[<span>0</span>,<span>1</span>]<span>*</span>y <span>-</span> A[<span>0</span>,<span>2</span>]<span>*</span>z</span>
<span id="cb273-4"><a href="#cb273-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb273-5"><a href="#cb273-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>f&#34;Solution: x=</span><span>{</span>x<span>}</span><span>, y=</span><span>{</span>y<span>}</span><span>, z=</span><span>{</span>z<span>}</span><span>&#34;</span>)</span></code></pre></div></div>
<div>
<pre><code>Solution: x=0.0, y=4.0, z=2.0</code></pre>
</div>
</div>
<ol start="6" type="1">
<li>Verification</li>
</ol>
<div id="9d0f23f5" data-execution_count="161">
<div><div id="cb275"><pre><code><span id="cb275-1"><a href="#cb275-1" aria-hidden="true" tabindex="-1"></a>coeff <span>=</span> np.array([</span>
<span id="cb275-2"><a href="#cb275-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>1</span>,<span>1</span>],</span>
<span id="cb275-3"><a href="#cb275-3" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>,<span>3</span>,<span>1</span>],</span>
<span id="cb275-4"><a href="#cb275-4" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>2</span>,<span>3</span>]</span>
<span id="cb275-5"><a href="#cb275-5" aria-hidden="true" tabindex="-1"></a>], dtype<span>=</span><span>float</span>)</span>
<span id="cb275-6"><a href="#cb275-6" aria-hidden="true" tabindex="-1"></a>const <span>=</span> np.array([<span>6</span>,<span>14</span>,<span>14</span>], dtype<span>=</span><span>float</span>)</span>
<span id="cb275-7"><a href="#cb275-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb275-8"><a href="#cb275-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Check with np.linalg.solve:&#34;</span>, np.linalg.solve(coeff,const))</span></code></pre></div></div>
<div>
<pre><code>Check with np.linalg.solve: [0. 4. 2.]</code></pre>
</div>
</div>
<p>The results match.</p>
</section>
<section id="try-it-yourself-25">
<h4 data-anchor-id="try-it-yourself-25">Try It Yourself</h4>
<ol type="1">
<li><p>Solve:</p>
<p><span>\[
2x + y = 5, \quad 4x - 6y = -2
\]</span></p>
<p>using Gaussian elimination manually in code.</p></li>
<li><p>Create a random 3×4 augmented matrix and reduce it step by step, printing after each row operation.</p></li>
<li><p>Compare your manual elimination to SymPy’s RREF with <code>Matrix.rref()</code>.</p></li>
</ol>
</section>
<section id="the-takeaway-9">
<h4 data-anchor-id="the-takeaway-9">The Takeaway</h4>
<ul>
<li>Gaussian elimination is a disciplined sequence of row operations.</li>
<li>It reduces the matrix to row-echelon form, from which back substitution is straightforward.</li>
<li>This method is the backbone of solving systems by hand and underlies many numerical algorithms.</li>
</ul>
</section>
</section>
<section id="back-substitution-and-solution-sets-finishing-cleanly">
<h3 data-anchor-id="back-substitution-and-solution-sets-finishing-cleanly">28. Back Substitution and Solution Sets (Finishing Cleanly)</h3>
<p>Once Gaussian elimination reduces a system to row-echelon form (REF), the final step is back substitution. This means solving variables starting from the last equation and working upward. In this lab, we’ll practice both unique and infinite solution cases.</p>
<section id="set-up-your-lab-27">
<h4 data-anchor-id="set-up-your-lab-27">Set Up Your Lab</h4>
<div id="392ff2cb" data-execution_count="162">
<div><div id="cb277"><pre><code><span id="cb277-1"><a href="#cb277-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb277-2"><a href="#cb277-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-27">
<h4 data-anchor-id="step-by-step-code-walkthrough-27">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Unique solution example</li>
</ol>
<p>System:</p>
<p><span>\[
x + y + z = 6, \quad 2y + 5z = -4, \quad z = 3
\]</span></p>
<p>Row-echelon form looks like:</p>
<p><span>\[
\begin{bmatrix}  
1 &amp; 1 &amp; 1 &amp; 6 \\  
0 &amp; 2 &amp; 5 &amp; -4 \\  
0 &amp; 0 &amp; 1 &amp; 3  
\end{bmatrix}
\]</span></p>
<p>Solve bottom-up:</p>
<div id="07d3e20c" data-execution_count="163">
<div><div id="cb278"><pre><code><span id="cb278-1"><a href="#cb278-1" aria-hidden="true" tabindex="-1"></a>z <span>=</span> <span>3</span></span>
<span id="cb278-2"><a href="#cb278-2" aria-hidden="true" tabindex="-1"></a>y <span>=</span> (<span>-</span><span>4</span> <span>-</span> <span>5</span><span>*</span>z)<span>/</span><span>2</span></span>
<span id="cb278-3"><a href="#cb278-3" aria-hidden="true" tabindex="-1"></a>x <span>=</span> <span>6</span> <span>-</span> y <span>-</span> z</span>
<span id="cb278-4"><a href="#cb278-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>f&#34;Solution: x=</span><span>{</span>x<span>}</span><span>, y=</span><span>{</span>y<span>}</span><span>, z=</span><span>{</span>z<span>}</span><span>&#34;</span>)</span></code></pre></div></div>
<div>
<pre><code>Solution: x=12.5, y=-9.5, z=3</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Infinite solution example</li>
</ol>
<p>System:</p>
<p><span>\[
x + y + z = 2, \quad 2x + 2y + 2z = 4
\]</span></p>
<p>After elimination:</p>
<p><span>\[
\begin{bmatrix}  
1 &amp; 1 &amp; 1 &amp; 2 \\  
0 &amp; 0 &amp; 0 &amp; 0  
\end{bmatrix}
\]</span></p>
<p>This means:</p>
<ul>
<li>Equation: <span>\(x + y + z = 2\)</span>.</li>
<li>Free variables: choose <span>\(y\)</span> and <span>\(z\)</span>.</li>
</ul>
<p>Let <span>\(y = s, z = t\)</span>. Then:</p>
<p><span>\[
x = 2 - s - t
\]</span></p>
<p>So the solution set is:</p>
<div id="f9d18c24" data-execution_count="164">
<div><div id="cb280"><pre><code><span id="cb280-1"><a href="#cb280-1" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> symbols</span>
<span id="cb280-2"><a href="#cb280-2" aria-hidden="true" tabindex="-1"></a>s, t <span>=</span> symbols(<span>&#39;s t&#39;</span>)</span>
<span id="cb280-3"><a href="#cb280-3" aria-hidden="true" tabindex="-1"></a>x <span>=</span> <span>2</span> <span>-</span> s <span>-</span> t</span>
<span id="cb280-4"><a href="#cb280-4" aria-hidden="true" tabindex="-1"></a>y <span>=</span> s</span>
<span id="cb280-5"><a href="#cb280-5" aria-hidden="true" tabindex="-1"></a>z <span>=</span> t</span>
<span id="cb280-6"><a href="#cb280-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;General solution:&#34;</span>)</span>
<span id="cb280-7"><a href="#cb280-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;x =&#34;</span>, x, <span>&#34;, y =&#34;</span>, y, <span>&#34;, z =&#34;</span>, z)</span></code></pre></div></div>
<div>
<pre><code>General solution:
x = -s - t + 2 , y = s , z = t</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Consistency check with RREF</li>
</ol>
<p>We can use SymPy to confirm solution sets:</p>
<div id="88ac642d" data-execution_count="165">
<div><div id="cb282"><pre><code><span id="cb282-1"><a href="#cb282-1" aria-hidden="true" tabindex="-1"></a>M <span>=</span> Matrix([</span>
<span id="cb282-2"><a href="#cb282-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>1</span>,<span>1</span>,<span>2</span>],</span>
<span id="cb282-3"><a href="#cb282-3" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>,<span>2</span>,<span>2</span>,<span>4</span>]</span>
<span id="cb282-4"><a href="#cb282-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb282-5"><a href="#cb282-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb282-6"><a href="#cb282-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;RREF form:</span><span>\n</span><span>&#34;</span>, M.rref()[<span>0</span>])</span></code></pre></div></div>
<div>
<pre><code>RREF form:
 Matrix([[1, 1, 1, 2], [0, 0, 0, 0]])</code></pre>
</div>
</div>
<p>The second row disappears, showing infinite solutions.</p>
<ol start="4" type="1">
<li>Encoding solution sets</li>
</ol>
<p>General solutions are often written in parametric vector form.</p>
<p>For the infinite solution above:</p>
<p><span>\[
(x,y,z) = (2,0,0) + s(-1,1,0) + t(-1,0,1)
\]</span></p>
<p>This shows the solution space is a plane in <span>\(\mathbb{R}^3\)</span>.</p>
</section>
<section id="try-it-yourself-26">
<h4 data-anchor-id="try-it-yourself-26">Try It Yourself</h4>
<ol type="1">
<li><p>Solve:</p>
<p><span>\[
x + 2y = 5, \quad y = 1
\]</span></p>
<p>Do back substitution by hand and check with NumPy.</p></li>
<li><p>Take the system:</p>
<p><span>\[
x + y + z = 1, \quad 2x + 2y + 2z = 2
\]</span></p>
<p>Write its solution set in parametric form.</p></li>
<li><p>Use <code>Matrix.rref()</code> on a 3×4 random augmented matrix. Identify pivot and free variables, then describe the solution set.</p></li>
</ol>
</section>
<section id="the-takeaway-10">
<h4 data-anchor-id="the-takeaway-10">The Takeaway</h4>
<ul>
<li>Back substitution is the cleanup step after Gaussian elimination.</li>
<li>It reveals whether the system has a unique solution or infinitely many.</li>
<li>Solutions can be expressed explicitly (unique case) or parametrically (infinite case).</li>
</ul>
</section>
</section>
<section id="rank-and-its-first-meaning-pivots-as-information">
<h3 data-anchor-id="rank-and-its-first-meaning-pivots-as-information">29. Rank and Its First Meaning (Pivots as Information)</h3>
<p>The rank of a matrix tells us how much independent information it contains. Rank is one of the most important concepts in linear algebra because it connects to pivots, independence, dimension, and the number of solutions to a system.</p>
<section id="set-up-your-lab-28">
<h4 data-anchor-id="set-up-your-lab-28">Set Up Your Lab</h4>
<div id="0b673d87" data-execution_count="166">
<div><div id="cb284"><pre><code><span id="cb284-1"><a href="#cb284-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb284-2"><a href="#cb284-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-28">
<h4 data-anchor-id="step-by-step-code-walkthrough-28">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Rank definition The rank is the number of pivots (leading ones) in the row-echelon form of a matrix.</li>
</ol>
<p>Example:</p>
<div id="f4a67a22" data-execution_count="167">
<div><div id="cb285"><pre><code><span id="cb285-1"><a href="#cb285-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> Matrix([</span>
<span id="cb285-2"><a href="#cb285-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>2</span>, <span>3</span>],</span>
<span id="cb285-3"><a href="#cb285-3" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>, <span>4</span>, <span>6</span>],</span>
<span id="cb285-4"><a href="#cb285-4" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>1</span>, <span>1</span>]</span>
<span id="cb285-5"><a href="#cb285-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb285-6"><a href="#cb285-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb285-7"><a href="#cb285-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;RREF:</span><span>\n</span><span>&#34;</span>, A.rref()[<span>0</span>])</span>
<span id="cb285-8"><a href="#cb285-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rank of A:&#34;</span>, A.rank())</span></code></pre></div></div>
<div>
<pre><code>RREF:
 Matrix([[1, 0, -1], [0, 1, 2], [0, 0, 0]])
Rank of A: 2</code></pre>
</div>
</div>
<ul>
<li>The second row is a multiple of the first, so the rank is less than 3.</li>
<li>Only two independent rows → rank = 2.</li>
</ul>
<ol start="2" type="1">
<li>Rank and solutions to <span>\(A·x = b\)</span></li>
</ol>
<p>Consider:</p>
<p><span>\[
\begin{cases}  
x + y + z = 3 \\  
2x + 2y + 2z = 6 \\  
x - y = 0  
\end{cases}
\]</span></p>
<div id="35026cd1" data-execution_count="168">
<div><div id="cb287"><pre><code><span id="cb287-1"><a href="#cb287-1" aria-hidden="true" tabindex="-1"></a>M <span>=</span> Matrix([</span>
<span id="cb287-2"><a href="#cb287-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>1</span>, <span>1</span>, <span>3</span>],</span>
<span id="cb287-3"><a href="#cb287-3" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>, <span>2</span>, <span>2</span>, <span>6</span>],</span>
<span id="cb287-4"><a href="#cb287-4" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>-</span><span>1</span>, <span>0</span>, <span>0</span>]</span>
<span id="cb287-5"><a href="#cb287-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb287-6"><a href="#cb287-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb287-7"><a href="#cb287-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;RREF:</span><span>\n</span><span>&#34;</span>, M.rref()[<span>0</span>])</span>
<span id="cb287-8"><a href="#cb287-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rank of coefficient matrix:&#34;</span>, M[:, :<span>-</span><span>1</span>].rank())</span>
<span id="cb287-9"><a href="#cb287-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rank of augmented matrix:&#34;</span>, M.rank())</span></code></pre></div></div>
<div>
<pre><code>RREF:
 Matrix([[1, 0, 1/2, 3/2], [0, 1, 1/2, 3/2], [0, 0, 0, 0]])
Rank of coefficient matrix: 2
Rank of augmented matrix: 2</code></pre>
</div>
</div>
<ul>
<li>If rank(A) = rank([A|b]) = number of variables → unique solution.</li>
<li>If rank(A) = rank([A|b]) &lt; number of variables → infinite solutions.</li>
<li>If rank(A) &lt; rank([A|b]) → no solution.</li>
</ul>
<ol start="3" type="1">
<li>NumPy comparison</li>
</ol>
<div id="ec8b453e" data-execution_count="169">
<div><div id="cb289"><pre><code><span id="cb289-1"><a href="#cb289-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([</span>
<span id="cb289-2"><a href="#cb289-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>2</span>, <span>3</span>],</span>
<span id="cb289-3"><a href="#cb289-3" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>, <span>4</span>, <span>6</span>],</span>
<span id="cb289-4"><a href="#cb289-4" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>1</span>, <span>1</span>]</span>
<span id="cb289-5"><a href="#cb289-5" aria-hidden="true" tabindex="-1"></a>], dtype<span>=</span><span>float</span>)</span>
<span id="cb289-6"><a href="#cb289-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb289-7"><a href="#cb289-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rank with NumPy:&#34;</span>, np.linalg.matrix_rank(A))</span></code></pre></div></div>

</div>
<ol start="4" type="1">
<li>Rank as “dimension of information”</li>
</ol>
<p>The rank equals:</p>
<ul>
<li>The number of independent rows.</li>
<li>The number of independent columns.</li>
<li>The dimension of the column space.</li>
</ul>
<div id="9eb404bd" data-execution_count="170">
<div><div id="cb291"><pre><code><span id="cb291-1"><a href="#cb291-1" aria-hidden="true" tabindex="-1"></a>B <span>=</span> Matrix([</span>
<span id="cb291-2"><a href="#cb291-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>2</span>],</span>
<span id="cb291-3"><a href="#cb291-3" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>,<span>4</span>],</span>
<span id="cb291-4"><a href="#cb291-4" aria-hidden="true" tabindex="-1"></a>    [<span>3</span>,<span>6</span>]</span>
<span id="cb291-5"><a href="#cb291-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb291-6"><a href="#cb291-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb291-7"><a href="#cb291-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rank of B:&#34;</span>, B.rank())</span></code></pre></div></div>

</div>
<p>All columns are multiples → only one independent direction → rank = 1.</p>
</section>
<section id="try-it-yourself-27">
<h4 data-anchor-id="try-it-yourself-27">Try It Yourself</h4>
<ol type="1">
<li><p>Compute the rank of:</p>
<p><span>\[
\begin{bmatrix}  
1 &amp; 2 &amp; 3 \\  
2 &amp; 4 &amp; 6 \\  
3 &amp; 6 &amp; 9  
\end{bmatrix}
\]</span></p>
<p>What do you expect?</p></li>
<li><p>Create a random 4×4 matrix with <code>np.random.randint</code>. Compute its rank with both SymPy and NumPy.</p></li>
<li><p>Test solution consistency using rank: build a system where rank(A) ≠ rank([A|b]) and show it has no solution.</p></li>
</ol>
</section>
<section id="the-takeaway-11">
<h4 data-anchor-id="the-takeaway-11">The Takeaway</h4>
<ul>
<li>Rank = number of pivots = dimension of independent information.</li>
<li>Rank reveals whether a system has no solution, one solution, or infinitely many.</li>
<li>Rank connects algebra (pivots) with geometry (dimension of subspaces).</li>
</ul>
</section>
</section>
<section id="lu-factorization-elimination-captured-as-l-and-u">
<h3 data-anchor-id="lu-factorization-elimination-captured-as-l-and-u">30. LU Factorization (Elimination Captured as L and U)</h3>
<p>Gaussian elimination can be recorded in a neat factorization:</p>
<p><span>\[
A = LU
\]</span></p>
<p>where <span>\(L\)</span> is a lower triangular matrix (recording the multipliers we used) and <span>\(U\)</span> is an upper triangular matrix (the result of elimination). This is called LU factorization. It’s a powerful tool for solving systems efficiently.</p>
<section id="set-up-your-lab-29">
<h4 data-anchor-id="set-up-your-lab-29">Set Up Your Lab</h4>
<div id="9b763c62" data-execution_count="171">
<div><div id="cb293"><pre><code><span id="cb293-1"><a href="#cb293-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb293-2"><a href="#cb293-2" aria-hidden="true" tabindex="-1"></a><span>from</span> scipy.linalg <span>import</span> lu</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-29">
<h4 data-anchor-id="step-by-step-code-walkthrough-29">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Example matrix</li>
</ol>
<div id="eef236fb" data-execution_count="172">
<div><div id="cb294"><pre><code><span id="cb294-1"><a href="#cb294-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([</span>
<span id="cb294-2"><a href="#cb294-2" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>, <span>3</span>, <span>1</span>],</span>
<span id="cb294-3"><a href="#cb294-3" aria-hidden="true" tabindex="-1"></a>    [<span>4</span>, <span>7</span>, <span>7</span>],</span>
<span id="cb294-4"><a href="#cb294-4" aria-hidden="true" tabindex="-1"></a>    [<span>6</span>, <span>18</span>, <span>22</span>]</span>
<span id="cb294-5"><a href="#cb294-5" aria-hidden="true" tabindex="-1"></a>], dtype<span>=</span><span>float</span>)</span>
<span id="cb294-6"><a href="#cb294-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb294-7"><a href="#cb294-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Matrix A:</span><span>\n</span><span>&#34;</span>, A)</span></code></pre></div></div>
<div>
<pre><code>Matrix A:
 [[ 2.  3.  1.]
 [ 4.  7.  7.]
 [ 6. 18. 22.]]</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>LU decomposition with SciPy</li>
</ol>
<div id="6cbe45f3" data-execution_count="173">
<div><div id="cb296"><pre><code><span id="cb296-1"><a href="#cb296-1" aria-hidden="true" tabindex="-1"></a>P, L, U <span>=</span> lu(A)</span>
<span id="cb296-2"><a href="#cb296-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb296-3"><a href="#cb296-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Permutation matrix P:</span><span>\n</span><span>&#34;</span>, P)</span>
<span id="cb296-4"><a href="#cb296-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Lower triangular L:</span><span>\n</span><span>&#34;</span>, L)</span>
<span id="cb296-5"><a href="#cb296-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Upper triangular U:</span><span>\n</span><span>&#34;</span>, U)</span></code></pre></div></div>
<div>
<pre><code>Permutation matrix P:
 [[0. 0. 1.]
 [0. 1. 0.]
 [1. 0. 0.]]
Lower triangular L:
 [[1.         0.         0.        ]
 [0.66666667 1.         0.        ]
 [0.33333333 0.6        1.        ]]
Upper triangular U:
 [[ 6.         18.         22.        ]
 [ 0.         -5.         -7.66666667]
 [ 0.          0.         -1.73333333]]</code></pre>
</div>
</div>
<p>Here, <span>\(P\)</span> handles row swaps (partial pivoting), <span>\(L\)</span> is lower triangular, and <span>\(U\)</span> is upper triangular.</p>
<ol start="3" type="1">
<li>Verifying the factorization</li>
</ol>
<div id="f8dbffe3" data-execution_count="174">
<div><div id="cb298"><pre><code><span id="cb298-1"><a href="#cb298-1" aria-hidden="true" tabindex="-1"></a>reconstructed <span>=</span> P <span>@</span> L <span>@</span> U</span>
<span id="cb298-2"><a href="#cb298-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Does P·L·U equal A?</span><span>\n</span><span>&#34;</span>, np.allclose(reconstructed, A))</span></code></pre></div></div>
<div>
<pre><code>Does P·L·U equal A?
 True</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Solving a system with LU</li>
</ol>
<p>Suppose we want to solve <span>\(Ax = b\)</span>. Instead of working directly with <span>\(A\)</span>, we solve in two steps:</p>
<ol type="1">
<li>Solve <span>\(Ly = Pb\)</span> (forward substitution).</li>
<li>Solve <span>\(Ux = y\)</span> (back substitution).</li>
</ol>
<div id="cc490e4e" data-execution_count="175">
<div><div id="cb300"><pre><code><span id="cb300-1"><a href="#cb300-1" aria-hidden="true" tabindex="-1"></a>b <span>=</span> np.array([<span>1</span>, <span>2</span>, <span>3</span>], dtype<span>=</span><span>float</span>)</span>
<span id="cb300-2"><a href="#cb300-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb300-3"><a href="#cb300-3" aria-hidden="true" tabindex="-1"></a><span># Step 1: Pb</span></span>
<span id="cb300-4"><a href="#cb300-4" aria-hidden="true" tabindex="-1"></a>Pb <span>=</span> P <span>@</span> b</span>
<span id="cb300-5"><a href="#cb300-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb300-6"><a href="#cb300-6" aria-hidden="true" tabindex="-1"></a><span># Step 2: forward substitution Ly = Pb</span></span>
<span id="cb300-7"><a href="#cb300-7" aria-hidden="true" tabindex="-1"></a>y <span>=</span> np.linalg.solve(L, Pb)</span>
<span id="cb300-8"><a href="#cb300-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb300-9"><a href="#cb300-9" aria-hidden="true" tabindex="-1"></a><span># Step 3: back substitution Ux = y</span></span>
<span id="cb300-10"><a href="#cb300-10" aria-hidden="true" tabindex="-1"></a>x <span>=</span> np.linalg.solve(U, y)</span>
<span id="cb300-11"><a href="#cb300-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb300-12"><a href="#cb300-12" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Solution x:&#34;</span>, x)</span></code></pre></div></div>
<div>
<pre><code>Solution x: [ 0.5 -0.  -0. ]</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Efficiency advantage</li>
</ol>
<p>If we have to solve many systems with the same <span>\(A\)</span> but different <span>\(b\)</span>, we only compute <span>\(LU\)</span> once, then reuse it. This saves a lot of computation.</p>
<ol start="6" type="1">
<li>NumPy’s built-in rank-revealing factorization</li>
</ol>
<p>While NumPy doesn’t have <code>lu</code> directly, it works seamlessly with SciPy. For large matrices, LU decomposition is the backbone of solvers like <code>np.linalg.solve</code>.</p>
</section>
<section id="try-it-yourself-28">
<h4 data-anchor-id="try-it-yourself-28">Try It Yourself</h4>
<ol type="1">
<li><p>Compute LU decomposition for</p>
<p><span>\[
A = \begin{bmatrix} 1 &amp; 2 &amp; 0 \\ 3 &amp; 4 &amp; 4 \\ 5 &amp; 6 &amp; 3 \end{bmatrix}
\]</span></p>
<p>Verify <span>\(P·L·U = A\)</span>.</p></li>
<li><p>Solve <span>\(Ax = b\)</span> with</p>
<p><span>\[
b = [3,7,8]
\]</span></p>
<p>using LU factorization.</p></li>
<li><p>Compare solving with LU factorization vs directly using <code>np.linalg.solve(A,b)</code>. Are the answers the same?</p></li>
</ol>
</section>
<section id="the-takeaway-12">
<h4 data-anchor-id="the-takeaway-12">The Takeaway</h4>
<ul>
<li>LU factorization captures Gaussian elimination in matrix form: <span>\(A = P·L·U\)</span>.</li>
<li>It allows fast repeated solving of systems with different right-hand sides.</li>
<li>LU decomposition is a core technique in numerical linear algebra and the basis of many solvers.</li>
</ul>
</section>
</section>
</section>
<section id="chapter-4.-vector-spaces-and-subspaces">
<h2 data-anchor-id="chapter-4.-vector-spaces-and-subspaces">Chapter 4. Vector Spaces and Subspaces</h2>
<section id="axioms-of-vector-spaces-what-space-really-means">
<h3 data-anchor-id="axioms-of-vector-spaces-what-space-really-means">31. Axioms of Vector Spaces (What “Space” Really Means)</h3>
<p>Vector spaces generalize what we’ve been doing with vectors and matrices. Instead of just <span>\(\mathbb{R}^n\)</span>, a vector space is any collection of objects (vectors) where addition and scalar multiplication follow specific axioms (rules). In this lab, we’ll explore these axioms concretely with Python.</p>
<section id="set-up-your-lab-30">
<h4 data-anchor-id="set-up-your-lab-30">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-30">
<h4 data-anchor-id="step-by-step-code-walkthrough-30">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Vector space example: <span>\(\mathbb{R}^2\)</span></li>
</ol>
<p>Let’s check two rules (axioms): closure under addition and scalar multiplication.</p>
<div id="640f924c" data-execution_count="177">
<div><div id="cb303"><pre><code><span id="cb303-1"><a href="#cb303-1" aria-hidden="true" tabindex="-1"></a>u <span>=</span> np.array([<span>1</span>, <span>2</span>])</span>
<span id="cb303-2"><a href="#cb303-2" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.array([<span>3</span>, <span>-</span><span>1</span>])</span>
<span id="cb303-3"><a href="#cb303-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb303-4"><a href="#cb303-4" aria-hidden="true" tabindex="-1"></a><span># Closure under addition</span></span>
<span id="cb303-5"><a href="#cb303-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;u + v =&#34;</span>, u <span>+</span> v)</span>
<span id="cb303-6"><a href="#cb303-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb303-7"><a href="#cb303-7" aria-hidden="true" tabindex="-1"></a><span># Closure under scalar multiplication</span></span>
<span id="cb303-8"><a href="#cb303-8" aria-hidden="true" tabindex="-1"></a>k <span>=</span> <span>5</span></span>
<span id="cb303-9"><a href="#cb303-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;k * u =&#34;</span>, k <span>*</span> u)</span></code></pre></div></div>
<div>
<pre><code>u + v = [4 1]
k * u = [ 5 10]</code></pre>
</div>
</div>
<p>Both results are still in <span>\(\mathbb{R}^2\)</span>.</p>
<ol start="2" type="1">
<li>Zero vector and additive inverses</li>
</ol>
<p>Every vector space must contain a zero vector, and every vector must have an additive inverse.</p>
<div id="e8f19c9e" data-execution_count="178">
<div><div id="cb305"><pre><code><span id="cb305-1"><a href="#cb305-1" aria-hidden="true" tabindex="-1"></a>zero <span>=</span> np.array([<span>0</span>, <span>0</span>])</span>
<span id="cb305-2"><a href="#cb305-2" aria-hidden="true" tabindex="-1"></a>inverse_u <span>=</span> <span>-</span>u</span>
<span id="cb305-3"><a href="#cb305-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Zero vector:&#34;</span>, zero)</span>
<span id="cb305-4"><a href="#cb305-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;u + (-u) =&#34;</span>, u <span>+</span> inverse_u)</span></code></pre></div></div>
<div>
<pre><code>Zero vector: [0 0]
u + (-u) = [0 0]</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Distributive and associative properties</li>
</ol>
<p>Check:</p>
<ul>
<li><span>\(a(u+v) = au + av\)</span></li>
<li><span>\((a+b)u = au + bu\)</span></li>
</ul>
<div id="4e3797ce" data-execution_count="179">
<div><div id="cb307"><pre><code><span id="cb307-1"><a href="#cb307-1" aria-hidden="true" tabindex="-1"></a>a, b <span>=</span> <span>2</span>, <span>3</span></span>
<span id="cb307-2"><a href="#cb307-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb307-3"><a href="#cb307-3" aria-hidden="true" tabindex="-1"></a>lhs1 <span>=</span> a <span>*</span> (u <span>+</span> v)</span>
<span id="cb307-4"><a href="#cb307-4" aria-hidden="true" tabindex="-1"></a>rhs1 <span>=</span> a<span>*</span>u <span>+</span> a<span>*</span>v</span>
<span id="cb307-5"><a href="#cb307-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;a(u+v) =&#34;</span>, lhs1, <span>&#34;, au+av =&#34;</span>, rhs1)</span>
<span id="cb307-6"><a href="#cb307-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb307-7"><a href="#cb307-7" aria-hidden="true" tabindex="-1"></a>lhs2 <span>=</span> (a<span>+</span>b) <span>*</span> u</span>
<span id="cb307-8"><a href="#cb307-8" aria-hidden="true" tabindex="-1"></a>rhs2 <span>=</span> a<span>*</span>u <span>+</span> b<span>*</span>u</span>
<span id="cb307-9"><a href="#cb307-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;(a+b)u =&#34;</span>, lhs2, <span>&#34;, au+bu =&#34;</span>, rhs2)</span></code></pre></div></div>
<div>
<pre><code>a(u+v) = [8 2] , au+av = [8 2]
(a+b)u = [ 5 10] , au+bu = [ 5 10]</code></pre>
</div>
</div>
<p>Both equalities hold → distributive laws confirmed.</p>
<ol start="4" type="1">
<li>A set that fails to be a vector space</li>
</ol>
<p>Consider only positive numbers with normal addition and scalar multiplication.</p>
<div id="3b2fd0b5" data-execution_count="180">
<div><div id="cb309"><pre><code><span id="cb309-1"><a href="#cb309-1" aria-hidden="true" tabindex="-1"></a>positive_numbers <span>=</span> [<span>1</span>, <span>2</span>, <span>3</span>]</span>
<span id="cb309-2"><a href="#cb309-2" aria-hidden="true" tabindex="-1"></a><span>try</span>:</span>
<span id="cb309-3"><a href="#cb309-3" aria-hidden="true" tabindex="-1"></a>    <span>print</span>(<span>&#34;Closure under negatives?&#34;</span>, <span>-</span><span>1</span> <span>*</span> np.array(positive_numbers))</span>
<span id="cb309-4"><a href="#cb309-4" aria-hidden="true" tabindex="-1"></a><span>except</span> <span>Exception</span> <span>as</span> e:</span>
<span id="cb309-5"><a href="#cb309-5" aria-hidden="true" tabindex="-1"></a>    <span>print</span>(<span>&#34;Error:&#34;</span>, e)</span></code></pre></div></div>
<div>
<pre><code>Closure under negatives? [-1 -2 -3]</code></pre>
</div>
</div>
<p>Negative results leave the set → not a vector space.</p>
<ol start="5" type="1">
<li>Python helper to check axioms</li>
</ol>
<p>We can quickly check if a set of vectors is closed under addition and scalar multiplication.</p>
<div id="68817a60" data-execution_count="181">
<div><div id="cb311"><pre><code><span id="cb311-1"><a href="#cb311-1" aria-hidden="true" tabindex="-1"></a><span>def</span> check_closure(vectors, scalars):</span>
<span id="cb311-2"><a href="#cb311-2" aria-hidden="true" tabindex="-1"></a>    <span>for</span> v <span>in</span> vectors:</span>
<span id="cb311-3"><a href="#cb311-3" aria-hidden="true" tabindex="-1"></a>        <span>for</span> u <span>in</span> vectors:</span>
<span id="cb311-4"><a href="#cb311-4" aria-hidden="true" tabindex="-1"></a>            <span>if</span> <span>not</span> <span>any</span>(np.array_equal(v<span>+</span>u, w) <span>for</span> w <span>in</span> vectors):</span>
<span id="cb311-5"><a href="#cb311-5" aria-hidden="true" tabindex="-1"></a>                <span>return</span> <span>False</span></span>
<span id="cb311-6"><a href="#cb311-6" aria-hidden="true" tabindex="-1"></a>        <span>for</span> k <span>in</span> scalars:</span>
<span id="cb311-7"><a href="#cb311-7" aria-hidden="true" tabindex="-1"></a>            <span>if</span> <span>not</span> <span>any</span>(np.array_equal(k<span>*</span>v, w) <span>for</span> w <span>in</span> vectors):</span>
<span id="cb311-8"><a href="#cb311-8" aria-hidden="true" tabindex="-1"></a>                <span>return</span> <span>False</span></span>
<span id="cb311-9"><a href="#cb311-9" aria-hidden="true" tabindex="-1"></a>    <span>return</span> <span>True</span></span>
<span id="cb311-10"><a href="#cb311-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb311-11"><a href="#cb311-11" aria-hidden="true" tabindex="-1"></a>vectors <span>=</span> [np.array([<span>0</span>,<span>0</span>]), np.array([<span>1</span>,<span>0</span>]), np.array([<span>0</span>,<span>1</span>]), np.array([<span>1</span>,<span>1</span>])]</span>
<span id="cb311-12"><a href="#cb311-12" aria-hidden="true" tabindex="-1"></a>scalars <span>=</span> [<span>0</span>,<span>1</span>,<span>-</span><span>1</span>]</span>
<span id="cb311-13"><a href="#cb311-13" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Closed under addition and scalar multiplication?&#34;</span>, check_closure(vectors, scalars))</span></code></pre></div></div>
<div>
<pre><code>Closed under addition and scalar multiplication? False</code></pre>
</div>
</div>
<p>This small set is closed → it forms a vector space (a subspace of <span>\(\mathbb{R}^2\)</span>).</p>
</section>
<section id="try-it-yourself-29">
<h4 data-anchor-id="try-it-yourself-29">Try It Yourself</h4>
<ol type="1">
<li>Verify that <span>\(\mathbb{R}^3\)</span> satisfies the vector space axioms using random vectors.</li>
<li>Test whether the set of all 2×2 matrices forms a vector space under normal addition and scalar multiplication.</li>
<li>Find an example of a set that fails closure (e.g., integers under division).</li>
</ol>
</section>
<section id="the-takeaway-13">
<h4 data-anchor-id="the-takeaway-13">The Takeaway</h4>
<ul>
<li>A vector space is any set where addition and scalar multiplication satisfy 10 standard axioms.</li>
<li>These rules ensure consistent algebraic behavior.</li>
<li>Many objects beyond arrows in <span>\(\mathbb{R}^n\)</span> (like polynomials or matrices) are vector spaces too.</li>
</ul>
</section>
</section>
<section id="subspaces-column-space-and-null-space-where-solutions-live">
<h3 data-anchor-id="subspaces-column-space-and-null-space-where-solutions-live">32. Subspaces, Column Space, and Null Space (Where Solutions Live)</h3>
<p>A subspace is a smaller vector space sitting inside a bigger one. For matrices, two subspaces show up all the time:</p>
<ul>
<li>Column space: all combinations of the matrix’s columns (possible outputs of <span>\(Ax\)</span>).</li>
<li>Null space: all vectors <span>\(x\)</span> such that <span>\(Ax = 0\)</span> (inputs that vanish).</li>
</ul>
<p>This lab explores both in Python.</p>
<section id="set-up-your-lab-31">
<h4 data-anchor-id="set-up-your-lab-31">Set Up Your Lab</h4>
<div id="23a11874" data-execution_count="182">
<div><div id="cb313"><pre><code><span id="cb313-1"><a href="#cb313-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb313-2"><a href="#cb313-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-31">
<h4 data-anchor-id="step-by-step-code-walkthrough-31">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Column space basics</li>
</ol>
<p>Take:</p>
<p><span>\[
A = \begin{bmatrix} 1 &amp; 2 \\ 2 &amp; 4 \\ 3 &amp; 6 \end{bmatrix}
\]</span></p>
<div id="abb321dc" data-execution_count="183">
<div><div id="cb314"><pre><code><span id="cb314-1"><a href="#cb314-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> Matrix([</span>
<span id="cb314-2"><a href="#cb314-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>2</span>],</span>
<span id="cb314-3"><a href="#cb314-3" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>,<span>4</span>],</span>
<span id="cb314-4"><a href="#cb314-4" aria-hidden="true" tabindex="-1"></a>    [<span>3</span>,<span>6</span>]</span>
<span id="cb314-5"><a href="#cb314-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb314-6"><a href="#cb314-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb314-7"><a href="#cb314-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Matrix A:</span><span>\n</span><span>&#34;</span>, A)</span>
<span id="cb314-8"><a href="#cb314-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Column space basis:</span><span>\n</span><span>&#34;</span>, A.columnspace())</span>
<span id="cb314-9"><a href="#cb314-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rank (dimension of column space):&#34;</span>, A.rank())</span></code></pre></div></div>
<div>
<pre><code>Matrix A:
 Matrix([[1, 2], [2, 4], [3, 6]])
Column space basis:
 [Matrix([
[1],
[2],
[3]])]
Rank (dimension of column space): 1</code></pre>
</div>
</div>
<ul>
<li>The second column is a multiple of the first → column space has dimension 1.</li>
<li>All outputs of <span>\(Ax\)</span> lie on a line in <span>\(\mathbb{R}^3\)</span>.</li>
</ul>
<ol start="2" type="1">
<li>Null space basics</li>
</ol>
<div id="72fb7eb0" data-execution_count="184">
<div><div id="cb316"><pre><code><span id="cb316-1"><a href="#cb316-1" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Null space basis:</span><span>\n</span><span>&#34;</span>, A.nullspace())</span></code></pre></div></div>
<div>
<pre><code>Null space basis:
 [Matrix([
[-2],
[ 1]])]</code></pre>
</div>
</div>
<p>The null space contains all <span>\(x\)</span> where <span>\(Ax=0\)</span>. Here, the null space is 1-dimensional (vectors like <span>\([-2,1]\)</span>).</p>
<ol start="3" type="1">
<li>A full-rank example</li>
</ol>
<div id="e75a5e8b" data-execution_count="185">
<div><div id="cb318"><pre><code><span id="cb318-1"><a href="#cb318-1" aria-hidden="true" tabindex="-1"></a>B <span>=</span> Matrix([</span>
<span id="cb318-2"><a href="#cb318-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>0</span>,<span>0</span>],</span>
<span id="cb318-3"><a href="#cb318-3" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>1</span>,<span>0</span>],</span>
<span id="cb318-4"><a href="#cb318-4" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>0</span>,<span>1</span>]</span>
<span id="cb318-5"><a href="#cb318-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb318-6"><a href="#cb318-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb318-7"><a href="#cb318-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Column space basis:</span><span>\n</span><span>&#34;</span>, B.columnspace())</span>
<span id="cb318-8"><a href="#cb318-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Null space basis:</span><span>\n</span><span>&#34;</span>, B.nullspace())</span></code></pre></div></div>
<div>
<pre><code>Column space basis:
 [Matrix([
[1],
[0],
[0]]), Matrix([
[0],
[1],
[0]]), Matrix([
[0],
[0],
[1]])]
Null space basis:
 []</code></pre>
</div>
</div>
<ul>
<li>Column space = all of <span>\(\mathbb{R}^3\)</span>.</li>
<li>Null space = only the zero vector.</li>
</ul>
<ol start="4" type="1">
<li>Geometry link</li>
</ol>
<p>For <span>\(A\)</span> (rank 1, 2 columns):</p>
<ul>
<li>Column space: line in <span>\(\mathbb{R}^3\)</span>.</li>
<li>Null space: line in <span>\(\mathbb{R}^2\)</span>.</li>
</ul>
<p>Together they explain the system <span>\(Ax = b\)</span>:</p>
<ul>
<li>If <span>\(b\)</span> is outside the column space, no solution exists.</li>
<li>If <span>\(b\)</span> is inside, solutions differ by a vector in the null space.</li>
</ul>
<ol start="5" type="1">
<li>Quick NumPy version</li>
</ol>
<p>NumPy doesn’t directly give null space, but we can compute it with SVD.</p>
<div id="7384887f" data-execution_count="186">
<div><div id="cb320"><pre><code><span id="cb320-1"><a href="#cb320-1" aria-hidden="true" tabindex="-1"></a><span>from</span> numpy.linalg <span>import</span> svd</span>
<span id="cb320-2"><a href="#cb320-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb320-3"><a href="#cb320-3" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>1</span>,<span>2</span>],[<span>2</span>,<span>4</span>],[<span>3</span>,<span>6</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb320-4"><a href="#cb320-4" aria-hidden="true" tabindex="-1"></a>U, S, Vt <span>=</span> svd(A)</span>
<span id="cb320-5"><a href="#cb320-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb320-6"><a href="#cb320-6" aria-hidden="true" tabindex="-1"></a>tol <span>=</span> <span>1e-10</span></span>
<span id="cb320-7"><a href="#cb320-7" aria-hidden="true" tabindex="-1"></a>null_mask <span>=</span> (S <span>&lt;=</span> tol)</span>
<span id="cb320-8"><a href="#cb320-8" aria-hidden="true" tabindex="-1"></a>null_space <span>=</span> Vt.T[:, null_mask]</span>
<span id="cb320-9"><a href="#cb320-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Null space (via SVD):</span><span>\n</span><span>&#34;</span>, null_space)</span></code></pre></div></div>
<div>
<pre><code>Null space (via SVD):
 [[-0.89442719]
 [ 0.4472136 ]]</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-30">
<h4 data-anchor-id="try-it-yourself-30">Try It Yourself</h4>
<ol type="1">
<li><p>Find the column space and null space of</p>
<p><span>\[
\begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1 \\ 0 &amp; 0 \end{bmatrix}
\]</span></p>
<p>How many dimensions does each have?</p></li>
<li><p>Generate a random 3×3 matrix. Compute its rank, column space, and null space.</p></li>
<li><p>Solve <span>\(Ax = b\)</span> with</p>
<p><span>\[
A = \begin{bmatrix} 1 &amp; 2 \\ 2 &amp; 4 \\ 3 &amp; 6 \end{bmatrix}, \quad b = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}
\]</span></p>
<p>and describe why it has infinitely many solutions.</p></li>
</ol>
</section>
<section id="the-takeaway-14">
<h4 data-anchor-id="the-takeaway-14">The Takeaway</h4>
<ul>
<li>The column space = all possible outputs of a matrix.</li>
<li>The null space = all inputs that map to zero.</li>
<li>These subspaces give the complete picture of what a matrix does.</li>
</ul>
</section>
</section>
<section id="span-and-generating-sets-coverage-of-a-space">
<h3 data-anchor-id="span-and-generating-sets-coverage-of-a-space">33. Span and Generating Sets (Coverage of a Space)</h3>
<p>The span of a set of vectors is all the linear combinations you can make from them. If a set of vectors can “cover” a whole space, we call it a generating set. This lab shows how to compute and visualize spans.</p>
<section id="set-up-your-lab-32">
<h4 data-anchor-id="set-up-your-lab-32">Set Up Your Lab</h4>
<div id="248ce6d5" data-execution_count="187">
<div><div id="cb322"><pre><code><span id="cb322-1"><a href="#cb322-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb322-2"><a href="#cb322-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span>
<span id="cb322-3"><a href="#cb322-3" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-32">
<h4 data-anchor-id="step-by-step-code-walkthrough-32">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Span in <span>\(\mathbb{R}^2\)</span></li>
</ol>
<p>Two vectors that aren’t multiples span the whole plane.</p>
<div id="c4e2e6e7" data-execution_count="188">
<div><div id="cb323"><pre><code><span id="cb323-1"><a href="#cb323-1" aria-hidden="true" tabindex="-1"></a>u <span>=</span> np.array([<span>1</span>, <span>0</span>])</span>
<span id="cb323-2"><a href="#cb323-2" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.array([<span>0</span>, <span>1</span>])</span>
<span id="cb323-3"><a href="#cb323-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb323-4"><a href="#cb323-4" aria-hidden="true" tabindex="-1"></a>M <span>=</span> Matrix.hstack(Matrix(u), Matrix(v))</span>
<span id="cb323-5"><a href="#cb323-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rank:&#34;</span>, M.rank())</span></code></pre></div></div>

</div>
<p>Rank = 2 → the span of <span>\(\{u,v\}\)</span> is all of <span>\(\mathbb{R}^2\)</span>.</p>
<ol start="2" type="1">
<li>Dependent vectors (smaller span)</li>
</ol>
<div id="fc19c39c" data-execution_count="189">
<div><div id="cb325"><pre><code><span id="cb325-1"><a href="#cb325-1" aria-hidden="true" tabindex="-1"></a>u <span>=</span> np.array([<span>1</span>, <span>2</span>])</span>
<span id="cb325-2"><a href="#cb325-2" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.array([<span>2</span>, <span>4</span>])</span>
<span id="cb325-3"><a href="#cb325-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb325-4"><a href="#cb325-4" aria-hidden="true" tabindex="-1"></a>M <span>=</span> Matrix.hstack(Matrix(u), Matrix(v))</span>
<span id="cb325-5"><a href="#cb325-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rank:&#34;</span>, M.rank())</span></code></pre></div></div>

</div>
<p>Rank = 1 → these vectors only span a line.</p>
<ol start="3" type="1">
<li>Visualizing a span</li>
</ol>
<p>Let’s see what the span of two vectors looks like.</p>
<div id="1e1f9ebb" data-execution_count="190">
<div><div id="cb327"><pre><code><span id="cb327-1"><a href="#cb327-1" aria-hidden="true" tabindex="-1"></a>u <span>=</span> np.array([<span>1</span>, <span>2</span>])</span>
<span id="cb327-2"><a href="#cb327-2" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.array([<span>2</span>, <span>1</span>])</span>
<span id="cb327-3"><a href="#cb327-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb327-4"><a href="#cb327-4" aria-hidden="true" tabindex="-1"></a>coeffs <span>=</span> np.linspace(<span>-</span><span>2</span>, <span>2</span>, <span>11</span>)</span>
<span id="cb327-5"><a href="#cb327-5" aria-hidden="true" tabindex="-1"></a>points <span>=</span> []</span>
<span id="cb327-6"><a href="#cb327-6" aria-hidden="true" tabindex="-1"></a><span>for</span> a <span>in</span> coeffs:</span>
<span id="cb327-7"><a href="#cb327-7" aria-hidden="true" tabindex="-1"></a>    <span>for</span> b <span>in</span> coeffs:</span>
<span id="cb327-8"><a href="#cb327-8" aria-hidden="true" tabindex="-1"></a>        points.append(a<span>*</span>u <span>+</span> b<span>*</span>v)</span>
<span id="cb327-9"><a href="#cb327-9" aria-hidden="true" tabindex="-1"></a>points <span>=</span> np.array(points)</span>
<span id="cb327-10"><a href="#cb327-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb327-11"><a href="#cb327-11" aria-hidden="true" tabindex="-1"></a>plt.scatter(points[:,<span>0</span>], points[:,<span>1</span>], s<span>=</span><span>10</span>)</span>
<span id="cb327-12"><a href="#cb327-12" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span>0</span>,color<span>=</span><span>&#39;black&#39;</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb327-13"><a href="#cb327-13" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span>0</span>,color<span>=</span><span>&#39;black&#39;</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb327-14"><a href="#cb327-14" aria-hidden="true" tabindex="-1"></a>plt.title(<span>&#34;Span of {u,v}&#34;</span>)</span>
<span id="cb327-15"><a href="#cb327-15" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb327-16"><a href="#cb327-16" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-191-output-1.png" width="569" height="431"/></p>
</figure>
</div>
</div>
</div>
<p>You’ll see a filled grid - the entire plane, because the two vectors are independent.</p>
<ol start="4" type="1">
<li>Generating set of a space</li>
</ol>
<p>For <span>\(\mathbb{R}^3\)</span>:</p>
<div id="01c900f8" data-execution_count="191">
<div><div id="cb328"><pre><code><span id="cb328-1"><a href="#cb328-1" aria-hidden="true" tabindex="-1"></a>basis <span>=</span> [Matrix([<span>1</span>,<span>0</span>,<span>0</span>]), Matrix([<span>0</span>,<span>1</span>,<span>0</span>]), Matrix([<span>0</span>,<span>0</span>,<span>1</span>])]</span>
<span id="cb328-2"><a href="#cb328-2" aria-hidden="true" tabindex="-1"></a>M <span>=</span> Matrix.hstack(<span>*</span>basis)</span>
<span id="cb328-3"><a href="#cb328-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rank:&#34;</span>, M.rank())</span></code></pre></div></div>

</div>
<p>Rank = 3 → this set spans the whole space.</p>
<ol start="5" type="1">
<li>Testing if a vector is in the span</li>
</ol>
<p>Example: Is <span>\([3,5]\)</span> in the span of <span>\([1,2]\)</span> and <span>\([2,1]\)</span>?</p>
<div id="d5069889" data-execution_count="192">
<div><div id="cb330"><pre><code><span id="cb330-1"><a href="#cb330-1" aria-hidden="true" tabindex="-1"></a>u <span>=</span> Matrix([<span>1</span>,<span>2</span>])</span>
<span id="cb330-2"><a href="#cb330-2" aria-hidden="true" tabindex="-1"></a>v <span>=</span> Matrix([<span>2</span>,<span>1</span>])</span>
<span id="cb330-3"><a href="#cb330-3" aria-hidden="true" tabindex="-1"></a>target <span>=</span> Matrix([<span>3</span>,<span>5</span>])</span>
<span id="cb330-4"><a href="#cb330-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb330-5"><a href="#cb330-5" aria-hidden="true" tabindex="-1"></a>M <span>=</span> Matrix.hstack(u,v)</span>
<span id="cb330-6"><a href="#cb330-6" aria-hidden="true" tabindex="-1"></a>solution <span>=</span> M.gauss_jordan_solve(target)</span>
<span id="cb330-7"><a href="#cb330-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Coefficients (a,b):&#34;</span>, solution)</span></code></pre></div></div>
<div>
<pre><code>Coefficients (a,b): (Matrix([
[7/3],
[1/3]]), Matrix(0, 1, []))</code></pre>
</div>
</div>
<p>If a solution exists, the target is in the span.</p>
</section>
<section id="try-it-yourself-31">
<h4 data-anchor-id="try-it-yourself-31">Try It Yourself</h4>
<ol type="1">
<li>Test if <span>\([4,6]\)</span> is in the span of <span>\([1,2]\)</span>.</li>
<li>Visualize the span of <span>\([1,0,0]\)</span> and <span>\([0,1,0]\)</span> in <span>\(\mathbb{R}^3\)</span>. What does it look like?</li>
<li>Create a random 3×3 matrix. Use <code>rank()</code> to check if its columns span <span>\(\mathbb{R}^3\)</span>.</li>
</ol>
</section>
<section id="the-takeaway-15">
<h4 data-anchor-id="the-takeaway-15">The Takeaway</h4>
<ul>
<li>Span = all linear combinations of a set of vectors.</li>
<li>Independent vectors span bigger spaces; dependent ones collapse to smaller spaces.</li>
<li>Generating sets are the foundation of bases and coordinate systems.</li>
</ul>
</section>
</section>
<section id="linear-independence-and-dependence-no-redundancy-vs.-redundancy">
<h3 data-anchor-id="linear-independence-and-dependence-no-redundancy-vs.-redundancy">34. Linear Independence and Dependence (No Redundancy vs. Redundancy)</h3>
<p>A set of vectors is linearly independent if none of them can be written as a combination of the others. If at least one can, the set is dependent. This distinction tells us whether a set of vectors has redundancy.</p>
<section id="set-up-your-lab-33">
<h4 data-anchor-id="set-up-your-lab-33">Set Up Your Lab</h4>
<div id="9fb82c4e" data-execution_count="193">
<div><div id="cb332"><pre><code><span id="cb332-1"><a href="#cb332-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb332-2"><a href="#cb332-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-33">
<h4 data-anchor-id="step-by-step-code-walkthrough-33">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Independent vectors example</li>
</ol>
<div id="c53a8ae7" data-execution_count="194">
<div><div id="cb333"><pre><code><span id="cb333-1"><a href="#cb333-1" aria-hidden="true" tabindex="-1"></a>v1 <span>=</span> Matrix([<span>1</span>, <span>0</span>, <span>0</span>])</span>
<span id="cb333-2"><a href="#cb333-2" aria-hidden="true" tabindex="-1"></a>v2 <span>=</span> Matrix([<span>0</span>, <span>1</span>, <span>0</span>])</span>
<span id="cb333-3"><a href="#cb333-3" aria-hidden="true" tabindex="-1"></a>v3 <span>=</span> Matrix([<span>0</span>, <span>0</span>, <span>1</span>])</span>
<span id="cb333-4"><a href="#cb333-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb333-5"><a href="#cb333-5" aria-hidden="true" tabindex="-1"></a>M <span>=</span> Matrix.hstack(v1, v2, v3)</span>
<span id="cb333-6"><a href="#cb333-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rank:&#34;</span>, M.rank(), <span>&#34; Number of vectors:&#34;</span>, M.shape[<span>1</span>])</span></code></pre></div></div>
<div>
<pre><code>Rank: 3  Number of vectors: 3</code></pre>
</div>
</div>
<p>Rank = 3, number of vectors = 3 → all independent.</p>
<ol start="2" type="1">
<li>Dependent vectors example</li>
</ol>
<div id="9968ffae" data-execution_count="195">
<div><div id="cb335"><pre><code><span id="cb335-1"><a href="#cb335-1" aria-hidden="true" tabindex="-1"></a>v1 <span>=</span> Matrix([<span>1</span>, <span>2</span>, <span>3</span>])</span>
<span id="cb335-2"><a href="#cb335-2" aria-hidden="true" tabindex="-1"></a>v2 <span>=</span> Matrix([<span>2</span>, <span>4</span>, <span>6</span>])</span>
<span id="cb335-3"><a href="#cb335-3" aria-hidden="true" tabindex="-1"></a>v3 <span>=</span> Matrix([<span>3</span>, <span>6</span>, <span>9</span>])</span>
<span id="cb335-4"><a href="#cb335-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb335-5"><a href="#cb335-5" aria-hidden="true" tabindex="-1"></a>M <span>=</span> Matrix.hstack(v1, v2, v3)</span>
<span id="cb335-6"><a href="#cb335-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rank:&#34;</span>, M.rank(), <span>&#34; Number of vectors:&#34;</span>, M.shape[<span>1</span>])</span></code></pre></div></div>
<div>
<pre><code>Rank: 1  Number of vectors: 3</code></pre>
</div>
</div>
<p>Rank = 1, number of vectors = 3 → they’re dependent (multiples of each other).</p>
<ol start="3" type="1">
<li>Checking dependence automatically</li>
</ol>
<p>A quick test: if rank &lt; number of vectors → dependent.</p>
<div id="3a3fd570" data-execution_count="196">
<div><div id="cb337"><pre><code><span id="cb337-1"><a href="#cb337-1" aria-hidden="true" tabindex="-1"></a><span>def</span> check_independence(vectors):</span>
<span id="cb337-2"><a href="#cb337-2" aria-hidden="true" tabindex="-1"></a>    M <span>=</span> Matrix.hstack(<span>*</span>vectors)</span>
<span id="cb337-3"><a href="#cb337-3" aria-hidden="true" tabindex="-1"></a>    <span>return</span> M.rank() <span>==</span> M.shape[<span>1</span>]</span>
<span id="cb337-4"><a href="#cb337-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb337-5"><a href="#cb337-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Independent?&#34;</span>, check_independence([Matrix([<span>1</span>,<span>0</span>]), Matrix([<span>0</span>,<span>1</span>])]))</span>
<span id="cb337-6"><a href="#cb337-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Independent?&#34;</span>, check_independence([Matrix([<span>1</span>,<span>2</span>]), Matrix([<span>2</span>,<span>4</span>])]))</span></code></pre></div></div>
<div>
<pre><code>Independent? True
Independent? False</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Solving for dependence relation</li>
</ol>
<p>If vectors are dependent, we can find coefficients <span>\(c_1, c_2, …\)</span> such that</p>
<p><span>\[
c_1 v_1 + c_2 v_2 + … + c_k v_k = 0
\]</span></p>
<p>with some <span>\(c_i \neq 0\)</span>.</p>
<div id="079e501c" data-execution_count="197">
<div><div id="cb339"><pre><code><span id="cb339-1"><a href="#cb339-1" aria-hidden="true" tabindex="-1"></a>M <span>=</span> Matrix.hstack(Matrix([<span>1</span>,<span>2</span>]), Matrix([<span>2</span>,<span>4</span>]))</span>
<span id="cb339-2"><a href="#cb339-2" aria-hidden="true" tabindex="-1"></a>null_space <span>=</span> M.nullspace()</span>
<span id="cb339-3"><a href="#cb339-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Dependence relation (coefficients):&#34;</span>, null_space)</span></code></pre></div></div>
<div>
<pre><code>Dependence relation (coefficients): [Matrix([
[-2],
[ 1]])]</code></pre>
</div>
</div>
<p>This shows the exact linear relation.</p>
<ol start="5" type="1">
<li>Random example</li>
</ol>
<div id="e1843605" data-execution_count="198">
<div><div id="cb341"><pre><code><span id="cb341-1"><a href="#cb341-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span>0</span>)</span>
<span id="cb341-2"><a href="#cb341-2" aria-hidden="true" tabindex="-1"></a>R <span>=</span> Matrix(np.random.randint(<span>-</span><span>3</span>, <span>4</span>, (<span>3</span>,<span>3</span>)))</span>
<span id="cb341-3"><a href="#cb341-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Random matrix:</span><span>\n</span><span>&#34;</span>, R)</span>
<span id="cb341-4"><a href="#cb341-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rank:&#34;</span>, R.rank())</span></code></pre></div></div>
<div>
<pre><code>Random matrix:
 Matrix([[1, 2, -3], [0, 0, 0], [-2, 0, 2]])
Rank: 2</code></pre>
</div>
</div>
<p>Depending on the rank, the columns may be independent (rank = 3) or dependent (rank &lt; 3).</p>
</section>
<section id="try-it-yourself-32">
<h4 data-anchor-id="try-it-yourself-32">Try It Yourself</h4>
<ol type="1">
<li>Test if <span>\([1,1,0], [0,1,1], [1,2,1]\)</span> are independent.</li>
<li>Generate 4 random vectors in <span>\(\mathbb{R}^3\)</span>. Can they ever be independent? Why or why not?</li>
<li>Find the dependence relation for <span>\([2,4], [3,6]\)</span>.</li>
</ol>
</section>
<section id="the-takeaway-16">
<h4 data-anchor-id="the-takeaway-16">The Takeaway</h4>
<ul>
<li>Independent set: no redundancy, each vector adds a new direction.</li>
<li>Dependent set: at least one vector is unnecessary (it lies in the span of others).</li>
<li>Independence is the key to defining basis and dimension.</li>
</ul>
</section>
</section>
<section id="basis-and-coordinates-naming-every-vector-uniquely">
<h3 data-anchor-id="basis-and-coordinates-naming-every-vector-uniquely">35. Basis and Coordinates (Naming Every Vector Uniquely)</h3>
<p>A basis is a set of independent vectors that span a space. It’s like choosing a coordinate system: every vector in the space can be expressed uniquely as a combination of basis vectors. In this lab, we’ll see how to find bases and compute coordinates relative to them.</p>
<section id="set-up-your-lab-34">
<h4 data-anchor-id="set-up-your-lab-34">Set Up Your Lab</h4>
<div id="c56e8fc3" data-execution_count="199">
<div><div id="cb343"><pre><code><span id="cb343-1"><a href="#cb343-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb343-2"><a href="#cb343-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-34">
<h4 data-anchor-id="step-by-step-code-walkthrough-34">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Standard basis in <span>\(\mathbb{R}^3\)</span></li>
</ol>
<div id="46c37a8d" data-execution_count="200">
<div><div id="cb344"><pre><code><span id="cb344-1"><a href="#cb344-1" aria-hidden="true" tabindex="-1"></a>e1 <span>=</span> Matrix([<span>1</span>,<span>0</span>,<span>0</span>])</span>
<span id="cb344-2"><a href="#cb344-2" aria-hidden="true" tabindex="-1"></a>e2 <span>=</span> Matrix([<span>0</span>,<span>1</span>,<span>0</span>])</span>
<span id="cb344-3"><a href="#cb344-3" aria-hidden="true" tabindex="-1"></a>e3 <span>=</span> Matrix([<span>0</span>,<span>0</span>,<span>1</span>])</span>
<span id="cb344-4"><a href="#cb344-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb344-5"><a href="#cb344-5" aria-hidden="true" tabindex="-1"></a>M <span>=</span> Matrix.hstack(e1, e2, e3)</span>
<span id="cb344-6"><a href="#cb344-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rank:&#34;</span>, M.rank())</span></code></pre></div></div>

</div>
<p>These three independent vectors form the standard basis of <span>\(\mathbb{R}^3\)</span>. Any vector like <span>\([2,5,-1]\)</span> can be expressed as</p>
<p><span>\[
2e_1 + 5e_2 - 1e_3
\]</span></p>
<ol start="2" type="1">
<li>Finding a basis from dependent vectors</li>
</ol>
<div id="0482e8f6" data-execution_count="201">
<div><div id="cb346"><pre><code><span id="cb346-1"><a href="#cb346-1" aria-hidden="true" tabindex="-1"></a>v1 <span>=</span> Matrix([<span>1</span>,<span>2</span>,<span>3</span>])</span>
<span id="cb346-2"><a href="#cb346-2" aria-hidden="true" tabindex="-1"></a>v2 <span>=</span> Matrix([<span>2</span>,<span>4</span>,<span>6</span>])</span>
<span id="cb346-3"><a href="#cb346-3" aria-hidden="true" tabindex="-1"></a>v3 <span>=</span> Matrix([<span>1</span>,<span>0</span>,<span>1</span>])</span>
<span id="cb346-4"><a href="#cb346-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb346-5"><a href="#cb346-5" aria-hidden="true" tabindex="-1"></a>M <span>=</span> Matrix.hstack(v1,v2,v3)</span>
<span id="cb346-6"><a href="#cb346-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Column space basis:&#34;</span>, M.columnspace())</span></code></pre></div></div>
<div>
<pre><code>Column space basis: [Matrix([
[1],
[2],
[3]]), Matrix([
[1],
[0],
[1]])]</code></pre>
</div>
</div>
<p>SymPy extracts independent columns automatically. This gives a basis for the column space.</p>
<ol start="3" type="1">
<li>Coordinates relative to a basis</li>
</ol>
<p>Suppose basis = <span>\(\{ [1,0], [1,1] \}\)</span>. Express vector <span>\([3,5]\)</span> in this basis.</p>
<div id="e49caec7" data-execution_count="202">
<div><div id="cb348"><pre><code><span id="cb348-1"><a href="#cb348-1" aria-hidden="true" tabindex="-1"></a>B <span>=</span> Matrix.hstack(Matrix([<span>1</span>,<span>0</span>]), Matrix([<span>1</span>,<span>1</span>]))</span>
<span id="cb348-2"><a href="#cb348-2" aria-hidden="true" tabindex="-1"></a>target <span>=</span> Matrix([<span>3</span>,<span>5</span>])</span>
<span id="cb348-3"><a href="#cb348-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb348-4"><a href="#cb348-4" aria-hidden="true" tabindex="-1"></a>coords <span>=</span> B.solve_least_squares(target)</span>
<span id="cb348-5"><a href="#cb348-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Coordinates in basis B:&#34;</span>, coords)</span></code></pre></div></div>
<div>
<pre><code>Coordinates in basis B: Matrix([[-2], [5]])</code></pre>
</div>
</div>
<p>So <span>\([3,5] = 3·[1,0] + 2·[1,1]\)</span>.</p>
<ol start="4" type="1">
<li>Basis change</li>
</ol>
<p>If we switch to a different basis, coordinates change but the vector stays the same.</p>
<div id="64af4393" data-execution_count="203">
<div><div id="cb350"><pre><code><span id="cb350-1"><a href="#cb350-1" aria-hidden="true" tabindex="-1"></a>new_basis <span>=</span> Matrix.hstack(Matrix([<span>2</span>,<span>1</span>]), Matrix([<span>1</span>,<span>2</span>]))</span>
<span id="cb350-2"><a href="#cb350-2" aria-hidden="true" tabindex="-1"></a>coords_new <span>=</span> new_basis.solve_least_squares(target)</span>
<span id="cb350-3"><a href="#cb350-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Coordinates in new basis:&#34;</span>, coords_new)</span></code></pre></div></div>
<div>
<pre><code>Coordinates in new basis: Matrix([[1/3], [7/3]])</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Random example</li>
</ol>
<p>Generate 3 random vectors in <span>\(\mathbb{R}^3\)</span>. Check if they form a basis.</p>
<div id="bf61379e" data-execution_count="204">
<div><div id="cb352"><pre><code><span id="cb352-1"><a href="#cb352-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span>1</span>)</span>
<span id="cb352-2"><a href="#cb352-2" aria-hidden="true" tabindex="-1"></a>R <span>=</span> Matrix(np.random.randint(<span>-</span><span>3</span>,<span>4</span>,(<span>3</span>,<span>3</span>)))</span>
<span id="cb352-3"><a href="#cb352-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Random matrix:</span><span>\n</span><span>&#34;</span>, R)</span>
<span id="cb352-4"><a href="#cb352-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rank:&#34;</span>, R.rank())</span></code></pre></div></div>
<div>
<pre><code>Random matrix:
 Matrix([[2, 0, 1], [-3, -2, 0], [2, -3, -3]])
Rank: 3</code></pre>
</div>
</div>
<p>If rank = 3 → basis for <span>\(\mathbb{R}^3\)</span>. Otherwise, only span a subspace.</p>
</section>
<section id="try-it-yourself-33">
<h4 data-anchor-id="try-it-yourself-33">Try It Yourself</h4>
<ol type="1">
<li>Check if <span>\([1,2], [3,4]\)</span> form a basis of <span>\(\mathbb{R}^2\)</span>.</li>
<li>Express vector <span>\([7,5]\)</span> in that basis.</li>
<li>Create 4 random vectors in <span>\(\mathbb{R}^3\)</span>. Find a basis for their span.</li>
</ol>
</section>
<section id="the-takeaway-17">
<h4 data-anchor-id="the-takeaway-17">The Takeaway</h4>
<ul>
<li>A basis = minimal set of vectors that span a space.</li>
<li>Every vector has a unique coordinate representation in a given basis.</li>
<li>Changing bases changes the coordinates, not the vector itself.</li>
</ul>
</section>
</section>
<section id="dimension-how-many-directions">
<h3 data-anchor-id="dimension-how-many-directions">36. Dimension (How Many Directions)</h3>
<p>The dimension of a vector space is the number of independent directions it has. Formally, it’s the number of vectors in any basis of the space. Dimension tells us the “size” of a space in terms of degrees of freedom.</p>
<section id="set-up-your-lab-35">
<h4 data-anchor-id="set-up-your-lab-35">Set Up Your Lab</h4>
<div id="5386066d" data-execution_count="205">
<div><div id="cb354"><pre><code><span id="cb354-1"><a href="#cb354-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb354-2"><a href="#cb354-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-35">
<h4 data-anchor-id="step-by-step-code-walkthrough-35">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Dimension of <span>\(\mathbb{R}^n\)</span></li>
</ol>
<p>The dimension of <span>\(\mathbb{R}^n\)</span> is <span>\(n\)</span>.</p>
<div id="9f615d93" data-execution_count="206">
<div><div id="cb355"><pre><code><span id="cb355-1"><a href="#cb355-1" aria-hidden="true" tabindex="-1"></a>n <span>=</span> <span>4</span></span>
<span id="cb355-2"><a href="#cb355-2" aria-hidden="true" tabindex="-1"></a>basis <span>=</span> [Matrix.eye(n)[:,i] <span>for</span> i <span>in</span> <span>range</span>(n)]</span>
<span id="cb355-3"><a href="#cb355-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Basis for R^4:&#34;</span>, basis)</span>
<span id="cb355-4"><a href="#cb355-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Dimension of R^4:&#34;</span>, <span>len</span>(basis))</span></code></pre></div></div>
<div>
<pre><code>Basis for R^4: [Matrix([
[1],
[0],
[0],
[0]]), Matrix([
[0],
[1],
[0],
[0]]), Matrix([
[0],
[0],
[1],
[0]]), Matrix([
[0],
[0],
[0],
[1]])]
Dimension of R^4: 4</code></pre>
</div>
</div>
<p>Each standard unit vector adds one independent direction → dimension = 4.</p>
<ol start="2" type="1">
<li>Dimension via rank</li>
</ol>
<p>The rank of a matrix equals the dimension of its column space.</p>
<div id="672075bc" data-execution_count="207">
<div><div id="cb357"><pre><code><span id="cb357-1"><a href="#cb357-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> Matrix([</span>
<span id="cb357-2"><a href="#cb357-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>2</span>,<span>3</span>],</span>
<span id="cb357-3"><a href="#cb357-3" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>,<span>4</span>,<span>6</span>],</span>
<span id="cb357-4"><a href="#cb357-4" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>0</span>,<span>1</span>]</span>
<span id="cb357-5"><a href="#cb357-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb357-6"><a href="#cb357-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb357-7"><a href="#cb357-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rank (dimension of column space):&#34;</span>, A.rank())</span></code></pre></div></div>
<div>
<pre><code>Rank (dimension of column space): 2</code></pre>
</div>
</div>
<p>Here, rank = 2 → the column space is a 2D plane inside <span>\(\mathbb{R}^3\)</span>.</p>
<ol start="3" type="1">
<li>Null space dimension</li>
</ol>
<p>The null space dimension is given by:</p>
<p><span>\[
\text{dim(Null(A))} = \#\text{variables} - \text{rank(A)}
\]</span></p>
<div id="1a5c37b6" data-execution_count="208">
<div><div id="cb359"><pre><code><span id="cb359-1"><a href="#cb359-1" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Null space basis:&#34;</span>, A.nullspace())</span>
<span id="cb359-2"><a href="#cb359-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Dimension of null space:&#34;</span>, <span>len</span>(A.nullspace()))</span></code></pre></div></div>
<div>
<pre><code>Null space basis: [Matrix([
[-1],
[-1],
[ 1]])]
Dimension of null space: 1</code></pre>
</div>
</div>
<p>This is the number of free variables in a solution.</p>
<ol start="4" type="1">
<li>Dimension in practice</li>
</ol>
<ul>
<li>A line through the origin in <span>\(\mathbb{R}^3\)</span> has dimension 1.</li>
<li>A plane through the origin has dimension 2.</li>
<li>The whole <span>\(\mathbb{R}^3\)</span> has dimension 3.</li>
</ul>
<p>Example:</p>
<div id="440ec6ac" data-execution_count="209">
<div><div id="cb361"><pre><code><span id="cb361-1"><a href="#cb361-1" aria-hidden="true" tabindex="-1"></a>v1 <span>=</span> Matrix([<span>1</span>,<span>2</span>,<span>3</span>])</span>
<span id="cb361-2"><a href="#cb361-2" aria-hidden="true" tabindex="-1"></a>v2 <span>=</span> Matrix([<span>2</span>,<span>4</span>,<span>6</span>])</span>
<span id="cb361-3"><a href="#cb361-3" aria-hidden="true" tabindex="-1"></a>span <span>=</span> Matrix.hstack(v1,v2)</span>
<span id="cb361-4"><a href="#cb361-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Dimension of span:&#34;</span>, span.rank())</span></code></pre></div></div>

</div>
<p>Result = 1 → they only generate a line.</p>
<ol start="5" type="1">
<li>Random example</li>
</ol>
<div id="51471bae" data-execution_count="210">
<div><div id="cb363"><pre><code><span id="cb363-1"><a href="#cb363-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span>2</span>)</span>
<span id="cb363-2"><a href="#cb363-2" aria-hidden="true" tabindex="-1"></a>R <span>=</span> Matrix(np.random.randint(<span>-</span><span>3</span>,<span>4</span>,(<span>4</span>,<span>4</span>)))</span>
<span id="cb363-3"><a href="#cb363-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Random 4x4 matrix:</span><span>\n</span><span>&#34;</span>, R)</span>
<span id="cb363-4"><a href="#cb363-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Column space dimension:&#34;</span>, R.rank())</span></code></pre></div></div>
<div>
<pre><code>Random 4x4 matrix:
 Matrix([[-3, 2, -3, 3], [0, -1, 0, -3], [-1, -2, 0, 2], [-1, 1, 1, 1]])
Column space dimension: 4</code></pre>
</div>
</div>
<p>Rank may be 4 (full space) or smaller (collapsed).</p>
</section>
<section id="try-it-yourself-34">
<h4 data-anchor-id="try-it-yourself-34">Try It Yourself</h4>
<ol type="1">
<li><p>Find the dimension of the column space of</p>
<p><span>\[
\begin{bmatrix}  
1 &amp; 1 &amp; 1 \\  
0 &amp; 1 &amp; 1 \\  
0 &amp; 0 &amp; 0  
\end{bmatrix}
\]</span></p></li>
<li><p>Compute the dimension of the null space of a 3×3 singular matrix.</p></li>
<li><p>Generate a 5×3 random matrix and compute its column space dimension.</p></li>
</ol>
</section>
<section id="the-takeaway-18">
<h4 data-anchor-id="the-takeaway-18">The Takeaway</h4>
<ul>
<li>Dimension = number of independent directions.</li>
<li>Found by counting basis vectors (or rank).</li>
<li>Dimensions describe lines (1D), planes (2D), and higher subspaces inside larger spaces.</li>
</ul>
</section>
</section>
<section id="ranknullity-theorem-dimensions-that-add-up">
<h3 data-anchor-id="ranknullity-theorem-dimensions-that-add-up">37. Rank–Nullity Theorem (Dimensions That Add Up)</h3>
<p>The rank–nullity theorem ties together the dimension of the column space and the null space of a matrix. It says:</p>
<p><span>\[
\text{rank}(A) + \text{nullity}(A) = \text{number of columns of } A
\]</span></p>
<p>This is a powerful consistency check in linear algebra.</p>
<section id="set-up-your-lab-36">
<h4 data-anchor-id="set-up-your-lab-36">Set Up Your Lab</h4>
<div id="70c2dd79" data-execution_count="211">
<div><div id="cb365"><pre><code><span id="cb365-1"><a href="#cb365-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb365-2"><a href="#cb365-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-36">
<h4 data-anchor-id="step-by-step-code-walkthrough-36">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Simple 3×3 example</li>
</ol>
<div id="1d4ef096" data-execution_count="212">
<div><div id="cb366"><pre><code><span id="cb366-1"><a href="#cb366-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> Matrix([</span>
<span id="cb366-2"><a href="#cb366-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>2</span>, <span>3</span>],</span>
<span id="cb366-3"><a href="#cb366-3" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>, <span>4</span>, <span>6</span>],</span>
<span id="cb366-4"><a href="#cb366-4" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>, <span>0</span>, <span>1</span>]</span>
<span id="cb366-5"><a href="#cb366-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb366-6"><a href="#cb366-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb366-7"><a href="#cb366-7" aria-hidden="true" tabindex="-1"></a>rank <span>=</span> A.rank()</span>
<span id="cb366-8"><a href="#cb366-8" aria-hidden="true" tabindex="-1"></a>nullity <span>=</span> <span>len</span>(A.nullspace())</span>
<span id="cb366-9"><a href="#cb366-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rank:&#34;</span>, rank)</span>
<span id="cb366-10"><a href="#cb366-10" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Nullity:&#34;</span>, nullity)</span>
<span id="cb366-11"><a href="#cb366-11" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rank + Nullity =&#34;</span>, rank <span>+</span> nullity)</span>
<span id="cb366-12"><a href="#cb366-12" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Number of columns =&#34;</span>, A.shape[<span>1</span>])</span></code></pre></div></div>
<div>
<pre><code>Rank: 2
Nullity: 1
Rank + Nullity = 3
Number of columns = 3</code></pre>
</div>
</div>
<p>You should see that rank + nullity = 3, the number of columns.</p>
<ol start="2" type="1">
<li>Full-rank case</li>
</ol>
<div id="54491b51" data-execution_count="213">
<div><div id="cb368"><pre><code><span id="cb368-1"><a href="#cb368-1" aria-hidden="true" tabindex="-1"></a>B <span>=</span> Matrix([</span>
<span id="cb368-2"><a href="#cb368-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>0</span>,<span>0</span>],</span>
<span id="cb368-3"><a href="#cb368-3" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>1</span>,<span>0</span>],</span>
<span id="cb368-4"><a href="#cb368-4" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>0</span>,<span>1</span>]</span>
<span id="cb368-5"><a href="#cb368-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb368-6"><a href="#cb368-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb368-7"><a href="#cb368-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rank:&#34;</span>, B.rank())</span>
<span id="cb368-8"><a href="#cb368-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Nullity:&#34;</span>, <span>len</span>(B.nullspace()))</span></code></pre></div></div>

</div>
<ul>
<li>Rank = 3 (all independent).</li>
<li>Nullity = 0 (only zero solution to <span>\(Bx=0\)</span>).</li>
<li>Rank + Nullity = 3 columns.</li>
</ul>
<ol start="3" type="1">
<li>Wide matrix (more columns than rows)</li>
</ol>
<div id="04f71e46" data-execution_count="214">
<div><div id="cb370"><pre><code><span id="cb370-1"><a href="#cb370-1" aria-hidden="true" tabindex="-1"></a>C <span>=</span> Matrix([</span>
<span id="cb370-2"><a href="#cb370-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>2</span>,<span>3</span>,<span>4</span>],</span>
<span id="cb370-3"><a href="#cb370-3" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>1</span>,<span>1</span>,<span>2</span>],</span>
<span id="cb370-4"><a href="#cb370-4" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>0</span>,<span>0</span>,<span>0</span>]</span>
<span id="cb370-5"><a href="#cb370-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb370-6"><a href="#cb370-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb370-7"><a href="#cb370-7" aria-hidden="true" tabindex="-1"></a>rank <span>=</span> C.rank()</span>
<span id="cb370-8"><a href="#cb370-8" aria-hidden="true" tabindex="-1"></a>nullity <span>=</span> <span>len</span>(C.nullspace())</span>
<span id="cb370-9"><a href="#cb370-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rank:&#34;</span>, rank, <span>&#34; Nullity:&#34;</span>, nullity, <span>&#34; Columns:&#34;</span>, C.shape[<span>1</span>])</span></code></pre></div></div>
<div>
<pre><code>Rank: 2  Nullity: 2  Columns: 4</code></pre>
</div>
</div>
<p>Here, nullity &gt; 0 because there are more variables than independent equations.</p>
<ol start="4" type="1">
<li>Verifying with random matrices</li>
</ol>
<div id="72e5f1e3" data-execution_count="215">
<div><div id="cb372"><pre><code><span id="cb372-1"><a href="#cb372-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span>3</span>)</span>
<span id="cb372-2"><a href="#cb372-2" aria-hidden="true" tabindex="-1"></a>R <span>=</span> Matrix(np.random.randint(<span>-</span><span>3</span>,<span>4</span>,(<span>4</span>,<span>5</span>)))</span>
<span id="cb372-3"><a href="#cb372-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Random 4x5 matrix:</span><span>\n</span><span>&#34;</span>, R)</span>
<span id="cb372-4"><a href="#cb372-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rank + Nullity =&#34;</span>, R.rank() <span>+</span> <span>len</span>(R.nullspace()))</span>
<span id="cb372-5"><a href="#cb372-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Number of columns =&#34;</span>, R.shape[<span>1</span>])</span></code></pre></div></div>
<div>
<pre><code>Random 4x5 matrix:
 Matrix([[-1, -3, -2, 0, -3], [-3, -3, 2, 2, 0], [-1, 0, -2, -2, -1], [2, 3, -3, 1, 1]])
Rank + Nullity = 5
Number of columns = 5</code></pre>
</div>
</div>
<p>Always consistent: rank + nullity = number of columns.</p>
<ol start="5" type="1">
<li>Geometric interpretation</li>
</ol>
<p>For an <span>\(m \times n\)</span> matrix:</p>
<ul>
<li>Rank(A) = dimension of outputs (column space).</li>
<li>Nullity(A) = dimension of hidden directions that collapse to 0.</li>
<li>Together, they use up all the “input dimensions” (n).</li>
</ul>
</section>
<section id="try-it-yourself-35">
<h4 data-anchor-id="try-it-yourself-35">Try It Yourself</h4>
<ol type="1">
<li><p>Compute rank and nullity of</p>
<p><span>\[
\begin{bmatrix}  
1 &amp; 1 &amp; 1 \\  
0 &amp; 1 &amp; 1  
\end{bmatrix}
\]</span></p>
<p>Check the theorem.</p></li>
<li><p>Create a 2×4 random integer matrix. Confirm that rank + nullity = 4.</p></li>
<li><p>Explain why a tall full-rank <span>\(5 \times 3\)</span> matrix must have nullity = 0.</p></li>
</ol>
</section>
<section id="the-takeaway-19">
<h4 data-anchor-id="the-takeaway-19">The Takeaway</h4>
<ul>
<li>Rank + Nullity = number of columns (always true).</li>
<li>Rank measures independent outputs; nullity measures hidden freedom.</li>
<li>This theorem connects solutions of <span>\(Ax=0\)</span> with the structure of <span>\(A\)</span>.</li>
</ul>
</section>
</section>
<section id="coordinates-relative-to-a-basis-changing-the-ruler">
<h3 data-anchor-id="coordinates-relative-to-a-basis-changing-the-ruler">38. Coordinates Relative to a Basis (Changing the “Ruler”)</h3>
<p>Once we choose a basis, every vector can be described with coordinates relative to that basis. This is like changing the “ruler” we use to measure vectors. In this lab, we’ll practice computing coordinates in different bases.</p>
<section id="set-up-your-lab-37">
<h4 data-anchor-id="set-up-your-lab-37">Set Up Your Lab</h4>
<div id="6e51ab7b" data-execution_count="216">
<div><div id="cb374"><pre><code><span id="cb374-1"><a href="#cb374-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb374-2"><a href="#cb374-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-37">
<h4 data-anchor-id="step-by-step-code-walkthrough-37">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Standard basis coordinates</li>
</ol>
<p>Vector <span>\(v = [4,5]\)</span> in <span>\(\mathbb{R}^2\)</span>:</p>
<div id="1e01d5bd" data-execution_count="217">
<div><div id="cb375"><pre><code><span id="cb375-1"><a href="#cb375-1" aria-hidden="true" tabindex="-1"></a>v <span>=</span> Matrix([<span>4</span>,<span>5</span>])</span>
<span id="cb375-2"><a href="#cb375-2" aria-hidden="true" tabindex="-1"></a>e1 <span>=</span> Matrix([<span>1</span>,<span>0</span>])</span>
<span id="cb375-3"><a href="#cb375-3" aria-hidden="true" tabindex="-1"></a>e2 <span>=</span> Matrix([<span>0</span>,<span>1</span>])</span>
<span id="cb375-4"><a href="#cb375-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb375-5"><a href="#cb375-5" aria-hidden="true" tabindex="-1"></a>B <span>=</span> Matrix.hstack(e1,e2)</span>
<span id="cb375-6"><a href="#cb375-6" aria-hidden="true" tabindex="-1"></a>coords <span>=</span> B.solve_least_squares(v)</span>
<span id="cb375-7"><a href="#cb375-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Coordinates in standard basis:&#34;</span>, coords)</span></code></pre></div></div>
<div>
<pre><code>Coordinates in standard basis: Matrix([[4], [5]])</code></pre>
</div>
</div>
<p>Result is just <span>\([4,5]\)</span>. Easy - the standard basis matches the components directly.</p>
<ol start="2" type="1">
<li>Non-standard basis</li>
</ol>
<p>Suppose basis = <span>\(\{ [1,1], [1,-1] \}\)</span>. Express <span>\(v = [4,5]\)</span> in this basis.</p>
<div id="81a03896" data-execution_count="218">
<div><div id="cb377"><pre><code><span id="cb377-1"><a href="#cb377-1" aria-hidden="true" tabindex="-1"></a>B2 <span>=</span> Matrix.hstack(Matrix([<span>1</span>,<span>1</span>]), Matrix([<span>1</span>,<span>-</span><span>1</span>]))</span>
<span id="cb377-2"><a href="#cb377-2" aria-hidden="true" tabindex="-1"></a>coords2 <span>=</span> B2.solve_least_squares(v)</span>
<span id="cb377-3"><a href="#cb377-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Coordinates in new basis:&#34;</span>, coords2)</span></code></pre></div></div>
<div>
<pre><code>Coordinates in new basis: Matrix([[9/2], [-1/2]])</code></pre>
</div>
</div>
<p>Now <span>\(v\)</span> has different coordinates.</p>
<ol start="3" type="1">
<li>Changing coordinates back</li>
</ol>
<p>To reconstruct the vector from coordinates:</p>
<div id="e7125077" data-execution_count="219">
<div><div id="cb379"><pre><code><span id="cb379-1"><a href="#cb379-1" aria-hidden="true" tabindex="-1"></a>reconstructed <span>=</span> B2 <span>*</span> coords2</span>
<span id="cb379-2"><a href="#cb379-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Reconstructed vector:&#34;</span>, reconstructed)</span></code></pre></div></div>
<div>
<pre><code>Reconstructed vector: Matrix([[4], [5]])</code></pre>
</div>
</div>
<p>It matches the original <span>\([4,5]\)</span>.</p>
<ol start="4" type="1">
<li>Random basis in <span>\(\mathbb{R}^3\)</span></li>
</ol>
<div id="700e082d" data-execution_count="220">
<div><div id="cb381"><pre><code><span id="cb381-1"><a href="#cb381-1" aria-hidden="true" tabindex="-1"></a>basis <span>=</span> Matrix.hstack(</span>
<span id="cb381-2"><a href="#cb381-2" aria-hidden="true" tabindex="-1"></a>    Matrix([<span>1</span>,<span>0</span>,<span>1</span>]),</span>
<span id="cb381-3"><a href="#cb381-3" aria-hidden="true" tabindex="-1"></a>    Matrix([<span>0</span>,<span>1</span>,<span>1</span>]),</span>
<span id="cb381-4"><a href="#cb381-4" aria-hidden="true" tabindex="-1"></a>    Matrix([<span>1</span>,<span>1</span>,<span>0</span>])</span>
<span id="cb381-5"><a href="#cb381-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb381-6"><a href="#cb381-6" aria-hidden="true" tabindex="-1"></a>v <span>=</span> Matrix([<span>2</span>,<span>3</span>,<span>4</span>])</span>
<span id="cb381-7"><a href="#cb381-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb381-8"><a href="#cb381-8" aria-hidden="true" tabindex="-1"></a>coords <span>=</span> basis.solve_least_squares(v)</span>
<span id="cb381-9"><a href="#cb381-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Coordinates of v in random basis:&#34;</span>, coords)</span></code></pre></div></div>
<div>
<pre><code>Coordinates of v in random basis: Matrix([[3/2], [5/2], [1/2]])</code></pre>
</div>
</div>
<p>Any independent set of 3 vectors in <span>\(\mathbb{R}^3\)</span> works as a basis.</p>
<ol start="5" type="1">
<li>Visualization in 2D</li>
</ol>
<p>Let’s compare coordinates in two bases.</p>
<div id="a322ae91" data-execution_count="221">
<div><div id="cb383"><pre><code><span id="cb383-1"><a href="#cb383-1" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span>
<span id="cb383-2"><a href="#cb383-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb383-3"><a href="#cb383-3" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.array([<span>4</span>,<span>5</span>])</span>
<span id="cb383-4"><a href="#cb383-4" aria-hidden="true" tabindex="-1"></a>b1 <span>=</span> np.array([<span>1</span>,<span>1</span>])</span>
<span id="cb383-5"><a href="#cb383-5" aria-hidden="true" tabindex="-1"></a>b2 <span>=</span> np.array([<span>1</span>,<span>-</span><span>1</span>])</span>
<span id="cb383-6"><a href="#cb383-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb383-7"><a href="#cb383-7" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>,<span>0</span>,v[<span>0</span>],v[<span>1</span>],angles<span>=</span><span>&#39;xy&#39;</span>,scale_units<span>=</span><span>&#39;xy&#39;</span>,scale<span>=</span><span>1</span>,color<span>=</span><span>&#39;blue&#39;</span>,label<span>=</span><span>&#39;v&#39;</span>)</span>
<span id="cb383-8"><a href="#cb383-8" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>,<span>0</span>,b1[<span>0</span>],b1[<span>1</span>],angles<span>=</span><span>&#39;xy&#39;</span>,scale_units<span>=</span><span>&#39;xy&#39;</span>,scale<span>=</span><span>1</span>,color<span>=</span><span>&#39;red&#39;</span>,label<span>=</span><span>&#39;basis1&#39;</span>)</span>
<span id="cb383-9"><a href="#cb383-9" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>,<span>0</span>,b2[<span>0</span>],b2[<span>1</span>],angles<span>=</span><span>&#39;xy&#39;</span>,scale_units<span>=</span><span>&#39;xy&#39;</span>,scale<span>=</span><span>1</span>,color<span>=</span><span>&#39;green&#39;</span>,label<span>=</span><span>&#39;basis2&#39;</span>)</span>
<span id="cb383-10"><a href="#cb383-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb383-11"><a href="#cb383-11" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span>-</span><span>1</span>,<span>6</span>)</span>
<span id="cb383-12"><a href="#cb383-12" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span>-</span><span>6</span>,<span>6</span>)</span>
<span id="cb383-13"><a href="#cb383-13" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb383-14"><a href="#cb383-14" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb383-15"><a href="#cb383-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-222-output-1.png" width="573" height="416"/></p>
</figure>
</div>
</div>
</div>
<p>Even though the basis vectors look different, they span the same space, and <span>\(v\)</span> can be expressed in terms of them.</p>
</section>
<section id="try-it-yourself-36">
<h4 data-anchor-id="try-it-yourself-36">Try It Yourself</h4>
<ol type="1">
<li>Express <span>\([7,3]\)</span> in the basis <span>\(\{[2,0], [0,3]\}\)</span>.</li>
<li>Pick three independent random vectors in <span>\(\mathbb{R}^3\)</span>. Write down the coordinates of <span>\([1,2,3]\)</span> in that basis.</li>
<li>Verify that reconstructing always gives the original vector.</li>
</ol>
</section>
<section id="the-takeaway-20">
<h4 data-anchor-id="the-takeaway-20">The Takeaway</h4>
<ul>
<li>A basis provides a coordinate system for vectors.</li>
<li>Coordinates depend on the basis, but the underlying vector doesn’t change.</li>
<li>Changing the basis is like changing the “ruler” you measure vectors with.</li>
</ul>
</section>
</section>
<section id="change-of-basis-matrices-moving-between-coordinate-systems">
<h3 data-anchor-id="change-of-basis-matrices-moving-between-coordinate-systems">39. Change-of-Basis Matrices (Moving Between Coordinate Systems)</h3>
<p>When we switch from one basis to another, we need a change-of-basis matrix. This matrix acts like a translator: it converts coordinates in one system to coordinates in another.</p>
<section id="set-up-your-lab-38">
<h4 data-anchor-id="set-up-your-lab-38">Set Up Your Lab</h4>
<div id="05b0cd38" data-execution_count="222">
<div><div id="cb384"><pre><code><span id="cb384-1"><a href="#cb384-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb384-2"><a href="#cb384-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-38">
<h4 data-anchor-id="step-by-step-code-walkthrough-38">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Two bases in <span>\(\mathbb{R}^2\)</span></li>
</ol>
<p>Let’s define:</p>
<ul>
<li>Basis <span>\(B = \{ [1,0], [0,1] \}\)</span> (standard basis).</li>
<li>Basis <span>\(C = \{ [1,1], [1,-1] \}\)</span>.</li>
</ul>
<div id="16054eed" data-execution_count="223">
<div><div id="cb385"><pre><code><span id="cb385-1"><a href="#cb385-1" aria-hidden="true" tabindex="-1"></a>B <span>=</span> Matrix.hstack(Matrix([<span>1</span>,<span>0</span>]), Matrix([<span>0</span>,<span>1</span>]))</span>
<span id="cb385-2"><a href="#cb385-2" aria-hidden="true" tabindex="-1"></a>C <span>=</span> Matrix.hstack(Matrix([<span>1</span>,<span>1</span>]), Matrix([<span>1</span>,<span>-</span><span>1</span>]))</span></code></pre></div></div>
</div>
<ol start="2" type="1">
<li>Change-of-basis matrix</li>
</ol>
<p>The matrix that converts C-coordinates → standard coordinates is just <span>\(C\)</span>.</p>
<div id="eff0e703" data-execution_count="224">
<div><div id="cb386"><pre><code><span id="cb386-1"><a href="#cb386-1" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;C (basis matrix):</span><span>\n</span><span>&#34;</span>, C)</span></code></pre></div></div>
<div>
<pre><code>C (basis matrix):
 Matrix([[1, 1], [1, -1]])</code></pre>
</div>
</div>
<p>To go the other way (standard → C), we compute the inverse of <span>\(C\)</span>.</p>
<div id="dc2a2a7b" data-execution_count="225">
<div><div id="cb388"><pre><code><span id="cb388-1"><a href="#cb388-1" aria-hidden="true" tabindex="-1"></a>C_inv <span>=</span> C.inv()</span>
<span id="cb388-2"><a href="#cb388-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;C inverse:</span><span>\n</span><span>&#34;</span>, C_inv)</span></code></pre></div></div>
<div>
<pre><code>C inverse:
 Matrix([[1/2, 1/2], [1/2, -1/2]])</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Converting coordinates</li>
</ol>
<p>Vector <span>\(v = [4,5]\)</span>.</p>
<ul>
<li>In standard basis:</li>
</ul>
<div id="6816b3d1" data-execution_count="226">
<div><div id="cb390"><pre><code><span id="cb390-1"><a href="#cb390-1" aria-hidden="true" tabindex="-1"></a>v <span>=</span> Matrix([<span>4</span>,<span>5</span>])</span>
<span id="cb390-2"><a href="#cb390-2" aria-hidden="true" tabindex="-1"></a>coords_in_standard <span>=</span> v</span>
<span id="cb390-3"><a href="#cb390-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Coordinates in standard basis:&#34;</span>, coords_in_standard)</span></code></pre></div></div>
<div>
<pre><code>Coordinates in standard basis: Matrix([[4], [5]])</code></pre>
</div>
</div>
<ul>
<li>In basis <span>\(C\)</span>:</li>
</ul>
<div id="1af938ce" data-execution_count="227">
<div><div id="cb392"><pre><code><span id="cb392-1"><a href="#cb392-1" aria-hidden="true" tabindex="-1"></a>coords_in_C <span>=</span> C_inv <span>*</span> v</span>
<span id="cb392-2"><a href="#cb392-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Coordinates in C basis:&#34;</span>, coords_in_C)</span></code></pre></div></div>
<div>
<pre><code>Coordinates in C basis: Matrix([[9/2], [-1/2]])</code></pre>
</div>
</div>
<ul>
<li>Convert back:</li>
</ul>
<div id="d65bdc32" data-execution_count="228">
<div><div id="cb394"><pre><code><span id="cb394-1"><a href="#cb394-1" aria-hidden="true" tabindex="-1"></a>reconstructed <span>=</span> C <span>*</span> coords_in_C</span>
<span id="cb394-2"><a href="#cb394-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Reconstructed vector:&#34;</span>, reconstructed)</span></code></pre></div></div>
<div>
<pre><code>Reconstructed vector: Matrix([[4], [5]])</code></pre>
</div>
</div>
<p>The reconstruction matches the original vector.</p>
<ol start="4" type="1">
<li>General formula</li>
</ol>
<p>If <span>\(P\)</span> is the change-of-basis matrix from basis <span>\(B\)</span> to basis <span>\(C\)</span>:</p>
<p><span>\[
[v]_C = P^{-1}[v]_B
\]</span></p>
<p><span>\[
[v]_B = P[v]_C
\]</span></p>
<p>Here, <span>\(P\)</span> is the matrix of new basis vectors written in terms of the old basis.</p>
<ol start="5" type="1">
<li>Random 3D example</li>
</ol>
<div id="176726ff" data-execution_count="229">
<div><div id="cb396"><pre><code><span id="cb396-1"><a href="#cb396-1" aria-hidden="true" tabindex="-1"></a>B <span>=</span> Matrix.eye(<span>3</span>)  <span># standard basis</span></span>
<span id="cb396-2"><a href="#cb396-2" aria-hidden="true" tabindex="-1"></a>C <span>=</span> Matrix.hstack(</span>
<span id="cb396-3"><a href="#cb396-3" aria-hidden="true" tabindex="-1"></a>    Matrix([<span>1</span>,<span>0</span>,<span>1</span>]),</span>
<span id="cb396-4"><a href="#cb396-4" aria-hidden="true" tabindex="-1"></a>    Matrix([<span>0</span>,<span>1</span>,<span>1</span>]),</span>
<span id="cb396-5"><a href="#cb396-5" aria-hidden="true" tabindex="-1"></a>    Matrix([<span>1</span>,<span>1</span>,<span>0</span>])</span>
<span id="cb396-6"><a href="#cb396-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb396-7"><a href="#cb396-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb396-8"><a href="#cb396-8" aria-hidden="true" tabindex="-1"></a>v <span>=</span> Matrix([<span>2</span>,<span>3</span>,<span>4</span>])</span>
<span id="cb396-9"><a href="#cb396-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb396-10"><a href="#cb396-10" aria-hidden="true" tabindex="-1"></a>C_inv <span>=</span> C.inv()</span>
<span id="cb396-11"><a href="#cb396-11" aria-hidden="true" tabindex="-1"></a>coords_in_C <span>=</span> C_inv <span>*</span> v</span>
<span id="cb396-12"><a href="#cb396-12" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Coordinates in new basis C:&#34;</span>, coords_in_C)</span>
<span id="cb396-13"><a href="#cb396-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb396-14"><a href="#cb396-14" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Back to standard:&#34;</span>, C <span>*</span> coords_in_C)</span></code></pre></div></div>
<div>
<pre><code>Coordinates in new basis C: Matrix([[3/2], [5/2], [1/2]])
Back to standard: Matrix([[2], [3], [4]])</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-37">
<h4 data-anchor-id="try-it-yourself-37">Try It Yourself</h4>
<ol type="1">
<li>Convert <span>\([7,3]\)</span> from the standard basis to the basis <span>\(\{[2,0], [0,3]\}\)</span>.</li>
<li>Pick a random invertible 3×3 matrix as a basis. Write a vector in that basis, then convert it back to the standard basis.</li>
<li>Prove that converting back and forth always returns the same vector.</li>
</ol>
</section>
<section id="the-takeaway-21">
<h4 data-anchor-id="the-takeaway-21">The Takeaway</h4>
<ul>
<li>A change-of-basis matrix converts coordinates between bases.</li>
<li>Going from new basis → old basis uses the basis matrix.</li>
<li>Going from old basis → new basis requires its inverse.</li>
<li>The vector itself never changes - only the description of it does.</li>
</ul>
</section>
</section>
<section id="affine-subspaces-lines-and-planes-not-through-the-origin">
<h3 data-anchor-id="affine-subspaces-lines-and-planes-not-through-the-origin">40. Affine Subspaces (Lines and Planes Not Through the Origin)</h3>
<p>So far, subspaces always passed through the origin. But many familiar objects - like lines offset from the origin or planes floating in space - are affine subspaces. They look like subspaces, just shifted away from zero.</p>
<section id="set-up-your-lab-39">
<h4 data-anchor-id="set-up-your-lab-39">Set Up Your Lab</h4>
<div id="3eb7f41c" data-execution_count="230">
<div><div id="cb398"><pre><code><span id="cb398-1"><a href="#cb398-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb398-2"><a href="#cb398-2" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span>
<span id="cb398-3"><a href="#cb398-3" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-39">
<h4 data-anchor-id="step-by-step-code-walkthrough-39">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Line through the origin (a subspace)</li>
</ol>
<p><span>\[
L = \{ t \cdot [1,2] : t \in \mathbb{R} \}
\]</span></p>
<div id="636712e9" data-execution_count="231">
<div><div id="cb399"><pre><code><span id="cb399-1"><a href="#cb399-1" aria-hidden="true" tabindex="-1"></a>t <span>=</span> np.linspace(<span>-</span><span>3</span>,<span>3</span>,<span>20</span>)</span>
<span id="cb399-2"><a href="#cb399-2" aria-hidden="true" tabindex="-1"></a>line_origin <span>=</span> np.array([t, <span>2</span><span>*</span>t]).T</span>
<span id="cb399-3"><a href="#cb399-3" aria-hidden="true" tabindex="-1"></a>plt.plot(line_origin[:,<span>0</span>], line_origin[:,<span>1</span>], label<span>=</span><span>&#34;Through origin&#34;</span>)</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-232-output-1.png" width="569" height="411"/></p>
</figure>
</div>
</div>
</div>
<ol start="2" type="1">
<li>Line not through the origin (affine subspace)</li>
</ol>
<p><span>\[
L&#39; = \{ [3,1] + t \cdot [1,2] : t \in \mathbb{R} \}
\]</span></p>
<div id="2b78c455" data-execution_count="232">
<div><div id="cb400"><pre><code><span id="cb400-1"><a href="#cb400-1" aria-hidden="true" tabindex="-1"></a>point <span>=</span> np.array([<span>3</span>,<span>1</span>])</span>
<span id="cb400-2"><a href="#cb400-2" aria-hidden="true" tabindex="-1"></a>direction <span>=</span> np.array([<span>1</span>,<span>2</span>])</span>
<span id="cb400-3"><a href="#cb400-3" aria-hidden="true" tabindex="-1"></a>line_shifted <span>=</span> np.array([point <span>+</span> k<span>*</span>direction <span>for</span> k <span>in</span> t])</span>
<span id="cb400-4"><a href="#cb400-4" aria-hidden="true" tabindex="-1"></a>plt.plot(line_shifted[:,<span>0</span>], line_shifted[:,<span>1</span>], label<span>=</span><span>&#34;Shifted line&#34;</span>)</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-233-output-1.png" width="569" height="411"/></p>
</figure>
</div>
</div>
</div>
<ol start="3" type="1">
<li>Visualizing together</li>
</ol>
<div id="8c58022f" data-execution_count="233">
<div><div id="cb401"><pre><code><span id="cb401-1"><a href="#cb401-1" aria-hidden="true" tabindex="-1"></a>plt.scatter(<span>*</span>point, color<span>=</span><span>&#34;red&#34;</span>, label<span>=</span><span>&#34;Shift point&#34;</span>)</span>
<span id="cb401-2"><a href="#cb401-2" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span>0</span>,color<span>=</span><span>&#39;black&#39;</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb401-3"><a href="#cb401-3" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span>0</span>,color<span>=</span><span>&#39;black&#39;</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb401-4"><a href="#cb401-4" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb401-5"><a href="#cb401-5" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb401-6"><a href="#cb401-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-234-output-1.png" width="571" height="411"/></p>
</figure>
</div>
</div>
</div>
<p>One line passes through the origin, the other is parallel but shifted.</p>
<ol start="4" type="1">
<li>Plane example</li>
</ol>
<p>A plane in <span>\(\mathbb{R}^3\)</span>:</p>
<p><span>\[
P = \{ [1,2,3] + s[1,0,0] + t[0,1,0] : s,t \in \mathbb{R} \}
\]</span></p>
<p>This is an affine plane parallel to the <span>\(xy\)</span>-plane, but shifted.</p>
<div id="9f66eb93" data-execution_count="234">
<div><div id="cb402"><pre><code><span id="cb402-1"><a href="#cb402-1" aria-hidden="true" tabindex="-1"></a>s_vals <span>=</span> np.linspace(<span>-</span><span>2</span>,<span>2</span>,<span>10</span>)</span>
<span id="cb402-2"><a href="#cb402-2" aria-hidden="true" tabindex="-1"></a>t_vals <span>=</span> np.linspace(<span>-</span><span>2</span>,<span>2</span>,<span>10</span>)</span>
<span id="cb402-3"><a href="#cb402-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb402-4"><a href="#cb402-4" aria-hidden="true" tabindex="-1"></a>points <span>=</span> []</span>
<span id="cb402-5"><a href="#cb402-5" aria-hidden="true" tabindex="-1"></a><span>for</span> s <span>in</span> s_vals:</span>
<span id="cb402-6"><a href="#cb402-6" aria-hidden="true" tabindex="-1"></a>    <span>for</span> t <span>in</span> t_vals:</span>
<span id="cb402-7"><a href="#cb402-7" aria-hidden="true" tabindex="-1"></a>        points.append([<span>1</span>,<span>2</span>,<span>3</span>] <span>+</span> s<span>*</span>np.array([<span>1</span>,<span>0</span>,<span>0</span>]) <span>+</span> t<span>*</span>np.array([<span>0</span>,<span>1</span>,<span>0</span>]))</span>
<span id="cb402-8"><a href="#cb402-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb402-9"><a href="#cb402-9" aria-hidden="true" tabindex="-1"></a>points <span>=</span> np.array(points)</span>
<span id="cb402-10"><a href="#cb402-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb402-11"><a href="#cb402-11" aria-hidden="true" tabindex="-1"></a><span>from</span> mpl_toolkits.mplot3d <span>import</span> Axes3D</span>
<span id="cb402-12"><a href="#cb402-12" aria-hidden="true" tabindex="-1"></a>fig <span>=</span> plt.figure()</span>
<span id="cb402-13"><a href="#cb402-13" aria-hidden="true" tabindex="-1"></a>ax <span>=</span> fig.add_subplot(<span>111</span>, projection<span>=</span><span>&#39;3d&#39;</span>)</span>
<span id="cb402-14"><a href="#cb402-14" aria-hidden="true" tabindex="-1"></a>ax.scatter(points[:,<span>0</span>], points[:,<span>1</span>], points[:,<span>2</span>])</span>
<span id="cb402-15"><a href="#cb402-15" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span>&#34;Affine plane in R^3&#34;</span>)</span>
<span id="cb402-16"><a href="#cb402-16" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-235-output-1.png" width="412" height="416"/></p>
</figure>
</div>
</div>
</div>
<ol start="5" type="1">
<li>Algebraic difference</li>
</ol>
<ul>
<li>A subspace must satisfy closure under addition and scalar multiplication, and must include 0.</li>
<li>An affine subspace is just a subspace plus a fixed shift vector.</li>
</ul>
</section>
<section id="try-it-yourself-38">
<h4 data-anchor-id="try-it-yourself-38">Try It Yourself</h4>
<ol type="1">
<li><p>Define a line in <span>\(\mathbb{R}^2\)</span>:</p>
<p><span>\[
(x,y) = (2,3) + t(1,-1)
\]</span></p>
<p>Plot it and compare with the subspace spanned by <span>\((1,-1)\)</span>.</p></li>
<li><p>Construct an affine plane in <span>\(\mathbb{R}^3\)</span> shifted by vector <span>\((5,5,5)\)</span>.</p></li>
<li><p>Show algebraically that subtracting the shift point turns an affine subspace back into a regular subspace.</p></li>
</ol>
</section>
<section id="the-takeaway-22">
<h4 data-anchor-id="the-takeaway-22">The Takeaway</h4>
<ul>
<li>Subspaces go through the origin.</li>
<li>Affine subspaces are shifted copies of subspaces.</li>
<li>They’re essential in geometry, computer graphics, and optimization (e.g., feasible regions in linear programming).</li>
</ul>
</section>
</section>
</section>
<section id="chapter-5.-linear-transformation-and-structure">
<h2 data-anchor-id="chapter-5.-linear-transformation-and-structure">Chapter 5. Linear Transformation and Structure</h2>
<section id="linear-transformations-preserving-lines-and-sums">
<h3 data-anchor-id="linear-transformations-preserving-lines-and-sums">41. Linear Transformations (Preserving Lines and Sums)</h3>
<p>A linear transformation is a function between vector spaces that preserves two key properties:</p>
<ol type="1">
<li>Additivity: <span>\(T(u+v) = T(u) + T(v)\)</span></li>
<li>Homogeneity: <span>\(T(cu) = cT(u)\)</span></li>
</ol>
<p>In practice, every linear transformation can be represented by a matrix. This lab will help you understand and experiment with linear transformations in Python.</p>
<section id="set-up-your-lab-40">
<h4 data-anchor-id="set-up-your-lab-40">Set Up Your Lab</h4>
<div id="aba519e8" data-execution_count="235">
<div><div id="cb403"><pre><code><span id="cb403-1"><a href="#cb403-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb403-2"><a href="#cb403-2" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-40">
<h4 data-anchor-id="step-by-step-code-walkthrough-40">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Simple linear transformation (scaling)</li>
</ol>
<p>Let’s scale vectors by 2 in the x-direction and by 0.5 in the y-direction.</p>
<div id="09875e6b" data-execution_count="236">
<div><div id="cb404"><pre><code><span id="cb404-1"><a href="#cb404-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([</span>
<span id="cb404-2"><a href="#cb404-2" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>, <span>0</span>],</span>
<span id="cb404-3"><a href="#cb404-3" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>, <span>0.5</span>]</span>
<span id="cb404-4"><a href="#cb404-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb404-5"><a href="#cb404-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb404-6"><a href="#cb404-6" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.array([<span>1</span>, <span>2</span>])</span>
<span id="cb404-7"><a href="#cb404-7" aria-hidden="true" tabindex="-1"></a>Tv <span>=</span> A <span>@</span> v</span>
<span id="cb404-8"><a href="#cb404-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Original v:&#34;</span>, v)</span>
<span id="cb404-9"><a href="#cb404-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Transformed Tv:&#34;</span>, Tv)</span></code></pre></div></div>
<div>
<pre><code>Original v: [1 2]
Transformed Tv: [2. 1.]</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Visualizing multiple vectors</li>
</ol>
<div id="4debd414" data-execution_count="237">
<div><div id="cb406"><pre><code><span id="cb406-1"><a href="#cb406-1" aria-hidden="true" tabindex="-1"></a>vectors <span>=</span> [np.array([<span>1</span>,<span>1</span>]), np.array([<span>2</span>,<span>0</span>]), np.array([<span>-</span><span>1</span>,<span>2</span>])]</span>
<span id="cb406-2"><a href="#cb406-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb406-3"><a href="#cb406-3" aria-hidden="true" tabindex="-1"></a><span>for</span> v <span>in</span> vectors:</span>
<span id="cb406-4"><a href="#cb406-4" aria-hidden="true" tabindex="-1"></a>    Tv <span>=</span> A <span>@</span> v</span>
<span id="cb406-5"><a href="#cb406-5" aria-hidden="true" tabindex="-1"></a>    plt.arrow(<span>0</span>,<span>0</span>,v[<span>0</span>],v[<span>1</span>],head_width<span>=</span><span>0.1</span>,color<span>=</span><span>&#39;blue&#39;</span>,length_includes_head<span>=</span><span>True</span>)</span>
<span id="cb406-6"><a href="#cb406-6" aria-hidden="true" tabindex="-1"></a>    plt.arrow(<span>0</span>,<span>0</span>,Tv[<span>0</span>],Tv[<span>1</span>],head_width<span>=</span><span>0.1</span>,color<span>=</span><span>&#39;red&#39;</span>,length_includes_head<span>=</span><span>True</span>)</span>
<span id="cb406-7"><a href="#cb406-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb406-8"><a href="#cb406-8" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span>0</span>,color<span>=</span><span>&#39;black&#39;</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb406-9"><a href="#cb406-9" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span>0</span>,color<span>=</span><span>&#39;black&#39;</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb406-10"><a href="#cb406-10" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span>-</span><span>3</span>,<span>5</span>)</span>
<span id="cb406-11"><a href="#cb406-11" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span>-</span><span>1</span>,<span>5</span>)</span>
<span id="cb406-12"><a href="#cb406-12" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb406-13"><a href="#cb406-13" aria-hidden="true" tabindex="-1"></a>plt.title(<span>&#34;Blue = original, Red = transformed&#34;</span>)</span>
<span id="cb406-14"><a href="#cb406-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-238-output-1.png" width="573" height="431"/></p>
</figure>
</div>
</div>
</div>
<p>Blue arrows are the original vectors; red arrows are the transformed ones. Notice how the transformation stretches and compresses consistently.</p>
<ol start="3" type="1">
<li>Rotation as a linear transformation</li>
</ol>
<p>Rotating vectors by <span>\(\theta = 90^\circ\)</span>:</p>
<div id="36cd5c80" data-execution_count="238">
<div><div id="cb407"><pre><code><span id="cb407-1"><a href="#cb407-1" aria-hidden="true" tabindex="-1"></a>theta <span>=</span> np.pi<span>/</span><span>2</span></span>
<span id="cb407-2"><a href="#cb407-2" aria-hidden="true" tabindex="-1"></a>R <span>=</span> np.array([</span>
<span id="cb407-3"><a href="#cb407-3" aria-hidden="true" tabindex="-1"></a>    [np.cos(theta), <span>-</span>np.sin(theta)],</span>
<span id="cb407-4"><a href="#cb407-4" aria-hidden="true" tabindex="-1"></a>    [np.sin(theta),  np.cos(theta)]</span>
<span id="cb407-5"><a href="#cb407-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb407-6"><a href="#cb407-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb407-7"><a href="#cb407-7" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.array([<span>1</span>,<span>0</span>])</span>
<span id="cb407-8"><a href="#cb407-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rotate [1,0] by 90°:&#34;</span>, R <span>@</span> v)</span></code></pre></div></div>
<div>
<pre><code>Rotate [1,0] by 90°: [6.123234e-17 1.000000e+00]</code></pre>
</div>
</div>
<p>The result is <span>\([0,1]\)</span>, a perfect rotation.</p>
<ol start="4" type="1">
<li>Checking linearity</li>
</ol>
<div id="dd7e8f74" data-execution_count="239">
<div><div id="cb409"><pre><code><span id="cb409-1"><a href="#cb409-1" aria-hidden="true" tabindex="-1"></a>u <span>=</span> np.array([<span>1</span>,<span>2</span>])</span>
<span id="cb409-2"><a href="#cb409-2" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.array([<span>3</span>,<span>4</span>])</span>
<span id="cb409-3"><a href="#cb409-3" aria-hidden="true" tabindex="-1"></a>c <span>=</span> <span>5</span></span>
<span id="cb409-4"><a href="#cb409-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb409-5"><a href="#cb409-5" aria-hidden="true" tabindex="-1"></a>lhs <span>=</span> A <span>@</span> (u<span>+</span>v)</span>
<span id="cb409-6"><a href="#cb409-6" aria-hidden="true" tabindex="-1"></a>rhs <span>=</span> A<span>@</span>u <span>+</span> A<span>@</span>v</span>
<span id="cb409-7"><a href="#cb409-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Additivity holds?&#34;</span>, np.allclose(lhs,rhs))</span>
<span id="cb409-8"><a href="#cb409-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb409-9"><a href="#cb409-9" aria-hidden="true" tabindex="-1"></a>lhs <span>=</span> A <span>@</span> (c<span>*</span>u)</span>
<span id="cb409-10"><a href="#cb409-10" aria-hidden="true" tabindex="-1"></a>rhs <span>=</span> c<span>*</span>(A<span>@</span>u)</span>
<span id="cb409-11"><a href="#cb409-11" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Homogeneity holds?&#34;</span>, np.allclose(lhs,rhs))</span></code></pre></div></div>
<div>
<pre><code>Additivity holds? True
Homogeneity holds? True</code></pre>
</div>
</div>
<p>Both checks return <code>True</code>, proving <span>\(T\)</span> is linear.</p>
<ol start="5" type="1">
<li>Non-linear example (for contrast)</li>
</ol>
<p>A transformation like <span>\(T(x,y) = (x^2, y)\)</span> is not linear.</p>
<div id="544a0ed6" data-execution_count="240">
<div><div id="cb411"><pre><code><span id="cb411-1"><a href="#cb411-1" aria-hidden="true" tabindex="-1"></a><span>def</span> nonlinear(v):</span>
<span id="cb411-2"><a href="#cb411-2" aria-hidden="true" tabindex="-1"></a>    <span>return</span> np.array([v[<span>0</span>]<span>**</span><span>2</span>, v[<span>1</span>]])</span>
<span id="cb411-3"><a href="#cb411-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb411-4"><a href="#cb411-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;T([2,3]) =&#34;</span>, nonlinear(np.array([<span>2</span>,<span>3</span>])))</span>
<span id="cb411-5"><a href="#cb411-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Check additivity:&#34;</span>, nonlinear(np.array([<span>1</span>,<span>2</span>])<span>+</span>np.array([<span>3</span>,<span>4</span>])) <span>==</span> (nonlinear([<span>1</span>,<span>2</span>])<span>+</span>nonlinear([<span>3</span>,<span>4</span>])))</span></code></pre></div></div>
<div>
<pre><code>T([2,3]) = [4 3]
Check additivity: [False  True]</code></pre>
</div>
</div>
<p>This fails the additivity test, so it’s not linear.</p>
</section>
<section id="try-it-yourself-39">
<h4 data-anchor-id="try-it-yourself-39">Try It Yourself</h4>
<ol type="1">
<li><p>Define a shear matrix</p>
<p><span>\[
S = \begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{bmatrix}
\]</span></p>
<p>Apply it to vectors and plot before/after.</p></li>
<li><p>Verify linearity for rotation by 45°.</p></li>
<li><p>Test whether <span>\(T(x,y) = (x+y, y)\)</span> is linear.</p></li>
</ol>
</section>
<section id="the-takeaway-23">
<h4 data-anchor-id="the-takeaway-23">The Takeaway</h4>
<ul>
<li>A linear transformation preserves vector addition and scalar multiplication.</li>
<li>Every linear transformation can be represented by a matrix.</li>
<li>Visualizing with arrows helps build geometric intuition: stretching, rotating, and shearing are all linear.</li>
</ul>
</section>
</section>
<section id="matrix-representation-of-a-linear-map-choosing-a-basis">
<h3 data-anchor-id="matrix-representation-of-a-linear-map-choosing-a-basis">42. Matrix Representation of a Linear Map (Choosing a Basis)</h3>
<p>Every linear transformation can be written as a matrix, but the exact matrix depends on the basis you choose. This lab shows how to build and interpret matrix representations.</p>
<section id="set-up-your-lab-41">
<h4 data-anchor-id="set-up-your-lab-41">Set Up Your Lab</h4>
<div id="e8929758" data-execution_count="241">
<div><div id="cb413"><pre><code><span id="cb413-1"><a href="#cb413-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb413-2"><a href="#cb413-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-41">
<h4 data-anchor-id="step-by-step-code-walkthrough-41">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>From transformation to matrix</li>
</ol>
<p>Suppose <span>\(T: \mathbb{R}^2 \to \mathbb{R}^2\)</span> is defined by:</p>
<p><span>\[
T(x,y) = (2x + y, \; x - y)
\]</span></p>
<p>To find its matrix in the standard basis, apply <span>\(T\)</span> to each basis vector:</p>
<div id="eefd2425" data-execution_count="242">
<div><div id="cb414"><pre><code><span id="cb414-1"><a href="#cb414-1" aria-hidden="true" tabindex="-1"></a>e1 <span>=</span> Matrix([<span>1</span>,<span>0</span>])</span>
<span id="cb414-2"><a href="#cb414-2" aria-hidden="true" tabindex="-1"></a>e2 <span>=</span> Matrix([<span>0</span>,<span>1</span>])</span>
<span id="cb414-3"><a href="#cb414-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb414-4"><a href="#cb414-4" aria-hidden="true" tabindex="-1"></a><span>def</span> T(v):</span>
<span id="cb414-5"><a href="#cb414-5" aria-hidden="true" tabindex="-1"></a>    x, y <span>=</span> v</span>
<span id="cb414-6"><a href="#cb414-6" aria-hidden="true" tabindex="-1"></a>    <span>return</span> Matrix([<span>2</span><span>*</span>x <span>+</span> y, x <span>-</span> y])</span>
<span id="cb414-7"><a href="#cb414-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb414-8"><a href="#cb414-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;T(e1):&#34;</span>, T(e1))</span>
<span id="cb414-9"><a href="#cb414-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;T(e2):&#34;</span>, T(e2))</span></code></pre></div></div>
<div>
<pre><code>T(e1): Matrix([[2], [1]])
T(e2): Matrix([[1], [-1]])</code></pre>
</div>
</div>
<p>Stacking results as columns gives the matrix:</p>
<div id="703e13a4" data-execution_count="243">
<div><div id="cb416"><pre><code><span id="cb416-1"><a href="#cb416-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> Matrix.hstack(T(e1), T(e2))</span>
<span id="cb416-2"><a href="#cb416-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Matrix representation in standard basis:</span><span>\n</span><span>&#34;</span>, A)</span></code></pre></div></div>
<div>
<pre><code>Matrix representation in standard basis:
 Matrix([[2, 1], [1, -1]])</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Using the matrix for computations</li>
</ol>
<div id="3a731b42" data-execution_count="244">
<div><div id="cb418"><pre><code><span id="cb418-1"><a href="#cb418-1" aria-hidden="true" tabindex="-1"></a>v <span>=</span> Matrix([<span>3</span>,<span>4</span>])</span>
<span id="cb418-2"><a href="#cb418-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;T(v) via definition:&#34;</span>, T(v))</span>
<span id="cb418-3"><a href="#cb418-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;T(v) via matrix:&#34;</span>, A<span>*</span>v)</span></code></pre></div></div>
<div>
<pre><code>T(v) via definition: Matrix([[10], [-1]])
T(v) via matrix: Matrix([[10], [-1]])</code></pre>
</div>
</div>
<p>Both methods match.</p>
<ol start="3" type="1">
<li>Matrix in a different basis</li>
</ol>
<p>Now suppose we use basis</p>
<p><span>\[
B = \{ [1,1], [1,-1] \}
\]</span></p>
<p>To represent <span>\(T\)</span> in this basis:</p>
<ol type="1">
<li>Build the change-of-basis matrix <span>\(P\)</span>.</li>
<li>Compute <span>\(A_B = P^{-1}AP\)</span>.</li>
</ol>
<div id="13047e66" data-execution_count="245">
<div><div id="cb420"><pre><code><span id="cb420-1"><a href="#cb420-1" aria-hidden="true" tabindex="-1"></a>B <span>=</span> Matrix.hstack(Matrix([<span>1</span>,<span>1</span>]), Matrix([<span>1</span>,<span>-</span><span>1</span>]))</span>
<span id="cb420-2"><a href="#cb420-2" aria-hidden="true" tabindex="-1"></a>P <span>=</span> B</span>
<span id="cb420-3"><a href="#cb420-3" aria-hidden="true" tabindex="-1"></a>A_B <span>=</span> P.inv() <span>*</span> A <span>*</span> P</span>
<span id="cb420-4"><a href="#cb420-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Matrix representation in new basis:</span><span>\n</span><span>&#34;</span>, A_B)</span></code></pre></div></div>
<div>
<pre><code>Matrix representation in new basis:
 Matrix([[3/2, 3/2], [3/2, -1/2]])</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Interpretation</li>
</ol>
<ul>
<li>In standard basis, <span>\(A\)</span> tells us how <span>\(T\)</span> acts on unit vectors.</li>
<li>In basis <span>\(B\)</span>, <span>\(A_B\)</span> shows how <span>\(T\)</span> looks when described using different coordinates.</li>
</ul>
<ol start="5" type="1">
<li>Random linear map in <span>\(\mathbb{R}^3\)</span></li>
</ol>
<div id="13e37e9e" data-execution_count="246">
<div><div id="cb422"><pre><code><span id="cb422-1"><a href="#cb422-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span>1</span>)</span>
<span id="cb422-2"><a href="#cb422-2" aria-hidden="true" tabindex="-1"></a>A3 <span>=</span> Matrix(np.random.randint(<span>-</span><span>3</span>,<span>4</span>,(<span>3</span>,<span>3</span>)))</span>
<span id="cb422-3"><a href="#cb422-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Random transformation matrix:</span><span>\n</span><span>&#34;</span>, A3)</span>
<span id="cb422-4"><a href="#cb422-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb422-5"><a href="#cb422-5" aria-hidden="true" tabindex="-1"></a>B3 <span>=</span> Matrix.hstack(Matrix([<span>1</span>,<span>0</span>,<span>1</span>]), Matrix([<span>0</span>,<span>1</span>,<span>1</span>]), Matrix([<span>1</span>,<span>1</span>,<span>0</span>]))</span>
<span id="cb422-6"><a href="#cb422-6" aria-hidden="true" tabindex="-1"></a>A3_B <span>=</span> B3.inv() <span>*</span> A3 <span>*</span> B3</span>
<span id="cb422-7"><a href="#cb422-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Representation in new basis:</span><span>\n</span><span>&#34;</span>, A3_B)</span></code></pre></div></div>
<div>
<pre><code>Random transformation matrix:
 Matrix([[2, 0, 1], [-3, -2, 0], [2, -3, -3]])
Representation in new basis:
 Matrix([[5/2, -3/2, 3], [-7/2, -9/2, -4], [1/2, 5/2, -1]])</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-40">
<h4 data-anchor-id="try-it-yourself-40">Try It Yourself</h4>
<ol type="1">
<li>Define <span>\(T(x,y) = (x+2y, 3x+y)\)</span>. Find its matrix in the standard basis.</li>
<li>Use a new basis <span>\(\{[2,0],[0,3]\}\)</span>. Compute the representation <span>\(A_B\)</span>.</li>
<li>Verify that applying <span>\(T\)</span> directly to a vector matches computing via <span>\(A_B\)</span> and change-of-basis.</li>
</ol>
</section>
<section id="the-takeaway-24">
<h4 data-anchor-id="the-takeaway-24">The Takeaway</h4>
<ul>
<li>A linear transformation becomes a matrix representation once a basis is chosen.</li>
<li>Columns of the matrix = images of basis vectors.</li>
<li>Changing the basis changes the matrix, but the transformation itself stays the same.</li>
</ul>
</section>
</section>
<section id="kernel-and-image-inputs-that-vanish-outputs-we-can-reach">
<h3 data-anchor-id="kernel-and-image-inputs-that-vanish-outputs-we-can-reach">43. Kernel and Image (Inputs That Vanish; Outputs We Can Reach)</h3>
<p>Two fundamental subspaces describe any linear transformation <span>\(T(x) = Ax\)</span>:</p>
<ul>
<li>Kernel (null space): all vectors <span>\(x\)</span> such that <span>\(Ax = 0\)</span>.</li>
<li>Image (column space): all possible outputs <span>\(Ax\)</span>.</li>
</ul>
<p>The kernel tells us what inputs collapse to zero, while the image tells us what outputs are achievable.</p>
<section id="set-up-your-lab-42">
<h4 data-anchor-id="set-up-your-lab-42">Set Up Your Lab</h4>
<div id="c36dc299" data-execution_count="247">
<div><div id="cb424"><pre><code><span id="cb424-1"><a href="#cb424-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb424-2"><a href="#cb424-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-42">
<h4 data-anchor-id="step-by-step-code-walkthrough-42">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Kernel of a matrix</li>
</ol>
<p>Consider</p>
<p><span>\[
A = \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 2 &amp; 4 &amp; 6 \end{bmatrix}
\]</span></p>
<div id="aca4b8fd" data-execution_count="248">
<div><div id="cb425"><pre><code><span id="cb425-1"><a href="#cb425-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> Matrix([</span>
<span id="cb425-2"><a href="#cb425-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>2</span>,<span>3</span>],</span>
<span id="cb425-3"><a href="#cb425-3" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>,<span>4</span>,<span>6</span>]</span>
<span id="cb425-4"><a href="#cb425-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb425-5"><a href="#cb425-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb425-6"><a href="#cb425-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Null space (kernel):&#34;</span>, A.nullspace())</span></code></pre></div></div>
<div>
<pre><code>Null space (kernel): [Matrix([
[-2],
[ 1],
[ 0]]), Matrix([
[-3],
[ 0],
[ 1]])]</code></pre>
</div>
</div>
<p>The null space basis shows dependencies among columns. Here, the kernel is 2-dimensional because columns are dependent.</p>
<ol start="2" type="1">
<li>Image (column space)</li>
</ol>
<div id="022cd2d3" data-execution_count="249">
<div><div id="cb427"><pre><code><span id="cb427-1"><a href="#cb427-1" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Column space (image):&#34;</span>, A.columnspace())</span>
<span id="cb427-2"><a href="#cb427-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rank (dimension of image):&#34;</span>, A.rank())</span></code></pre></div></div>
<div>
<pre><code>Column space (image): [Matrix([
[1],
[2]])]
Rank (dimension of image): 1</code></pre>
</div>
</div>
<p>The image is spanned by <span>\([1,2]^T\)</span>. So all outputs of <span>\(A\)</span> are multiples of this vector.</p>
<ol start="3" type="1">
<li>Interpretation</li>
</ol>
<ul>
<li>Kernel vectors → directions that map to zero.</li>
<li>Image vectors → directions we can actually reach in the output space.</li>
</ul>
<p>If <span>\(x \in \ker(A)\)</span>, then <span>\(Ax = 0\)</span>. If <span>\(b\)</span> is not in the image, the system <span>\(Ax = b\)</span> has no solution.</p>
<ol start="4" type="1">
<li>Example with full rank</li>
</ol>
<div id="f04f6b4b" data-execution_count="250">
<div><div id="cb429"><pre><code><span id="cb429-1"><a href="#cb429-1" aria-hidden="true" tabindex="-1"></a>B <span>=</span> Matrix([</span>
<span id="cb429-2"><a href="#cb429-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>0</span>,<span>0</span>],</span>
<span id="cb429-3"><a href="#cb429-3" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>1</span>,<span>0</span>],</span>
<span id="cb429-4"><a href="#cb429-4" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>0</span>,<span>1</span>]</span>
<span id="cb429-5"><a href="#cb429-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb429-6"><a href="#cb429-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb429-7"><a href="#cb429-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Kernel of B:&#34;</span>, B.nullspace())</span>
<span id="cb429-8"><a href="#cb429-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Image of B:&#34;</span>, B.columnspace())</span></code></pre></div></div>
<div>
<pre><code>Kernel of B: []
Image of B: [Matrix([
[1],
[0],
[0]]), Matrix([
[0],
[1],
[0]]), Matrix([
[0],
[0],
[1]])]</code></pre>
</div>
</div>
<ul>
<li>Kernel = only zero vector.</li>
<li>Image = all of <span>\(\mathbb{R}^3\)</span>.</li>
</ul>
<ol start="5" type="1">
<li>NumPy version (image via column space)</li>
</ol>
<div id="646fed4b" data-execution_count="251">
<div><div id="cb431"><pre><code><span id="cb431-1"><a href="#cb431-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>1</span>,<span>2</span>,<span>3</span>],[<span>2</span>,<span>4</span>,<span>6</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb431-2"><a href="#cb431-2" aria-hidden="true" tabindex="-1"></a>rank <span>=</span> np.linalg.matrix_rank(A)</span>
<span id="cb431-3"><a href="#cb431-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rank with NumPy:&#34;</span>, rank)</span></code></pre></div></div>

</div>
<p>NumPy doesn’t compute null spaces directly, but we can use SVD for that if needed.</p>
</section>
<section id="try-it-yourself-41">
<h4 data-anchor-id="try-it-yourself-41">Try It Yourself</h4>
<ol type="1">
<li><p>Compute kernel and image for</p>
<p><span>\[
\begin{bmatrix} 1 &amp; 1 \\ 1 &amp; 1 \end{bmatrix}
\]</span></p>
<p>What do they look like?</p></li>
<li><p>Take a random 3×4 matrix and find its kernel and image dimensions.</p></li>
<li><p>Solve <span>\(Ax = b\)</span> for a matrix <span>\(A\)</span>. Try two different <span>\(b\)</span>: one inside the image, one outside. Observe the difference.</p></li>
</ol>
</section>
<section id="the-takeaway-25">
<h4 data-anchor-id="the-takeaway-25">The Takeaway</h4>
<ul>
<li>Kernel = inputs that vanish under <span>\(A\)</span>.</li>
<li>Image = outputs that can be reached by <span>\(A\)</span>.</li>
<li>Together, they fully describe what a linear map does: what it “kills” and what it “produces.”</li>
</ul>
</section>
</section>
<section id="invertibility-and-isomorphisms-perfectly-reversible-maps">
<h3 data-anchor-id="invertibility-and-isomorphisms-perfectly-reversible-maps">44. Invertibility and Isomorphisms (Perfectly Reversible Maps)</h3>
<p>A matrix (or linear map) is invertible if it has an inverse <span>\(A^{-1}\)</span> such that</p>
<p><span>\[
A^{-1}A = I \quad \text{and} \quad AA^{-1} = I
\]</span></p>
<p>An invertible map is also called an isomorphism, because it preserves all information - every input has exactly one output, and every output comes from exactly one input.</p>
<section id="set-up-your-lab-43">
<h4 data-anchor-id="set-up-your-lab-43">Set Up Your Lab</h4>
<div id="aa3bee28" data-execution_count="252">
<div><div id="cb433"><pre><code><span id="cb433-1"><a href="#cb433-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb433-2"><a href="#cb433-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-43">
<h4 data-anchor-id="step-by-step-code-walkthrough-43">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Checking invertibility</li>
</ol>
<div id="02b909d0" data-execution_count="253">
<div><div id="cb434"><pre><code><span id="cb434-1"><a href="#cb434-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> Matrix([</span>
<span id="cb434-2"><a href="#cb434-2" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>,<span>1</span>],</span>
<span id="cb434-3"><a href="#cb434-3" aria-hidden="true" tabindex="-1"></a>    [<span>5</span>,<span>3</span>]</span>
<span id="cb434-4"><a href="#cb434-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb434-5"><a href="#cb434-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb434-6"><a href="#cb434-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Determinant:&#34;</span>, A.det())</span>
<span id="cb434-7"><a href="#cb434-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Is invertible?&#34;</span>, A.det() <span>!=</span> <span>0</span>)</span></code></pre></div></div>
<div>
<pre><code>Determinant: 1
Is invertible? True</code></pre>
</div>
</div>
<p>If determinant ≠ 0 → invertible.</p>
<ol start="2" type="1">
<li>Computing the inverse</li>
</ol>
<div id="7fe6f3b9" data-execution_count="254">
<div><div id="cb436"><pre><code><span id="cb436-1"><a href="#cb436-1" aria-hidden="true" tabindex="-1"></a>A_inv <span>=</span> A.inv()</span>
<span id="cb436-2"><a href="#cb436-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Inverse matrix:</span><span>\n</span><span>&#34;</span>, A_inv)</span>
<span id="cb436-3"><a href="#cb436-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb436-4"><a href="#cb436-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Check A*A_inv = I:</span><span>\n</span><span>&#34;</span>, A <span>*</span> A_inv)</span></code></pre></div></div>
<div>
<pre><code>Inverse matrix:
 Matrix([[3, -1], [-5, 2]])
Check A*A_inv = I:
 Matrix([[1, 0], [0, 1]])</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Solving systems with inverses</li>
</ol>
<p>For <span>\(Ax = b\)</span>, if <span>\(A\)</span> is invertible:</p>
<div id="027788cf" data-execution_count="255">
<div><div id="cb438"><pre><code><span id="cb438-1"><a href="#cb438-1" aria-hidden="true" tabindex="-1"></a>b <span>=</span> Matrix([<span>1</span>,<span>2</span>])</span>
<span id="cb438-2"><a href="#cb438-2" aria-hidden="true" tabindex="-1"></a>x <span>=</span> A_inv <span>*</span> b</span>
<span id="cb438-3"><a href="#cb438-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Solution x:&#34;</span>, x)</span></code></pre></div></div>
<div>
<pre><code>Solution x: Matrix([[1], [-1]])</code></pre>
</div>
</div>
<p>This is equivalent to <code>A.solve(b)</code> in SymPy or <code>np.linalg.solve</code> in NumPy.</p>
<ol start="4" type="1">
<li>Non-invertible (singular) example</li>
</ol>
<div id="995292d9" data-execution_count="256">
<div><div id="cb440"><pre><code><span id="cb440-1"><a href="#cb440-1" aria-hidden="true" tabindex="-1"></a>B <span>=</span> Matrix([</span>
<span id="cb440-2"><a href="#cb440-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>2</span>],</span>
<span id="cb440-3"><a href="#cb440-3" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>,<span>4</span>]</span>
<span id="cb440-4"><a href="#cb440-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb440-5"><a href="#cb440-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb440-6"><a href="#cb440-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Determinant:&#34;</span>, B.det())</span>
<span id="cb440-7"><a href="#cb440-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Is invertible?&#34;</span>, B.det() <span>!=</span> <span>0</span>)</span></code></pre></div></div>
<div>
<pre><code>Determinant: 0
Is invertible? False</code></pre>
</div>
</div>
<p>Determinant = 0 → no inverse. The matrix collapses space onto a line, losing information.</p>
<ol start="5" type="1">
<li>NumPy version</li>
</ol>
<div id="b59354a1" data-execution_count="257">
<div><div id="cb442"><pre><code><span id="cb442-1"><a href="#cb442-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>2</span>,<span>1</span>],[<span>5</span>,<span>3</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb442-2"><a href="#cb442-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Determinant:&#34;</span>, np.linalg.det(A))</span>
<span id="cb442-3"><a href="#cb442-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Inverse:</span><span>\n</span><span>&#34;</span>, np.linalg.inv(A))</span></code></pre></div></div>
<div>
<pre><code>Determinant: 1.0000000000000002
Inverse:
 [[ 3. -1.]
 [-5.  2.]]</code></pre>
</div>
</div>
<ol start="6" type="1">
<li>Geometric intuition</li>
</ol>
<ul>
<li>Invertible transformation = reversible (like rotating, scaling by nonzero).</li>
<li>Non-invertible transformation = squashing space into a lower dimension (like flattening a plane onto a line).</li>
</ul>
</section>
<section id="try-it-yourself-42">
<h4 data-anchor-id="try-it-yourself-42">Try It Yourself</h4>
<ol type="1">
<li><p>Test whether</p>
<p><span>\[
\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}
\]</span></p>
<p>is invertible and find its inverse.</p></li>
<li><p>Compute the determinant of a 3×3 random integer matrix. If it’s nonzero, find its inverse.</p></li>
<li><p>Create a singular 3×3 matrix (make one row a multiple of another). Confirm it has no inverse.</p></li>
</ol>
</section>
<section id="the-takeaway-26">
<h4 data-anchor-id="the-takeaway-26">The Takeaway</h4>
<ul>
<li>Invertible matrix ↔︎ isomorphism: perfectly reversible, no information lost.</li>
<li>Determinant ≠ 0 → invertible; determinant = 0 → singular.</li>
<li>Inverses are useful conceptually, but in computation we usually solve systems directly instead of calculating <span>\(A^{-1}\)</span>.</li>
</ul>
</section>
</section>
<section id="composition-powers-and-iteration-doing-it-again-and-again">
<h3 data-anchor-id="composition-powers-and-iteration-doing-it-again-and-again">45. Composition, Powers, and Iteration (Doing It Again and Again)</h3>
<p>Linear transformations can be chained together. Applying one after another is called composition, and in matrix form this becomes multiplication. Repeated application of the same transformation leads to powers of a matrix.</p>
<section id="set-up-your-lab-44">
<h4 data-anchor-id="set-up-your-lab-44">Set Up Your Lab</h4>
<div id="699c0c7e" data-execution_count="258">
<div><div id="cb444"><pre><code><span id="cb444-1"><a href="#cb444-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb444-2"><a href="#cb444-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-44">
<h4 data-anchor-id="step-by-step-code-walkthrough-44">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Composition of transformations</li>
</ol>
<p>Suppose we have two linear maps:</p>
<ul>
<li><span>\(T_1\)</span>: rotate by 90°</li>
<li><span>\(T_2\)</span>: scale x by 2</li>
</ul>
<div id="0b4c98d5" data-execution_count="259">
<div><div id="cb445"><pre><code><span id="cb445-1"><a href="#cb445-1" aria-hidden="true" tabindex="-1"></a>theta <span>=</span> np.pi<span>/</span><span>2</span></span>
<span id="cb445-2"><a href="#cb445-2" aria-hidden="true" tabindex="-1"></a>R <span>=</span> np.array([</span>
<span id="cb445-3"><a href="#cb445-3" aria-hidden="true" tabindex="-1"></a>    [np.cos(theta), <span>-</span>np.sin(theta)],</span>
<span id="cb445-4"><a href="#cb445-4" aria-hidden="true" tabindex="-1"></a>    [np.sin(theta),  np.cos(theta)]</span>
<span id="cb445-5"><a href="#cb445-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb445-6"><a href="#cb445-6" aria-hidden="true" tabindex="-1"></a>S <span>=</span> np.array([</span>
<span id="cb445-7"><a href="#cb445-7" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>,<span>0</span>],</span>
<span id="cb445-8"><a href="#cb445-8" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>1</span>]</span>
<span id="cb445-9"><a href="#cb445-9" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb445-10"><a href="#cb445-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb445-11"><a href="#cb445-11" aria-hidden="true" tabindex="-1"></a><span># Compose: apply R then S</span></span>
<span id="cb445-12"><a href="#cb445-12" aria-hidden="true" tabindex="-1"></a>C <span>=</span> S <span>@</span> R</span>
<span id="cb445-13"><a href="#cb445-13" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Composite matrix:</span><span>\n</span><span>&#34;</span>, C)</span></code></pre></div></div>
<div>
<pre><code>Composite matrix:
 [[ 1.2246468e-16 -2.0000000e+00]
 [ 1.0000000e+00  6.1232340e-17]]</code></pre>
</div>
</div>
<p>Applying the composite matrix is equivalent to applying both maps in sequence.</p>
<ol start="2" type="1">
<li>Verifying with a vector</li>
</ol>
<div id="5f5226f6" data-execution_count="260">
<div><div id="cb447"><pre><code><span id="cb447-1"><a href="#cb447-1" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.array([<span>1</span>,<span>1</span>])</span>
<span id="cb447-2"><a href="#cb447-2" aria-hidden="true" tabindex="-1"></a>step1 <span>=</span> R <span>@</span> v</span>
<span id="cb447-3"><a href="#cb447-3" aria-hidden="true" tabindex="-1"></a>step2 <span>=</span> S <span>@</span> step1</span>
<span id="cb447-4"><a href="#cb447-4" aria-hidden="true" tabindex="-1"></a>composite <span>=</span> C <span>@</span> v</span>
<span id="cb447-5"><a href="#cb447-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb447-6"><a href="#cb447-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Step-by-step:&#34;</span>, step2)</span>
<span id="cb447-7"><a href="#cb447-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Composite:&#34;</span>, composite)</span></code></pre></div></div>
<div>
<pre><code>Step-by-step: [-2.  1.]
Composite: [-2.  1.]</code></pre>
</div>
</div>
<p>Both results are the same → composition = matrix multiplication.</p>
<ol start="3" type="1">
<li>Powers of a matrix</li>
</ol>
<p>Repeatedly applying a transformation corresponds to matrix powers.</p>
<p>Example: scaling by 2.</p>
<div id="97823ab5" data-execution_count="261">
<div><div id="cb449"><pre><code><span id="cb449-1"><a href="#cb449-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>2</span>,<span>0</span>],[<span>0</span>,<span>2</span>]])</span>
<span id="cb449-2"><a href="#cb449-2" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.array([<span>1</span>,<span>1</span>])</span>
<span id="cb449-3"><a href="#cb449-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb449-4"><a href="#cb449-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;A @ v =&#34;</span>, A <span>@</span> v)</span>
<span id="cb449-5"><a href="#cb449-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;A^2 @ v =&#34;</span>, np.linalg.matrix_power(A,<span>2</span>) <span>@</span> v)</span>
<span id="cb449-6"><a href="#cb449-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;A^5 @ v =&#34;</span>, np.linalg.matrix_power(A,<span>5</span>) <span>@</span> v)</span></code></pre></div></div>
<div>
<pre><code>A @ v = [2 2]
A^2 @ v = [4 4]
A^5 @ v = [32 32]</code></pre>
</div>
</div>
<p>Each step doubles the scaling effect.</p>
<ol start="4" type="1">
<li>Iteration dynamics</li>
</ol>
<p>Let’s iterate a transformation many times and see what happens.</p>
<p>Example:</p>
<p><span>\[
A = \begin{bmatrix} 0.5 &amp; 0 \\ 0 &amp; 0.5 \end{bmatrix}
\]</span></p>
<div id="b37d0270" data-execution_count="262">
<div><div id="cb451"><pre><code><span id="cb451-1"><a href="#cb451-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>0.5</span>,<span>0</span>],[<span>0</span>,<span>0.5</span>]])</span>
<span id="cb451-2"><a href="#cb451-2" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.array([<span>4</span>,<span>4</span>])</span>
<span id="cb451-3"><a href="#cb451-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb451-4"><a href="#cb451-4" aria-hidden="true" tabindex="-1"></a><span>for</span> i <span>in</span> <span>range</span>(<span>5</span>):</span>
<span id="cb451-5"><a href="#cb451-5" aria-hidden="true" tabindex="-1"></a>    v <span>=</span> A <span>@</span> v</span>
<span id="cb451-6"><a href="#cb451-6" aria-hidden="true" tabindex="-1"></a>    <span>print</span>(<span>f&#34;Step </span><span>{</span>i<span>+</span><span>1</span><span>}</span><span>:&#34;</span>, v)</span></code></pre></div></div>
<div>
<pre><code>Step 1: [2. 2.]
Step 2: [1. 1.]
Step 3: [0.5 0.5]
Step 4: [0.25 0.25]
Step 5: [0.125 0.125]</code></pre>
</div>
</div>
<p>Each step shrinks the vector → iteration can reveal stability.</p>
<ol start="5" type="1">
<li>Random example</li>
</ol>
<div id="187b110b" data-execution_count="263">
<div><div id="cb453"><pre><code><span id="cb453-1"><a href="#cb453-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span>0</span>)</span>
<span id="cb453-2"><a href="#cb453-2" aria-hidden="true" tabindex="-1"></a>M <span>=</span> np.random.randint(<span>-</span><span>2</span>,<span>3</span>,(<span>2</span>,<span>2</span>))</span>
<span id="cb453-3"><a href="#cb453-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Random matrix:</span><span>\n</span><span>&#34;</span>, M)</span>
<span id="cb453-4"><a href="#cb453-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb453-5"><a href="#cb453-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;M^2:</span><span>\n</span><span>&#34;</span>, np.linalg.matrix_power(M,<span>2</span>))</span>
<span id="cb453-6"><a href="#cb453-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;M^3:</span><span>\n</span><span>&#34;</span>, np.linalg.matrix_power(M,<span>3</span>))</span></code></pre></div></div>
<div>
<pre><code>Random matrix:
 [[ 2 -2]
 [ 1  1]]
M^2:
 [[ 2 -6]
 [ 3 -1]]
M^3:
 [[ -2 -10]
 [  5  -7]]</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-43">
<h4 data-anchor-id="try-it-yourself-43">Try It Yourself</h4>
<ol type="1">
<li>Create two transformations: reflection across x-axis and scaling by 3. Compose them.</li>
<li>Take a shear matrix and compute <span>\(A^5\)</span>. What happens to a vector after repeated application?</li>
<li>Experiment with a rotation matrix raised to higher powers. What cycle do you see?</li>
</ol>
</section>
<section id="the-takeaway-27">
<h4 data-anchor-id="the-takeaway-27">The Takeaway</h4>
<ul>
<li>Composition of linear maps = matrix multiplication.</li>
<li>Powers of a matrix represent repeated application.</li>
<li>Iteration reveals long-term dynamics: shrinking, growing, or oscillating behavior.</li>
</ul>
</section>
</section>
<section id="similarity-and-conjugation-same-action-different-basis">
<h3 data-anchor-id="similarity-and-conjugation-same-action-different-basis">46. Similarity and Conjugation (Same Action, Different Basis)</h3>
<p>Two matrices <span>\(A\)</span> and <span>\(B\)</span> are called similar if there exists an invertible matrix <span>\(P\)</span> such that</p>
<p><span>\[
B = P^{-1} A P
\]</span></p>
<p>This means <span>\(A\)</span> and <span>\(B\)</span> represent the same linear transformation, but in different bases. This lab explores similarity and why it matters.</p>
<section id="set-up-your-lab-45">
<h4 data-anchor-id="set-up-your-lab-45">Set Up Your Lab</h4>
<div id="6dd9fbe4" data-execution_count="264">
<div><div id="cb455"><pre><code><span id="cb455-1"><a href="#cb455-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb455-2"><a href="#cb455-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-45">
<h4 data-anchor-id="step-by-step-code-walkthrough-45">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Example with a change of basis</li>
</ol>
<div id="de12b0eb" data-execution_count="265">
<div><div id="cb456"><pre><code><span id="cb456-1"><a href="#cb456-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> Matrix([</span>
<span id="cb456-2"><a href="#cb456-2" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>,<span>1</span>],</span>
<span id="cb456-3"><a href="#cb456-3" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>2</span>]</span>
<span id="cb456-4"><a href="#cb456-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb456-5"><a href="#cb456-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb456-6"><a href="#cb456-6" aria-hidden="true" tabindex="-1"></a>P <span>=</span> Matrix([</span>
<span id="cb456-7"><a href="#cb456-7" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>1</span>],</span>
<span id="cb456-8"><a href="#cb456-8" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>1</span>]</span>
<span id="cb456-9"><a href="#cb456-9" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb456-10"><a href="#cb456-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb456-11"><a href="#cb456-11" aria-hidden="true" tabindex="-1"></a>B <span>=</span> P.inv() <span>*</span> A <span>*</span> P</span>
<span id="cb456-12"><a href="#cb456-12" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Original A:</span><span>\n</span><span>&#34;</span>, A)</span>
<span id="cb456-13"><a href="#cb456-13" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Similar matrix B:</span><span>\n</span><span>&#34;</span>, B)</span></code></pre></div></div>
<div>
<pre><code>Original A:
 Matrix([[2, 1], [0, 2]])
Similar matrix B:
 Matrix([[2, 1], [0, 2]])</code></pre>
</div>
</div>
<p>Here, <span>\(A\)</span> and <span>\(B\)</span> are similar: they describe the same transformation in different coordinates.</p>
<ol start="2" type="1">
<li>Eigenvalues stay the same</li>
</ol>
<p>Similarity preserves eigenvalues.</p>
<div id="95d9b21d" data-execution_count="266">
<div><div id="cb458"><pre><code><span id="cb458-1"><a href="#cb458-1" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvalues of A:&#34;</span>, A.eigenvals())</span>
<span id="cb458-2"><a href="#cb458-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvalues of B:&#34;</span>, B.eigenvals())</span></code></pre></div></div>
<div>
<pre><code>Eigenvalues of A: {2: 2}
Eigenvalues of B: {2: 2}</code></pre>
</div>
</div>
<p>Both matrices have the same eigenvalues, even though their entries differ.</p>
<ol start="3" type="1">
<li>Similarity and diagonalization</li>
</ol>
<p>If a matrix is diagonalizable, there exists <span>\(P\)</span> such that</p>
<p><span>\[
D = P^{-1} A P
\]</span></p>
<p>where <span>\(D\)</span> is diagonal.</p>
<div id="d304255e" data-execution_count="267">
<div><div id="cb460"><pre><code><span id="cb460-1"><a href="#cb460-1" aria-hidden="true" tabindex="-1"></a>C <span>=</span> Matrix([</span>
<span id="cb460-2"><a href="#cb460-2" aria-hidden="true" tabindex="-1"></a>    [<span>4</span>,<span>1</span>],</span>
<span id="cb460-3"><a href="#cb460-3" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>2</span>]</span>
<span id="cb460-4"><a href="#cb460-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb460-5"><a href="#cb460-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb460-6"><a href="#cb460-6" aria-hidden="true" tabindex="-1"></a>P, D <span>=</span> C.diagonalize()</span>
<span id="cb460-7"><a href="#cb460-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Diagonal form D:</span><span>\n</span><span>&#34;</span>, D)</span>
<span id="cb460-8"><a href="#cb460-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Check similarity (P^-1 C P = D):</span><span>\n</span><span>&#34;</span>, P.inv()<span>*</span>C<span>*</span>P)</span></code></pre></div></div>
<div>
<pre><code>Diagonal form D:
 Matrix([[2, 0], [0, 4]])
Check similarity (P^-1 C P = D):
 Matrix([[2, 0], [0, 4]])</code></pre>
</div>
</div>
<p>Diagonalization is a special case of similarity, where the new matrix is as simple as possible.</p>
<ol start="4" type="1">
<li>NumPy version</li>
</ol>
<div id="01596e17" data-execution_count="268">
<div><div id="cb462"><pre><code><span id="cb462-1"><a href="#cb462-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>2</span>,<span>1</span>],[<span>0</span>,<span>2</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb462-2"><a href="#cb462-2" aria-hidden="true" tabindex="-1"></a>eigvals, eigvecs <span>=</span> np.linalg.eig(A)</span>
<span id="cb462-3"><a href="#cb462-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvalues:&#34;</span>, eigvals)</span>
<span id="cb462-4"><a href="#cb462-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvectors (basis P):</span><span>\n</span><span>&#34;</span>, eigvecs)</span></code></pre></div></div>
<div>
<pre><code>Eigenvalues: [2. 2.]
Eigenvectors (basis P):
 [[ 1.0000000e+00 -1.0000000e+00]
 [ 0.0000000e+00  4.4408921e-16]]</code></pre>
</div>
</div>
<p>Here, eigenvectors form the change-of-basis matrix <span>\(P\)</span>.</p>
<ol start="5" type="1">
<li>Geometric interpretation</li>
</ol>
<ul>
<li>Similar matrices = same transformation, different “ruler” (basis).</li>
<li>Diagonalization = finding a ruler that makes the transformation look like pure stretching along axes.</li>
</ul>
</section>
<section id="try-it-yourself-44">
<h4 data-anchor-id="try-it-yourself-44">Try It Yourself</h4>
<ol type="1">
<li><p>Take</p>
<p><span>\[
A = \begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{bmatrix}
\]</span></p>
<p>and find a matrix <span>\(P\)</span> that gives a similar <span>\(B\)</span>.</p></li>
<li><p>Show that two similar matrices have the same determinant and trace.</p></li>
<li><p>For a random 3×3 matrix, check if it is diagonalizable using SymPy’s <code>.diagonalize()</code> method.</p></li>
</ol>
</section>
<section id="the-takeaway-28">
<h4 data-anchor-id="the-takeaway-28">The Takeaway</h4>
<ul>
<li>Similarity = same linear map, different basis.</li>
<li>Similar matrices share eigenvalues, determinant, and trace.</li>
<li>Diagonalization is the simplest similarity form, making repeated computations (like powers) much easier.</li>
</ul>
</section>
</section>
<section id="projections-and-reflections-idempotent-and-involutive-maps">
<h3 data-anchor-id="projections-and-reflections-idempotent-and-involutive-maps">47. Projections and Reflections (Idempotent and Involutive Maps)</h3>
<p>Two very common geometric linear maps are projections and reflections. They show up in graphics, physics, and optimization.</p>
<ul>
<li>A projection squashes vectors onto a subspace (like dropping a shadow).</li>
<li>A reflection flips vectors across a line or plane (like a mirror).</li>
</ul>
<section id="set-up-your-lab-46">
<h4 data-anchor-id="set-up-your-lab-46">Set Up Your Lab</h4>
<div id="de26b86e" data-execution_count="269">
<div><div id="cb464"><pre><code><span id="cb464-1"><a href="#cb464-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb464-2"><a href="#cb464-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span>
<span id="cb464-3"><a href="#cb464-3" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-46">
<h4 data-anchor-id="step-by-step-code-walkthrough-46">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Projection onto a line</li>
</ol>
<p>If we want to project onto the line spanned by <span>\(u\)</span>, the projection matrix is:</p>
<p><span>\[
P = \frac{uu^T}{u^T u}
\]</span></p>
<div id="69dd8354" data-execution_count="270">
<div><div id="cb465"><pre><code><span id="cb465-1"><a href="#cb465-1" aria-hidden="true" tabindex="-1"></a>u <span>=</span> np.array([<span>2</span>,<span>1</span>], dtype<span>=</span><span>float</span>)</span>
<span id="cb465-2"><a href="#cb465-2" aria-hidden="true" tabindex="-1"></a>u <span>=</span> u <span>/</span> np.linalg.norm(u)   <span># normalize</span></span>
<span id="cb465-3"><a href="#cb465-3" aria-hidden="true" tabindex="-1"></a>P <span>=</span> np.outer(u,u)</span>
<span id="cb465-4"><a href="#cb465-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb465-5"><a href="#cb465-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Projection matrix:</span><span>\n</span><span>&#34;</span>, P)</span></code></pre></div></div>
<div>
<pre><code>Projection matrix:
 [[0.8 0.4]
 [0.4 0.2]]</code></pre>
</div>
</div>
<p>Apply projection:</p>
<div id="f46e93c1" data-execution_count="271">
<div><div id="cb467"><pre><code><span id="cb467-1"><a href="#cb467-1" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.array([<span>3</span>,<span>4</span>], dtype<span>=</span><span>float</span>)</span>
<span id="cb467-2"><a href="#cb467-2" aria-hidden="true" tabindex="-1"></a>proj_v <span>=</span> P <span>@</span> v</span>
<span id="cb467-3"><a href="#cb467-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Original v:&#34;</span>, v)</span>
<span id="cb467-4"><a href="#cb467-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Projection of v onto u:&#34;</span>, proj_v)</span></code></pre></div></div>
<div>
<pre><code>Original v: [3. 4.]
Projection of v onto u: [4. 2.]</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Visualization of projection</li>
</ol>
<div id="08fb08be" data-execution_count="272">
<div><div id="cb469"><pre><code><span id="cb469-1"><a href="#cb469-1" aria-hidden="true" tabindex="-1"></a>plt.arrow(<span>0</span>,<span>0</span>,v[<span>0</span>],v[<span>1</span>],head_width<span>=</span><span>0.1</span>,color<span>=</span><span>&#34;blue&#34;</span>,length_includes_head<span>=</span><span>True</span>)</span>
<span id="cb469-2"><a href="#cb469-2" aria-hidden="true" tabindex="-1"></a>plt.arrow(<span>0</span>,<span>0</span>,proj_v[<span>0</span>],proj_v[<span>1</span>],head_width<span>=</span><span>0.1</span>,color<span>=</span><span>&#34;red&#34;</span>,length_includes_head<span>=</span><span>True</span>)</span>
<span id="cb469-3"><a href="#cb469-3" aria-hidden="true" tabindex="-1"></a>plt.arrow(proj_v[<span>0</span>],proj_v[<span>1</span>],v[<span>0</span>]<span>-</span>proj_v[<span>0</span>],v[<span>1</span>]<span>-</span>proj_v[<span>1</span>],head_width<span>=</span><span>0.1</span>,color<span>=</span><span>&#34;gray&#34;</span>,linestyle<span>=</span><span>&#34;dashed&#34;</span>)</span>
<span id="cb469-4"><a href="#cb469-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb469-5"><a href="#cb469-5" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span>0</span>,color<span>=</span><span>&#39;black&#39;</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb469-6"><a href="#cb469-6" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span>0</span>,color<span>=</span><span>&#39;black&#39;</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb469-7"><a href="#cb469-7" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb469-8"><a href="#cb469-8" aria-hidden="true" tabindex="-1"></a>plt.title(<span>&#34;Blue = original, Red = projection, Gray = error vector&#34;</span>)</span>
<span id="cb469-9"><a href="#cb469-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-273-output-1.png" width="558" height="431"/></p>
</figure>
</div>
</div>
</div>
<p>The projection is the closest point on the line to the original vector.</p>
<ol start="3" type="1">
<li>Reflection across a line</li>
</ol>
<p>The reflection matrix across the line spanned by <span>\(u\)</span> is:</p>
<p><span>\[
R = 2P - I
\]</span></p>
<div id="14ec34e6" data-execution_count="273">
<div><div id="cb470"><pre><code><span id="cb470-1"><a href="#cb470-1" aria-hidden="true" tabindex="-1"></a>I <span>=</span> np.eye(<span>2</span>)</span>
<span id="cb470-2"><a href="#cb470-2" aria-hidden="true" tabindex="-1"></a>R <span>=</span> <span>2</span><span>*</span>P <span>-</span> I</span>
<span id="cb470-3"><a href="#cb470-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb470-4"><a href="#cb470-4" aria-hidden="true" tabindex="-1"></a>reflect_v <span>=</span> R <span>@</span> v</span>
<span id="cb470-5"><a href="#cb470-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Reflection of v across line u:&#34;</span>, reflect_v)</span></code></pre></div></div>
<div>
<pre><code>Reflection of v across line u: [ 5.0000000e+00 -4.4408921e-16]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Checking algebraic properties</li>
</ol>
<ul>
<li>Projection: <span>\(P^2 = P\)</span> (idempotent).</li>
<li>Reflection: <span>\(R^2 = I\)</span> (involutive).</li>
</ul>
<div id="a2d8560d" data-execution_count="274">
<div><div id="cb472"><pre><code><span id="cb472-1"><a href="#cb472-1" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;P^2 =</span><span>\n</span><span>&#34;</span>, P <span>@</span> P)</span>
<span id="cb472-2"><a href="#cb472-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;R^2 =</span><span>\n</span><span>&#34;</span>, R <span>@</span> R)</span></code></pre></div></div>
<div>
<pre><code>P^2 =
 [[0.8 0.4]
 [0.4 0.2]]
R^2 =
 [[ 1.00000000e+00 -1.59872116e-16]
 [-1.59872116e-16  1.00000000e+00]]</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Projection in higher dimensions</li>
</ol>
<p>Project onto the plane spanned by two vectors in <span>\(\mathbb{R}^3\)</span>.</p>
<div id="f8219988" data-execution_count="275">
<div><div id="cb474"><pre><code><span id="cb474-1"><a href="#cb474-1" aria-hidden="true" tabindex="-1"></a>u1 <span>=</span> np.array([<span>1</span>,<span>0</span>,<span>0</span>], dtype<span>=</span><span>float</span>)</span>
<span id="cb474-2"><a href="#cb474-2" aria-hidden="true" tabindex="-1"></a>u2 <span>=</span> np.array([<span>0</span>,<span>1</span>,<span>0</span>], dtype<span>=</span><span>float</span>)</span>
<span id="cb474-3"><a href="#cb474-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb474-4"><a href="#cb474-4" aria-hidden="true" tabindex="-1"></a>U <span>=</span> np.column_stack((u1,u2))   <span># basis for plane</span></span>
<span id="cb474-5"><a href="#cb474-5" aria-hidden="true" tabindex="-1"></a>P_plane <span>=</span> U <span>@</span> np.linalg.inv(U.T <span>@</span> U) <span>@</span> U.T</span>
<span id="cb474-6"><a href="#cb474-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb474-7"><a href="#cb474-7" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.array([<span>1</span>,<span>2</span>,<span>3</span>], dtype<span>=</span><span>float</span>)</span>
<span id="cb474-8"><a href="#cb474-8" aria-hidden="true" tabindex="-1"></a>proj_plane <span>=</span> P_plane <span>@</span> v</span>
<span id="cb474-9"><a href="#cb474-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Projection onto xy-plane:&#34;</span>, proj_plane)</span></code></pre></div></div>
<div>
<pre><code>Projection onto xy-plane: [1. 2. 0.]</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-45">
<h4 data-anchor-id="try-it-yourself-45">Try It Yourself</h4>
<ol type="1">
<li>Project <span>\([4,5]\)</span> onto the x-axis and verify the result.</li>
<li>Reflect <span>\([1,2]\)</span> across the line <span>\(y=x\)</span>.</li>
<li>Create a random 3D vector and project it onto the plane spanned by <span>\([1,1,0]\)</span> and <span>\([0,1,1]\)</span>.</li>
</ol>
</section>
<section id="the-takeaway-29">
<h4 data-anchor-id="the-takeaway-29">The Takeaway</h4>
<ul>
<li>Projection: idempotent (<span>\(P^2 = P\)</span>), finds the closest vector in a subspace.</li>
<li>Reflection: involutive (<span>\(R^2 = I\)</span>), flips across a line/plane but preserves lengths.</li>
<li>Both are simple but powerful examples of linear transformations with clear geometry.</li>
</ul>
</section>
</section>
<section id="rotations-and-shear-geometric-intuition">
<h3 data-anchor-id="rotations-and-shear-geometric-intuition">48. Rotations and Shear (Geometric Intuition)</h3>
<p>Two transformations often used in geometry, graphics, and physics are rotations and shears. Both are linear maps, but they behave differently:</p>
<ul>
<li>Rotation preserves lengths and angles.</li>
<li>Shear preserves area (in 2D) but distorts shapes, turning squares into parallelograms.</li>
</ul>
<section id="set-up-your-lab-47">
<h4 data-anchor-id="set-up-your-lab-47">Set Up Your Lab</h4>
<div id="8138157f" data-execution_count="276">
<div><div id="cb476"><pre><code><span id="cb476-1"><a href="#cb476-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb476-2"><a href="#cb476-2" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-47">
<h4 data-anchor-id="step-by-step-code-walkthrough-47">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Rotation in 2D</li>
</ol>
<p>The rotation matrix by angle <span>\(\theta\)</span> is:</p>
<p><span>\[
R(\theta) = \begin{bmatrix}
\cos\theta &amp; -\sin\theta \\
\sin\theta &amp; \cos\theta
\end{bmatrix}
\]</span></p>
<div id="ee1be257" data-execution_count="277">
<div><div id="cb477"><pre><code><span id="cb477-1"><a href="#cb477-1" aria-hidden="true" tabindex="-1"></a><span>def</span> rotation_matrix(theta):</span>
<span id="cb477-2"><a href="#cb477-2" aria-hidden="true" tabindex="-1"></a>    <span>return</span> np.array([</span>
<span id="cb477-3"><a href="#cb477-3" aria-hidden="true" tabindex="-1"></a>        [np.cos(theta), <span>-</span>np.sin(theta)],</span>
<span id="cb477-4"><a href="#cb477-4" aria-hidden="true" tabindex="-1"></a>        [np.sin(theta),  np.cos(theta)]</span>
<span id="cb477-5"><a href="#cb477-5" aria-hidden="true" tabindex="-1"></a>    ])</span>
<span id="cb477-6"><a href="#cb477-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb477-7"><a href="#cb477-7" aria-hidden="true" tabindex="-1"></a>theta <span>=</span> np.pi<span>/</span><span>4</span>   <span># 45 degrees</span></span>
<span id="cb477-8"><a href="#cb477-8" aria-hidden="true" tabindex="-1"></a>R <span>=</span> rotation_matrix(theta)</span>
<span id="cb477-9"><a href="#cb477-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb477-10"><a href="#cb477-10" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.array([<span>2</span>,<span>1</span>])</span>
<span id="cb477-11"><a href="#cb477-11" aria-hidden="true" tabindex="-1"></a>rotated_v <span>=</span> R <span>@</span> v</span>
<span id="cb477-12"><a href="#cb477-12" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Original v:&#34;</span>, v)</span>
<span id="cb477-13"><a href="#cb477-13" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rotated v (45°):&#34;</span>, rotated_v)</span></code></pre></div></div>
<div>
<pre><code>Original v: [2 1]
Rotated v (45°): [0.70710678 2.12132034]</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Visualizing rotation</li>
</ol>
<div id="d95c464c" data-execution_count="278">
<div><div id="cb479"><pre><code><span id="cb479-1"><a href="#cb479-1" aria-hidden="true" tabindex="-1"></a>plt.arrow(<span>0</span>,<span>0</span>,v[<span>0</span>],v[<span>1</span>],head_width<span>=</span><span>0.1</span>,color<span>=</span><span>&#34;blue&#34;</span>,length_includes_head<span>=</span><span>True</span>)</span>
<span id="cb479-2"><a href="#cb479-2" aria-hidden="true" tabindex="-1"></a>plt.arrow(<span>0</span>,<span>0</span>,rotated_v[<span>0</span>],rotated_v[<span>1</span>],head_width<span>=</span><span>0.1</span>,color<span>=</span><span>&#34;red&#34;</span>,length_includes_head<span>=</span><span>True</span>)</span>
<span id="cb479-3"><a href="#cb479-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb479-4"><a href="#cb479-4" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span>0</span>,color<span>=</span><span>&#39;black&#39;</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb479-5"><a href="#cb479-5" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span>0</span>,color<span>=</span><span>&#39;black&#39;</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb479-6"><a href="#cb479-6" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb479-7"><a href="#cb479-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span>&#34;Blue = original, Red = rotated (45°)&#34;</span>)</span>
<span id="cb479-8"><a href="#cb479-8" aria-hidden="true" tabindex="-1"></a>plt.axis(<span>&#34;equal&#34;</span>)</span>
<span id="cb479-9"><a href="#cb479-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-279-output-1.png" width="571" height="431"/></p>
</figure>
</div>
</div>
</div>
<p>The vector rotates counterclockwise by 45°.</p>
<ol start="3" type="1">
<li>Shear in 2D</li>
</ol>
<p>A shear along the x-axis by factor <span>\(k\)</span>:</p>
<p><span>\[
S = \begin{bmatrix}
1 &amp; k \\
0 &amp; 1
\end{bmatrix}
\]</span></p>
<div id="6634c5cb" data-execution_count="279">
<div><div id="cb480"><pre><code><span id="cb480-1"><a href="#cb480-1" aria-hidden="true" tabindex="-1"></a>k <span>=</span> <span>1.0</span></span>
<span id="cb480-2"><a href="#cb480-2" aria-hidden="true" tabindex="-1"></a>S <span>=</span> np.array([</span>
<span id="cb480-3"><a href="#cb480-3" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,k],</span>
<span id="cb480-4"><a href="#cb480-4" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>1</span>]</span>
<span id="cb480-5"><a href="#cb480-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb480-6"><a href="#cb480-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb480-7"><a href="#cb480-7" aria-hidden="true" tabindex="-1"></a>sheared_v <span>=</span> S <span>@</span> v</span>
<span id="cb480-8"><a href="#cb480-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Sheared v:&#34;</span>, sheared_v)</span></code></pre></div></div>

</div>
<ol start="4" type="1">
<li>Visualizing shear</li>
</ol>
<div id="4f31d543" data-execution_count="280">
<div><div id="cb482"><pre><code><span id="cb482-1"><a href="#cb482-1" aria-hidden="true" tabindex="-1"></a>plt.arrow(<span>0</span>,<span>0</span>,v[<span>0</span>],v[<span>1</span>],head_width<span>=</span><span>0.1</span>,color<span>=</span><span>&#34;blue&#34;</span>,length_includes_head<span>=</span><span>True</span>)</span>
<span id="cb482-2"><a href="#cb482-2" aria-hidden="true" tabindex="-1"></a>plt.arrow(<span>0</span>,<span>0</span>,sheared_v[<span>0</span>],sheared_v[<span>1</span>],head_width<span>=</span><span>0.1</span>,color<span>=</span><span>&#34;green&#34;</span>,length_includes_head<span>=</span><span>True</span>)</span>
<span id="cb482-3"><a href="#cb482-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb482-4"><a href="#cb482-4" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span>0</span>,color<span>=</span><span>&#39;black&#39;</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb482-5"><a href="#cb482-5" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span>0</span>,color<span>=</span><span>&#39;black&#39;</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb482-6"><a href="#cb482-6" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb482-7"><a href="#cb482-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span>&#34;Blue = original, Green = sheared&#34;</span>)</span>
<span id="cb482-8"><a href="#cb482-8" aria-hidden="true" tabindex="-1"></a>plt.axis(<span>&#34;equal&#34;</span>)</span>
<span id="cb482-9"><a href="#cb482-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-281-output-1.png" width="582" height="431"/></p>
</figure>
</div>
</div>
</div>
<p>The shear moves the vector sideways, distorting its angle.</p>
<ol start="5" type="1">
<li>Properties check</li>
</ol>
<ul>
<li>Rotation preserves length:</li>
</ul>
<div id="60690774" data-execution_count="281">
<div><div id="cb483"><pre><code><span id="cb483-1"><a href="#cb483-1" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;||v|| =&#34;</span>, np.linalg.norm(v))</span>
<span id="cb483-2"><a href="#cb483-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;||R v|| =&#34;</span>, np.linalg.norm(rotated_v))</span></code></pre></div></div>
<div>
<pre><code>||v|| = 2.23606797749979
||R v|| = 2.2360679774997894</code></pre>
</div>
</div>
<ul>
<li>Shear preserves area (determinant = 1):</li>
</ul>
<div id="5e097c88" data-execution_count="282">
<div><div id="cb485"><pre><code><span id="cb485-1"><a href="#cb485-1" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(S) =&#34;</span>, np.linalg.det(S))</span></code></pre></div></div>

</div>
</section>
<section id="try-it-yourself-46">
<h4 data-anchor-id="try-it-yourself-46">Try It Yourself</h4>
<ol type="1">
<li>Rotate <span>\([1,0]\)</span> by 90° and check it becomes <span>\([0,1]\)</span>.</li>
<li>Apply shear with <span>\(k=2\)</span> to a square (points <span>\((0,0),(1,0),(1,1),(0,1)\)</span>) and plot before/after.</li>
<li>Combine rotation and shear: apply shear first, then rotation. What happens?</li>
</ol>
</section>
<section id="the-takeaway-30">
<h4 data-anchor-id="the-takeaway-30">The Takeaway</h4>
<ul>
<li>Rotation: length- and angle-preserving, determinant = 1.</li>
<li>Shear: shape-distorting but area-preserving, determinant = 1.</li>
<li>Both are linear maps that provide geometric intuition and real-world modeling tools.</li>
</ul>
</section>
</section>
<section id="rank-and-operator-viewpoint-rank-beyond-elimination">
<h3 data-anchor-id="rank-and-operator-viewpoint-rank-beyond-elimination">49. Rank and Operator Viewpoint (Rank Beyond Elimination)</h3>
<p>The rank of a matrix tells us how much “information” a linear map carries. Algebraically, it is the dimension of the image (column space). Geometrically, it measures how many independent directions survive the transformation.</p>
<p>From the operator viewpoint:</p>
<ul>
<li>A matrix <span>\(A\)</span> is not just a table of numbers - it is a linear operator that maps vectors to other vectors.</li>
<li>The rank is the dimension of the output space that <span>\(A\)</span> actually reaches.</li>
</ul>
<section id="set-up-your-lab-48">
<h4 data-anchor-id="set-up-your-lab-48">Set Up Your Lab</h4>
<div id="c5e36221" data-execution_count="283">
<div><div id="cb487"><pre><code><span id="cb487-1"><a href="#cb487-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb487-2"><a href="#cb487-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-48">
<h4 data-anchor-id="step-by-step-code-walkthrough-48">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Rank via elimination (SymPy)</li>
</ol>
<div id="61affbdc" data-execution_count="284">
<div><div id="cb488"><pre><code><span id="cb488-1"><a href="#cb488-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> Matrix([</span>
<span id="cb488-2"><a href="#cb488-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>2</span>,<span>3</span>],</span>
<span id="cb488-3"><a href="#cb488-3" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>,<span>4</span>,<span>6</span>],</span>
<span id="cb488-4"><a href="#cb488-4" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>1</span>,<span>1</span>]</span>
<span id="cb488-5"><a href="#cb488-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb488-6"><a href="#cb488-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb488-7"><a href="#cb488-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Matrix A:</span><span>\n</span><span>&#34;</span>, A)</span>
<span id="cb488-8"><a href="#cb488-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rank of A:&#34;</span>, A.rank())</span></code></pre></div></div>
<div>
<pre><code>Matrix A:
 Matrix([[1, 2, 3], [2, 4, 6], [1, 1, 1]])
Rank of A: 2</code></pre>
</div>
</div>
<p>Here, the second row is a multiple of the first → less independence → rank &lt; 3.</p>
<ol start="2" type="1">
<li>Rank via NumPy</li>
</ol>
<div id="7e538165" data-execution_count="285">
<div><div id="cb490"><pre><code><span id="cb490-1"><a href="#cb490-1" aria-hidden="true" tabindex="-1"></a>A_np <span>=</span> np.array([[<span>1</span>,<span>2</span>,<span>3</span>],[<span>2</span>,<span>4</span>,<span>6</span>],[<span>1</span>,<span>1</span>,<span>1</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb490-2"><a href="#cb490-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rank (NumPy):&#34;</span>, np.linalg.matrix_rank(A_np))</span></code></pre></div></div>

</div>
<ol start="3" type="1">
<li>Operator viewpoint</li>
</ol>
<p>Let’s apply <span>\(A\)</span> to random vectors:</p>
<div id="8e7fa128" data-execution_count="286">
<div><div id="cb492"><pre><code><span id="cb492-1"><a href="#cb492-1" aria-hidden="true" tabindex="-1"></a><span>for</span> v <span>in</span> [np.array([<span>1</span>,<span>0</span>,<span>0</span>]), np.array([<span>0</span>,<span>1</span>,<span>0</span>]), np.array([<span>0</span>,<span>0</span>,<span>1</span>])]:</span>
<span id="cb492-2"><a href="#cb492-2" aria-hidden="true" tabindex="-1"></a>    <span>print</span>(<span>&#34;A @&#34;</span>, v, <span>&#34;=&#34;</span>, A_np <span>@</span> v)</span></code></pre></div></div>
<div>
<pre><code>A @ [1 0 0] = [1. 2. 1.]
A @ [0 1 0] = [2. 4. 1.]
A @ [0 0 1] = [3. 6. 1.]</code></pre>
</div>
</div>
<p>Even though we started in 3D, all outputs lie in a plane in <span>\(\mathbb{R}^3\)</span>. That’s why rank = 2.</p>
<ol start="4" type="1">
<li>Full rank vs reduced rank</li>
</ol>
<ul>
<li>Full rank: the transformation preserves dimension (no collapse).</li>
<li>Reduced rank: the transformation collapses onto a lower-dimensional subspace.</li>
</ul>
<p>Example full-rank:</p>
<div id="fbf0fc2f" data-execution_count="287">
<div><div id="cb494"><pre><code><span id="cb494-1"><a href="#cb494-1" aria-hidden="true" tabindex="-1"></a>B <span>=</span> Matrix([</span>
<span id="cb494-2"><a href="#cb494-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>0</span>,<span>0</span>],</span>
<span id="cb494-3"><a href="#cb494-3" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>1</span>,<span>0</span>],</span>
<span id="cb494-4"><a href="#cb494-4" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>0</span>,<span>1</span>]</span>
<span id="cb494-5"><a href="#cb494-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb494-6"><a href="#cb494-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb494-7"><a href="#cb494-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rank of B:&#34;</span>, B.rank())</span></code></pre></div></div>

</div>
<ol start="5" type="1">
<li>Connection to nullity</li>
</ol>
<p>The rank-nullity theorem:</p>
<p><span>\[
\text{rank}(A) + \text{nullity}(A) = \text{number of columns of } A
\]</span></p>
<p>Check with SymPy:</p>
<div id="9f45440e" data-execution_count="288">
<div><div id="cb496"><pre><code><span id="cb496-1"><a href="#cb496-1" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Null space (basis):&#34;</span>, A.nullspace())</span>
<span id="cb496-2"><a href="#cb496-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Nullity:&#34;</span>, <span>len</span>(A.nullspace()))</span>
<span id="cb496-3"><a href="#cb496-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rank + Nullity =&#34;</span>, A.rank() <span>+</span> <span>len</span>(A.nullspace()))</span></code></pre></div></div>
<div>
<pre><code>Null space (basis): [Matrix([
[ 1],
[-2],
[ 1]])]
Nullity: 1
Rank + Nullity = 3</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-47">
<h4 data-anchor-id="try-it-yourself-47">Try It Yourself</h4>
<ol type="1">
<li><p>Take</p>
<p><span>\[
\begin{bmatrix} 1 &amp; 1 \\ 1 &amp; 1 \end{bmatrix}
\]</span></p>
<p>and compute its rank. Why is it 1?</p></li>
<li><p>For a random 4×4 matrix, use <code>np.linalg.matrix_rank</code> to check if it’s invertible.</p></li>
<li><p>Verify rank-nullity theorem for a 3×5 random integer matrix.</p></li>
</ol>
</section>
<section id="the-takeaway-31">
<h4 data-anchor-id="the-takeaway-31">The Takeaway</h4>
<ul>
<li>Rank = dimension of the image (how many independent outputs a transformation has).</li>
<li>Operator viewpoint: rank shows how much of the input space survives after transformation.</li>
<li>Rank-nullity links the image and kernel - together they fully describe a linear operator.</li>
</ul>
</section>
</section>
<section id="block-matrices-and-block-maps-divide-and-conquer-structure">
<h3 data-anchor-id="block-matrices-and-block-maps-divide-and-conquer-structure">50. Block Matrices and Block Maps (Divide and Conquer Structure)</h3>
<p>Sometimes matrices can be arranged in blocks (submatrices). Treating a big matrix as smaller pieces helps simplify calculations, especially in systems with structure (networks, coupled equations, or partitioned variables).</p>
<section id="set-up-your-lab-49">
<h4 data-anchor-id="set-up-your-lab-49">Set Up Your Lab</h4>
<div id="b62e27a6" data-execution_count="289">
<div><div id="cb498"><pre><code><span id="cb498-1"><a href="#cb498-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb498-2"><a href="#cb498-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-49">
<h4 data-anchor-id="step-by-step-code-walkthrough-49">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Constructing block matrices</li>
</ol>
<p>We can build a block matrix from smaller pieces:</p>
<div id="4044e4c4" data-execution_count="290">
<div><div id="cb499"><pre><code><span id="cb499-1"><a href="#cb499-1" aria-hidden="true" tabindex="-1"></a>A11 <span>=</span> Matrix([[<span>1</span>,<span>2</span>],[<span>3</span>,<span>4</span>]])</span>
<span id="cb499-2"><a href="#cb499-2" aria-hidden="true" tabindex="-1"></a>A12 <span>=</span> Matrix([[<span>5</span>,<span>6</span>],[<span>7</span>,<span>8</span>]])</span>
<span id="cb499-3"><a href="#cb499-3" aria-hidden="true" tabindex="-1"></a>A21 <span>=</span> Matrix([[<span>9</span>,<span>10</span>]])</span>
<span id="cb499-4"><a href="#cb499-4" aria-hidden="true" tabindex="-1"></a>A22 <span>=</span> Matrix([[<span>11</span>,<span>12</span>]])</span>
<span id="cb499-5"><a href="#cb499-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb499-6"><a href="#cb499-6" aria-hidden="true" tabindex="-1"></a><span># Combine into a block matrix</span></span>
<span id="cb499-7"><a href="#cb499-7" aria-hidden="true" tabindex="-1"></a>A <span>=</span> Matrix.vstack(</span>
<span id="cb499-8"><a href="#cb499-8" aria-hidden="true" tabindex="-1"></a>    Matrix.hstack(A11, A12),</span>
<span id="cb499-9"><a href="#cb499-9" aria-hidden="true" tabindex="-1"></a>    Matrix.hstack(A21, A22)</span>
<span id="cb499-10"><a href="#cb499-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb499-11"><a href="#cb499-11" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Block matrix A:</span><span>\n</span><span>&#34;</span>, A)</span></code></pre></div></div>
<div>
<pre><code>Block matrix A:
 Matrix([[1, 2, 5, 6], [3, 4, 7, 8], [9, 10, 11, 12]])</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Block multiplication</li>
</ol>
<p>If a matrix is partitioned into blocks, multiplication follows block rules:</p>
<p><span>\[
\begin{bmatrix} A &amp; B \\ C &amp; D \end{bmatrix}
\begin{bmatrix} x \\ y \end{bmatrix}
= \begin{bmatrix} Ax + By \\ Cx + Dy \end{bmatrix}
\]</span></p>
<p>Example:</p>
<div id="42fccb67" data-execution_count="291">
<div><div id="cb501"><pre><code><span id="cb501-1"><a href="#cb501-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> Matrix([</span>
<span id="cb501-2"><a href="#cb501-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>2</span>,<span>5</span>,<span>6</span>],</span>
<span id="cb501-3"><a href="#cb501-3" aria-hidden="true" tabindex="-1"></a>    [<span>3</span>,<span>4</span>,<span>7</span>,<span>8</span>],</span>
<span id="cb501-4"><a href="#cb501-4" aria-hidden="true" tabindex="-1"></a>    [<span>9</span>,<span>10</span>,<span>11</span>,<span>12</span>]</span>
<span id="cb501-5"><a href="#cb501-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb501-6"><a href="#cb501-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb501-7"><a href="#cb501-7" aria-hidden="true" tabindex="-1"></a>x <span>=</span> Matrix([<span>1</span>,<span>1</span>,<span>2</span>,<span>2</span>])</span>
<span id="cb501-8"><a href="#cb501-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;A * x =&#34;</span>, A<span>*</span>x)</span></code></pre></div></div>
<div>
<pre><code>A * x = Matrix([[25], [37], [65]])</code></pre>
</div>
</div>
<p>Here the vector is split into blocks <span>\([x,y]\)</span>.</p>
<ol start="3" type="1">
<li>Block diagonal matrices</li>
</ol>
<p>Block diagonal = independent subproblems:</p>
<div id="f8cd3d71" data-execution_count="292">
<div><div id="cb503"><pre><code><span id="cb503-1"><a href="#cb503-1" aria-hidden="true" tabindex="-1"></a>B1 <span>=</span> Matrix([[<span>2</span>,<span>0</span>],[<span>0</span>,<span>2</span>]])</span>
<span id="cb503-2"><a href="#cb503-2" aria-hidden="true" tabindex="-1"></a>B2 <span>=</span> Matrix([[<span>3</span>,<span>1</span>],[<span>0</span>,<span>3</span>]])</span>
<span id="cb503-3"><a href="#cb503-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb503-4"><a href="#cb503-4" aria-hidden="true" tabindex="-1"></a>BlockDiag <span>=</span> Matrix([</span>
<span id="cb503-5"><a href="#cb503-5" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>,<span>0</span>,<span>0</span>,<span>0</span>],</span>
<span id="cb503-6"><a href="#cb503-6" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>2</span>,<span>0</span>,<span>0</span>],</span>
<span id="cb503-7"><a href="#cb503-7" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>0</span>,<span>3</span>,<span>1</span>],</span>
<span id="cb503-8"><a href="#cb503-8" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>0</span>,<span>0</span>,<span>3</span>]</span>
<span id="cb503-9"><a href="#cb503-9" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb503-10"><a href="#cb503-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb503-11"><a href="#cb503-11" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Block diagonal matrix:</span><span>\n</span><span>&#34;</span>, BlockDiag)</span></code></pre></div></div>
<div>
<pre><code>Block diagonal matrix:
 Matrix([[2, 0, 0, 0], [0, 2, 0, 0], [0, 0, 3, 1], [0, 0, 0, 3]])</code></pre>
</div>
</div>
<p>Applying this matrix acts separately on each block - like running two smaller transformations in parallel.</p>
<ol start="4" type="1">
<li>Inverse of block diagonal</li>
</ol>
<p>The inverse of a block diagonal is just the block diagonal of inverses:</p>
<div id="237dc21d" data-execution_count="293">
<div><div id="cb505"><pre><code><span id="cb505-1"><a href="#cb505-1" aria-hidden="true" tabindex="-1"></a>B1_inv <span>=</span> B1.inv()</span>
<span id="cb505-2"><a href="#cb505-2" aria-hidden="true" tabindex="-1"></a>B2_inv <span>=</span> B2.inv()</span>
<span id="cb505-3"><a href="#cb505-3" aria-hidden="true" tabindex="-1"></a>BlockDiagInv <span>=</span> Matrix([</span>
<span id="cb505-4"><a href="#cb505-4" aria-hidden="true" tabindex="-1"></a>    [B1_inv[<span>0</span>,<span>0</span>],<span>0</span>,<span>0</span>,<span>0</span>],</span>
<span id="cb505-5"><a href="#cb505-5" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,B1_inv[<span>1</span>,<span>1</span>],<span>0</span>,<span>0</span>],</span>
<span id="cb505-6"><a href="#cb505-6" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>0</span>,B2_inv[<span>0</span>,<span>0</span>],B2_inv[<span>0</span>,<span>1</span>]],</span>
<span id="cb505-7"><a href="#cb505-7" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>0</span>,B2_inv[<span>1</span>,<span>0</span>],B2_inv[<span>1</span>,<span>1</span>]]</span>
<span id="cb505-8"><a href="#cb505-8" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb505-9"><a href="#cb505-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Inverse block diag:</span><span>\n</span><span>&#34;</span>, BlockDiagInv)</span></code></pre></div></div>
<div>
<pre><code>Inverse block diag:
 Matrix([[1/2, 0, 0, 0], [0, 1/2, 0, 0], [0, 0, 1/3, -1/9], [0, 0, 0, 1/3]])</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Practical example - coupled equations</li>
</ol>
<p>Suppose we have two independent systems:</p>
<ul>
<li>System 1: <span>\(Ax = b\)</span></li>
<li>System 2: <span>\(Cy = d\)</span></li>
</ul>
<p>We can represent both together:</p>
<p><span>\[
\begin{bmatrix} A &amp; 0 \\ 0 &amp; C \end{bmatrix}
\begin{bmatrix} x \\ y \end{bmatrix}
= \begin{bmatrix} b \\ d \end{bmatrix}
\]</span></p>
<p>This shows how block matrices organize multiple systems in one big equation.</p>
</section>
<section id="try-it-yourself-48">
<h4 data-anchor-id="try-it-yourself-48">Try It Yourself</h4>
<ol type="1">
<li>Build a block diagonal matrix with three 2×2 blocks. Apply it to a vector.</li>
<li>Verify block multiplication rule by manually computing <span>\(Ax + By\)</span> and <span>\(Cx + Dy\)</span>.</li>
<li>Write two small systems of equations and combine them into one block system.</li>
</ol>
</section>
<section id="the-takeaway-32">
<h4 data-anchor-id="the-takeaway-32">The Takeaway</h4>
<ul>
<li>Block matrices let us break down big systems into smaller parts.</li>
<li>Block diagonal matrices = independent subsystems.</li>
<li>Thinking in blocks simplifies algebra, programming, and numerical computation.</li>
</ul>
</section>
</section>
</section>
<section id="chapter-6.-determinants-and-volume">
<h2 data-anchor-id="chapter-6.-determinants-and-volume">Chapter 6. Determinants and volume</h2>
<section id="areas-volumes-and-signed-scale-factors-geometric-entry-point">
<h3 data-anchor-id="areas-volumes-and-signed-scale-factors-geometric-entry-point">51. Areas, Volumes, and Signed Scale Factors (Geometric Entry Point)</h3>
<p>The determinant of a matrix has a deep geometric meaning: it tells us how a linear transformation scales area (in 2D), volume (in 3D), or higher-dimensional content. It can also flip orientation (sign).</p>
<section id="set-up-your-lab-50">
<h4 data-anchor-id="set-up-your-lab-50">Set Up Your Lab</h4>
<div id="71c5e507" data-execution_count="294">
<div><div id="cb507"><pre><code><span id="cb507-1"><a href="#cb507-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb507-2"><a href="#cb507-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-50">
<h4 data-anchor-id="step-by-step-code-walkthrough-50">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Determinant in 2D (area scaling)</li>
</ol>
<p>Let’s take a matrix that stretches and shears:</p>
<div id="2cea41a3" data-execution_count="295">
<div><div id="cb508"><pre><code><span id="cb508-1"><a href="#cb508-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> Matrix([</span>
<span id="cb508-2"><a href="#cb508-2" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>,<span>1</span>],</span>
<span id="cb508-3"><a href="#cb508-3" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>1</span>]</span>
<span id="cb508-4"><a href="#cb508-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb508-5"><a href="#cb508-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb508-6"><a href="#cb508-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Determinant:&#34;</span>, A.det())</span></code></pre></div></div>

</div>
<p>The determinant = 1 → areas are preserved, even though the shape is distorted.</p>
<ol start="2" type="1">
<li>Unit square under transformation</li>
</ol>
<p>Transform the square with corners <span>\((0,0),(1,0),(1,1),(0,1)\)</span>:</p>
<div id="066c6553" data-execution_count="296">
<div><div id="cb510"><pre><code><span id="cb510-1"><a href="#cb510-1" aria-hidden="true" tabindex="-1"></a>square <span>=</span> Matrix([</span>
<span id="cb510-2"><a href="#cb510-2" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>0</span>],</span>
<span id="cb510-3"><a href="#cb510-3" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>0</span>],</span>
<span id="cb510-4"><a href="#cb510-4" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>1</span>],</span>
<span id="cb510-5"><a href="#cb510-5" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>1</span>]</span>
<span id="cb510-6"><a href="#cb510-6" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb510-7"><a href="#cb510-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb510-8"><a href="#cb510-8" aria-hidden="true" tabindex="-1"></a>transformed <span>=</span> (A <span>*</span> square.T).T</span>
<span id="cb510-9"><a href="#cb510-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Original square:</span><span>\n</span><span>&#34;</span>, square)</span>
<span id="cb510-10"><a href="#cb510-10" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Transformed square:</span><span>\n</span><span>&#34;</span>, transformed)</span></code></pre></div></div>
<div>
<pre><code>Original square:
 Matrix([[0, 0], [1, 0], [1, 1], [0, 1]])
Transformed square:
 Matrix([[0, 0], [2, 1], [3, 2], [1, 1]])</code></pre>
</div>
</div>
<p>The area of the transformed shape equals <span>\(|\det(A)|\)</span>.</p>
<ol start="3" type="1">
<li>Determinant in 3D (volume scaling)</li>
</ol>
<div id="fb061fe8" data-execution_count="297">
<div><div id="cb512"><pre><code><span id="cb512-1"><a href="#cb512-1" aria-hidden="true" tabindex="-1"></a>B <span>=</span> Matrix([</span>
<span id="cb512-2"><a href="#cb512-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>2</span>,<span>0</span>],</span>
<span id="cb512-3"><a href="#cb512-3" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>1</span>,<span>0</span>],</span>
<span id="cb512-4"><a href="#cb512-4" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>0</span>,<span>3</span>]</span>
<span id="cb512-5"><a href="#cb512-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb512-6"><a href="#cb512-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb512-7"><a href="#cb512-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Determinant:&#34;</span>, B.det())</span></code></pre></div></div>

</div>
<p><span>\(\det(B)=3\)</span> means that volumes are scaled by 3.</p>
<ol start="4" type="1">
<li>Negative determinant = orientation flip</li>
</ol>
<div id="353c8f1d" data-execution_count="298">
<div><div id="cb514"><pre><code><span id="cb514-1"><a href="#cb514-1" aria-hidden="true" tabindex="-1"></a>C <span>=</span> Matrix([</span>
<span id="cb514-2"><a href="#cb514-2" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>1</span>],</span>
<span id="cb514-3"><a href="#cb514-3" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>0</span>]</span>
<span id="cb514-4"><a href="#cb514-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb514-5"><a href="#cb514-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb514-6"><a href="#cb514-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Determinant:&#34;</span>, C.det())</span></code></pre></div></div>

</div>
<p>The determinant = -1 → area preserved but orientation flipped (like a mirror reflection).</p>
<ol start="5" type="1">
<li>NumPy version</li>
</ol>
<div id="bacd8dee" data-execution_count="299">
<div><div id="cb516"><pre><code><span id="cb516-1"><a href="#cb516-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>2</span>,<span>1</span>],[<span>1</span>,<span>1</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb516-2"><a href="#cb516-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Det (NumPy):&#34;</span>, np.linalg.det(A))</span></code></pre></div></div>

</div>
</section>
<section id="try-it-yourself-49">
<h4 data-anchor-id="try-it-yourself-49">Try It Yourself</h4>
<ol type="1">
<li><p>Take</p>
<p><span>\[
\begin{bmatrix} 3 &amp; 0 \\ 0 &amp; 2 \end{bmatrix}
\]</span></p>
<p>and compute the determinant. Verify it scales areas by 6.</p></li>
<li><p>Build a 3×3 shear matrix and check how it affects volume.</p></li>
<li><p>Test a reflection matrix and confirm that the determinant is negative.</p></li>
</ol>
</section>
<section id="the-takeaway-33">
<h4 data-anchor-id="the-takeaway-33">The Takeaway</h4>
<ul>
<li>Determinant measures how a linear map scales area, volume, or hypervolume.</li>
<li>Positive determinant = preserves orientation; negative = flips it.</li>
<li>Magnitude of determinant = scaling factor of geometric content.</li>
</ul>
</section>
</section>
<section id="determinant-via-linear-rules-multilinearity-sign-normalization">
<h3 data-anchor-id="determinant-via-linear-rules-multilinearity-sign-normalization">52. Determinant via Linear Rules (Multilinearity, Sign, Normalization)</h3>
<p>The determinant isn’t just a formula; it’s defined by three elegant rules that make it unique. These rules capture its geometric meaning as a volume-scaling factor.</p>
<ol type="1">
<li>Multilinearity: Linear in each row (or column).</li>
<li>Sign Change: Swapping two rows flips the sign.</li>
<li>Normalization: The determinant of the identity matrix is 1.</li>
</ol>
<section id="set-up-your-lab-51">
<h4 data-anchor-id="set-up-your-lab-51">Set Up Your Lab</h4>
<div id="1ff33309" data-execution_count="300">
<div><div id="cb518"><pre><code><span id="cb518-1"><a href="#cb518-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb518-2"><a href="#cb518-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-51">
<h4 data-anchor-id="step-by-step-code-walkthrough-51">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Multilinearity</li>
</ol>
<p>If one row is scaled, the determinant scales the same way.</p>
<div id="cdb0ef6d" data-execution_count="301">
<div><div id="cb519"><pre><code><span id="cb519-1"><a href="#cb519-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> Matrix([[<span>1</span>,<span>2</span>],[<span>3</span>,<span>4</span>]])</span>
<span id="cb519-2"><a href="#cb519-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(A):&#34;</span>, A.det())</span>
<span id="cb519-3"><a href="#cb519-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb519-4"><a href="#cb519-4" aria-hidden="true" tabindex="-1"></a>B <span>=</span> Matrix([[<span>2</span>,<span>4</span>],[<span>3</span>,<span>4</span>]])  <span># first row doubled</span></span>
<span id="cb519-5"><a href="#cb519-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(B):&#34;</span>, B.det())</span></code></pre></div></div>

</div>
<p>You’ll see <code>det(B) = 2 * det(A)</code>.</p>
<ol start="2" type="1">
<li>Sign change by row swap</li>
</ol>
<div id="7cec5d4c" data-execution_count="302">
<div><div id="cb521"><pre><code><span id="cb521-1"><a href="#cb521-1" aria-hidden="true" tabindex="-1"></a>C <span>=</span> Matrix([[<span>1</span>,<span>2</span>],[<span>3</span>,<span>4</span>]])</span>
<span id="cb521-2"><a href="#cb521-2" aria-hidden="true" tabindex="-1"></a>C_swapped <span>=</span> Matrix([[<span>3</span>,<span>4</span>],[<span>1</span>,<span>2</span>]])</span>
<span id="cb521-3"><a href="#cb521-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb521-4"><a href="#cb521-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(C):&#34;</span>, C.det())</span>
<span id="cb521-5"><a href="#cb521-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(C_swapped):&#34;</span>, C_swapped.det())</span></code></pre></div></div>
<div>
<pre><code>det(C): -2
det(C_swapped): 2</code></pre>
</div>
</div>
<p>Swapping rows flips the sign of the determinant.</p>
<ol start="3" type="1">
<li>Normalization rule</li>
</ol>
<div id="b17d1fc0" data-execution_count="303">
<div><div id="cb523"><pre><code><span id="cb523-1"><a href="#cb523-1" aria-hidden="true" tabindex="-1"></a>I <span>=</span> Matrix.eye(<span>3</span>)</span>
<span id="cb523-2"><a href="#cb523-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(I):&#34;</span>, I.det())</span></code></pre></div></div>

</div>
<p>The determinant of the identity is always 1 - this fixes the scaling baseline.</p>
<ol start="4" type="1">
<li>Combining rules (example in 3×3)</li>
</ol>
<div id="91d1d320" data-execution_count="304">
<div><div id="cb525"><pre><code><span id="cb525-1"><a href="#cb525-1" aria-hidden="true" tabindex="-1"></a>M <span>=</span> Matrix([[<span>1</span>,<span>2</span>,<span>3</span>],[<span>4</span>,<span>5</span>,<span>6</span>],[<span>7</span>,<span>8</span>,<span>9</span>]])</span>
<span id="cb525-2"><a href="#cb525-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(M):&#34;</span>, M.det())</span></code></pre></div></div>

</div>
<p>Here, rows are linearly dependent, so the determinant is 0 - consistent with multilinearity (since one row can be written as a combo of others).</p>
<ol start="5" type="1">
<li>NumPy check</li>
</ol>
<div id="8d75fc67" data-execution_count="305">
<div><div id="cb527"><pre><code><span id="cb527-1"><a href="#cb527-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>1</span>,<span>2</span>],[<span>3</span>,<span>4</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb527-2"><a href="#cb527-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(A) NumPy:&#34;</span>, np.linalg.det(A))</span></code></pre></div></div>
<div>
<pre><code>det(A) NumPy: -2.0000000000000004</code></pre>
</div>
</div>
<p>Both SymPy and NumPy confirm the same result.</p>
</section>
<section id="try-it-yourself-50">
<h4 data-anchor-id="try-it-yourself-50">Try It Yourself</h4>
<ol type="1">
<li>Scale a row of a 3×3 matrix by 3. Confirm the determinant scales by 3.</li>
<li>Swap two rows twice in a row - does the determinant return to its original value?</li>
<li>Compute determinant of a triangular matrix. What pattern do you see?</li>
</ol>
</section>
<section id="the-takeaway-34">
<h4 data-anchor-id="the-takeaway-34">The Takeaway</h4>
<ul>
<li>Determinant is defined by multilinearity, sign change, and normalization.</li>
<li>These rules uniquely pin down the determinant’s behavior.</li>
<li>Every formula (cofactor expansion, row-reduction method, etc.) comes from these core principles.</li>
</ul>
</section>
</section>
<section id="determinant-and-row-operations-how-each-move-changes-det">
<h3 data-anchor-id="determinant-and-row-operations-how-each-move-changes-det">53. Determinant and Row Operations (How Each Move Changes det)</h3>
<p>Row operations are at the heart of Gaussian elimination, and the determinant has simple, predictable reactions to them. Understanding these reactions gives both computational shortcuts and geometric intuition.</p>
<section id="the-three-key-rules">
<h4 data-anchor-id="the-three-key-rules">The Three Key Rules</h4>
<ol type="1">
<li>Row swap: Swapping two rows flips the sign of the determinant.</li>
<li>Row scaling: Multiplying a row by a scalar <span>\(c\)</span> multiplies the determinant by <span>\(c\)</span>.</li>
<li>Row replacement: Adding a multiple of one row to another leaves the determinant unchanged.</li>
</ol>
</section>
<section id="set-up-your-lab-52">
<h4 data-anchor-id="set-up-your-lab-52">Set Up Your Lab</h4>
<div id="5f8cb320" data-execution_count="306">
<div><div id="cb529"><pre><code><span id="cb529-1"><a href="#cb529-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb529-2"><a href="#cb529-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-52">
<h4 data-anchor-id="step-by-step-code-walkthrough-52">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Row swap</li>
</ol>
<div id="15012150" data-execution_count="307">
<div><div id="cb530"><pre><code><span id="cb530-1"><a href="#cb530-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> Matrix([[<span>1</span>,<span>2</span>],[<span>3</span>,<span>4</span>]])</span>
<span id="cb530-2"><a href="#cb530-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(A):&#34;</span>, A.det())</span>
<span id="cb530-3"><a href="#cb530-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb530-4"><a href="#cb530-4" aria-hidden="true" tabindex="-1"></a>A_swapped <span>=</span> Matrix([[<span>3</span>,<span>4</span>],[<span>1</span>,<span>2</span>]])</span>
<span id="cb530-5"><a href="#cb530-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(after swap):&#34;</span>, A_swapped.det())</span></code></pre></div></div>
<div>
<pre><code>det(A): -2
det(after swap): 2</code></pre>
</div>
</div>
<p>The result flips sign.</p>
<ol start="2" type="1">
<li>Row scaling</li>
</ol>
<div id="0a416b46" data-execution_count="308">
<div><div id="cb532"><pre><code><span id="cb532-1"><a href="#cb532-1" aria-hidden="true" tabindex="-1"></a>B <span>=</span> Matrix([[<span>1</span>,<span>2</span>],[<span>3</span>,<span>4</span>]])</span>
<span id="cb532-2"><a href="#cb532-2" aria-hidden="true" tabindex="-1"></a>B_scaled <span>=</span> Matrix([[<span>2</span>,<span>4</span>],[<span>3</span>,<span>4</span>]])  <span># first row × 2</span></span>
<span id="cb532-3"><a href="#cb532-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb532-4"><a href="#cb532-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(B):&#34;</span>, B.det())</span>
<span id="cb532-5"><a href="#cb532-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(after scaling row 1 by 2):&#34;</span>, B_scaled.det())</span></code></pre></div></div>
<div>
<pre><code>det(B): -2
det(after scaling row 1 by 2): -4</code></pre>
</div>
</div>
<p>Determinant is multiplied by 2.</p>
<ol start="3" type="1">
<li>Row replacement (no change)</li>
</ol>
<div id="1bc15aa2" data-execution_count="309">
<div><div id="cb534"><pre><code><span id="cb534-1"><a href="#cb534-1" aria-hidden="true" tabindex="-1"></a>C <span>=</span> Matrix([[<span>1</span>,<span>2</span>],[<span>3</span>,<span>4</span>]])</span>
<span id="cb534-2"><a href="#cb534-2" aria-hidden="true" tabindex="-1"></a>C_replaced <span>=</span> Matrix([[<span>1</span>,<span>2</span>],[<span>3</span><span>-</span><span>2</span><span>*</span><span>1</span>, <span>4</span><span>-</span><span>2</span><span>*</span><span>2</span>]])  <span># row2 → row2 - 2*row1</span></span>
<span id="cb534-3"><a href="#cb534-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb534-4"><a href="#cb534-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(C):&#34;</span>, C.det())</span>
<span id="cb534-5"><a href="#cb534-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(after row replacement):&#34;</span>, C_replaced.det())</span></code></pre></div></div>
<div>
<pre><code>det(C): -2
det(after row replacement): -2</code></pre>
</div>
</div>
<p>Determinant stays the same.</p>
<ol start="4" type="1">
<li>Triangular form shortcut</li>
</ol>
<p>Since elimination only uses row replacement (which doesn’t change the determinant) and row swaps/scales (which we can track), the determinant of a triangular matrix is just the product of its diagonal entries.</p>
<div id="8f802720" data-execution_count="310">
<div><div id="cb536"><pre><code><span id="cb536-1"><a href="#cb536-1" aria-hidden="true" tabindex="-1"></a>D <span>=</span> Matrix([[<span>2</span>,<span>1</span>,<span>3</span>],[<span>0</span>,<span>4</span>,<span>5</span>],[<span>0</span>,<span>0</span>,<span>6</span>]])</span>
<span id="cb536-2"><a href="#cb536-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(D):&#34;</span>, D.det())</span>
<span id="cb536-3"><a href="#cb536-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Product of diagonals:&#34;</span>, <span>2</span><span>*</span><span>4</span><span>*</span><span>6</span>)</span></code></pre></div></div>
<div>
<pre><code>det(D): 48
Product of diagonals: 48</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>NumPy confirmation</li>
</ol>
<div id="aacca2ba" data-execution_count="311">
<div><div id="cb538"><pre><code><span id="cb538-1"><a href="#cb538-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>1</span>,<span>2</span>,<span>3</span>],[<span>0</span>,<span>4</span>,<span>5</span>],[<span>1</span>,<span>0</span>,<span>6</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb538-2"><a href="#cb538-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(A):&#34;</span>, np.linalg.det(A))</span></code></pre></div></div>
<div>
<pre><code>det(A): 22.000000000000004</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-51">
<h4 data-anchor-id="try-it-yourself-51">Try It Yourself</h4>
<ol type="1">
<li><p>Take</p>
<p><span>\[
\begin{bmatrix} 2 &amp; 3 \\ 4 &amp; 6 \end{bmatrix}
\]</span></p>
<p>and scale the second row by <span>\(\tfrac{1}{2}\)</span>. Compare determinants before and after.</p></li>
<li><p>Do Gaussian elimination on a 3×3 matrix, and track how each row operation changes the determinant.</p></li>
<li><p>Compute determinant by reducing to triangular form and compare with SymPy’s <code>.det()</code>.</p></li>
</ol>
</section>
<section id="the-takeaway-35">
<h4 data-anchor-id="the-takeaway-35">The Takeaway</h4>
<ul>
<li>Determinant reacts predictably to row operations.</li>
<li>Row replacement is “safe” (no change), scaling multiplies by the factor, and swapping flips the sign.</li>
<li>This makes elimination not just a solving tool, but also a method to compute determinants efficiently.</li>
</ul>
</section>
</section>
<section id="triangular-matrices-and-product-of-diagonals-fast-wins">
<h3 data-anchor-id="triangular-matrices-and-product-of-diagonals-fast-wins">54. Triangular Matrices and Product of Diagonals (Fast Wins)</h3>
<p>For triangular matrices (upper or lower), the determinant is simply the product of the diagonal entries. This rule is one of the biggest shortcuts in linear algebra - no expansion or elimination needed.</p>
<section id="why-it-works">
<h4 data-anchor-id="why-it-works">Why It Works</h4>
<ul>
<li>Triangular matrices already look like the end result of Gaussian elimination.</li>
<li>Since row replacement operations don’t change the determinant, what’s left is just the product of the diagonal.</li>
</ul>
</section>
<section id="set-up-your-lab-53">
<h4 data-anchor-id="set-up-your-lab-53">Set Up Your Lab</h4>
<div id="71f0e038" data-execution_count="312">
<div><div id="cb540"><pre><code><span id="cb540-1"><a href="#cb540-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb540-2"><a href="#cb540-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-53">
<h4 data-anchor-id="step-by-step-code-walkthrough-53">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Upper triangular example</li>
</ol>
<div id="8776e95a" data-execution_count="313">
<div><div id="cb541"><pre><code><span id="cb541-1"><a href="#cb541-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> Matrix([</span>
<span id="cb541-2"><a href="#cb541-2" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>,<span>1</span>,<span>3</span>],</span>
<span id="cb541-3"><a href="#cb541-3" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>4</span>,<span>5</span>],</span>
<span id="cb541-4"><a href="#cb541-4" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>0</span>,<span>6</span>]</span>
<span id="cb541-5"><a href="#cb541-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb541-6"><a href="#cb541-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb541-7"><a href="#cb541-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(A):&#34;</span>, A.det())</span>
<span id="cb541-8"><a href="#cb541-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Product of diagonals:&#34;</span>, <span>2</span><span>*</span><span>4</span><span>*</span><span>6</span>)</span></code></pre></div></div>
<div>
<pre><code>det(A): 48
Product of diagonals: 48</code></pre>
</div>
</div>
<p>Both values match exactly.</p>
<ol start="2" type="1">
<li>Lower triangular example</li>
</ol>
<div id="287924be" data-execution_count="314">
<div><div id="cb543"><pre><code><span id="cb543-1"><a href="#cb543-1" aria-hidden="true" tabindex="-1"></a>B <span>=</span> Matrix([</span>
<span id="cb543-2"><a href="#cb543-2" aria-hidden="true" tabindex="-1"></a>    [<span>7</span>,<span>0</span>,<span>0</span>],</span>
<span id="cb543-3"><a href="#cb543-3" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>,<span>5</span>,<span>0</span>],</span>
<span id="cb543-4"><a href="#cb543-4" aria-hidden="true" tabindex="-1"></a>    [<span>3</span>,<span>4</span>,<span>9</span>]</span>
<span id="cb543-5"><a href="#cb543-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb543-6"><a href="#cb543-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb543-7"><a href="#cb543-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(B):&#34;</span>, B.det())</span>
<span id="cb543-8"><a href="#cb543-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Product of diagonals:&#34;</span>, <span>7</span><span>*</span><span>5</span><span>*</span><span>9</span>)</span></code></pre></div></div>
<div>
<pre><code>det(B): 315
Product of diagonals: 315</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Diagonal matrix (special case)</li>
</ol>
<p>For diagonal matrices, determinant = product of diagonal entries directly.</p>
<div id="3b1859a8" data-execution_count="315">
<div><div id="cb545"><pre><code><span id="cb545-1"><a href="#cb545-1" aria-hidden="true" tabindex="-1"></a>C <span>=</span> Matrix.diag(<span>3</span>,<span>5</span>,<span>7</span>)</span>
<span id="cb545-2"><a href="#cb545-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(C):&#34;</span>, C.det())</span>
<span id="cb545-3"><a href="#cb545-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Product of diagonals:&#34;</span>, <span>3</span><span>*</span><span>5</span><span>*</span><span>7</span>)</span></code></pre></div></div>
<div>
<pre><code>det(C): 105
Product of diagonals: 105</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>NumPy version</li>
</ol>
<div id="5f7c0777" data-execution_count="316">
<div><div id="cb547"><pre><code><span id="cb547-1"><a href="#cb547-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>2</span>,<span>1</span>,<span>3</span>],[<span>0</span>,<span>4</span>,<span>5</span>],[<span>0</span>,<span>0</span>,<span>6</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb547-2"><a href="#cb547-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(A):&#34;</span>, np.linalg.det(A))</span>
<span id="cb547-3"><a href="#cb547-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Product of diagonals:&#34;</span>, np.prod(np.diag(A)))</span></code></pre></div></div>
<div>
<pre><code>det(A): 47.999999999999986
Product of diagonals: 48.0</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Quick elimination to triangular form</li>
</ol>
<p>Even for non-triangular matrices, elimination reduces them to triangular form, where this rule applies.</p>
<div id="158a699d" data-execution_count="317">
<div><div id="cb549"><pre><code><span id="cb549-1"><a href="#cb549-1" aria-hidden="true" tabindex="-1"></a>D <span>=</span> Matrix([[<span>1</span>,<span>2</span>,<span>3</span>],[<span>4</span>,<span>5</span>,<span>6</span>],[<span>7</span>,<span>8</span>,<span>10</span>]])</span>
<span id="cb549-2"><a href="#cb549-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(D) via SymPy:&#34;</span>, D.det())</span>
<span id="cb549-3"><a href="#cb549-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(D) via LU decomposition:&#34;</span>, D.LUdecomposition()[<span>0</span>].det() <span>*</span> D.LUdecomposition()[<span>1</span>].det())</span></code></pre></div></div>
<div>
<pre><code>det(D) via SymPy: -3
det(D) via LU decomposition: -3</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-52">
<h4 data-anchor-id="try-it-yourself-52">Try It Yourself</h4>
<ol type="1">
<li>Compute the determinant of a 4×4 diagonal matrix quickly.</li>
<li>Verify that triangular matrices with a zero on the diagonal always have determinant 0.</li>
<li>Use SymPy to check that elimination to triangular form preserves determinant (except for swaps/scales).</li>
</ol>
</section>
<section id="the-takeaway-36">
<h4 data-anchor-id="the-takeaway-36">The Takeaway</h4>
<ul>
<li><p>For triangular (and diagonal) matrices:</p>
<p><span>\[
\det(A) = \prod_{i} a_{ii}
\]</span></p></li>
<li><p>This shortcut makes determinant computation trivial.</p></li>
<li><p>Gaussian elimination leverages this fact: once reduced to triangular form, the determinant is just the product of pivots (with sign adjustments for swaps).</p></li>
</ul>
</section>
</section>
<section id="detab-detadetb-multiplicative-magic">
<h3 data-anchor-id="detab-detadetb-multiplicative-magic">55. det(AB) = det(A)det(B) (Multiplicative Magic)</h3>
<p>One of the most elegant properties of determinants is multiplicativity:</p>
<p><span>\[
\det(AB) = \det(A)\,\det(B)
\]</span></p>
<p>This rule is powerful because it connects algebra (matrix multiplication) with geometry (volume scaling).</p>
<section id="geometric-intuition">
<h4 data-anchor-id="geometric-intuition">Geometric Intuition</h4>
<ul>
<li>If <span>\(A\)</span> scales volumes by factor <span>\(\det(A)\)</span>, and <span>\(B\)</span> scales them by <span>\(\det(B)\)</span>, then applying <span>\(B\)</span> followed by <span>\(A\)</span> scales volumes by <span>\(\det(A)\det(B)\)</span>.</li>
<li>This property works in all dimensions.</li>
</ul>
</section>
<section id="set-up-your-lab-54">
<h4 data-anchor-id="set-up-your-lab-54">Set Up Your Lab</h4>
<div id="73e32013" data-execution_count="318">
<div><div id="cb551"><pre><code><span id="cb551-1"><a href="#cb551-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb551-2"><a href="#cb551-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-54">
<h4 data-anchor-id="step-by-step-code-walkthrough-54">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>2×2 example</li>
</ol>
<div id="5ae84434" data-execution_count="319">
<div><div id="cb552"><pre><code><span id="cb552-1"><a href="#cb552-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> Matrix([[<span>2</span>,<span>1</span>],[<span>0</span>,<span>3</span>]])</span>
<span id="cb552-2"><a href="#cb552-2" aria-hidden="true" tabindex="-1"></a>B <span>=</span> Matrix([[<span>1</span>,<span>4</span>],[<span>2</span>,<span>5</span>]])</span>
<span id="cb552-3"><a href="#cb552-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb552-4"><a href="#cb552-4" aria-hidden="true" tabindex="-1"></a>detA <span>=</span> A.det()</span>
<span id="cb552-5"><a href="#cb552-5" aria-hidden="true" tabindex="-1"></a>detB <span>=</span> B.det()</span>
<span id="cb552-6"><a href="#cb552-6" aria-hidden="true" tabindex="-1"></a>detAB <span>=</span> (A<span>*</span>B).det()</span>
<span id="cb552-7"><a href="#cb552-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb552-8"><a href="#cb552-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(A):&#34;</span>, detA)</span>
<span id="cb552-9"><a href="#cb552-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(B):&#34;</span>, detB)</span>
<span id="cb552-10"><a href="#cb552-10" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(AB):&#34;</span>, detAB)</span>
<span id="cb552-11"><a href="#cb552-11" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(A)*det(B):&#34;</span>, detA<span>*</span>detB)</span></code></pre></div></div>
<div>
<pre><code>det(A): 6
det(B): -3
det(AB): -18
det(A)*det(B): -18</code></pre>
</div>
</div>
<p>The two results match.</p>
<ol start="2" type="1">
<li>3×3 random matrix check</li>
</ol>
<div id="2508f6be" data-execution_count="320">
<div><div id="cb554"><pre><code><span id="cb554-1"><a href="#cb554-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span>1</span>)</span>
<span id="cb554-2"><a href="#cb554-2" aria-hidden="true" tabindex="-1"></a>A <span>=</span> Matrix(np.random.randint(<span>-</span><span>3</span>,<span>4</span>,(<span>3</span>,<span>3</span>)))</span>
<span id="cb554-3"><a href="#cb554-3" aria-hidden="true" tabindex="-1"></a>B <span>=</span> Matrix(np.random.randint(<span>-</span><span>3</span>,<span>4</span>,(<span>3</span>,<span>3</span>)))</span>
<span id="cb554-4"><a href="#cb554-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb554-5"><a href="#cb554-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(A):&#34;</span>, A.det())</span>
<span id="cb554-6"><a href="#cb554-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(B):&#34;</span>, B.det())</span>
<span id="cb554-7"><a href="#cb554-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(AB):&#34;</span>, (A<span>*</span>B).det())</span>
<span id="cb554-8"><a href="#cb554-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(A)*det(B):&#34;</span>, A.det()<span>*</span>B.det())</span></code></pre></div></div>
<div>
<pre><code>det(A): 25
det(B): -15
det(AB): -375
det(A)*det(B): -375</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Special cases</li>
</ol>
<ul>
<li>If <span>\(\det(A)=0\)</span>, then <span>\(\det(AB)=0\)</span>.</li>
<li>If <span>\(\det(A)=\pm1\)</span>, it acts like a “volume-preserving” transformation (rotation/reflection).</li>
</ul>
<div id="c0f48a84" data-execution_count="321">
<div><div id="cb556"><pre><code><span id="cb556-1"><a href="#cb556-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> Matrix([[<span>1</span>,<span>0</span>],[<span>0</span>,<span>0</span>]])  <span># singular</span></span>
<span id="cb556-2"><a href="#cb556-2" aria-hidden="true" tabindex="-1"></a>B <span>=</span> Matrix([[<span>2</span>,<span>3</span>],[<span>4</span>,<span>5</span>]])</span>
<span id="cb556-3"><a href="#cb556-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb556-4"><a href="#cb556-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(A):&#34;</span>, A.det())</span>
<span id="cb556-5"><a href="#cb556-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(AB):&#34;</span>, (A<span>*</span>B).det())</span></code></pre></div></div>

</div>
<p>Both are 0.</p>
<ol start="4" type="1">
<li>NumPy version</li>
</ol>
<div id="0ef4be24" data-execution_count="322">
<div><div id="cb558"><pre><code><span id="cb558-1"><a href="#cb558-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>2</span>,<span>1</span>],[<span>0</span>,<span>3</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb558-2"><a href="#cb558-2" aria-hidden="true" tabindex="-1"></a>B <span>=</span> np.array([[<span>1</span>,<span>4</span>],[<span>2</span>,<span>5</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb558-3"><a href="#cb558-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb558-4"><a href="#cb558-4" aria-hidden="true" tabindex="-1"></a>lhs <span>=</span> np.linalg.det(A <span>@</span> B)</span>
<span id="cb558-5"><a href="#cb558-5" aria-hidden="true" tabindex="-1"></a>rhs <span>=</span> np.linalg.det(A) <span>*</span> np.linalg.det(B)</span>
<span id="cb558-6"><a href="#cb558-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb558-7"><a href="#cb558-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(AB) =&#34;</span>, lhs)</span>
<span id="cb558-8"><a href="#cb558-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(A)*det(B) =&#34;</span>, rhs)</span></code></pre></div></div>
<div>
<pre><code>det(AB) = -17.999999999999996
det(A)*det(B) = -17.999999999999996</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-53">
<h4 data-anchor-id="try-it-yourself-53">Try It Yourself</h4>
<ol type="1">
<li>Construct two triangular matrices and verify multiplicativity (diagonal products multiply too).</li>
<li>Test the property with an orthogonal matrix <span>\(Q\)</span> (<span>\(\det(Q)=\pm 1\)</span>). What happens?</li>
<li>Try with one matrix singular - confirm the product is always singular.</li>
</ol>
</section>
<section id="the-takeaway-37">
<h4 data-anchor-id="the-takeaway-37">The Takeaway</h4>
<ul>
<li>Determinant is multiplicative, not additive.</li>
<li><span>\(\det(AB) = \det(A)\det(B)\)</span> is a cornerstone identity in linear algebra.</li>
<li>This property connects geometry (volume scaling) with algebra (matrix multiplication).</li>
</ul>
</section>
</section>
<section id="invertibility-and-zero-determinant-flat-vs.-full-volume">
<h3 data-anchor-id="invertibility-and-zero-determinant-flat-vs.-full-volume">56. Invertibility and Zero Determinant (Flat vs. Full Volume)</h3>
<p>The determinant gives a quick test for invertibility:</p>
<ul>
<li>If <span>\(\det(A) \neq 0\)</span>, the matrix is invertible.</li>
<li>If <span>\(\det(A) = 0\)</span>, the matrix is singular (non-invertible).</li>
</ul>
<p>Geometrically:</p>
<ul>
<li>Nonzero determinant → transformation keeps full dimension (no collapse).</li>
<li>Zero determinant → transformation flattens space into a lower dimension (volume = 0).</li>
</ul>
<section id="set-up-your-lab-55">
<h4 data-anchor-id="set-up-your-lab-55">Set Up Your Lab</h4>
<div id="5003767c" data-execution_count="323">
<div><div id="cb560"><pre><code><span id="cb560-1"><a href="#cb560-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb560-2"><a href="#cb560-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span>
<span id="cb560-3"><a href="#cb560-3" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy.matrices.common <span>import</span> NonInvertibleMatrixError</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-55">
<h4 data-anchor-id="step-by-step-code-walkthrough-55">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Invertible example</li>
</ol>
<div id="495c8043" data-execution_count="324">
<div><div id="cb561"><pre><code><span id="cb561-1"><a href="#cb561-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> Matrix([[<span>2</span>,<span>1</span>],[<span>5</span>,<span>3</span>]])</span>
<span id="cb561-2"><a href="#cb561-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(A):&#34;</span>, A.det())</span>
<span id="cb561-3"><a href="#cb561-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Inverse exists?&#34;</span>, A.det() <span>!=</span> <span>0</span>)</span>
<span id="cb561-4"><a href="#cb561-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;A inverse:</span><span>\n</span><span>&#34;</span>, A.inv())</span></code></pre></div></div>
<div>
<pre><code>det(A): 1
Inverse exists? True
A inverse:
 Matrix([[3, -1], [-5, 2]])</code></pre>
</div>
</div>
<p>The determinant is nonzero → invertible.</p>
<ol start="2" type="1">
<li>Singular example (zero determinant)</li>
</ol>
<div id="99cad634" data-execution_count="325">
<div><div id="cb563"><pre><code><span id="cb563-1"><a href="#cb563-1" aria-hidden="true" tabindex="-1"></a>B <span>=</span> Matrix([[<span>1</span>,<span>2</span>],[<span>2</span>,<span>4</span>]])</span>
<span id="cb563-2"><a href="#cb563-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(B):&#34;</span>, B.det())</span>
<span id="cb563-3"><a href="#cb563-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Inverse exists?&#34;</span>, B.det() <span>!=</span> <span>0</span>)</span></code></pre></div></div>
<div>
<pre><code>det(B): 0
Inverse exists? False</code></pre>
</div>
</div>
<p>Since the second row is a multiple of the first, determinant = 0 → no inverse.</p>
<ol start="3" type="1">
<li>Solving systems with determinant check</li>
</ol>
<p>If <span>\(\det(A)=0\)</span>, the system <span>\(Ax=b\)</span> may have no solutions or infinitely many.</p>
<div id="155ea5d1" data-execution_count="326">
<div><div id="cb565"><pre><code><span id="cb565-1"><a href="#cb565-1" aria-hidden="true" tabindex="-1"></a><span># 3. Solving systems with determinant check</span></span>
<span id="cb565-2"><a href="#cb565-2" aria-hidden="true" tabindex="-1"></a>b <span>=</span> Matrix([<span>1</span>,<span>2</span>])</span>
<span id="cb565-3"><a href="#cb565-3" aria-hidden="true" tabindex="-1"></a><span>try</span>:</span>
<span id="cb565-4"><a href="#cb565-4" aria-hidden="true" tabindex="-1"></a>    <span>print</span>(<span>&#34;Solve Ax=b with singular B:&#34;</span>, B.solve(b))</span>
<span id="cb565-5"><a href="#cb565-5" aria-hidden="true" tabindex="-1"></a><span>except</span> NonInvertibleMatrixError <span>as</span> e:</span>
<span id="cb565-6"><a href="#cb565-6" aria-hidden="true" tabindex="-1"></a>    <span>print</span>(<span>&#34;Error when solving Ax=b:&#34;</span>, e)</span></code></pre></div></div>
<div>
<pre><code>Error when solving Ax=b: Matrix det == 0; not invertible.</code></pre>
</div>
</div>
<p>SymPy indicates inconsistency or multiple solutions.</p>
<ol start="4" type="1">
<li>Higher-dimensional example</li>
</ol>
<div id="26c78cdc" data-execution_count="327">
<div><div id="cb567"><pre><code><span id="cb567-1"><a href="#cb567-1" aria-hidden="true" tabindex="-1"></a>C <span>=</span> Matrix([</span>
<span id="cb567-2"><a href="#cb567-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>0</span>,<span>0</span>],</span>
<span id="cb567-3"><a href="#cb567-3" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>2</span>,<span>0</span>],</span>
<span id="cb567-4"><a href="#cb567-4" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>0</span>,<span>3</span>]</span>
<span id="cb567-5"><a href="#cb567-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb567-6"><a href="#cb567-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(C):&#34;</span>, C.det())</span>
<span id="cb567-7"><a href="#cb567-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Invertible?&#34;</span>, C.det() <span>!=</span> <span>0</span>)</span></code></pre></div></div>
<div>
<pre><code>det(C): 6
Invertible? True</code></pre>
</div>
</div>
<p>Diagonal entries all nonzero → invertible.</p>
<ol start="5" type="1">
<li>NumPy version</li>
</ol>
<div id="287a6afc" data-execution_count="328">
<div><div id="cb569"><pre><code><span id="cb569-1"><a href="#cb569-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>2</span>,<span>1</span>],[<span>5</span>,<span>3</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb569-2"><a href="#cb569-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(A):&#34;</span>, np.linalg.det(A))</span>
<span id="cb569-3"><a href="#cb569-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Inverse:</span><span>\n</span><span>&#34;</span>, np.linalg.inv(A))</span>
<span id="cb569-4"><a href="#cb569-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb569-5"><a href="#cb569-5" aria-hidden="true" tabindex="-1"></a>B <span>=</span> np.array([[<span>1</span>,<span>2</span>],[<span>2</span>,<span>4</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb569-6"><a href="#cb569-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(B):&#34;</span>, np.linalg.det(B))</span>
<span id="cb569-7"><a href="#cb569-7" aria-hidden="true" tabindex="-1"></a><span># np.linalg.inv(B) would fail because det=0</span></span></code></pre></div></div>
<div>
<pre><code>det(A): 1.0000000000000002
Inverse:
 [[ 3. -1.]
 [-5.  2.]]
det(B): 0.0</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-54">
<h4 data-anchor-id="try-it-yourself-54">Try It Yourself</h4>
<ol type="1">
<li>Build a 3×3 matrix with determinant 0 by making one row a multiple of another. Confirm singularity.</li>
<li>Generate a random 4×4 matrix and check whether it’s invertible using <code>.det()</code>.</li>
<li>Test if two different 2×2 matrices are invertible, then multiply them together - is the product invertible too?</li>
</ol>
</section>
<section id="the-takeaway-38">
<h4 data-anchor-id="the-takeaway-38">The Takeaway</h4>
<ul>
<li><span>\(\det(A) \neq 0 \implies\)</span> invertible (full volume).</li>
<li><span>\(\det(A) = 0 \implies\)</span> singular (space collapsed).</li>
<li>Determinant gives both algebraic and geometric insight into when a matrix is reversible.</li>
</ul>
</section>
</section>
<section id="cofactor-expansion-laplaces-method">
<h3 data-anchor-id="cofactor-expansion-laplaces-method">57. Cofactor Expansion (Laplace’s Method)</h3>
<p>The cofactor expansion is a systematic way to compute determinants using minors. It’s not efficient for large matrices, but it reveals the recursive structure of determinants.</p>
<section id="definition">
<h4 data-anchor-id="definition">Definition</h4>
<p>For an <span>\(n \times n\)</span> matrix <span>\(A\)</span>,</p>
<p><span>\[
\det(A) = \sum_{j=1}^{n} (-1)^{i+j} a_{ij} \det(M_{ij})
\]</span></p>
<p>where:</p>
<ul>
<li><span>\(i\)</span> = chosen row (or column),</li>
<li><span>\(M_{ij}\)</span> = minor matrix after removing row <span>\(i\)</span>, column <span>\(j\)</span>.</li>
</ul>
</section>
<section id="set-up-your-lab-56">
<h4 data-anchor-id="set-up-your-lab-56">Set Up Your Lab</h4>
<div id="454a38ee" data-execution_count="329">
<div><div id="cb571"><pre><code><span id="cb571-1"><a href="#cb571-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb571-2"><a href="#cb571-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix, symbols</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-56">
<h4 data-anchor-id="step-by-step-code-walkthrough-56">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>2×2 case (base rule)</li>
</ol>
<div id="08e9ecac" data-execution_count="330">
<div><div id="cb572"><pre><code><span id="cb572-1"><a href="#cb572-1" aria-hidden="true" tabindex="-1"></a><span># declare symbols</span></span>
<span id="cb572-2"><a href="#cb572-2" aria-hidden="true" tabindex="-1"></a>a, b, c, d <span>=</span> symbols(<span>&#39;a b c d&#39;</span>)</span>
<span id="cb572-3"><a href="#cb572-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb572-4"><a href="#cb572-4" aria-hidden="true" tabindex="-1"></a><span># build the matrix</span></span>
<span id="cb572-5"><a href="#cb572-5" aria-hidden="true" tabindex="-1"></a>A <span>=</span> Matrix([[a, b],[c, d]])</span>
<span id="cb572-6"><a href="#cb572-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb572-7"><a href="#cb572-7" aria-hidden="true" tabindex="-1"></a><span># compute determinant</span></span>
<span id="cb572-8"><a href="#cb572-8" aria-hidden="true" tabindex="-1"></a>detA <span>=</span> A.det()</span>
<span id="cb572-9"><a href="#cb572-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Determinant 2x2:&#34;</span>, detA)</span></code></pre></div></div>
<div>
<pre><code>Determinant 2x2: a*d - b*c</code></pre>
</div>
</div>
<p>Formula: <span>\(\det(A) = ad - bc\)</span>.</p>
<ol start="2" type="1">
<li>3×3 example using cofactor expansion</li>
</ol>
<div id="02cf46c6" data-execution_count="331">
<div><div id="cb574"><pre><code><span id="cb574-1"><a href="#cb574-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> Matrix([</span>
<span id="cb574-2"><a href="#cb574-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>2</span>,<span>3</span>],</span>
<span id="cb574-3"><a href="#cb574-3" aria-hidden="true" tabindex="-1"></a>    [<span>4</span>,<span>5</span>,<span>6</span>],</span>
<span id="cb574-4"><a href="#cb574-4" aria-hidden="true" tabindex="-1"></a>    [<span>7</span>,<span>8</span>,<span>9</span>]</span>
<span id="cb574-5"><a href="#cb574-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb574-6"><a href="#cb574-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb574-7"><a href="#cb574-7" aria-hidden="true" tabindex="-1"></a>detA <span>=</span> A.det()</span>
<span id="cb574-8"><a href="#cb574-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Determinant via SymPy:&#34;</span>, detA)</span></code></pre></div></div>

</div>
<p>Let’s compute manually along the first row:</p>
<div id="64497be9" data-execution_count="332">
<div><div id="cb576"><pre><code><span id="cb576-1"><a href="#cb576-1" aria-hidden="true" tabindex="-1"></a>cofactor_expansion <span>=</span> (</span>
<span id="cb576-2"><a href="#cb576-2" aria-hidden="true" tabindex="-1"></a>    <span>1</span> <span>*</span> Matrix([[<span>5</span>,<span>6</span>],[<span>8</span>,<span>9</span>]]).det()</span>
<span id="cb576-3"><a href="#cb576-3" aria-hidden="true" tabindex="-1"></a>    <span>-</span> <span>2</span> <span>*</span> Matrix([[<span>4</span>,<span>6</span>],[<span>7</span>,<span>9</span>]]).det()</span>
<span id="cb576-4"><a href="#cb576-4" aria-hidden="true" tabindex="-1"></a>    <span>+</span> <span>3</span> <span>*</span> Matrix([[<span>4</span>,<span>5</span>],[<span>7</span>,<span>8</span>]]).det()</span>
<span id="cb576-5"><a href="#cb576-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb576-6"><a href="#cb576-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Cofactor expansion result:&#34;</span>, cofactor_expansion)</span></code></pre></div></div>
<div>
<pre><code>Cofactor expansion result: 0</code></pre>
</div>
</div>
<p>Both match (here = 0 because rows are dependent).</p>
<ol start="3" type="1">
<li>Expansion along different rows/columns</li>
</ol>
<p>The result is the same no matter which row/column you expand along.</p>
<div id="118c6339" data-execution_count="333">
<div><div id="cb578"><pre><code><span id="cb578-1"><a href="#cb578-1" aria-hidden="true" tabindex="-1"></a>cofactor_col1 <span>=</span> (</span>
<span id="cb578-2"><a href="#cb578-2" aria-hidden="true" tabindex="-1"></a>    <span>1</span> <span>*</span> Matrix([[<span>2</span>,<span>3</span>],[<span>8</span>,<span>9</span>]]).det()</span>
<span id="cb578-3"><a href="#cb578-3" aria-hidden="true" tabindex="-1"></a>    <span>-</span> <span>4</span> <span>*</span> Matrix([[<span>2</span>,<span>3</span>],[<span>5</span>,<span>6</span>]]).det()</span>
<span id="cb578-4"><a href="#cb578-4" aria-hidden="true" tabindex="-1"></a>    <span>+</span> <span>7</span> <span>*</span> Matrix([[<span>2</span>,<span>3</span>],[<span>5</span>,<span>6</span>]]).det()</span>
<span id="cb578-5"><a href="#cb578-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb578-6"><a href="#cb578-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Expansion along col1:&#34;</span>, cofactor_col1)</span></code></pre></div></div>
<div>
<pre><code>Expansion along col1: -15</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Larger example (4×4)</li>
</ol>
<div id="27a15c1b" data-execution_count="334">
<div><div id="cb580"><pre><code><span id="cb580-1"><a href="#cb580-1" aria-hidden="true" tabindex="-1"></a>B <span>=</span> Matrix([</span>
<span id="cb580-2"><a href="#cb580-2" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>,<span>0</span>,<span>1</span>,<span>3</span>],</span>
<span id="cb580-3"><a href="#cb580-3" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>2</span>,<span>0</span>,<span>4</span>],</span>
<span id="cb580-4"><a href="#cb580-4" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>1</span>,<span>1</span>,<span>0</span>],</span>
<span id="cb580-5"><a href="#cb580-5" aria-hidden="true" tabindex="-1"></a>    [<span>3</span>,<span>0</span>,<span>2</span>,<span>1</span>]</span>
<span id="cb580-6"><a href="#cb580-6" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb580-7"><a href="#cb580-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb580-8"><a href="#cb580-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Determinant 4x4:&#34;</span>, B.det())</span></code></pre></div></div>

</div>
<p>SymPy handles it directly, but conceptually it’s still the same recursive expansion.</p>
<ol start="5" type="1">
<li>NumPy vs SymPy</li>
</ol>
<div id="31b9b26e" data-execution_count="335">
<div><div id="cb582"><pre><code><span id="cb582-1"><a href="#cb582-1" aria-hidden="true" tabindex="-1"></a>B_np <span>=</span> np.array([[<span>2</span>,<span>0</span>,<span>1</span>,<span>3</span>],[<span>1</span>,<span>2</span>,<span>0</span>,<span>4</span>],[<span>0</span>,<span>1</span>,<span>1</span>,<span>0</span>],[<span>3</span>,<span>0</span>,<span>2</span>,<span>1</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb582-2"><a href="#cb582-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;NumPy determinant:&#34;</span>, np.linalg.det(B_np))</span></code></pre></div></div>

</div>
</section>
<section id="try-it-yourself-55">
<h4 data-anchor-id="try-it-yourself-55">Try It Yourself</h4>
<ol type="1">
<li>Compute a 3×3 determinant manually using cofactor expansion and confirm with <code>.det()</code>.</li>
<li>Expand along a different row and check that the result is unchanged.</li>
<li>Build a 4×4 diagonal matrix and expand it - what simplification do you notice?</li>
</ol>
</section>
<section id="the-takeaway-39">
<h4 data-anchor-id="the-takeaway-39">The Takeaway</h4>
<ul>
<li>Cofactor expansion defines determinant recursively.</li>
<li>Works on any row or column, with consistent results.</li>
<li>Important for proofs and theory, though not practical for computation on large matrices.</li>
</ul>
</section>
</section>
<section id="permutations-and-sign-the-combinatorial-core">
<h3 data-anchor-id="permutations-and-sign-the-combinatorial-core">58. Permutations and Sign (The Combinatorial Core)</h3>
<p>The determinant can also be defined using permutations of indices. This looks abstract, but it’s the most fundamental definition:</p>
<p><span>\[
\det(A) = \sum_{\sigma \in S_n} \text{sgn}(\sigma) \prod_{i=1}^n a_{i,\sigma(i)}
\]</span></p>
<ul>
<li><span>\(S_n\)</span> = set of all permutations of <span>\(\{1,\dots,n\}\)</span></li>
<li><span>\(\text{sgn}(\sigma)\)</span> = +1 if the permutation is even, -1 if odd</li>
<li>Each term = one product of entries, one from each row and column</li>
</ul>
<p>This formula explains why determinants mix signs, why row swaps flip the determinant, and why dependence kills it.</p>
<section id="set-up-your-lab-57">
<h4 data-anchor-id="set-up-your-lab-57">Set Up Your Lab</h4>
<div id="c8b61d11" data-execution_count="336">
<div><div id="cb584"><pre><code><span id="cb584-1"><a href="#cb584-1" aria-hidden="true" tabindex="-1"></a><span>import</span> itertools</span>
<span id="cb584-2"><a href="#cb584-2" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb584-3"><a href="#cb584-3" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-57">
<h4 data-anchor-id="step-by-step-code-walkthrough-57">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Determinant by permutation expansion (3×3)</li>
</ol>
<div id="ee9092fb" data-execution_count="337">
<div><div id="cb585"><pre><code><span id="cb585-1"><a href="#cb585-1" aria-hidden="true" tabindex="-1"></a><span>def</span> determinant_permutation(A):</span>
<span id="cb585-2"><a href="#cb585-2" aria-hidden="true" tabindex="-1"></a>    n <span>=</span> A.shape[<span>0</span>]</span>
<span id="cb585-3"><a href="#cb585-3" aria-hidden="true" tabindex="-1"></a>    total <span>=</span> <span>0</span></span>
<span id="cb585-4"><a href="#cb585-4" aria-hidden="true" tabindex="-1"></a>    <span>for</span> perm <span>in</span> itertools.permutations(<span>range</span>(n)):</span>
<span id="cb585-5"><a href="#cb585-5" aria-hidden="true" tabindex="-1"></a>        sign <span>=</span> (<span>-</span><span>1</span>)<span>**</span>(<span>sum</span>(<span>1</span> <span>for</span> i <span>in</span> <span>range</span>(n) <span>for</span> j <span>in</span> <span>range</span>(i) <span>if</span> perm[j] <span>&gt;</span> perm[i]))</span>
<span id="cb585-6"><a href="#cb585-6" aria-hidden="true" tabindex="-1"></a>        product <span>=</span> <span>1</span></span>
<span id="cb585-7"><a href="#cb585-7" aria-hidden="true" tabindex="-1"></a>        <span>for</span> i <span>in</span> <span>range</span>(n):</span>
<span id="cb585-8"><a href="#cb585-8" aria-hidden="true" tabindex="-1"></a>            product <span>*=</span> A[i, perm[i]]</span>
<span id="cb585-9"><a href="#cb585-9" aria-hidden="true" tabindex="-1"></a>        total <span>+=</span> sign <span>*</span> product</span>
<span id="cb585-10"><a href="#cb585-10" aria-hidden="true" tabindex="-1"></a>    <span>return</span> total</span>
<span id="cb585-11"><a href="#cb585-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb585-12"><a href="#cb585-12" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>1</span>,<span>2</span>,<span>3</span>],</span>
<span id="cb585-13"><a href="#cb585-13" aria-hidden="true" tabindex="-1"></a>              [<span>4</span>,<span>5</span>,<span>6</span>],</span>
<span id="cb585-14"><a href="#cb585-14" aria-hidden="true" tabindex="-1"></a>              [<span>7</span>,<span>8</span>,<span>9</span>]])</span>
<span id="cb585-15"><a href="#cb585-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb585-16"><a href="#cb585-16" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Permutation formula det:&#34;</span>, determinant_permutation(A))</span>
<span id="cb585-17"><a href="#cb585-17" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;NumPy det:&#34;</span>, np.linalg.det(A))</span></code></pre></div></div>
<div>
<pre><code>Permutation formula det: 0
NumPy det: -9.51619735392994e-16</code></pre>
</div>
</div>
<p>Both results ≈ 0, since rows are dependent.</p>
<ol start="2" type="1">
<li>Count permutations</li>
</ol>
<p>For <span>\(n=3\)</span>, there are <span>\(3! = 6\)</span> terms:</p>
<p><span>\[
\det(A) = a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32}  
- a_{13}a_{22}a_{31} - a_{11}a_{23}a_{32} - a_{12}a_{21}a_{33}
\]</span></p>
<p>You can see the alternating signs explicitly.</p>
<ol start="3" type="1">
<li>Verification with SymPy</li>
</ol>
<div id="eeb431c5" data-execution_count="338">
<div><div id="cb587"><pre><code><span id="cb587-1"><a href="#cb587-1" aria-hidden="true" tabindex="-1"></a>M <span>=</span> Matrix([[<span>2</span>,<span>1</span>,<span>0</span>],</span>
<span id="cb587-2"><a href="#cb587-2" aria-hidden="true" tabindex="-1"></a>            [<span>1</span>,<span>3</span>,<span>4</span>],</span>
<span id="cb587-3"><a href="#cb587-3" aria-hidden="true" tabindex="-1"></a>            [<span>0</span>,<span>2</span>,<span>5</span>]])</span>
<span id="cb587-4"><a href="#cb587-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;SymPy det:&#34;</span>, M.det())</span></code></pre></div></div>

</div>
<p>Matches the permutation expansion.</p>
<ol start="4" type="1">
<li>Growth of terms</li>
</ol>
<ul>
<li>2×2 → 2 terms</li>
<li>3×3 → 6 terms</li>
<li>4×4 → 24 terms</li>
<li><span>\(n\)</span> → <span>\(n!\)</span> terms (factorial growth!)</li>
</ul>
<p>This is why cofactor or LU is preferred computationally.</p>
</section>
<section id="try-it-yourself-56">
<h4 data-anchor-id="try-it-yourself-56">Try It Yourself</h4>
<ol type="1">
<li>Write out the 2×2 permutation formula explicitly and check it equals <span>\(ad - bc\)</span>.</li>
<li>Expand a 3×3 determinant by hand using the six terms.</li>
<li>Modify the code to count how many multiplications are required for a 5×5 matrix using the permutation definition.</li>
</ol>
</section>
<section id="the-takeaway-40">
<h4 data-anchor-id="the-takeaway-40">The Takeaway</h4>
<ul>
<li>Determinant = signed sum over all permutations.</li>
<li>Signs come from permutation parity (even/odd swaps).</li>
<li>This definition is the combinatorial foundation that unifies all determinant properties.</li>
</ul>
</section>
</section>
<section id="cramers-rule-solving-with-determinants-and-when-not-to-use-it">
<h3 data-anchor-id="cramers-rule-solving-with-determinants-and-when-not-to-use-it">59. Cramer’s Rule (Solving with Determinants, and When Not to Use It)</h3>
<p>Cramer’s Rule gives an explicit formula for solving a system of linear equations <span>\(Ax = b\)</span> using determinants. It is elegant but inefficient for large systems.</p>
<p>For <span>\(A \in \mathbb{R}^{n \times n}\)</span> with <span>\(\det(A) \neq 0\)</span>:</p>
<p><span>\[
x_i = \frac{\det(A_i)}{\det(A)}
\]</span></p>
<p>where <span>\(A_i\)</span> is <span>\(A\)</span> with its <span>\(i\)</span>-th column replaced by <span>\(b\)</span>.</p>
<section id="set-up-your-lab-58">
<h4 data-anchor-id="set-up-your-lab-58">Set Up Your Lab</h4>
<div id="f4aec65f" data-execution_count="339">
<div><div id="cb589"><pre><code><span id="cb589-1"><a href="#cb589-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb589-2"><a href="#cb589-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-58">
<h4 data-anchor-id="step-by-step-code-walkthrough-58">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Simple 2×2 example</li>
</ol>
<p>Solve:</p>
<p><span>\[
\begin{cases}
2x + y = 5 \\
x - y = 1
\end{cases}
\]</span></p>
<div id="9d98db65" data-execution_count="340">
<div><div id="cb590"><pre><code><span id="cb590-1"><a href="#cb590-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> Matrix([[<span>2</span>,<span>1</span>],[<span>1</span>,<span>-</span><span>1</span>]])</span>
<span id="cb590-2"><a href="#cb590-2" aria-hidden="true" tabindex="-1"></a>b <span>=</span> Matrix([<span>5</span>,<span>1</span>])</span>
<span id="cb590-3"><a href="#cb590-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb590-4"><a href="#cb590-4" aria-hidden="true" tabindex="-1"></a>detA <span>=</span> A.det()</span>
<span id="cb590-5"><a href="#cb590-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(A):&#34;</span>, detA)</span>
<span id="cb590-6"><a href="#cb590-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb590-7"><a href="#cb590-7" aria-hidden="true" tabindex="-1"></a><span># Replace columns</span></span>
<span id="cb590-8"><a href="#cb590-8" aria-hidden="true" tabindex="-1"></a>A1 <span>=</span> A.copy()</span>
<span id="cb590-9"><a href="#cb590-9" aria-hidden="true" tabindex="-1"></a>A1[:,<span>0</span>] <span>=</span> b</span>
<span id="cb590-10"><a href="#cb590-10" aria-hidden="true" tabindex="-1"></a>A2 <span>=</span> A.copy()</span>
<span id="cb590-11"><a href="#cb590-11" aria-hidden="true" tabindex="-1"></a>A2[:,<span>1</span>] <span>=</span> b</span>
<span id="cb590-12"><a href="#cb590-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb590-13"><a href="#cb590-13" aria-hidden="true" tabindex="-1"></a>x1 <span>=</span> A1.det() <span>/</span> detA</span>
<span id="cb590-14"><a href="#cb590-14" aria-hidden="true" tabindex="-1"></a>x2 <span>=</span> A2.det() <span>/</span> detA</span>
<span id="cb590-15"><a href="#cb590-15" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Solution via Cramer&#39;s Rule:&#34;</span>, [x1, x2])</span>
<span id="cb590-16"><a href="#cb590-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb590-17"><a href="#cb590-17" aria-hidden="true" tabindex="-1"></a><span># Check with built-in solver</span></span>
<span id="cb590-18"><a href="#cb590-18" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;SymPy solve:&#34;</span>, A.LUsolve(b))</span></code></pre></div></div>
<div>
<pre><code>det(A): -3
Solution via Cramer&#39;s Rule: [2, 1]
SymPy solve: Matrix([[2], [1]])</code></pre>
</div>
</div>
<p>Both give the same solution.</p>
<ol start="2" type="1">
<li>3×3 example</li>
</ol>
<div id="5b0995a9" data-execution_count="341">
<div><div id="cb592"><pre><code><span id="cb592-1"><a href="#cb592-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> Matrix([</span>
<span id="cb592-2"><a href="#cb592-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>2</span>,<span>3</span>],</span>
<span id="cb592-3"><a href="#cb592-3" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>1</span>,<span>4</span>],</span>
<span id="cb592-4"><a href="#cb592-4" aria-hidden="true" tabindex="-1"></a>    [<span>5</span>,<span>6</span>,<span>0</span>]</span>
<span id="cb592-5"><a href="#cb592-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb592-6"><a href="#cb592-6" aria-hidden="true" tabindex="-1"></a>b <span>=</span> Matrix([<span>7</span>,<span>8</span>,<span>9</span>])</span>
<span id="cb592-7"><a href="#cb592-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb592-8"><a href="#cb592-8" aria-hidden="true" tabindex="-1"></a>detA <span>=</span> A.det()</span>
<span id="cb592-9"><a href="#cb592-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(A):&#34;</span>, detA)</span>
<span id="cb592-10"><a href="#cb592-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb592-11"><a href="#cb592-11" aria-hidden="true" tabindex="-1"></a>solutions <span>=</span> []</span>
<span id="cb592-12"><a href="#cb592-12" aria-hidden="true" tabindex="-1"></a><span>for</span> i <span>in</span> <span>range</span>(A.shape[<span>1</span>]):</span>
<span id="cb592-13"><a href="#cb592-13" aria-hidden="true" tabindex="-1"></a>    Ai <span>=</span> A.copy()</span>
<span id="cb592-14"><a href="#cb592-14" aria-hidden="true" tabindex="-1"></a>    Ai[:,i] <span>=</span> b</span>
<span id="cb592-15"><a href="#cb592-15" aria-hidden="true" tabindex="-1"></a>    solutions.append(Ai.det()<span>/</span>detA)</span>
<span id="cb592-16"><a href="#cb592-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb592-17"><a href="#cb592-17" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Solution via Cramer&#39;s Rule:&#34;</span>, solutions)</span>
<span id="cb592-18"><a href="#cb592-18" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;SymPy solve:&#34;</span>, A.LUsolve(b))</span></code></pre></div></div>
<div>
<pre><code>det(A): 1
Solution via Cramer&#39;s Rule: [21, -16, 6]
SymPy solve: Matrix([[21], [-16], [6]])</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>NumPy version (inefficient but illustrative)</li>
</ol>
<div id="7a0855f9" data-execution_count="342">
<div><div id="cb594"><pre><code><span id="cb594-1"><a href="#cb594-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>2</span>,<span>1</span>],[<span>1</span>,<span>-</span><span>1</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb594-2"><a href="#cb594-2" aria-hidden="true" tabindex="-1"></a>b <span>=</span> np.array([<span>5</span>,<span>1</span>], dtype<span>=</span><span>float</span>)</span>
<span id="cb594-3"><a href="#cb594-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb594-4"><a href="#cb594-4" aria-hidden="true" tabindex="-1"></a>detA <span>=</span> np.linalg.det(A)</span>
<span id="cb594-5"><a href="#cb594-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb594-6"><a href="#cb594-6" aria-hidden="true" tabindex="-1"></a>solutions <span>=</span> []</span>
<span id="cb594-7"><a href="#cb594-7" aria-hidden="true" tabindex="-1"></a><span>for</span> i <span>in</span> <span>range</span>(A.shape[<span>1</span>]):</span>
<span id="cb594-8"><a href="#cb594-8" aria-hidden="true" tabindex="-1"></a>    Ai <span>=</span> A.copy()</span>
<span id="cb594-9"><a href="#cb594-9" aria-hidden="true" tabindex="-1"></a>    Ai[:,i] <span>=</span> b</span>
<span id="cb594-10"><a href="#cb594-10" aria-hidden="true" tabindex="-1"></a>    solutions.append(np.linalg.det(Ai)<span>/</span>detA)</span>
<span id="cb594-11"><a href="#cb594-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb594-12"><a href="#cb594-12" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Solution:&#34;</span>, solutions)</span></code></pre></div></div>
<div>
<pre><code>Solution: [np.float64(2.0000000000000004), np.float64(1.0)]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Why not use it in practice?</li>
</ol>
<ul>
<li>Requires computing <span>\(n+1\)</span> determinants.</li>
<li>Determinant computation via cofactor expansion is factorial-time.</li>
<li>Gaussian elimination or LU is far more efficient.</li>
</ul>
</section>
<section id="try-it-yourself-57">
<h4 data-anchor-id="try-it-yourself-57">Try It Yourself</h4>
<ol type="1">
<li>Solve a 3×3 system using Cramer’s Rule and confirm with <code>A.solve(b)</code>.</li>
<li>Try Cramer’s Rule when <span>\(\det(A)=0\)</span>. What happens?</li>
<li>Compare runtime of Cramer’s Rule vs LU for a random 5×5 matrix.</li>
</ol>
</section>
<section id="the-takeaway-41">
<h4 data-anchor-id="the-takeaway-41">The Takeaway</h4>
<ul>
<li>Cramer’s Rule gives explicit formulas for solutions using determinants.</li>
<li>Beautiful for theory, useful for small cases, but not computationally practical.</li>
<li>It highlights the deep connection between determinants and solving linear systems.</li>
</ul>
</section>
</section>
<section id="computing-determinants-in-practice-use-lu-mind-stability">
<h3 data-anchor-id="computing-determinants-in-practice-use-lu-mind-stability">60. Computing Determinants in Practice (Use LU, Mind Stability)</h3>
<p>While definitions like cofactor expansion and permutations are beautiful, they are too slow for large matrices. In practice, determinants are computed using row reduction or LU decomposition, with careful attention to numerical stability.</p>
<section id="set-up-your-lab-59">
<h4 data-anchor-id="set-up-your-lab-59">Set Up Your Lab</h4>
<div id="d68bd293" data-execution_count="343">
<div><div id="cb596"><pre><code><span id="cb596-1"><a href="#cb596-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb596-2"><a href="#cb596-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-59">
<h4 data-anchor-id="step-by-step-code-walkthrough-59">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Cofactor expansion is too slow</li>
</ol>
<div id="9ab3ce3b" data-execution_count="344">
<div><div id="cb597"><pre><code><span id="cb597-1"><a href="#cb597-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> Matrix([</span>
<span id="cb597-2"><a href="#cb597-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>2</span>,<span>3</span>],</span>
<span id="cb597-3"><a href="#cb597-3" aria-hidden="true" tabindex="-1"></a>    [<span>4</span>,<span>5</span>,<span>6</span>],</span>
<span id="cb597-4"><a href="#cb597-4" aria-hidden="true" tabindex="-1"></a>    [<span>7</span>,<span>8</span>,<span>10</span>]</span>
<span id="cb597-5"><a href="#cb597-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb597-6"><a href="#cb597-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det via cofactor expansion:&#34;</span>, A.det())</span></code></pre></div></div>
<div>
<pre><code>det via cofactor expansion: -3</code></pre>
</div>
</div>
<p>This works for 3×3, but complexity grows factorially.</p>
<ol start="2" type="1">
<li>Determinant via triangular form (LU decomposition)</li>
</ol>
<p>LU decomposition factorizes <span>\(A = LU\)</span>, where <span>\(L\)</span> is lower triangular and <span>\(U\)</span> is upper triangular. Determinant = product of diagonals of <span>\(U\)</span>, up to sign corrections for row swaps.</p>
<div id="a2d32cbb" data-execution_count="345">
<div><div id="cb599"><pre><code><span id="cb599-1"><a href="#cb599-1" aria-hidden="true" tabindex="-1"></a>L, U, perm <span>=</span> A.LUdecomposition()</span>
<span id="cb599-2"><a href="#cb599-2" aria-hidden="true" tabindex="-1"></a>detA <span>=</span> A.det()</span>
<span id="cb599-3"><a href="#cb599-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;L:</span><span>\n</span><span>&#34;</span>, L)</span>
<span id="cb599-4"><a href="#cb599-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;U:</span><span>\n</span><span>&#34;</span>, U)</span>
<span id="cb599-5"><a href="#cb599-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Permutation matrix:</span><span>\n</span><span>&#34;</span>, perm)</span>
<span id="cb599-6"><a href="#cb599-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det via LU product:&#34;</span>, detA)</span></code></pre></div></div>
<div>
<pre><code>L:
 Matrix([[1, 0, 0], [4, 1, 0], [7, 2, 1]])
U:
 Matrix([[1, 2, 3], [0, -3, -6], [0, 0, 1]])
Permutation matrix:
 []
det via LU product: -3</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>NumPy efficient method</li>
</ol>
<div id="c933a5fa" data-execution_count="346">
<div><div id="cb601"><pre><code><span id="cb601-1"><a href="#cb601-1" aria-hidden="true" tabindex="-1"></a>A_np <span>=</span> np.array([[<span>1</span>,<span>2</span>,<span>3</span>],[<span>4</span>,<span>5</span>,<span>6</span>],[<span>7</span>,<span>8</span>,<span>10</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb601-2"><a href="#cb601-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;NumPy det:&#34;</span>, np.linalg.det(A_np))</span></code></pre></div></div>
<div>
<pre><code>NumPy det: -3.000000000000001</code></pre>
</div>
</div>
<p>NumPy uses optimized routines (LAPACK under the hood).</p>
<ol start="4" type="1">
<li>Large random matrix</li>
</ol>
<div id="64550d72" data-execution_count="347">
<div><div id="cb603"><pre><code><span id="cb603-1"><a href="#cb603-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span>0</span>)</span>
<span id="cb603-2"><a href="#cb603-2" aria-hidden="true" tabindex="-1"></a>B <span>=</span> np.random.rand(<span>5</span>,<span>5</span>)</span>
<span id="cb603-3"><a href="#cb603-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;NumPy det (5x5):&#34;</span>, np.linalg.det(B))</span></code></pre></div></div>
<div>
<pre><code>NumPy det (5x5): 0.009658225505885114</code></pre>
</div>
</div>
<p>Computes quickly even for larger matrices.</p>
<ol start="5" type="1">
<li>Stability issues</li>
</ol>
<p>Determinants of large or ill-conditioned matrices can suffer from floating-point errors. For example, if rows are nearly dependent:</p>
<div id="61363ba8" data-execution_count="348">
<div><div id="cb605"><pre><code><span id="cb605-1"><a href="#cb605-1" aria-hidden="true" tabindex="-1"></a>C <span>=</span> np.array([[<span>1</span>,<span>2</span>,<span>3</span>],[<span>2</span>,<span>4.0000001</span>,<span>6</span>],[<span>3</span>,<span>6</span>,<span>9</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb605-2"><a href="#cb605-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;det(C):&#34;</span>, np.linalg.det(C))</span></code></pre></div></div>
<div>
<pre><code>det(C): -4.996003624823549e-23</code></pre>
</div>
</div>
<p>The result may not be exactly 0 due to floating-point approximations.</p>
</section>
<section id="try-it-yourself-58">
<h4 data-anchor-id="try-it-yourself-58">Try It Yourself</h4>
<ol type="1">
<li>Compute the determinant of a random 10×10 matrix with <code>np.linalg.det</code>.</li>
<li>Compare results between SymPy (exact rational arithmetic) and NumPy (floating-point).</li>
<li>Test determinant of a nearly singular matrix - notice numerical instability.</li>
</ol>
</section>
<section id="the-takeaway-42">
<h4 data-anchor-id="the-takeaway-42">The Takeaway</h4>
<ul>
<li>Determinants in practice are computed with LU decomposition or equivalent.</li>
<li>Always be mindful of numerical stability - small errors matter when determinant ≈ 0.</li>
<li>For exact answers (small cases), use symbolic tools like SymPy; for speed, use NumPy.</li>
</ul>
</section>
</section>
</section>
<section id="chapter-7.-eigenvalues-eigenvectors-and-dynamics">
<h2 data-anchor-id="chapter-7.-eigenvalues-eigenvectors-and-dynamics">Chapter 7. Eigenvalues, Eigenvectors, and Dynamics</h2>
<section id="eigenvalues-and-eigenvectors-directions-that-stay-put">
<h3 data-anchor-id="eigenvalues-and-eigenvectors-directions-that-stay-put">61. Eigenvalues and Eigenvectors (Directions That Stay Put)</h3>
<p>An eigenvector of a matrix <span>\(A\)</span> is a special vector that doesn’t change direction when multiplied by <span>\(A\)</span>. Instead, it only gets stretched or shrunk by a scalar called the eigenvalue.</p>
<p>Formally:</p>
<p><span>\[
A v = \lambda v
\]</span></p>
<p>where <span>\(v\)</span> is an eigenvector and <span>\(\lambda\)</span> is the eigenvalue.</p>
<p>Geometrically: eigenvectors are “preferred directions” of a linear transformation.</p>
<section id="set-up-your-lab-60">
<h4 data-anchor-id="set-up-your-lab-60">Set Up Your Lab</h4>
<div id="e70236e9" data-execution_count="349">
<div><div id="cb607"><pre><code><span id="cb607-1"><a href="#cb607-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb607-2"><a href="#cb607-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-60">
<h4 data-anchor-id="step-by-step-code-walkthrough-60">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>A simple 2×2 example</li>
</ol>
<div id="604db1bd" data-execution_count="350">
<div><div id="cb608"><pre><code><span id="cb608-1"><a href="#cb608-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> Matrix([</span>
<span id="cb608-2"><a href="#cb608-2" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>,<span>1</span>],</span>
<span id="cb608-3"><a href="#cb608-3" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>2</span>]</span>
<span id="cb608-4"><a href="#cb608-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb608-5"><a href="#cb608-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb608-6"><a href="#cb608-6" aria-hidden="true" tabindex="-1"></a>eigs <span>=</span> A.eigenvects()</span>
<span id="cb608-7"><a href="#cb608-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvalues and eigenvectors:&#34;</span>, eigs)</span></code></pre></div></div>
<div>
<pre><code>Eigenvalues and eigenvectors: [(1, 1, [Matrix([
[-1],
[ 1]])]), (3, 1, [Matrix([
[1],
[1]])])]</code></pre>
</div>
</div>
<p>This outputs eigenvalues and their associated eigenvectors.</p>
<ol start="2" type="1">
<li>Verify the eigen equation</li>
</ol>
<p>Pick one eigenpair <span>\((\lambda, v)\)</span>:</p>
<div id="a0a85681" data-execution_count="351">
<div><div id="cb610"><pre><code><span id="cb610-1"><a href="#cb610-1" aria-hidden="true" tabindex="-1"></a>lam <span>=</span> eigs[<span>0</span>][<span>0</span>]</span>
<span id="cb610-2"><a href="#cb610-2" aria-hidden="true" tabindex="-1"></a>v <span>=</span> eigs[<span>0</span>][<span>2</span>][<span>0</span>]</span>
<span id="cb610-3"><a href="#cb610-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Check Av = λv:&#34;</span>, A<span>*</span>v, lam<span>*</span>v)</span></code></pre></div></div>
<div>
<pre><code>Check Av = λv: Matrix([[-1], [1]]) Matrix([[-1], [1]])</code></pre>
</div>
</div>
<p>Both sides match → confirming the eigenpair.</p>
<ol start="3" type="1">
<li>NumPy version</li>
</ol>
<div id="eee97db5" data-execution_count="352">
<div><div id="cb612"><pre><code><span id="cb612-1"><a href="#cb612-1" aria-hidden="true" tabindex="-1"></a>A_np <span>=</span> np.array([[<span>2</span>,<span>1</span>],[<span>1</span>,<span>2</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb612-2"><a href="#cb612-2" aria-hidden="true" tabindex="-1"></a>eigvals, eigvecs <span>=</span> np.linalg.eig(A_np)</span>
<span id="cb612-3"><a href="#cb612-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb612-4"><a href="#cb612-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvalues:&#34;</span>, eigvals)</span>
<span id="cb612-5"><a href="#cb612-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvectors:</span><span>\n</span><span>&#34;</span>, eigvecs)</span></code></pre></div></div>
<div>
<pre><code>Eigenvalues: [3. 1.]
Eigenvectors:
 [[ 0.70710678 -0.70710678]
 [ 0.70710678  0.70710678]]</code></pre>
</div>
</div>
<p>Columns of <code>eigvecs</code> are eigenvectors.</p>
<ol start="4" type="1">
<li>Geometric interpretation (plot)</li>
</ol>
<div id="5efaee68" data-execution_count="353">
<div><div id="cb614"><pre><code><span id="cb614-1"><a href="#cb614-1" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span>
<span id="cb614-2"><a href="#cb614-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb614-3"><a href="#cb614-3" aria-hidden="true" tabindex="-1"></a>v1 <span>=</span> np.array(eigvecs[:,<span>0</span>])</span>
<span id="cb614-4"><a href="#cb614-4" aria-hidden="true" tabindex="-1"></a>v2 <span>=</span> np.array(eigvecs[:,<span>1</span>])</span>
<span id="cb614-5"><a href="#cb614-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb614-6"><a href="#cb614-6" aria-hidden="true" tabindex="-1"></a>plt.arrow(<span>0</span>,<span>0</span>,v1[<span>0</span>],v1[<span>1</span>],head_width<span>=</span><span>0.1</span>,color<span>=</span><span>&#34;blue&#34;</span>,length_includes_head<span>=</span><span>True</span>)</span>
<span id="cb614-7"><a href="#cb614-7" aria-hidden="true" tabindex="-1"></a>plt.arrow(<span>0</span>,<span>0</span>,v2[<span>0</span>],v2[<span>1</span>],head_width<span>=</span><span>0.1</span>,color<span>=</span><span>&#34;red&#34;</span>,length_includes_head<span>=</span><span>True</span>)</span>
<span id="cb614-8"><a href="#cb614-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb614-9"><a href="#cb614-9" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span>0</span>,color<span>=</span><span>&#34;black&#34;</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb614-10"><a href="#cb614-10" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span>0</span>,color<span>=</span><span>&#34;black&#34;</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb614-11"><a href="#cb614-11" aria-hidden="true" tabindex="-1"></a>plt.axis(<span>&#34;equal&#34;</span>)</span>
<span id="cb614-12"><a href="#cb614-12" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb614-13"><a href="#cb614-13" aria-hidden="true" tabindex="-1"></a>plt.title(<span>&#34;Eigenvectors: directions that stay put&#34;</span>)</span>
<span id="cb614-14"><a href="#cb614-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-354-output-1.png" width="571" height="431"/></p>
</figure>
</div>
</div>
</div>
<p>Both eigenvectors define directions where the transformation acts by scaling only.</p>
<ol start="5" type="1">
<li>Random 3×3 matrix example</li>
</ol>
<div id="742dc66d" data-execution_count="354">
<div><div id="cb615"><pre><code><span id="cb615-1"><a href="#cb615-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span>1</span>)</span>
<span id="cb615-2"><a href="#cb615-2" aria-hidden="true" tabindex="-1"></a>B <span>=</span> Matrix(np.random.randint(<span>-</span><span>2</span>,<span>3</span>,(<span>3</span>,<span>3</span>)))</span>
<span id="cb615-3"><a href="#cb615-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Matrix B:</span><span>\n</span><span>&#34;</span>, B)</span>
<span id="cb615-4"><a href="#cb615-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvalues/vectors:&#34;</span>, B.eigenvects())</span></code></pre></div></div>
<div>
<pre><code>Matrix B:
 Matrix([[1, 2, -2], [-1, 1, -2], [-2, -1, 2]])
Eigenvalues/vectors: [(4/3 + (-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3) + 13/(9*(-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)), 1, [Matrix([
[ -16/27 - 91/(81*(-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)) + (4/3 + (-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3) + 13/(9*(-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)))**2/9 - 7*(-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)/9],
[50/27 + 5*(-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)/9 - 2*(4/3 + (-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3) + 13/(9*(-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)))**2/9 + 65/(81*(-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3))],
[                                                                                                                                                                                                                                                              1]])]), (4/3 + 13/(9*(-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)) + (-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3), 1, [Matrix([
[ -16/27 - 7*(-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)/9 + (4/3 + 13/(9*(-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)) + (-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3))**2/9 - 91/(81*(-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3))],
[50/27 + 65/(81*(-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)) - 2*(4/3 + 13/(9*(-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)) + (-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3))**2/9 + 5*(-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)/9],
[                                                                                                                                                                                                                                                              1]])]), (13/(9*(2*sqrt(43)/3 + 127/27)**(1/3)) + 4/3 + (2*sqrt(43)/3 + 127/27)**(1/3), 1, [Matrix([
[  -7*(2*sqrt(43)/3 + 127/27)**(1/3)/9 - 16/27 - 91/(81*(2*sqrt(43)/3 + 127/27)**(1/3)) + (13/(9*(2*sqrt(43)/3 + 127/27)**(1/3)) + 4/3 + (2*sqrt(43)/3 + 127/27)**(1/3))**2/9],
[-2*(13/(9*(2*sqrt(43)/3 + 127/27)**(1/3)) + 4/3 + (2*sqrt(43)/3 + 127/27)**(1/3))**2/9 + 65/(81*(2*sqrt(43)/3 + 127/27)**(1/3)) + 5*(2*sqrt(43)/3 + 127/27)**(1/3)/9 + 50/27],
[                                                                                                                                                                           1]])])]</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-59">
<h4 data-anchor-id="try-it-yourself-59">Try It Yourself</h4>
<ol type="1">
<li><p>Compute eigenvalues and eigenvectors of</p>
<p><span>\[
\begin{bmatrix} 3 &amp; 0 \\ 0 &amp; 2 \end{bmatrix}
\]</span></p>
<p>and verify that they match the diagonal entries.</p></li>
<li><p>Use NumPy to find eigenvectors of a rotation matrix by 90°. What do you notice?</p></li>
<li><p>For a singular matrix, check if 0 is an eigenvalue.</p></li>
</ol>
</section>
<section id="the-takeaway-43">
<h4 data-anchor-id="the-takeaway-43">The Takeaway</h4>
<ul>
<li>Eigenvalues = scale factors; eigenvectors = directions that stay put.</li>
<li>The eigen equation <span>\(Av=\lambda v\)</span> captures the essence of a matrix’s action.</li>
<li>They form the foundation for deeper topics like diagonalization, stability, and dynamics.</li>
</ul>
</section>
</section>
<section id="characteristic-polynomial-where-eigenvalues-come-from">
<h3 data-anchor-id="characteristic-polynomial-where-eigenvalues-come-from">62. Characteristic Polynomial (Where Eigenvalues Come From)</h3>
<p>Eigenvalues don’t appear out of thin air - they come from the characteristic polynomial of a matrix. For a square matrix <span>\(A\)</span>,</p>
<p><span>\[
p(\lambda) = \det(A - \lambda I)
\]</span></p>
<p>The roots of this polynomial are the eigenvalues of <span>\(A\)</span>.</p>
<section id="set-up-your-lab-61">
<h4 data-anchor-id="set-up-your-lab-61">Set Up Your Lab</h4>
<div id="8137db83" data-execution_count="355">
<div><div id="cb617"><pre><code><span id="cb617-1"><a href="#cb617-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb617-2"><a href="#cb617-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix, symbols</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-61">
<h4 data-anchor-id="step-by-step-code-walkthrough-61">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>2×2 example</li>
</ol>
<p><span>\[
A = \begin{bmatrix} 2 &amp; 1 \\ 1 &amp; 2 \end{bmatrix}
\]</span></p>
<div id="a33dbb77" data-execution_count="356">
<div><div id="cb618"><pre><code><span id="cb618-1"><a href="#cb618-1" aria-hidden="true" tabindex="-1"></a>λ <span>=</span> symbols(<span>&#39;λ&#39;</span>)</span>
<span id="cb618-2"><a href="#cb618-2" aria-hidden="true" tabindex="-1"></a>A <span>=</span> Matrix([[<span>2</span>,<span>1</span>],[<span>1</span>,<span>2</span>]])</span>
<span id="cb618-3"><a href="#cb618-3" aria-hidden="true" tabindex="-1"></a>char_poly <span>=</span> A.charpoly(λ)</span>
<span id="cb618-4"><a href="#cb618-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Characteristic polynomial:&#34;</span>, char_poly.as_expr())</span>
<span id="cb618-5"><a href="#cb618-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvalues (roots):&#34;</span>, char_poly.all_roots())</span></code></pre></div></div>
<div>
<pre><code>Characteristic polynomial: λ**2 - 4*λ + 3
Eigenvalues (roots): [1, 3]</code></pre>
</div>
</div>
<p>Polynomial: <span>\(\lambda^2 - 4\lambda + 3\)</span>. Roots: <span>\(\lambda = 3, 1\)</span>.</p>
<ol start="2" type="1">
<li>Verify with eigen computation</li>
</ol>
<div id="a617732a" data-execution_count="357">
<div><div id="cb620"><pre><code><span id="cb620-1"><a href="#cb620-1" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvalues directly:&#34;</span>, A.eigenvals())</span></code></pre></div></div>
<div>
<pre><code>Eigenvalues directly: {3: 1, 1: 1}</code></pre>
</div>
</div>
<p>Matches the roots of the polynomial.</p>
<ol start="3" type="1">
<li>3×3 example</li>
</ol>
<div id="f52be399" data-execution_count="358">
<div><div id="cb622"><pre><code><span id="cb622-1"><a href="#cb622-1" aria-hidden="true" tabindex="-1"></a>B <span>=</span> Matrix([</span>
<span id="cb622-2"><a href="#cb622-2" aria-hidden="true" tabindex="-1"></a>    [<span>1</span>,<span>2</span>,<span>3</span>],</span>
<span id="cb622-3"><a href="#cb622-3" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>1</span>,<span>4</span>],</span>
<span id="cb622-4"><a href="#cb622-4" aria-hidden="true" tabindex="-1"></a>    [<span>5</span>,<span>6</span>,<span>0</span>]</span>
<span id="cb622-5"><a href="#cb622-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb622-6"><a href="#cb622-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb622-7"><a href="#cb622-7" aria-hidden="true" tabindex="-1"></a>char_poly_B <span>=</span> B.charpoly(λ)</span>
<span id="cb622-8"><a href="#cb622-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Characteristic polynomial of B:&#34;</span>, char_poly_B.as_expr())</span>
<span id="cb622-9"><a href="#cb622-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvalues of B:&#34;</span>, char_poly_B.all_roots())</span></code></pre></div></div>
<div>
<pre><code>Characteristic polynomial of B: λ**3 - 2*λ**2 - 38*λ - 1
Eigenvalues of B: [CRootOf(x**3 - 2*x**2 - 38*x - 1, 0), CRootOf(x**3 - 2*x**2 - 38*x - 1, 1), CRootOf(x**3 - 2*x**2 - 38*x - 1, 2)]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>NumPy version</li>
</ol>
<p>NumPy doesn’t give the polynomial directly, but eigenvalues can be checked:</p>
<div id="8508629a" data-execution_count="359">
<div><div id="cb624"><pre><code><span id="cb624-1"><a href="#cb624-1" aria-hidden="true" tabindex="-1"></a>B_np <span>=</span> np.array([[<span>1</span>,<span>2</span>,<span>3</span>],[<span>0</span>,<span>1</span>,<span>4</span>],[<span>5</span>,<span>6</span>,<span>0</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb624-2"><a href="#cb624-2" aria-hidden="true" tabindex="-1"></a>eigvals <span>=</span> np.linalg.eigvals(B_np)</span>
<span id="cb624-3"><a href="#cb624-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;NumPy eigenvalues:&#34;</span>, eigvals)</span></code></pre></div></div>
<div>
<pre><code>NumPy eigenvalues: [-5.2296696  -0.02635282  7.25602242]</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Relation to trace and determinant</li>
</ol>
<p>For a 2×2 matrix</p>
<p><span>\[
A = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix},
\]</span></p>
<p>the characteristic polynomial is</p>
<p><span>\[
\lambda^2 - (a+d)\lambda + (ad - bc).
\]</span></p>
<ul>
<li>Coefficient of <span>\(\lambda\)</span>: <span>\(-\text{trace}(A)\)</span>.</li>
<li>Constant term: <span>\(\det(A)\)</span>.</li>
</ul>
<div id="9d84d870" data-execution_count="360">
<div><div id="cb626"><pre><code><span id="cb626-1"><a href="#cb626-1" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Trace:&#34;</span>, A.trace())</span>
<span id="cb626-2"><a href="#cb626-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Determinant:&#34;</span>, A.det())</span></code></pre></div></div>

</div>
</section>
<section id="try-it-yourself-60">
<h4 data-anchor-id="try-it-yourself-60">Try It Yourself</h4>
<ol type="1">
<li><p>Compute the characteristic polynomial of</p>
<p><span>\[
\begin{bmatrix} 4 &amp; 0 \\ 0 &amp; 5 \end{bmatrix}
\]</span></p>
<p>and confirm eigenvalues are 4 and 5.</p></li>
<li><p>Check the relationship between polynomial coefficients, trace, and determinant for a 3×3 case.</p></li>
<li><p>Verify with NumPy that the roots of the polynomial equal the eigenvalues.</p></li>
</ol>
</section>
<section id="the-takeaway-44">
<h4 data-anchor-id="the-takeaway-44">The Takeaway</h4>
<ul>
<li>The characteristic polynomial encodes eigenvalues as its roots.</li>
<li>Coefficients are tied to invariants: trace and determinant.</li>
<li>This polynomial viewpoint is the bridge from algebraic formulas to geometric eigen-behavior.</li>
</ul>
</section>
</section>
<section id="algebraic-vs.-geometric-multiplicity-how-many-and-how-independent">
<h3 data-anchor-id="algebraic-vs.-geometric-multiplicity-how-many-and-how-independent">63. Algebraic vs. Geometric Multiplicity (How Many and How Independent)</h3>
<p>Eigenvalues can repeat, and when they do, two notions of multiplicity arise:</p>
<ul>
<li>Algebraic multiplicity: how many times the eigenvalue appears as a root of the characteristic polynomial.</li>
<li>Geometric multiplicity: the dimension of the eigenspace (number of independent eigenvectors).</li>
</ul>
<p>Always:</p>
<p><span>\[
1 \leq \text{geometric multiplicity} \leq \text{algebraic multiplicity}
\]</span></p>
<section id="set-up-your-lab-62">
<h4 data-anchor-id="set-up-your-lab-62">Set Up Your Lab</h4>
<div id="afb7b717" data-execution_count="361">
<div><div id="cb628"><pre><code><span id="cb628-1"><a href="#cb628-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb628-2"><a href="#cb628-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-62">
<h4 data-anchor-id="step-by-step-code-walkthrough-62">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Matrix with repeated eigenvalue</li>
</ol>
<div id="e37ca5a6" data-execution_count="362">
<div><div id="cb629"><pre><code><span id="cb629-1"><a href="#cb629-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> Matrix([</span>
<span id="cb629-2"><a href="#cb629-2" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>,<span>1</span>],</span>
<span id="cb629-3"><a href="#cb629-3" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>2</span>]</span>
<span id="cb629-4"><a href="#cb629-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb629-5"><a href="#cb629-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb629-6"><a href="#cb629-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvalues and algebraic multiplicity:&#34;</span>, A.eigenvals())</span>
<span id="cb629-7"><a href="#cb629-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvectors:&#34;</span>, A.eigenvects())</span></code></pre></div></div>
<div>
<pre><code>Eigenvalues and algebraic multiplicity: {2: 2}
Eigenvectors: [(2, 2, [Matrix([
[1],
[0]])])]</code></pre>
</div>
</div>
<ul>
<li>Eigenvalue 2 has algebraic multiplicity = 2.</li>
<li>But only 1 independent eigenvector → geometric multiplicity = 1.</li>
</ul>
<ol start="2" type="1">
<li>Diagonal matrix with repetition</li>
</ol>
<div id="73b04e56" data-execution_count="363">
<div><div id="cb631"><pre><code><span id="cb631-1"><a href="#cb631-1" aria-hidden="true" tabindex="-1"></a>B <span>=</span> Matrix([</span>
<span id="cb631-2"><a href="#cb631-2" aria-hidden="true" tabindex="-1"></a>    [<span>3</span>,<span>0</span>,<span>0</span>],</span>
<span id="cb631-3"><a href="#cb631-3" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>3</span>,<span>0</span>],</span>
<span id="cb631-4"><a href="#cb631-4" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>0</span>,<span>3</span>]</span>
<span id="cb631-5"><a href="#cb631-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb631-6"><a href="#cb631-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb631-7"><a href="#cb631-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvalues:&#34;</span>, B.eigenvals())</span>
<span id="cb631-8"><a href="#cb631-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvectors:&#34;</span>, B.eigenvects())</span></code></pre></div></div>
<div>
<pre><code>Eigenvalues: {3: 3}
Eigenvectors: [(3, 3, [Matrix([
[1],
[0],
[0]]), Matrix([
[0],
[1],
[0]]), Matrix([
[0],
[0],
[1]])])]</code></pre>
</div>
</div>
<p>Here, eigenvalue 3 has algebraic multiplicity = 3, and geometric multiplicity = 3.</p>
<ol start="3" type="1">
<li>NumPy check</li>
</ol>
<div id="dcc7594b" data-execution_count="364">
<div><div id="cb633"><pre><code><span id="cb633-1"><a href="#cb633-1" aria-hidden="true" tabindex="-1"></a>A_np <span>=</span> np.array([[<span>2</span>,<span>1</span>],[<span>0</span>,<span>2</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb633-2"><a href="#cb633-2" aria-hidden="true" tabindex="-1"></a>eigvals, eigvecs <span>=</span> np.linalg.eig(A_np)</span>
<span id="cb633-3"><a href="#cb633-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvalues:&#34;</span>, eigvals)</span>
<span id="cb633-4"><a href="#cb633-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvectors:</span><span>\n</span><span>&#34;</span>, eigvecs)</span></code></pre></div></div>
<div>
<pre><code>Eigenvalues: [2. 2.]
Eigenvectors:
 [[ 1.0000000e+00 -1.0000000e+00]
 [ 0.0000000e+00  4.4408921e-16]]</code></pre>
</div>
</div>
<p>NumPy won’t show multiplicities directly, but you can see repeated eigenvalues.</p>
<ol start="4" type="1">
<li>Comparing two cases</li>
</ol>
<ul>
<li>Defective matrix: Algebraic &gt; geometric (like the upper triangular <span>\(A\)</span>).</li>
<li>Diagonalizable matrix: Algebraic = geometric (like <span>\(B\)</span>).</li>
</ul>
<p>This distinction determines whether a matrix can be diagonalized.</p>
</section>
<section id="try-it-yourself-61">
<h4 data-anchor-id="try-it-yourself-61">Try It Yourself</h4>
<ol type="1">
<li><p>Compute algebraic and geometric multiplicities of</p>
<p><span>\[
\begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{bmatrix}
\]</span></p>
<p>(hint: only one eigenvector).</p></li>
<li><p>Take a diagonal matrix with repeated entries - what happens to multiplicities?</p></li>
<li><p>Test a random 3×3 singular matrix. Does 0 have algebraic multiplicity &gt; 1?</p></li>
</ol>
</section>
<section id="the-takeaway-45">
<h4 data-anchor-id="the-takeaway-45">The Takeaway</h4>
<ul>
<li>Algebraic multiplicity = count of root in characteristic polynomial.</li>
<li>Geometric multiplicity = dimension of eigenspace.</li>
<li>If they match for all eigenvalues → matrix is diagonalizable.</li>
</ul>
</section>
</section>
<section id="diagonalization-when-a-matrix-becomes-simple">
<h3 data-anchor-id="diagonalization-when-a-matrix-becomes-simple">64. Diagonalization (When a Matrix Becomes Simple)</h3>
<p>A matrix <span>\(A\)</span> is diagonalizable if it can be written as</p>
<p><span>\[
A = P D P^{-1}
\]</span></p>
<ul>
<li><span>\(D\)</span> is diagonal (containing eigenvalues).</li>
<li>Columns of <span>\(P\)</span> are the eigenvectors.</li>
</ul>
<p>This means <span>\(A\)</span> acts like simple scaling in a “better” coordinate system.</p>
<section id="set-up-your-lab-63">
<h4 data-anchor-id="set-up-your-lab-63">Set Up Your Lab</h4>
<div id="cdee09d8" data-execution_count="365">
<div><div id="cb635"><pre><code><span id="cb635-1"><a href="#cb635-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb635-2"><a href="#cb635-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-63">
<h4 data-anchor-id="step-by-step-code-walkthrough-63">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>A diagonalizable 2×2 matrix</li>
</ol>
<div id="b6dea603" data-execution_count="366">
<div><div id="cb636"><pre><code><span id="cb636-1"><a href="#cb636-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> Matrix([</span>
<span id="cb636-2"><a href="#cb636-2" aria-hidden="true" tabindex="-1"></a>    [<span>4</span>,<span>1</span>],</span>
<span id="cb636-3"><a href="#cb636-3" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>,<span>3</span>]</span>
<span id="cb636-4"><a href="#cb636-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb636-5"><a href="#cb636-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb636-6"><a href="#cb636-6" aria-hidden="true" tabindex="-1"></a>P, D <span>=</span> A.diagonalize()</span>
<span id="cb636-7"><a href="#cb636-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;P (eigenvectors):&#34;</span>)</span>
<span id="cb636-8"><a href="#cb636-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(P)</span>
<span id="cb636-9"><a href="#cb636-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;D (eigenvalues on diagonal):&#34;</span>)</span>
<span id="cb636-10"><a href="#cb636-10" aria-hidden="true" tabindex="-1"></a><span>print</span>(D)</span>
<span id="cb636-11"><a href="#cb636-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb636-12"><a href="#cb636-12" aria-hidden="true" tabindex="-1"></a><span># Verify A = P D P^-1</span></span>
<span id="cb636-13"><a href="#cb636-13" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Check:&#34;</span>, P<span>*</span>D<span>*</span>P.inv())</span></code></pre></div></div>
<div>
<pre><code>P (eigenvectors):
Matrix([[-1, 1], [2, 1]])
D (eigenvalues on diagonal):
Matrix([[2, 0], [0, 5]])
Check: Matrix([[4, 1], [2, 3]])</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>A non-diagonalizable matrix</li>
</ol>
<div id="2d3d7ba0" data-execution_count="367">
<div><div id="cb638"><pre><code><span id="cb638-1"><a href="#cb638-1" aria-hidden="true" tabindex="-1"></a>B <span>=</span> Matrix([</span>
<span id="cb638-2"><a href="#cb638-2" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>,<span>1</span>],</span>
<span id="cb638-3"><a href="#cb638-3" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>2</span>]</span>
<span id="cb638-4"><a href="#cb638-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb638-5"><a href="#cb638-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb638-6"><a href="#cb638-6" aria-hidden="true" tabindex="-1"></a><span>try</span>:</span>
<span id="cb638-7"><a href="#cb638-7" aria-hidden="true" tabindex="-1"></a>    P, D <span>=</span> B.diagonalize()</span>
<span id="cb638-8"><a href="#cb638-8" aria-hidden="true" tabindex="-1"></a>    <span>print</span>(<span>&#34;Diagonalization successful&#34;</span>)</span>
<span id="cb638-9"><a href="#cb638-9" aria-hidden="true" tabindex="-1"></a><span>except</span> <span>Exception</span> <span>as</span> e:</span>
<span id="cb638-10"><a href="#cb638-10" aria-hidden="true" tabindex="-1"></a>    <span>print</span>(<span>&#34;Not diagonalizable:&#34;</span>, e)</span></code></pre></div></div>
<div>
<pre><code>Not diagonalizable: Matrix is not diagonalizable</code></pre>
</div>
</div>
<p>This fails because eigenvalue 2 has algebraic multiplicity 2 but geometric multiplicity 1.</p>
<ol start="3" type="1">
<li>Diagonalization with NumPy</li>
</ol>
<p>NumPy doesn’t diagonalize explicitly, but we can build <span>\(P\)</span> and <span>\(D\)</span> ourselves:</p>
<div id="74fcdc53" data-execution_count="368">
<div><div id="cb640"><pre><code><span id="cb640-1"><a href="#cb640-1" aria-hidden="true" tabindex="-1"></a>A_np <span>=</span> np.array([[<span>4</span>,<span>1</span>],[<span>2</span>,<span>3</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb640-2"><a href="#cb640-2" aria-hidden="true" tabindex="-1"></a>eigvals, eigvecs <span>=</span> np.linalg.eig(A_np)</span>
<span id="cb640-3"><a href="#cb640-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb640-4"><a href="#cb640-4" aria-hidden="true" tabindex="-1"></a>P <span>=</span> eigvecs</span>
<span id="cb640-5"><a href="#cb640-5" aria-hidden="true" tabindex="-1"></a>D <span>=</span> np.diag(eigvals)</span>
<span id="cb640-6"><a href="#cb640-6" aria-hidden="true" tabindex="-1"></a>Pinv <span>=</span> np.linalg.inv(P)</span>
<span id="cb640-7"><a href="#cb640-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb640-8"><a href="#cb640-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Check A = PDP^-1:</span><span>\n</span><span>&#34;</span>, P <span>@</span> D <span>@</span> Pinv)</span></code></pre></div></div>
<div>
<pre><code>Check A = PDP^-1:
 [[4. 1.]
 [2. 3.]]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Powers of a diagonalizable matrix</li>
</ol>
<p>One reason diagonalization is powerful:</p>
<p><span>\[
A^k = P D^k P^{-1}
\]</span></p>
<p>Since <span>\(D^k\)</span> is trivial (just raise each diagonal entry to power <span>\(k\)</span>).</p>
<div id="02191ad1" data-execution_count="369">
<div><div id="cb642"><pre><code><span id="cb642-1"><a href="#cb642-1" aria-hidden="true" tabindex="-1"></a>k <span>=</span> <span>5</span></span>
<span id="cb642-2"><a href="#cb642-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb642-3"><a href="#cb642-3" aria-hidden="true" tabindex="-1"></a>A_power <span>=</span> np.linalg.matrix_power(A, k)</span>
<span id="cb642-4"><a href="#cb642-4" aria-hidden="true" tabindex="-1"></a>D_power <span>=</span> np.linalg.matrix_power(D, k)</span>
<span id="cb642-5"><a href="#cb642-5" aria-hidden="true" tabindex="-1"></a>A_via_diag <span>=</span> P <span>@</span> D_power <span>@</span> np.linalg.inv(P)</span>
<span id="cb642-6"><a href="#cb642-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb642-7"><a href="#cb642-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;A^5 via diagonalization:</span><span>\n</span><span>&#34;</span>, A_via_diag)</span>
<span id="cb642-8"><a href="#cb642-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Direct A^5:</span><span>\n</span><span>&#34;</span>, A_power)</span></code></pre></div></div>
<div>
<pre><code>A^5 via diagonalization:
 [[2094. 1031.]
 [2062. 1063.]]
Direct A^5:
 [[2094 1031]
 [2062 1063]]</code></pre>
</div>
</div>
<p>Both match.</p>
</section>
<section id="try-it-yourself-62">
<h4 data-anchor-id="try-it-yourself-62">Try It Yourself</h4>
<ol type="1">
<li><p>Check whether</p>
<p><span>\[
\begin{bmatrix} 5 &amp; 0 \\ 0 &amp; 5 \end{bmatrix}
\]</span></p>
<p>is diagonalizable.</p></li>
<li><p>Try diagonalizing a rotation matrix by 90°. Do you get complex eigenvalues?</p></li>
<li><p>Verify the formula <span>\(A^k = P D^k P^{-1}\)</span> for a 3×3 diagonalizable matrix.</p></li>
</ol>
</section>
<section id="the-takeaway-46">
<h4 data-anchor-id="the-takeaway-46">The Takeaway</h4>
<ul>
<li>Diagonalization rewrites a matrix in its simplest form.</li>
<li>Works if there are enough independent eigenvectors.</li>
<li>It makes powers of <span>\(A\)</span> easy, and is the gateway to analyzing dynamics.</li>
</ul>
</section>
</section>
<section id="powers-of-a-matrix-long-term-behavior-via-eigenvalues">
<h3 data-anchor-id="powers-of-a-matrix-long-term-behavior-via-eigenvalues">65. Powers of a Matrix (Long-Term Behavior via Eigenvalues)</h3>
<p>One of the most useful applications of eigenvalues and diagonalization is computing powers of a matrix:</p>
<p><span>\[
A^k = P D^k P^{-1}
\]</span></p>
<p>where <span>\(D\)</span> is diagonal with eigenvalues of <span>\(A\)</span>. Each eigenvalue <span>\(\lambda\)</span> raised to <span>\(k\)</span> dictates how its eigenvector direction grows, decays, or oscillates over time.</p>
<section id="set-up-your-lab-64">
<h4 data-anchor-id="set-up-your-lab-64">Set Up Your Lab</h4>
<div id="2941621f" data-execution_count="370">
<div><div id="cb644"><pre><code><span id="cb644-1"><a href="#cb644-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb644-2"><a href="#cb644-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-64">
<h4 data-anchor-id="step-by-step-code-walkthrough-64">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Simple diagonal matrix</li>
</ol>
<p>If <span>\(D = \text{diag}(2,3)\)</span>:</p>
<div id="a0e1cb10" data-execution_count="371">
<div><div id="cb645"><pre><code><span id="cb645-1"><a href="#cb645-1" aria-hidden="true" tabindex="-1"></a>D <span>=</span> Matrix([[<span>2</span>,<span>0</span>],[<span>0</span>,<span>3</span>]])</span>
<span id="cb645-2"><a href="#cb645-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;D^5 =&#34;</span>)</span>
<span id="cb645-3"><a href="#cb645-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(D<span>**</span><span>5</span>)</span></code></pre></div></div>
<div>
<pre><code>D^5 =
Matrix([[32, 0], [0, 243]])</code></pre>
</div>
</div>
<p>Eigenvalues are 2 and 3. Raising to the 5th power just raises each eigenvalue to the 5th: <span>\(2^5, 3^5\)</span>.</p>
<ol start="2" type="1">
<li>Non-diagonal matrix</li>
</ol>
<div id="8e6caa7c" data-execution_count="372">
<div><div id="cb647"><pre><code><span id="cb647-1"><a href="#cb647-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> Matrix([</span>
<span id="cb647-2"><a href="#cb647-2" aria-hidden="true" tabindex="-1"></a>    [<span>4</span>,<span>1</span>],</span>
<span id="cb647-3"><a href="#cb647-3" aria-hidden="true" tabindex="-1"></a>    [<span>2</span>,<span>3</span>]</span>
<span id="cb647-4"><a href="#cb647-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb647-5"><a href="#cb647-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb647-6"><a href="#cb647-6" aria-hidden="true" tabindex="-1"></a>P, D <span>=</span> A.diagonalize()</span>
<span id="cb647-7"><a href="#cb647-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;D (eigenvalues):&#34;</span>)</span>
<span id="cb647-8"><a href="#cb647-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(D)</span>
<span id="cb647-9"><a href="#cb647-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb647-10"><a href="#cb647-10" aria-hidden="true" tabindex="-1"></a><span># Compute A^10 via diagonalization</span></span>
<span id="cb647-11"><a href="#cb647-11" aria-hidden="true" tabindex="-1"></a>A10 <span>=</span> P <span>*</span> (D<span>**</span><span>10</span>) <span>*</span> P.inv()</span>
<span id="cb647-12"><a href="#cb647-12" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;A^10 =&#34;</span>)</span>
<span id="cb647-13"><a href="#cb647-13" aria-hidden="true" tabindex="-1"></a><span>print</span>(A10)</span></code></pre></div></div>
<div>
<pre><code>D (eigenvalues):
Matrix([[2, 0], [0, 5]])
A^10 =
Matrix([[6510758, 3254867], [6509734, 3255891]])</code></pre>
</div>
</div>
<p>Much easier than multiplying <span>\(A\)</span> ten times!</p>
<ol start="3" type="1">
<li>NumPy version</li>
</ol>
<div id="0e95ef08" data-execution_count="373">
<div><div id="cb649"><pre><code><span id="cb649-1"><a href="#cb649-1" aria-hidden="true" tabindex="-1"></a>A_np <span>=</span> np.array([[<span>4</span>,<span>1</span>],[<span>2</span>,<span>3</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb649-2"><a href="#cb649-2" aria-hidden="true" tabindex="-1"></a>eigvals, eigvecs <span>=</span> np.linalg.eig(A_np)</span>
<span id="cb649-3"><a href="#cb649-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb649-4"><a href="#cb649-4" aria-hidden="true" tabindex="-1"></a>k <span>=</span> <span>10</span></span>
<span id="cb649-5"><a href="#cb649-5" aria-hidden="true" tabindex="-1"></a>D_power <span>=</span> np.diag(eigvals<span>**</span>k)</span>
<span id="cb649-6"><a href="#cb649-6" aria-hidden="true" tabindex="-1"></a>A10_np <span>=</span> eigvecs <span>@</span> D_power <span>@</span> np.linalg.inv(eigvecs)</span>
<span id="cb649-7"><a href="#cb649-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb649-8"><a href="#cb649-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;A^10 via eigen-decomposition:</span><span>\n</span><span>&#34;</span>, A10_np)</span></code></pre></div></div>
<div>
<pre><code>A^10 via eigen-decomposition:
 [[6510758. 3254867.]
 [6509734. 3255891.]]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Long-term behavior</li>
</ol>
<p>Eigenvalues tell us what happens as <span>\(k \to \infty\)</span>:</p>
<ul>
<li>If <span>\(|\lambda| &lt; 1\)</span> → decay to 0.</li>
<li>If <span>\(|\lambda| &gt; 1\)</span> → grows unbounded.</li>
<li>If <span>\(|\lambda| = 1\)</span> → oscillates or stabilizes.</li>
</ul>
<div id="f0707dee" data-execution_count="374">
<div><div id="cb651"><pre><code><span id="cb651-1"><a href="#cb651-1" aria-hidden="true" tabindex="-1"></a>B <span>=</span> Matrix([</span>
<span id="cb651-2"><a href="#cb651-2" aria-hidden="true" tabindex="-1"></a>    [<span>0.5</span>,<span>0</span>],</span>
<span id="cb651-3"><a href="#cb651-3" aria-hidden="true" tabindex="-1"></a>    [<span>0</span>,<span>1.2</span>]</span>
<span id="cb651-4"><a href="#cb651-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb651-5"><a href="#cb651-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb651-6"><a href="#cb651-6" aria-hidden="true" tabindex="-1"></a>P, D <span>=</span> B.diagonalize()</span>
<span id="cb651-7"><a href="#cb651-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvalues:&#34;</span>, D)</span>
<span id="cb651-8"><a href="#cb651-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;B^20:&#34;</span>, P<span>*</span>(D<span>**</span><span>20</span>)<span>*</span>P.inv())</span></code></pre></div></div>
<div>
<pre><code>Eigenvalues: Matrix([[0.500000000000000, 0], [0, 1.20000000000000]])
B^20: Matrix([[9.53674316406250e-7, 0], [0, 38.3375999244747]])</code></pre>
</div>
</div>
<p>Here, the component along eigenvalue 0.5 decays, while eigenvalue 1.2 grows.</p>
</section>
<section id="try-it-yourself-63">
<h4 data-anchor-id="try-it-yourself-63">Try It Yourself</h4>
<ol type="1">
<li>Compute <span>\(A^{50}\)</span> for a diagonal matrix with eigenvalues 0.9 and 1.1. Which component dominates?</li>
<li>Take a stochastic (Markov) matrix and compute powers. Do the rows stabilize?</li>
<li>Experiment with complex eigenvalues (like a rotation) and check if the powers oscillate.</li>
</ol>
</section>
<section id="the-takeaway-47">
<h4 data-anchor-id="the-takeaway-47">The Takeaway</h4>
<ul>
<li>Matrix powers are simple when using eigenvalues.</li>
<li>Long-term dynamics are controlled by eigenvalue magnitudes.</li>
<li>This insight is critical in Markov chains, stability analysis, and dynamical systems.</li>
</ul>
</section>
</section>
<section id="real-vs.-complex-spectra-rotations-and-oscillations">
<h3 data-anchor-id="real-vs.-complex-spectra-rotations-and-oscillations">66. Real vs. Complex Spectra (Rotations and Oscillations)</h3>
<p>Not all eigenvalues are real. Some matrices, especially those involving rotations, have complex eigenvalues. Complex eigenvalues often describe oscillations or rotations in systems.</p>
<section id="set-up-your-lab-65">
<h4 data-anchor-id="set-up-your-lab-65">Set Up Your Lab</h4>
<div id="1e419a10" data-execution_count="375">
<div><div id="cb653"><pre><code><span id="cb653-1"><a href="#cb653-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb653-2"><a href="#cb653-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-65">
<h4 data-anchor-id="step-by-step-code-walkthrough-65">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Rotation matrix in 2D</li>
</ol>
<p>A 90° rotation matrix:</p>
<p><span>\[
R = \begin{bmatrix} 0 &amp; -1 \\ 1 &amp; 0 \end{bmatrix}
\]</span></p>
<div id="31a2ef4a" data-execution_count="376">
<div><div id="cb654"><pre><code><span id="cb654-1"><a href="#cb654-1" aria-hidden="true" tabindex="-1"></a>R <span>=</span> Matrix([[<span>0</span>, <span>-</span><span>1</span>],</span>
<span id="cb654-2"><a href="#cb654-2" aria-hidden="true" tabindex="-1"></a>            [<span>1</span>,  <span>0</span>]])</span>
<span id="cb654-3"><a href="#cb654-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb654-4"><a href="#cb654-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Characteristic polynomial:&#34;</span>, R.charpoly())</span>
<span id="cb654-5"><a href="#cb654-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvalues:&#34;</span>, R.eigenvals())</span></code></pre></div></div>
<div>
<pre><code>Characteristic polynomial: PurePoly(lambda**2 + 1, lambda, domain=&#39;ZZ&#39;)
Eigenvalues: {-I: 1, I: 1}</code></pre>
</div>
</div>
<p>Result: eigenvalues are <span>\(i\)</span> and <span>\(-i\)</span> (purely imaginary).</p>
<ol start="2" type="1">
<li>Verify eigen-equation with complex numbers</li>
</ol>
<div id="ba7b49d9" data-execution_count="377">
<div><div id="cb656"><pre><code><span id="cb656-1"><a href="#cb656-1" aria-hidden="true" tabindex="-1"></a>eigs <span>=</span> R.eigenvects()</span>
<span id="cb656-2"><a href="#cb656-2" aria-hidden="true" tabindex="-1"></a><span>for</span> eig <span>in</span> eigs:</span>
<span id="cb656-3"><a href="#cb656-3" aria-hidden="true" tabindex="-1"></a>    lam <span>=</span> eig[<span>0</span>]</span>
<span id="cb656-4"><a href="#cb656-4" aria-hidden="true" tabindex="-1"></a>    v <span>=</span> eig[<span>2</span>][<span>0</span>]</span>
<span id="cb656-5"><a href="#cb656-5" aria-hidden="true" tabindex="-1"></a>    <span>print</span>(<span>f&#34;λ = </span><span>{</span>lam<span>}</span><span>, Av = </span><span>{</span>R<span>*</span>v<span>}</span><span>, λv = </span><span>{</span>lam<span>*</span>v<span>}</span><span>&#34;</span>)</span></code></pre></div></div>
<div>
<pre><code>λ = -I, Av = Matrix([[-1], [-I]]), λv = Matrix([[-1], [-I]])
λ = I, Av = Matrix([[-1], [I]]), λv = Matrix([[-1], [I]])</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>NumPy version</li>
</ol>
<div id="959f2be4" data-execution_count="378">
<div><div id="cb658"><pre><code><span id="cb658-1"><a href="#cb658-1" aria-hidden="true" tabindex="-1"></a>R_np <span>=</span> np.array([[<span>0</span>,<span>-</span><span>1</span>],[<span>1</span>,<span>0</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb658-2"><a href="#cb658-2" aria-hidden="true" tabindex="-1"></a>eigvals, eigvecs <span>=</span> np.linalg.eig(R_np)</span>
<span id="cb658-3"><a href="#cb658-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvalues:&#34;</span>, eigvals)</span>
<span id="cb658-4"><a href="#cb658-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvectors:</span><span>\n</span><span>&#34;</span>, eigvecs)</span></code></pre></div></div>
<div>
<pre><code>Eigenvalues: [0.+1.j 0.-1.j]
Eigenvectors:
 [[0.70710678+0.j         0.70710678-0.j        ]
 [0.        -0.70710678j 0.        +0.70710678j]]</code></pre>
</div>
</div>
<p>NumPy shows complex eigenvalues with <code>j</code> (Python’s imaginary unit).</p>
<ol start="4" type="1">
<li>Rotation by arbitrary angle</li>
</ol>
<p>General 2D rotation:</p>
<p><span>\[
R(\theta) = \begin{bmatrix} \cos\theta &amp; -\sin\theta \\ \sin\theta &amp; \cos\theta \end{bmatrix}
\]</span></p>
<p>Eigenvalues:</p>
<p><span>\[
\lambda = e^{\pm i\theta} = \cos\theta \pm i\sin\theta
\]</span></p>
<div id="772ead5f" data-execution_count="379">
<div><div id="cb660"><pre><code><span id="cb660-1"><a href="#cb660-1" aria-hidden="true" tabindex="-1"></a>theta <span>=</span> np.pi<span>/</span><span>4</span>  <span># 45 degrees</span></span>
<span id="cb660-2"><a href="#cb660-2" aria-hidden="true" tabindex="-1"></a>R_theta <span>=</span> np.array([[np.cos(theta), <span>-</span>np.sin(theta)],</span>
<span id="cb660-3"><a href="#cb660-3" aria-hidden="true" tabindex="-1"></a>                    [np.sin(theta),  np.cos(theta)]])</span>
<span id="cb660-4"><a href="#cb660-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb660-5"><a href="#cb660-5" aria-hidden="true" tabindex="-1"></a>eigvals, eigvecs <span>=</span> np.linalg.eig(R_theta)</span>
<span id="cb660-6"><a href="#cb660-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvalues (rotation 45°):&#34;</span>, eigvals)</span></code></pre></div></div>
<div>
<pre><code>Eigenvalues (rotation 45°): [0.70710678+0.70710678j 0.70710678-0.70710678j]</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Oscillation insight</li>
</ol>
<ul>
<li>Complex eigenvalues with <span>\(|\lambda|=1\)</span> → pure oscillation (no growth).</li>
<li>If <span>\(|\lambda|&lt;1\)</span> → decaying spiral.</li>
<li>If <span>\(|\lambda|&gt;1\)</span> → growing spiral.</li>
</ul>
<p>Example:</p>
<div id="07cbe87b" data-execution_count="380">
<div><div id="cb662"><pre><code><span id="cb662-1"><a href="#cb662-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>0.8</span>, <span>-</span><span>0.6</span>],</span>
<span id="cb662-2"><a href="#cb662-2" aria-hidden="true" tabindex="-1"></a>              [<span>0.6</span>,  <span>0.8</span>]])</span>
<span id="cb662-3"><a href="#cb662-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb662-4"><a href="#cb662-4" aria-hidden="true" tabindex="-1"></a>eigvals, _ <span>=</span> np.linalg.eig(A)</span>
<span id="cb662-5"><a href="#cb662-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvalues:&#34;</span>, eigvals)</span></code></pre></div></div>
<div>
<pre><code>Eigenvalues: [0.8+0.6j 0.8-0.6j]</code></pre>
</div>
</div>
<p>These eigenvalues lie inside the unit circle → spiral decay.</p>
</section>
<section id="try-it-yourself-64">
<h4 data-anchor-id="try-it-yourself-64">Try It Yourself</h4>
<ol type="1">
<li>Compute eigenvalues of a 180° rotation. What happens?</li>
<li>Modify the rotation matrix to include scaling (e.g., multiply by 1.1). Do the eigenvalues lie outside the unit circle?</li>
<li>Plot the trajectory of repeatedly applying a rotation matrix to a vector.</li>
</ol>
</section>
<section id="the-takeaway-48">
<h4 data-anchor-id="the-takeaway-48">The Takeaway</h4>
<ul>
<li>Complex eigenvalues naturally appear in rotations and oscillatory systems.</li>
<li>Their magnitude controls growth or decay; their angle controls oscillation.</li>
<li>This is a key link between linear algebra and dynamics in physics and engineering.</li>
</ul>
</section>
</section>
<section id="defective-matrices-and-a-peek-at-jordan-form-when-diagonalization-fails">
<h3 data-anchor-id="defective-matrices-and-a-peek-at-jordan-form-when-diagonalization-fails">67. Defective Matrices and a Peek at Jordan Form (When Diagonalization Fails)</h3>
<p>Not every matrix has enough independent eigenvectors to be diagonalized. Such matrices are called defective. To handle them, mathematicians use the Jordan normal form, which extends diagonalization with extra structure.</p>
<section id="set-up-your-lab-66">
<h4 data-anchor-id="set-up-your-lab-66">Set Up Your Lab</h4>
<div id="240fb6c2" data-execution_count="381">
<div><div id="cb664"><pre><code><span id="cb664-1"><a href="#cb664-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb664-2"><a href="#cb664-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-66">
<h4 data-anchor-id="step-by-step-code-walkthrough-66">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>A defective example</li>
</ol>
<p><span>\[
A = \begin{bmatrix} 2 &amp; 1 \\ 0 &amp; 2 \end{bmatrix}
\]</span></p>
<div id="af88f139" data-execution_count="382">
<div><div id="cb665"><pre><code><span id="cb665-1"><a href="#cb665-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> Matrix([[<span>2</span>,<span>1</span>],</span>
<span id="cb665-2"><a href="#cb665-2" aria-hidden="true" tabindex="-1"></a>            [<span>0</span>,<span>2</span>]])</span>
<span id="cb665-3"><a href="#cb665-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb665-4"><a href="#cb665-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvalues:&#34;</span>, A.eigenvals())</span>
<span id="cb665-5"><a href="#cb665-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvectors:&#34;</span>, A.eigenvects())</span></code></pre></div></div>
<div>
<pre><code>Eigenvalues: {2: 2}
Eigenvectors: [(2, 2, [Matrix([
[1],
[0]])])]</code></pre>
</div>
</div>
<ul>
<li>Eigenvalue 2 has algebraic multiplicity = 2.</li>
<li>Only 1 eigenvector exists → geometric multiplicity = 1.</li>
</ul>
<p>Thus <span>\(A\)</span> is defective, not diagonalizable.</p>
<ol start="2" type="1">
<li>Attempt diagonalization</li>
</ol>
<div id="7c003c11" data-execution_count="383">
<div><div id="cb667"><pre><code><span id="cb667-1"><a href="#cb667-1" aria-hidden="true" tabindex="-1"></a><span>try</span>:</span>
<span id="cb667-2"><a href="#cb667-2" aria-hidden="true" tabindex="-1"></a>    P, D <span>=</span> A.diagonalize()</span>
<span id="cb667-3"><a href="#cb667-3" aria-hidden="true" tabindex="-1"></a>    <span>print</span>(<span>&#34;Diagonal form:&#34;</span>, D)</span>
<span id="cb667-4"><a href="#cb667-4" aria-hidden="true" tabindex="-1"></a><span>except</span> <span>Exception</span> <span>as</span> e:</span>
<span id="cb667-5"><a href="#cb667-5" aria-hidden="true" tabindex="-1"></a>    <span>print</span>(<span>&#34;Diagonalization failed:&#34;</span>, e)</span></code></pre></div></div>
<div>
<pre><code>Diagonalization failed: Matrix is not diagonalizable</code></pre>
</div>
</div>
<p>You’ll see an error - confirming <span>\(A\)</span> is not diagonalizable.</p>
<ol start="3" type="1">
<li>Jordan form in SymPy</li>
</ol>
<div id="5ab5033a" data-execution_count="384">
<div><div id="cb669"><pre><code><span id="cb669-1"><a href="#cb669-1" aria-hidden="true" tabindex="-1"></a>J, P <span>=</span> A.jordan_form()</span>
<span id="cb669-2"><a href="#cb669-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Jordan form J:&#34;</span>)</span>
<span id="cb669-3"><a href="#cb669-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(J)</span>
<span id="cb669-4"><a href="#cb669-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;P (generalized eigenvectors):&#34;</span>)</span>
<span id="cb669-5"><a href="#cb669-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(P)</span></code></pre></div></div>
<div>
<pre><code>Jordan form J:
Matrix([[1, 0], [0, 1]])
P (generalized eigenvectors):
Matrix([[2, 1], [0, 2]])</code></pre>
</div>
</div>
<p>The Jordan form shows a Jordan block:</p>
<p><span>\[
J = \begin{bmatrix} 2 &amp; 1 \\ 0 &amp; 2 \end{bmatrix}
\]</span></p>
<p>This block structure represents the failure of diagonalization.</p>
<ol start="4" type="1">
<li>NumPy perspective</li>
</ol>
<p>NumPy doesn’t compute Jordan form, but you can see repeated eigenvalues and lack of eigenvectors:</p>
<div id="ff458933" data-execution_count="385">
<div><div id="cb671"><pre><code><span id="cb671-1"><a href="#cb671-1" aria-hidden="true" tabindex="-1"></a>A_np <span>=</span> np.array([[<span>2</span>,<span>1</span>],[<span>0</span>,<span>2</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb671-2"><a href="#cb671-2" aria-hidden="true" tabindex="-1"></a>eigvals, eigvecs <span>=</span> np.linalg.eig(A_np)</span>
<span id="cb671-3"><a href="#cb671-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvalues:&#34;</span>, eigvals)</span>
<span id="cb671-4"><a href="#cb671-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvectors:</span><span>\n</span><span>&#34;</span>, eigvecs)</span></code></pre></div></div>
<div>
<pre><code>Eigenvalues: [2. 2.]
Eigenvectors:
 [[ 1.0000000e+00 -1.0000000e+00]
 [ 0.0000000e+00  4.4408921e-16]]</code></pre>
</div>
</div>
<p>The eigenvectors matrix has fewer independent columns than expected.</p>
<ol start="5" type="1">
<li>Generalized eigenvectors</li>
</ol>
<p>Jordan form introduces generalized eigenvectors, which satisfy:</p>
<p><span>\[
(A - \lambda I)^k v = 0 \quad \text{for some } k&gt;1
\]</span></p>
<p>They “fill the gap” when ordinary eigenvectors are insufficient.</p>
</section>
<section id="try-it-yourself-65">
<h4 data-anchor-id="try-it-yourself-65">Try It Yourself</h4>
<ol type="1">
<li><p>Test diagonalizability of</p>
<p><span>\[
\begin{bmatrix} 3 &amp; 1 \\ 0 &amp; 3 \end{bmatrix}
\]</span></p>
<p>and compare with its Jordan form.</p></li>
<li><p>Try a 3×3 defective matrix with one Jordan block of size 3.</p></li>
<li><p>Verify that Jordan blocks still capture the correct eigenvalues.</p></li>
</ol>
</section>
<section id="the-takeaway-49">
<h4 data-anchor-id="the-takeaway-49">The Takeaway</h4>
<ul>
<li>Defective matrices lack enough eigenvectors for diagonalization.</li>
<li>Jordan form replaces diagonalization with blocks, keeping eigenvalues on the diagonal.</li>
<li>Understanding Jordan blocks is essential for advanced linear algebra and differential equations.</li>
</ul>
</section>
</section>
<section id="stability-and-spectral-radius-grow-decay-or-oscillate">
<h3 data-anchor-id="stability-and-spectral-radius-grow-decay-or-oscillate">68. Stability and Spectral Radius (Grow, Decay, or Oscillate)</h3>
<p>The spectral radius of a matrix <span>\(A\)</span> is defined as</p>
<p><span>\[
\rho(A) = \max_i |\lambda_i|
\]</span></p>
<p>where <span>\(\lambda_i\)</span> are the eigenvalues. It tells us the long-term behavior of repeated applications of <span>\(A\)</span>:</p>
<ul>
<li>If <span>\(\rho(A) &lt; 1\)</span> → powers of <span>\(A\)</span> tend to 0 (stable/decay).</li>
<li>If <span>\(\rho(A) = 1\)</span> → powers neither blow up nor vanish (neutral, may oscillate).</li>
<li>If <span>\(\rho(A) &gt; 1\)</span> → powers diverge (unstable/growth).</li>
</ul>
<section id="set-up-your-lab-67">
<h4 data-anchor-id="set-up-your-lab-67">Set Up Your Lab</h4>
<div id="bc2e59a4" data-execution_count="386">
<div><div id="cb673"><pre><code><span id="cb673-1"><a href="#cb673-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb673-2"><a href="#cb673-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-67">
<h4 data-anchor-id="step-by-step-code-walkthrough-67">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Stable matrix (<span>\(\rho &lt; 1\)</span>)</li>
</ol>
<div id="5432057f" data-execution_count="387">
<div><div id="cb674"><pre><code><span id="cb674-1"><a href="#cb674-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>0.5</span>, <span>0</span>],</span>
<span id="cb674-2"><a href="#cb674-2" aria-hidden="true" tabindex="-1"></a>              [<span>0</span>, <span>0.3</span>]])</span>
<span id="cb674-3"><a href="#cb674-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb674-4"><a href="#cb674-4" aria-hidden="true" tabindex="-1"></a>eigvals <span>=</span> np.linalg.eigvals(A)</span>
<span id="cb674-5"><a href="#cb674-5" aria-hidden="true" tabindex="-1"></a>spectral_radius <span>=</span> <span>max</span>(<span>abs</span>(eigvals))</span>
<span id="cb674-6"><a href="#cb674-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb674-7"><a href="#cb674-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvalues:&#34;</span>, eigvals)</span>
<span id="cb674-8"><a href="#cb674-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Spectral radius:&#34;</span>, spectral_radius)</span>
<span id="cb674-9"><a href="#cb674-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb674-10"><a href="#cb674-10" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;A^10:</span><span>\n</span><span>&#34;</span>, np.linalg.matrix_power(A, <span>10</span>))</span></code></pre></div></div>
<div>
<pre><code>Eigenvalues: [0.5 0.3]
Spectral radius: 0.5
A^10:
 [[9.765625e-04 0.000000e+00]
 [0.000000e+00 5.904900e-06]]</code></pre>
</div>
</div>
<p>All entries shrink toward zero.</p>
<ol start="2" type="1">
<li>Unstable matrix (<span>\(\rho &gt; 1\)</span>)</li>
</ol>
<div id="acd293dd" data-execution_count="388">
<div><div id="cb676"><pre><code><span id="cb676-1"><a href="#cb676-1" aria-hidden="true" tabindex="-1"></a>B <span>=</span> np.array([[<span>1.2</span>, <span>0</span>],</span>
<span id="cb676-2"><a href="#cb676-2" aria-hidden="true" tabindex="-1"></a>              [<span>0</span>, <span>0.9</span>]])</span>
<span id="cb676-3"><a href="#cb676-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb676-4"><a href="#cb676-4" aria-hidden="true" tabindex="-1"></a>eigvals <span>=</span> np.linalg.eigvals(B)</span>
<span id="cb676-5"><a href="#cb676-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvalues:&#34;</span>, eigvals, <span>&#34;Spectral radius:&#34;</span>, <span>max</span>(<span>abs</span>(eigvals)))</span>
<span id="cb676-6"><a href="#cb676-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;B^10:</span><span>\n</span><span>&#34;</span>, np.linalg.matrix_power(B, <span>10</span>))</span></code></pre></div></div>
<div>
<pre><code>Eigenvalues: [1.2 0.9] Spectral radius: 1.2
B^10:
 [[6.19173642 0.        ]
 [0.         0.34867844]]</code></pre>
</div>
</div>
<p>The component along eigenvalue 1.2 grows quickly.</p>
<ol start="3" type="1">
<li>Neutral/oscillatory case (<span>\(\rho = 1\)</span>)</li>
</ol>
<p>90° rotation matrix:</p>
<div id="40e391b1" data-execution_count="389">
<div><div id="cb678"><pre><code><span id="cb678-1"><a href="#cb678-1" aria-hidden="true" tabindex="-1"></a>R <span>=</span> np.array([[<span>0</span>, <span>-</span><span>1</span>],</span>
<span id="cb678-2"><a href="#cb678-2" aria-hidden="true" tabindex="-1"></a>              [<span>1</span>,  <span>0</span>]])</span>
<span id="cb678-3"><a href="#cb678-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb678-4"><a href="#cb678-4" aria-hidden="true" tabindex="-1"></a>eigvals <span>=</span> np.linalg.eigvals(R)</span>
<span id="cb678-5"><a href="#cb678-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvalues:&#34;</span>, eigvals)</span>
<span id="cb678-6"><a href="#cb678-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Spectral radius:&#34;</span>, <span>max</span>(<span>abs</span>(eigvals)))</span>
<span id="cb678-7"><a href="#cb678-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;R^4:</span><span>\n</span><span>&#34;</span>, np.linalg.matrix_power(R, <span>4</span>))</span></code></pre></div></div>
<div>
<pre><code>Eigenvalues: [0.+1.j 0.-1.j]
Spectral radius: 1.0
R^4:
 [[1 0]
 [0 1]]</code></pre>
</div>
</div>
<p>Eigenvalues are ±i, with modulus 1 → pure oscillation.</p>
<ol start="4" type="1">
<li>Spectral radius with SymPy</li>
</ol>
<div id="6145d4c9" data-execution_count="390">
<div><div id="cb680"><pre><code><span id="cb680-1"><a href="#cb680-1" aria-hidden="true" tabindex="-1"></a>M <span>=</span> Matrix([[<span>2</span>,<span>1</span>],[<span>1</span>,<span>2</span>]])</span>
<span id="cb680-2"><a href="#cb680-2" aria-hidden="true" tabindex="-1"></a>eigs <span>=</span> M.eigenvals()</span>
<span id="cb680-3"><a href="#cb680-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvalues:&#34;</span>, eigs)</span>
<span id="cb680-4"><a href="#cb680-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Spectral radius:&#34;</span>, <span>max</span>(<span>abs</span>(ev) <span>for</span> ev <span>in</span> eigs))</span></code></pre></div></div>
<div>
<pre><code>Eigenvalues: {3: 1, 1: 1}
Spectral radius: 3</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-66">
<h4 data-anchor-id="try-it-yourself-66">Try It Yourself</h4>
<ol type="1">
<li>Build a diagonal matrix with entries 0.8, 1.0, and 1.1. Predict which direction dominates as powers grow.</li>
<li>Apply a random matrix repeatedly to a vector. Does it shrink, grow, or oscillate?</li>
<li>Check if a Markov chain transition matrix always has spectral radius 1.</li>
</ol>
</section>
<section id="the-takeaway-50">
<h4 data-anchor-id="the-takeaway-50">The Takeaway</h4>
<ul>
<li>The spectral radius is the key number that predicts growth, decay, or oscillation.</li>
<li>Long-term stability in dynamical systems is governed entirely by eigenvalue magnitudes.</li>
<li>This connects linear algebra directly to control theory, Markov chains, and differential equations.</li>
</ul>
</section>
</section>
<section id="markov-chains-and-steady-states-probabilities-as-linear-algebra">
<h3 data-anchor-id="markov-chains-and-steady-states-probabilities-as-linear-algebra">69. Markov Chains and Steady States (Probabilities as Linear Algebra)</h3>
<p>A Markov chain is a process that moves between states according to probabilities. The transitions are encoded in a stochastic matrix <span>\(P\)</span>:</p>
<ul>
<li>Each entry <span>\(p_{ij} \geq 0\)</span></li>
<li>Each row sums to 1</li>
</ul>
<p>If we start with a probability vector <span>\(v_0\)</span>, then after <span>\(k\)</span> steps:</p>
<p><span>\[
v_k = v_0 P^k
\]</span></p>
<p>A steady state is a probability vector <span>\(v\)</span> such that <span>\(vP = v\)</span>. It corresponds to eigenvalue <span>\(\lambda = 1\)</span>.</p>
<section id="set-up-your-lab-68">
<h4 data-anchor-id="set-up-your-lab-68">Set Up Your Lab</h4>
<div id="7cde0441" data-execution_count="391">
<div><div id="cb682"><pre><code><span id="cb682-1"><a href="#cb682-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb682-2"><a href="#cb682-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-68">
<h4 data-anchor-id="step-by-step-code-walkthrough-68">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Simple two-state chain</li>
</ol>
<div id="455a26a2" data-execution_count="392">
<div><div id="cb683"><pre><code><span id="cb683-1"><a href="#cb683-1" aria-hidden="true" tabindex="-1"></a>P <span>=</span> np.array([</span>
<span id="cb683-2"><a href="#cb683-2" aria-hidden="true" tabindex="-1"></a>    [<span>0.9</span>, <span>0.1</span>],</span>
<span id="cb683-3"><a href="#cb683-3" aria-hidden="true" tabindex="-1"></a>    [<span>0.5</span>, <span>0.5</span>]</span>
<span id="cb683-4"><a href="#cb683-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb683-5"><a href="#cb683-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb683-6"><a href="#cb683-6" aria-hidden="true" tabindex="-1"></a>v0 <span>=</span> np.array([<span>1.0</span>, <span>0.0</span>])  <span># start in state 1</span></span>
<span id="cb683-7"><a href="#cb683-7" aria-hidden="true" tabindex="-1"></a><span>for</span> k <span>in</span> [<span>1</span>, <span>2</span>, <span>5</span>, <span>10</span>, <span>50</span>]:</span>
<span id="cb683-8"><a href="#cb683-8" aria-hidden="true" tabindex="-1"></a>    vk <span>=</span> v0 <span>@</span> np.linalg.matrix_power(P, k)</span>
<span id="cb683-9"><a href="#cb683-9" aria-hidden="true" tabindex="-1"></a>    <span>print</span>(<span>f&#34;Step </span><span>{</span>k<span>}</span><span>: </span><span>{</span>vk<span>}</span><span>&#34;</span>)</span></code></pre></div></div>
<div>
<pre><code>Step 1: [0.9 0.1]
Step 2: [0.86 0.14]
Step 5: [0.83504 0.16496]
Step 10: [0.83335081 0.16664919]
Step 50: [0.83333333 0.16666667]</code></pre>
</div>
</div>
<p>The distribution stabilizes as <span>\(k\)</span> increases.</p>
<ol start="2" type="1">
<li>Steady state via eigenvector</li>
</ol>
<p>Find eigenvector for eigenvalue 1:</p>
<div id="cb0a204a" data-execution_count="393">
<div><div id="cb685"><pre><code><span id="cb685-1"><a href="#cb685-1" aria-hidden="true" tabindex="-1"></a>eigvals, eigvecs <span>=</span> np.linalg.eig(P.T)</span>
<span id="cb685-2"><a href="#cb685-2" aria-hidden="true" tabindex="-1"></a>steady_state <span>=</span> eigvecs[:, np.isclose(eigvals, <span>1</span>)]</span>
<span id="cb685-3"><a href="#cb685-3" aria-hidden="true" tabindex="-1"></a>steady_state <span>=</span> steady_state <span>/</span> steady_state.<span>sum</span>()</span>
<span id="cb685-4"><a href="#cb685-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Steady state:&#34;</span>, steady_state.real.flatten())</span></code></pre></div></div>
<div>
<pre><code>Steady state: [0.83333333 0.16666667]</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>SymPy exact check</li>
</ol>
<div id="80b906eb" data-execution_count="394">
<div><div id="cb687"><pre><code><span id="cb687-1"><a href="#cb687-1" aria-hidden="true" tabindex="-1"></a>P_sym <span>=</span> Matrix([[<span>0.9</span>,<span>0.1</span>],[<span>0.5</span>,<span>0.5</span>]])</span>
<span id="cb687-2"><a href="#cb687-2" aria-hidden="true" tabindex="-1"></a>steady <span>=</span> P_sym.eigenvects()</span>
<span id="cb687-3"><a href="#cb687-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigen info:&#34;</span>, steady)</span></code></pre></div></div>
<div>
<pre><code>Eigen info: [(1.00000000000000, 1, [Matrix([
[0.707106781186548],
[0.707106781186547]])]), (0.400000000000000, 1, [Matrix([
[-0.235702260395516],
[  1.17851130197758]])])]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>A 3-state example</li>
</ol>
<div id="b7878813" data-execution_count="395">
<div><div id="cb689"><pre><code><span id="cb689-1"><a href="#cb689-1" aria-hidden="true" tabindex="-1"></a>Q <span>=</span> np.array([</span>
<span id="cb689-2"><a href="#cb689-2" aria-hidden="true" tabindex="-1"></a>    [<span>0.3</span>, <span>0.7</span>, <span>0.0</span>],</span>
<span id="cb689-3"><a href="#cb689-3" aria-hidden="true" tabindex="-1"></a>    [<span>0.2</span>, <span>0.5</span>, <span>0.3</span>],</span>
<span id="cb689-4"><a href="#cb689-4" aria-hidden="true" tabindex="-1"></a>    [<span>0.1</span>, <span>0.2</span>, <span>0.7</span>]</span>
<span id="cb689-5"><a href="#cb689-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb689-6"><a href="#cb689-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb689-7"><a href="#cb689-7" aria-hidden="true" tabindex="-1"></a>eigvals, eigvecs <span>=</span> np.linalg.eig(Q.T)</span>
<span id="cb689-8"><a href="#cb689-8" aria-hidden="true" tabindex="-1"></a>steady <span>=</span> eigvecs[:, np.isclose(eigvals, <span>1</span>)]</span>
<span id="cb689-9"><a href="#cb689-9" aria-hidden="true" tabindex="-1"></a>steady <span>=</span> steady <span>/</span> steady.<span>sum</span>()</span>
<span id="cb689-10"><a href="#cb689-10" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Steady state for Q:&#34;</span>, steady.real.flatten())</span></code></pre></div></div>
<div>
<pre><code>Steady state for Q: [0.17647059 0.41176471 0.41176471]</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-67">
<h4 data-anchor-id="try-it-yourself-67">Try It Yourself</h4>
<ol type="1">
<li>Create a transition matrix where one state is absorbing (e.g., row = [0,0,1]). What happens to the steady state?</li>
<li>Simulate a random walk on 3 states. Does the steady state distribute evenly?</li>
<li>Compare long-run simulation with eigenvector computation.</li>
</ol>
</section>
<section id="the-takeaway-51">
<h4 data-anchor-id="the-takeaway-51">The Takeaway</h4>
<ul>
<li>Markov chains evolve by repeated multiplication with a stochastic matrix.</li>
<li>Steady states are eigenvectors with eigenvalue 1.</li>
<li>This framework powers real applications like PageRank, weather models, and queuing systems.</li>
</ul>
</section>
</section>
<section id="linear-differential-systems-solutions-via-eigen-decomposition">
<h3 data-anchor-id="linear-differential-systems-solutions-via-eigen-decomposition">70. Linear Differential Systems (Solutions via Eigen-Decomposition)</h3>
<p>Linear differential equations often reduce to systems of the form:</p>
<p><span>\[
\frac{d}{dt}x(t) = A x(t)
\]</span></p>
<p>where <span>\(A\)</span> is a matrix and <span>\(x(t)\)</span> is a vector of functions. The solution is given by the matrix exponential:</p>
<p><span>\[
x(t) = e^{At} x(0)
\]</span></p>
<p>If <span>\(A\)</span> is diagonalizable, this becomes simple using eigenvalues and eigenvectors.</p>
<section id="set-up-your-lab-69">
<h4 data-anchor-id="set-up-your-lab-69">Set Up Your Lab</h4>
<div id="025b57c6" data-execution_count="396">
<div><div id="cb691"><pre><code><span id="cb691-1"><a href="#cb691-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb691-2"><a href="#cb691-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sympy <span>import</span> Matrix, exp, symbols</span>
<span id="cb691-3"><a href="#cb691-3" aria-hidden="true" tabindex="-1"></a><span>from</span> scipy.linalg <span>import</span> expm</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-69">
<h4 data-anchor-id="step-by-step-code-walkthrough-69">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Simple system with diagonal matrix</li>
</ol>
<p><span>\[
A = \begin{bmatrix} -1 &amp; 0 \\ 0 &amp; 2 \end{bmatrix}
\]</span></p>
<div id="84603a9b" data-execution_count="397">
<div><div id="cb692"><pre><code><span id="cb692-1"><a href="#cb692-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> Matrix([[<span>-</span><span>1</span>,<span>0</span>],</span>
<span id="cb692-2"><a href="#cb692-2" aria-hidden="true" tabindex="-1"></a>            [<span>0</span>, <span>2</span>]])</span>
<span id="cb692-3"><a href="#cb692-3" aria-hidden="true" tabindex="-1"></a>t <span>=</span> symbols(<span>&#39;t&#39;</span>)</span>
<span id="cb692-4"><a href="#cb692-4" aria-hidden="true" tabindex="-1"></a>expAt <span>=</span> (A<span>*</span>t).exp()</span>
<span id="cb692-5"><a href="#cb692-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;e^</span><span>{At}</span><span> =&#34;</span>)</span>
<span id="cb692-6"><a href="#cb692-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(expAt)</span></code></pre></div></div>
<div>
<pre><code>e^{At} =
Matrix([[exp(-t), 0], [0, exp(2*t)]])</code></pre>
</div>
</div>
<p>Solution:</p>
<p><span>\[
x(t) = \begin{bmatrix} e^{-t} &amp; 0 \\ 0 &amp; e^{2t} \end{bmatrix} x(0)
\]</span></p>
<p>One component decays, the other grows.</p>
<ol start="2" type="1">
<li>Non-diagonal example</li>
</ol>
<div id="65e26cf2" data-execution_count="398">
<div><div id="cb694"><pre><code><span id="cb694-1"><a href="#cb694-1" aria-hidden="true" tabindex="-1"></a>B <span>=</span> Matrix([[<span>0</span>,<span>1</span>],</span>
<span id="cb694-2"><a href="#cb694-2" aria-hidden="true" tabindex="-1"></a>            [<span>-</span><span>2</span>,<span>-</span><span>3</span>]])</span>
<span id="cb694-3"><a href="#cb694-3" aria-hidden="true" tabindex="-1"></a>expBt <span>=</span> (B<span>*</span>t).exp()</span>
<span id="cb694-4"><a href="#cb694-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;e^</span><span>{Bt}</span><span> =&#34;</span>)</span>
<span id="cb694-5"><a href="#cb694-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(expBt)</span></code></pre></div></div>
<div>
<pre><code>e^{Bt} =
Matrix([[2*exp(-t) - exp(-2*t), exp(-t) - exp(-2*t)], [-2*exp(-t) + 2*exp(-2*t), -exp(-t) + 2*exp(-2*t)]])</code></pre>
</div>
</div>
<p>Here the solution involves exponentials and possibly sines/cosines (oscillatory behavior).</p>
<ol start="3" type="1">
<li>Numeric computation with SciPy</li>
</ol>
<div id="fe38411c" data-execution_count="399">
<div><div id="cb696"><pre><code><span id="cb696-1"><a href="#cb696-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb696-2"><a href="#cb696-2" aria-hidden="true" tabindex="-1"></a><span>from</span> scipy.linalg <span>import</span> expm</span>
<span id="cb696-3"><a href="#cb696-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb696-4"><a href="#cb696-4" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>-</span><span>1</span>,<span>0</span>],[<span>0</span>,<span>2</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb696-5"><a href="#cb696-5" aria-hidden="true" tabindex="-1"></a>t <span>=</span> <span>1.0</span></span>
<span id="cb696-6"><a href="#cb696-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Matrix exponential e^</span><span>{At}</span><span> at t=1:</span><span>\n</span><span>&#34;</span>, expm(A<span>*</span>t))</span></code></pre></div></div>
<div>
<pre><code>Matrix exponential e^{At} at t=1:
 [[0.36787944 0.        ]
 [0.         7.3890561 ]]</code></pre>
</div>
</div>
<p>This computes <span>\(e^{At}\)</span> numerically.</p>
<ol start="4" type="1">
<li>Simulation of a trajectory</li>
</ol>
<div id="7e2a7903" data-execution_count="400">
<div><div id="cb698"><pre><code><span id="cb698-1"><a href="#cb698-1" aria-hidden="true" tabindex="-1"></a>x0 <span>=</span> np.array([<span>1.0</span>, <span>1.0</span>])</span>
<span id="cb698-2"><a href="#cb698-2" aria-hidden="true" tabindex="-1"></a><span>for</span> t <span>in</span> [<span>0</span>, <span>0.5</span>, <span>1</span>, <span>2</span>]:</span>
<span id="cb698-3"><a href="#cb698-3" aria-hidden="true" tabindex="-1"></a>    xt <span>=</span> expm(A<span>*</span>t) <span>@</span> x0</span>
<span id="cb698-4"><a href="#cb698-4" aria-hidden="true" tabindex="-1"></a>    <span>print</span>(<span>f&#34;x(</span><span>{</span>t<span>}</span><span>) = </span><span>{</span>xt<span>}</span><span>&#34;</span>)</span></code></pre></div></div>
<div>
<pre><code>x(0) = [1. 1.]
x(0.5) = [0.60653066 2.71828183]
x(1) = [0.36787944 7.3890561 ]
x(2) = [ 0.13533528 54.59815003]</code></pre>
</div>
</div>
<p>One coordinate decays, the other explodes with time.</p>
</section>
<section id="try-it-yourself-68">
<h4 data-anchor-id="try-it-yourself-68">Try It Yourself</h4>
<ol type="1">
<li>Solve the system <span>\(\dot{x} = \begin{bmatrix} 0 &amp; 1 \\ -1 &amp; 0 \end{bmatrix}x\)</span>. What kind of motion do you see?</li>
<li>Use SciPy to simulate a system with eigenvalues less than 0. Does it always decay?</li>
<li>Try an unstable system with eigenvalues &gt; 0 and watch how trajectories diverge.</li>
</ol>
</section>
<section id="the-takeaway-52">
<h4 data-anchor-id="the-takeaway-52">The Takeaway</h4>
<ul>
<li>Linear systems <span>\(\dot{x} = Ax\)</span> are solved via the matrix exponential.</li>
<li>Eigenvalues determine stability: negative real parts = stable, positive = unstable, imaginary = oscillations.</li>
<li>This ties linear algebra directly to differential equations and dynamical systems.</li>
</ul>
</section>
</section>
</section>
<section id="chapter-8.-orthogonality-least-squars-and-qr">
<h2 data-anchor-id="chapter-8.-orthogonality-least-squars-and-qr">Chapter 8. Orthogonality, least squars, and QR</h2>
<section id="inner-products-beyond-dot-product-custom-notions-of-angle">
<h3 data-anchor-id="inner-products-beyond-dot-product-custom-notions-of-angle">71. Inner Products Beyond Dot Product (Custom Notions of Angle)</h3>
<p>The dot product is the standard inner product in <span>\(\mathbb{R}^n\)</span>, but linear algebra allows us to define more general inner products that measure length and angle in different ways.</p>
<p>An inner product on a vector space is a function <span>\(\langle u, v \rangle\)</span> that satisfies:</p>
<ol type="1">
<li>Linearity in the first argument.</li>
<li>Symmetry: <span>\(\langle u, v \rangle = \langle v, u \rangle\)</span>.</li>
<li>Positive definiteness: <span>\(\langle v, v \rangle \geq 0\)</span> and equals 0 only if <span>\(v=0\)</span>.</li>
</ol>
<section id="set-up-your-lab-70">
<h4 data-anchor-id="set-up-your-lab-70">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-70">
<h4 data-anchor-id="step-by-step-code-walkthrough-70">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Standard dot product</li>
</ol>
<div id="6ebd434c" data-execution_count="402">
<div><div id="cb701"><pre><code><span id="cb701-1"><a href="#cb701-1" aria-hidden="true" tabindex="-1"></a>u <span>=</span> np.array([<span>1</span>,<span>2</span>,<span>3</span>])</span>
<span id="cb701-2"><a href="#cb701-2" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.array([<span>4</span>,<span>5</span>,<span>6</span>])</span>
<span id="cb701-3"><a href="#cb701-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb701-4"><a href="#cb701-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Dot product:&#34;</span>, np.dot(u,v))</span></code></pre></div></div>

</div>
<p>This is the familiar formula: <span>\(1·4 + 2·5 + 3·6 = 32\)</span>.</p>
<ol start="2" type="1">
<li>Weighted inner product</li>
</ol>
<p>We can define:</p>
<p><span>\[
\langle u, v \rangle_W = u^T W v
\]</span></p>
<p>where <span>\(W\)</span> is a positive definite matrix.</p>
<div id="2295fdeb" data-execution_count="403">
<div><div id="cb703"><pre><code><span id="cb703-1"><a href="#cb703-1" aria-hidden="true" tabindex="-1"></a>W <span>=</span> np.array([[<span>2</span>,<span>0</span>,<span>0</span>],</span>
<span id="cb703-2"><a href="#cb703-2" aria-hidden="true" tabindex="-1"></a>              [<span>0</span>,<span>1</span>,<span>0</span>],</span>
<span id="cb703-3"><a href="#cb703-3" aria-hidden="true" tabindex="-1"></a>              [<span>0</span>,<span>0</span>,<span>3</span>]])</span>
<span id="cb703-4"><a href="#cb703-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb703-5"><a href="#cb703-5" aria-hidden="true" tabindex="-1"></a><span>def</span> weighted_inner(u,v,W):</span>
<span id="cb703-6"><a href="#cb703-6" aria-hidden="true" tabindex="-1"></a>    <span>return</span> u.T <span>@</span> W <span>@</span> v</span>
<span id="cb703-7"><a href="#cb703-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb703-8"><a href="#cb703-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Weighted inner product:&#34;</span>, weighted_inner(u,v,W))</span></code></pre></div></div>
<div>
<pre><code>Weighted inner product: 72</code></pre>
</div>
</div>
<p>Here, some coordinates “count more” than others.</p>
<ol start="3" type="1">
<li>Check symmetry and positivity</li>
</ol>
<div id="a9f8b616" data-execution_count="404">
<div><div id="cb705"><pre><code><span id="cb705-1"><a href="#cb705-1" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;⟨u,v⟩ == ⟨v,u⟩ ?&#34;</span>, weighted_inner(u,v,W) <span>==</span> weighted_inner(v,u,W))</span>
<span id="cb705-2"><a href="#cb705-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;⟨u,u⟩ (should be &gt;0):&#34;</span>, weighted_inner(u,u,W))</span></code></pre></div></div>
<div>
<pre><code>⟨u,v⟩ == ⟨v,u⟩ ? True
⟨u,u⟩ (should be &gt;0): 33</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Angle with weighted inner product</li>
</ol>
<p><span>\[
\cos\theta = \frac{\langle u,v \rangle_W}{\|u\|_W \, \|v\|_W}
\]</span></p>
<div id="ddb96cd3" data-execution_count="405">
<div><div id="cb707"><pre><code><span id="cb707-1"><a href="#cb707-1" aria-hidden="true" tabindex="-1"></a><span>def</span> weighted_norm(u,W):</span>
<span id="cb707-2"><a href="#cb707-2" aria-hidden="true" tabindex="-1"></a>    <span>return</span> np.sqrt(weighted_inner(u,u,W))</span>
<span id="cb707-3"><a href="#cb707-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb707-4"><a href="#cb707-4" aria-hidden="true" tabindex="-1"></a>cos_theta <span>=</span> weighted_inner(u,v,W) <span>/</span> (weighted_norm(u,W) <span>*</span> weighted_norm(v,W))</span>
<span id="cb707-5"><a href="#cb707-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Cosine of angle (weighted):&#34;</span>, cos_theta)</span></code></pre></div></div>
<div>
<pre><code>Cosine of angle (weighted): 0.97573875381809</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Custom example: correlation inner product</li>
</ol>
<p>For statistics, an inner product can be defined as covariance or correlation. Example with mean-centered vectors:</p>
<div id="306d54b4" data-execution_count="406">
<div><div id="cb709"><pre><code><span id="cb709-1"><a href="#cb709-1" aria-hidden="true" tabindex="-1"></a>x <span>=</span> np.array([<span>2</span>,<span>4</span>,<span>6</span>])</span>
<span id="cb709-2"><a href="#cb709-2" aria-hidden="true" tabindex="-1"></a>y <span>=</span> np.array([<span>1</span>,<span>3</span>,<span>5</span>])</span>
<span id="cb709-3"><a href="#cb709-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb709-4"><a href="#cb709-4" aria-hidden="true" tabindex="-1"></a>x_centered <span>=</span> x <span>-</span> x.mean()</span>
<span id="cb709-5"><a href="#cb709-5" aria-hidden="true" tabindex="-1"></a>y_centered <span>=</span> y <span>-</span> y.mean()</span>
<span id="cb709-6"><a href="#cb709-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb709-7"><a href="#cb709-7" aria-hidden="true" tabindex="-1"></a>corr_inner <span>=</span> np.dot(x_centered,y_centered)</span>
<span id="cb709-8"><a href="#cb709-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Correlation-style inner product:&#34;</span>, corr_inner)</span></code></pre></div></div>
<div>
<pre><code>Correlation-style inner product: 8.0</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-69">
<h4 data-anchor-id="try-it-yourself-69">Try It Yourself</h4>
<ol type="1">
<li>Define a custom inner product with <span>\(W = \text{diag}(1,10,100)\)</span>. How does it change angles between vectors?</li>
<li>Verify positivity: compute <span>\(\langle v, v \rangle_W\)</span> for a random vector <span>\(v\)</span>.</li>
<li>Compare dot product vs weighted inner product on the same pair of vectors.</li>
</ol>
</section>
<section id="the-takeaway-53">
<h4 data-anchor-id="the-takeaway-53">The Takeaway</h4>
<ul>
<li>Inner products generalize the dot product to new “geometries.”</li>
<li>By changing the weight matrix <span>\(W\)</span>, you change how lengths and angles are measured.</li>
<li>This flexibility is essential in statistics, optimization, and machine learning.</li>
</ul>
</section>
</section>
<section id="orthogonality-and-orthonormal-bases-perpendicular-power">
<h3 data-anchor-id="orthogonality-and-orthonormal-bases-perpendicular-power">72. Orthogonality and Orthonormal Bases (Perpendicular Power)</h3>
<p>Two vectors are orthogonal if their inner product is zero:</p>
<p><span>\[
\langle u, v \rangle = 0
\]</span></p>
<p>If, in addition, each vector has length 1, the set is orthonormal. Orthonormal bases are extremely useful because they simplify computations: projections, decompositions, and coordinate changes all become clean.</p>
<section id="set-up-your-lab-71">
<h4 data-anchor-id="set-up-your-lab-71">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-71">
<h4 data-anchor-id="step-by-step-code-walkthrough-71">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Check orthogonality</li>
</ol>
<div id="a037b3e3" data-execution_count="408">
<div><div id="cb712"><pre><code><span id="cb712-1"><a href="#cb712-1" aria-hidden="true" tabindex="-1"></a>u <span>=</span> np.array([<span>1</span>, <span>-</span><span>1</span>])</span>
<span id="cb712-2"><a href="#cb712-2" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.array([<span>1</span>, <span>1</span>])</span>
<span id="cb712-3"><a href="#cb712-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb712-4"><a href="#cb712-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Dot product:&#34;</span>, np.dot(u,v))</span></code></pre></div></div>

</div>
<p>Since the dot product is 0, they’re orthogonal.</p>
<ol start="2" type="1">
<li>Normalizing vectors</li>
</ol>
<p><span>\[
\hat{u} = \frac{u}{\|u\|}
\]</span></p>
<div id="036d482b" data-execution_count="409">
<div><div id="cb714"><pre><code><span id="cb714-1"><a href="#cb714-1" aria-hidden="true" tabindex="-1"></a><span>def</span> normalize(vec):</span>
<span id="cb714-2"><a href="#cb714-2" aria-hidden="true" tabindex="-1"></a>    <span>return</span> vec <span>/</span> np.linalg.norm(vec)</span>
<span id="cb714-3"><a href="#cb714-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb714-4"><a href="#cb714-4" aria-hidden="true" tabindex="-1"></a>u_norm <span>=</span> normalize(u)</span>
<span id="cb714-5"><a href="#cb714-5" aria-hidden="true" tabindex="-1"></a>v_norm <span>=</span> normalize(v)</span>
<span id="cb714-6"><a href="#cb714-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb714-7"><a href="#cb714-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Normalized u:&#34;</span>, u_norm)</span>
<span id="cb714-8"><a href="#cb714-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Normalized v:&#34;</span>, v_norm)</span></code></pre></div></div>
<div>
<pre><code>Normalized u: [ 0.70710678 -0.70710678]
Normalized v: [0.70710678 0.70710678]</code></pre>
</div>
</div>
<p>Now both have length 1.</p>
<ol start="3" type="1">
<li>Form an orthonormal basis</li>
</ol>
<div id="95a9b9aa" data-execution_count="410">
<div><div id="cb716"><pre><code><span id="cb716-1"><a href="#cb716-1" aria-hidden="true" tabindex="-1"></a>basis <span>=</span> np.column_stack((u_norm, v_norm))</span>
<span id="cb716-2"><a href="#cb716-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Orthonormal basis:</span><span>\n</span><span>&#34;</span>, basis)</span>
<span id="cb716-3"><a href="#cb716-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb716-4"><a href="#cb716-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Check inner products:</span><span>\n</span><span>&#34;</span>, basis.T <span>@</span> basis)</span></code></pre></div></div>
<div>
<pre><code>Orthonormal basis:
 [[ 0.70710678  0.70710678]
 [-0.70710678  0.70710678]]
Check inner products:
 [[ 1.00000000e+00 -2.23711432e-17]
 [-2.23711432e-17  1.00000000e+00]]</code></pre>
</div>
</div>
<p>The result is the identity matrix → perfectly orthonormal.</p>
<ol start="4" type="1">
<li>Apply to coordinates</li>
</ol>
<p>If <span>\(x = [2,3]\)</span>, coordinates in the orthonormal basis are:</p>
<div id="0452785a" data-execution_count="411">
<div><div id="cb718"><pre><code><span id="cb718-1"><a href="#cb718-1" aria-hidden="true" tabindex="-1"></a>x <span>=</span> np.array([<span>2</span>,<span>3</span>])</span>
<span id="cb718-2"><a href="#cb718-2" aria-hidden="true" tabindex="-1"></a>coords <span>=</span> basis.T <span>@</span> x</span>
<span id="cb718-3"><a href="#cb718-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Coordinates in new basis:&#34;</span>, coords)</span>
<span id="cb718-4"><a href="#cb718-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Reconstruction:&#34;</span>, basis <span>@</span> coords)</span></code></pre></div></div>
<div>
<pre><code>Coordinates in new basis: [-0.70710678  3.53553391]
Reconstruction: [2. 3.]</code></pre>
</div>
</div>
<p>It reconstructs exactly.</p>
<ol start="5" type="1">
<li>Random example with QR</li>
</ol>
<p>Any set of linearly independent vectors can be orthonormalized (Gram–Schmidt, or QR decomposition):</p>
<div id="bd09a889" data-execution_count="412">
<div><div id="cb720"><pre><code><span id="cb720-1"><a href="#cb720-1" aria-hidden="true" tabindex="-1"></a>M <span>=</span> np.random.rand(<span>3</span>,<span>3</span>)</span>
<span id="cb720-2"><a href="#cb720-2" aria-hidden="true" tabindex="-1"></a>Q, R <span>=</span> np.linalg.qr(M)</span>
<span id="cb720-3"><a href="#cb720-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Q (orthonormal basis):</span><span>\n</span><span>&#34;</span>, Q)</span>
<span id="cb720-4"><a href="#cb720-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Check Q^T Q = I:</span><span>\n</span><span>&#34;</span>, Q.T <span>@</span> Q)</span></code></pre></div></div>
<div>
<pre><code>Q (orthonormal basis):
 [[-0.37617518  0.91975919 -0.111961  ]
 [-0.82070726 -0.38684608 -0.42046368]
 [-0.430037   -0.06628079  0.90037494]]
Check Q^T Q = I:
 [[1.00000000e+00 5.55111512e-17 5.55111512e-17]
 [5.55111512e-17 1.00000000e+00 3.47849792e-17]
 [5.55111512e-17 3.47849792e-17 1.00000000e+00]]</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-70">
<h4 data-anchor-id="try-it-yourself-70">Try It Yourself</h4>
<ol type="1">
<li>Create two 3D vectors and check if they’re orthogonal.</li>
<li>Normalize them to form an orthonormal set.</li>
<li>Use <code>np.linalg.qr</code> on a 4×3 random matrix and verify that the columns of <span>\(Q\)</span> are orthonormal.</li>
</ol>
</section>
<section id="the-takeaway-54">
<h4 data-anchor-id="the-takeaway-54">The Takeaway</h4>
<ul>
<li>Orthogonality means perpendicularity; orthonormality adds unit length.</li>
<li>Orthonormal bases simplify coordinate systems, making inner products and projections easy.</li>
<li>QR decomposition is the practical tool to generate orthonormal bases in higher dimensions.</li>
</ul>
</section>
</section>
<section id="gramschmidt-process-constructing-orthonormal-bases">
<h3 data-anchor-id="gramschmidt-process-constructing-orthonormal-bases">73. Gram–Schmidt Process (Constructing Orthonormal Bases)</h3>
<p>The Gram–Schmidt process takes a set of linearly independent vectors and turns them into an orthonormal basis. This is crucial for working with subspaces, projections, and numerical stability.</p>
<p>Given vectors <span>\(v_1, v_2, \dots, v_n\)</span>:</p>
<ol type="1">
<li><p>Set <span>\(u_1 = v_1\)</span>.</p></li>
<li><p>Subtract projections to make each new vector orthogonal to the earlier ones:</p>
<p><span>\[
u_k = v_k - \sum_{j=1}^{k-1} \frac{\langle v_k, u_j \rangle}{\langle u_j, u_j \rangle} u_j
\]</span></p></li>
<li><p>Normalize:</p>
<p><span>\[
e_k = \frac{u_k}{\|u_k\|}
\]</span></p></li>
</ol>
<p>The set <span>\(\{e_1, e_2, \dots, e_n\}\)</span> is orthonormal.</p>
<section id="set-up-your-lab-72">
<h4 data-anchor-id="set-up-your-lab-72">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-72">
<h4 data-anchor-id="step-by-step-code-walkthrough-72">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Define vectors</li>
</ol>
<div id="8a5f3246" data-execution_count="414">
<div><div id="cb723"><pre><code><span id="cb723-1"><a href="#cb723-1" aria-hidden="true" tabindex="-1"></a>v1 <span>=</span> np.array([<span>1.0</span>, <span>1.0</span>, <span>0.0</span>])</span>
<span id="cb723-2"><a href="#cb723-2" aria-hidden="true" tabindex="-1"></a>v2 <span>=</span> np.array([<span>1.0</span>, <span>0.0</span>, <span>1.0</span>])</span>
<span id="cb723-3"><a href="#cb723-3" aria-hidden="true" tabindex="-1"></a>v3 <span>=</span> np.array([<span>0.0</span>, <span>1.0</span>, <span>1.0</span>])</span>
<span id="cb723-4"><a href="#cb723-4" aria-hidden="true" tabindex="-1"></a>V <span>=</span> [v1, v2, v3]</span></code></pre></div></div>
</div>
<ol start="2" type="1">
<li>Implement Gram–Schmidt</li>
</ol>
<div id="5034c58e" data-execution_count="415">
<div><div id="cb724"><pre><code><span id="cb724-1"><a href="#cb724-1" aria-hidden="true" tabindex="-1"></a><span>def</span> gram_schmidt(V):</span>
<span id="cb724-2"><a href="#cb724-2" aria-hidden="true" tabindex="-1"></a>    U <span>=</span> []</span>
<span id="cb724-3"><a href="#cb724-3" aria-hidden="true" tabindex="-1"></a>    <span>for</span> v <span>in</span> V:</span>
<span id="cb724-4"><a href="#cb724-4" aria-hidden="true" tabindex="-1"></a>        u <span>=</span> v.copy()</span>
<span id="cb724-5"><a href="#cb724-5" aria-hidden="true" tabindex="-1"></a>        <span>for</span> uj <span>in</span> U:</span>
<span id="cb724-6"><a href="#cb724-6" aria-hidden="true" tabindex="-1"></a>            u <span>-=</span> np.dot(v, uj) <span>/</span> np.dot(uj, uj) <span>*</span> uj</span>
<span id="cb724-7"><a href="#cb724-7" aria-hidden="true" tabindex="-1"></a>        U.append(u)</span>
<span id="cb724-8"><a href="#cb724-8" aria-hidden="true" tabindex="-1"></a>    <span># Normalize</span></span>
<span id="cb724-9"><a href="#cb724-9" aria-hidden="true" tabindex="-1"></a>    E <span>=</span> [u<span>/</span>np.linalg.norm(u) <span>for</span> u <span>in</span> U]</span>
<span id="cb724-10"><a href="#cb724-10" aria-hidden="true" tabindex="-1"></a>    <span>return</span> np.array(E)</span>
<span id="cb724-11"><a href="#cb724-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb724-12"><a href="#cb724-12" aria-hidden="true" tabindex="-1"></a>E <span>=</span> gram_schmidt(V)</span>
<span id="cb724-13"><a href="#cb724-13" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Orthonormal basis:</span><span>\n</span><span>&#34;</span>, E)</span>
<span id="cb724-14"><a href="#cb724-14" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Check orthonormality:</span><span>\n</span><span>&#34;</span>, np.<span>round</span>(E <span>@</span> E.T, <span>6</span>))</span></code></pre></div></div>
<div>
<pre><code>Orthonormal basis:
 [[ 0.70710678  0.70710678  0.        ]
 [ 0.40824829 -0.40824829  0.81649658]
 [-0.57735027  0.57735027  0.57735027]]
Check orthonormality:
 [[1. 0. 0.]
 [0. 1. 0.]
 [0. 0. 1.]]</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Compare with NumPy QR</li>
</ol>
<div id="adac0897" data-execution_count="416">
<div><div id="cb726"><pre><code><span id="cb726-1"><a href="#cb726-1" aria-hidden="true" tabindex="-1"></a>Q, R <span>=</span> np.linalg.qr(np.column_stack(V))</span>
<span id="cb726-2"><a href="#cb726-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;QR-based orthonormal basis:</span><span>\n</span><span>&#34;</span>, Q)</span>
<span id="cb726-3"><a href="#cb726-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Check Q^T Q = I:</span><span>\n</span><span>&#34;</span>, np.<span>round</span>(Q.T <span>@</span> Q, <span>6</span>))</span></code></pre></div></div>
<div>
<pre><code>QR-based orthonormal basis:
 [[-0.70710678  0.40824829 -0.57735027]
 [-0.70710678 -0.40824829  0.57735027]
 [-0.          0.81649658  0.57735027]]
Check Q^T Q = I:
 [[ 1.  0. -0.]
 [ 0.  1. -0.]
 [-0. -0.  1.]]</code></pre>
</div>
</div>
<p>Both methods give orthonormal bases.</p>
<ol start="4" type="1">
<li>Application: projection</li>
</ol>
<p>To project a vector <span>\(x\)</span> onto the span of <span>\(V\)</span>:</p>
<div id="31c8f020" data-execution_count="417">
<div><div id="cb728"><pre><code><span id="cb728-1"><a href="#cb728-1" aria-hidden="true" tabindex="-1"></a>x <span>=</span> np.array([<span>2.0</span>, <span>2.0</span>, <span>2.0</span>])</span>
<span id="cb728-2"><a href="#cb728-2" aria-hidden="true" tabindex="-1"></a>proj <span>=</span> <span>sum</span>((x <span>@</span> e) <span>*</span> e <span>for</span> e <span>in</span> E)</span>
<span id="cb728-3"><a href="#cb728-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Projection of x onto span(V):&#34;</span>, proj)</span></code></pre></div></div>
<div>
<pre><code>Projection of x onto span(V): [2. 2. 2.]</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-71">
<h4 data-anchor-id="try-it-yourself-71">Try It Yourself</h4>
<ol type="1">
<li>Run Gram–Schmidt on two vectors in 2D. Compare with just normalizing and checking orthogonality.</li>
<li>Replace one vector with a linear combination of others. What happens?</li>
<li>Use QR decomposition on a 4×3 random matrix and compare with Gram–Schmidt.</li>
</ol>
</section>
<section id="the-takeaway-55">
<h4 data-anchor-id="the-takeaway-55">The Takeaway</h4>
<ul>
<li>Gram–Schmidt converts arbitrary independent vectors into an orthonormal basis.</li>
<li>Orthonormal bases simplify projections, decompositions, and computations.</li>
<li>In practice, QR decomposition is often used as a numerically stable implementation.</li>
</ul>
</section>
</section>
<section id="orthogonal-projections-onto-subspaces-closest-point-principle">
<h3 data-anchor-id="orthogonal-projections-onto-subspaces-closest-point-principle">74. Orthogonal Projections onto Subspaces (Closest Point Principle)</h3>
<p>Given a subspace spanned by vectors, the orthogonal projection of a vector <span>\(x\)</span> onto the subspace is the point in the subspace that is closest to <span>\(x\)</span>. This is a cornerstone idea in least squares, data fitting, and signal processing.</p>
<section id="formula-recap">
<h4 data-anchor-id="formula-recap">Formula Recap</h4>
<p>If <span>\(Q\)</span> is a matrix with orthonormal columns spanning the subspace, the projection of <span>\(x\)</span> is:</p>
<p><span>\[
\text{proj}(x) = Q Q^T x
\]</span></p>
</section>
<section id="set-up-your-lab-73">
<h4 data-anchor-id="set-up-your-lab-73">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-73">
<h4 data-anchor-id="step-by-step-code-walkthrough-73">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Projection onto a line (1D subspace)</li>
</ol>
<p>Suppose the subspace is spanned by <span>\(u = [1,2]\)</span>.</p>
<div id="086fb728" data-execution_count="419">
<div><div id="cb731"><pre><code><span id="cb731-1"><a href="#cb731-1" aria-hidden="true" tabindex="-1"></a>u <span>=</span> np.array([<span>1.0</span>,<span>2.0</span>])</span>
<span id="cb731-2"><a href="#cb731-2" aria-hidden="true" tabindex="-1"></a>x <span>=</span> np.array([<span>3.0</span>,<span>1.0</span>])</span>
<span id="cb731-3"><a href="#cb731-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb731-4"><a href="#cb731-4" aria-hidden="true" tabindex="-1"></a>u_norm <span>=</span> u <span>/</span> np.linalg.norm(u)</span>
<span id="cb731-5"><a href="#cb731-5" aria-hidden="true" tabindex="-1"></a>proj <span>=</span> np.dot(x, u_norm) <span>*</span> u_norm</span>
<span id="cb731-6"><a href="#cb731-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Projection of x onto span(u):&#34;</span>, proj)</span></code></pre></div></div>
<div>
<pre><code>Projection of x onto span(u): [1. 2.]</code></pre>
</div>
</div>
<p>This gives the closest point to <span>\(x\)</span> along the line spanned by <span>\(u\)</span>.</p>
<ol start="2" type="1">
<li>Projection onto a plane (2D subspace in 3D)</li>
</ol>
<div id="c7c42046" data-execution_count="420">
<div><div id="cb733"><pre><code><span id="cb733-1"><a href="#cb733-1" aria-hidden="true" tabindex="-1"></a>u1 <span>=</span> np.array([<span>1.0</span>,<span>0.0</span>,<span>0.0</span>])</span>
<span id="cb733-2"><a href="#cb733-2" aria-hidden="true" tabindex="-1"></a>u2 <span>=</span> np.array([<span>0.0</span>,<span>1.0</span>,<span>0.0</span>])</span>
<span id="cb733-3"><a href="#cb733-3" aria-hidden="true" tabindex="-1"></a>Q <span>=</span> np.column_stack([u1,u2])   <span># Orthonormal basis for xy-plane</span></span>
<span id="cb733-4"><a href="#cb733-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb733-5"><a href="#cb733-5" aria-hidden="true" tabindex="-1"></a>x <span>=</span> np.array([<span>2.0</span>,<span>3.0</span>,<span>5.0</span>])</span>
<span id="cb733-6"><a href="#cb733-6" aria-hidden="true" tabindex="-1"></a>proj <span>=</span> Q <span>@</span> Q.T <span>@</span> x</span>
<span id="cb733-7"><a href="#cb733-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Projection of x onto xy-plane:&#34;</span>, proj)</span></code></pre></div></div>
<div>
<pre><code>Projection of x onto xy-plane: [2. 3. 0.]</code></pre>
</div>
</div>
<p>Result drops the z-component → projection onto the plane.</p>
<ol start="3" type="1">
<li>General projection using QR</li>
</ol>
<div id="3cdebdd9" data-execution_count="421">
<div><div id="cb735"><pre><code><span id="cb735-1"><a href="#cb735-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>1</span>,<span>1</span>,<span>0</span>],</span>
<span id="cb735-2"><a href="#cb735-2" aria-hidden="true" tabindex="-1"></a>              [<span>0</span>,<span>1</span>,<span>1</span>],</span>
<span id="cb735-3"><a href="#cb735-3" aria-hidden="true" tabindex="-1"></a>              [<span>1</span>,<span>0</span>,<span>1</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb735-4"><a href="#cb735-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb735-5"><a href="#cb735-5" aria-hidden="true" tabindex="-1"></a>Q, R <span>=</span> np.linalg.qr(A)</span>
<span id="cb735-6"><a href="#cb735-6" aria-hidden="true" tabindex="-1"></a>Q <span>=</span> Q[:, :<span>2</span>]   <span># take first 2 independent columns</span></span>
<span id="cb735-7"><a href="#cb735-7" aria-hidden="true" tabindex="-1"></a>x <span>=</span> np.array([<span>2</span>,<span>2</span>,<span>2</span>], dtype<span>=</span><span>float</span>)</span>
<span id="cb735-8"><a href="#cb735-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb735-9"><a href="#cb735-9" aria-hidden="true" tabindex="-1"></a>proj <span>=</span> Q <span>@</span> Q.T <span>@</span> x</span>
<span id="cb735-10"><a href="#cb735-10" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Projection of x onto span(A):&#34;</span>, proj)</span></code></pre></div></div>
<div>
<pre><code>Projection of x onto span(A): [2.66666667 1.33333333 1.33333333]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Visualization (2D case)</li>
</ol>
<div id="3356fbdc" data-execution_count="422">
<div><div id="cb737"><pre><code><span id="cb737-1"><a href="#cb737-1" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span>
<span id="cb737-2"><a href="#cb737-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb737-3"><a href="#cb737-3" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>,<span>0</span>,x[<span>0</span>],x[<span>1</span>],angles<span>=</span><span>&#39;xy&#39;</span>,scale_units<span>=</span><span>&#39;xy&#39;</span>,scale<span>=</span><span>1</span>,color<span>=</span><span>&#39;red&#39;</span>,label<span>=</span><span>&#34;x&#34;</span>)</span>
<span id="cb737-4"><a href="#cb737-4" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>,<span>0</span>,proj[<span>0</span>],proj[<span>1</span>],angles<span>=</span><span>&#39;xy&#39;</span>,scale_units<span>=</span><span>&#39;xy&#39;</span>,scale<span>=</span><span>1</span>,color<span>=</span><span>&#39;blue&#39;</span>,label<span>=</span><span>&#34;Projection&#34;</span>)</span>
<span id="cb737-5"><a href="#cb737-5" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span>0</span>,<span>0</span>,u[<span>0</span>],u[<span>1</span>],angles<span>=</span><span>&#39;xy&#39;</span>,scale_units<span>=</span><span>&#39;xy&#39;</span>,scale<span>=</span><span>1</span>,color<span>=</span><span>&#39;green&#39;</span>,label<span>=</span><span>&#34;Subspace&#34;</span>)</span>
<span id="cb737-6"><a href="#cb737-6" aria-hidden="true" tabindex="-1"></a>plt.axis(<span>&#39;equal&#39;</span>)<span>;</span> plt.grid()<span>;</span> plt.legend()<span>;</span> plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-423-output-1.png" width="590" height="411"/></p>
</figure>
</div>
</div>
</div>
</section>
<section id="try-it-yourself-72">
<h4 data-anchor-id="try-it-yourself-72">Try It Yourself</h4>
<ol type="1">
<li>Project a vector onto the line spanned by <span>\([2,1]\)</span>.</li>
<li>Project <span>\([1,2,3]\)</span> onto the plane spanned by <span>\([1,0,1]\)</span> and <span>\([0,1,1]\)</span>.</li>
<li>Compare projection via formula <span>\(Q Q^T x\)</span> with manually solving least squares.</li>
</ol>
</section>
<section id="the-takeaway-56">
<h4 data-anchor-id="the-takeaway-56">The Takeaway</h4>
<ul>
<li>Orthogonal projection finds the closest point in a subspace.</li>
<li>Formula <span>\(Q Q^T x\)</span> works perfectly when <span>\(Q\)</span> has orthonormal columns.</li>
<li>Projections are the foundation of least squares, PCA, and many geometric algorithms.</li>
</ul>
</section>
</section>
<section id="least-squares-problems-fit-when-exact-solve-is-impossible">
<h3 data-anchor-id="least-squares-problems-fit-when-exact-solve-is-impossible">75. Least-Squares Problems (Fit When Exact Solve Is Impossible)</h3>
<p>Sometimes a system of equations <span>\(Ax = b\)</span> has no exact solution - usually because it’s overdetermined (more equations than unknowns). In this case, we look for an approximate solution <span>\(x^*\)</span> that minimizes the error:</p>
<p><span>\[
x^* = \arg\min_x \|Ax - b\|^2
\]</span></p>
<p>This is the least-squares solution, which geometrically is the projection of <span>\(b\)</span> onto the column space of <span>\(A\)</span>.</p>
<section id="set-up-your-lab-74">
<h4 data-anchor-id="set-up-your-lab-74">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-74">
<h4 data-anchor-id="step-by-step-code-walkthrough-74">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Overdetermined system</li>
</ol>
<p>3 equations, 2 unknowns:</p>
<div id="8f69aaee" data-execution_count="424">
<div><div id="cb739"><pre><code><span id="cb739-1"><a href="#cb739-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>1</span>,<span>1</span>],</span>
<span id="cb739-2"><a href="#cb739-2" aria-hidden="true" tabindex="-1"></a>              [<span>1</span>,<span>2</span>],</span>
<span id="cb739-3"><a href="#cb739-3" aria-hidden="true" tabindex="-1"></a>              [<span>1</span>,<span>3</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb739-4"><a href="#cb739-4" aria-hidden="true" tabindex="-1"></a>b <span>=</span> np.array([<span>6</span>, <span>0</span>, <span>0</span>], dtype<span>=</span><span>float</span>)</span></code></pre></div></div>
</div>
<ol start="2" type="1">
<li>Solve least squares with NumPy</li>
</ol>
<div id="edfb6240" data-execution_count="425">
<div><div id="cb740"><pre><code><span id="cb740-1"><a href="#cb740-1" aria-hidden="true" tabindex="-1"></a>x_star, residuals, rank, s <span>=</span> np.linalg.lstsq(A, b, rcond<span>=</span><span>None</span>)</span>
<span id="cb740-2"><a href="#cb740-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Least squares solution:&#34;</span>, x_star)</span>
<span id="cb740-3"><a href="#cb740-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Residual norm squared:&#34;</span>, residuals)</span></code></pre></div></div>
<div>
<pre><code>Least squares solution: [ 8. -3.]
Residual norm squared: [6.]</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Compare with normal equations</li>
</ol>
<p><span>\[
A^T A x = A^T b
\]</span></p>
<div id="12303360" data-execution_count="426">
<div><div id="cb742"><pre><code><span id="cb742-1"><a href="#cb742-1" aria-hidden="true" tabindex="-1"></a>x_normal <span>=</span> np.linalg.inv(A.T <span>@</span> A) <span>@</span> (A.T <span>@</span> b)</span>
<span id="cb742-2"><a href="#cb742-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Solution via normal equations:&#34;</span>, x_normal)</span></code></pre></div></div>
<div>
<pre><code>Solution via normal equations: [ 8. -3.]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Geometric picture</li>
</ol>
<p>The least-squares solution projects <span>\(b\)</span> onto the column space of <span>\(A\)</span>:</p>
<div id="56f4051a" data-execution_count="427">
<div><div id="cb744"><pre><code><span id="cb744-1"><a href="#cb744-1" aria-hidden="true" tabindex="-1"></a>proj <span>=</span> A <span>@</span> x_star</span>
<span id="cb744-2"><a href="#cb744-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Projection of b onto Col(A):&#34;</span>, proj)</span>
<span id="cb744-3"><a href="#cb744-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Original b:&#34;</span>, b)</span>
<span id="cb744-4"><a href="#cb744-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Error vector (b - proj):&#34;</span>, b <span>-</span> proj)</span></code></pre></div></div>
<div>
<pre><code>Projection of b onto Col(A): [ 5.  2. -1.]
Original b: [6. 0. 0.]
Error vector (b - proj): [ 1. -2.  1.]</code></pre>
</div>
</div>
<p>The error vector is orthogonal to the column space.</p>
<ol start="5" type="1">
<li>Verify orthogonality condition</li>
</ol>
<p><span>\[
A^T (b - Ax^*) = 0
\]</span></p>
<div id="10b4733e" data-execution_count="428">
<div><div id="cb746"><pre><code><span id="cb746-1"><a href="#cb746-1" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Check orthogonality:&#34;</span>, A.T <span>@</span> (b <span>-</span> A <span>@</span> x_star))</span></code></pre></div></div>
<div>
<pre><code>Check orthogonality: [0. 0.]</code></pre>
</div>
</div>
<p>The result should be (close to) zero.</p>
</section>
<section id="try-it-yourself-73">
<h4 data-anchor-id="try-it-yourself-73">Try It Yourself</h4>
<ol type="1">
<li>Create a taller <span>\(A\)</span> (say 5×2) with random numbers and solve least squares for a random <span>\(b\)</span>.</li>
<li>Compare the residual from <code>np.linalg.lstsq</code> with geometric intuition (projection).</li>
<li>Modify <span>\(b\)</span> so that the system has an exact solution. Check if least squares gives it exactly.</li>
</ol>
</section>
<section id="the-takeaway-57">
<h4 data-anchor-id="the-takeaway-57">The Takeaway</h4>
<ul>
<li>Least-squares finds the best-fit solution when no exact solution exists.</li>
<li>It works by projecting <span>\(b\)</span> onto the column space of <span>\(A\)</span>.</li>
<li>This principle underlies regression, curve fitting, and countless applications in data science.</li>
</ul>
</section>
</section>
<section id="normal-equations-and-geometry-of-residuals-why-it-works">
<h3 data-anchor-id="normal-equations-and-geometry-of-residuals-why-it-works">76. Normal Equations and Geometry of Residuals (Why It Works)</h3>
<p>The least-squares solution can be found by solving the normal equations:</p>
<p><span>\[
A^T A x = A^T b
\]</span></p>
<p>This comes from the condition that the residual vector</p>
<p><span>\[
r = b - Ax
\]</span></p>
<p>is orthogonal to the column space of <span>\(A\)</span>.</p>
<section id="set-up-your-lab-75">
<h4 data-anchor-id="set-up-your-lab-75">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-75">
<h4 data-anchor-id="step-by-step-code-walkthrough-75">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Build an overdetermined system</li>
</ol>
<div id="20d8f734" data-execution_count="430">
<div><div id="cb749"><pre><code><span id="cb749-1"><a href="#cb749-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>1</span>,<span>1</span>],</span>
<span id="cb749-2"><a href="#cb749-2" aria-hidden="true" tabindex="-1"></a>              [<span>1</span>,<span>2</span>],</span>
<span id="cb749-3"><a href="#cb749-3" aria-hidden="true" tabindex="-1"></a>              [<span>1</span>,<span>3</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb749-4"><a href="#cb749-4" aria-hidden="true" tabindex="-1"></a>b <span>=</span> np.array([<span>6</span>, <span>0</span>, <span>0</span>], dtype<span>=</span><span>float</span>)</span></code></pre></div></div>
</div>
<ol start="2" type="1">
<li>Solve least squares via normal equations</li>
</ol>
<div id="f2d9f0e2" data-execution_count="431">
<div><div id="cb750"><pre><code><span id="cb750-1"><a href="#cb750-1" aria-hidden="true" tabindex="-1"></a>ATA <span>=</span> A.T <span>@</span> A</span>
<span id="cb750-2"><a href="#cb750-2" aria-hidden="true" tabindex="-1"></a>ATb <span>=</span> A.T <span>@</span> b</span>
<span id="cb750-3"><a href="#cb750-3" aria-hidden="true" tabindex="-1"></a>x_star <span>=</span> np.linalg.solve(ATA, ATb)</span>
<span id="cb750-4"><a href="#cb750-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb750-5"><a href="#cb750-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Least-squares solution x*:&#34;</span>, x_star)</span></code></pre></div></div>
<div>
<pre><code>Least-squares solution x*: [ 8. -3.]</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Compute residual and check orthogonality</li>
</ol>
<div id="5729da1a" data-execution_count="432">
<div><div id="cb752"><pre><code><span id="cb752-1"><a href="#cb752-1" aria-hidden="true" tabindex="-1"></a>residual <span>=</span> b <span>-</span> A <span>@</span> x_star</span>
<span id="cb752-2"><a href="#cb752-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Residual vector:&#34;</span>, residual)</span>
<span id="cb752-3"><a href="#cb752-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Check A^T r ≈ 0:&#34;</span>, A.T <span>@</span> residual)</span></code></pre></div></div>
<div>
<pre><code>Residual vector: [ 1. -2.  1.]
Check A^T r ≈ 0: [0. 0.]</code></pre>
</div>
</div>
<p>This verifies the residual is perpendicular to the column space of <span>\(A\)</span>.</p>
<ol start="4" type="1">
<li>Compare with NumPy’s least squares solver</li>
</ol>
<div id="bec5c567" data-execution_count="433">
<div><div id="cb754"><pre><code><span id="cb754-1"><a href="#cb754-1" aria-hidden="true" tabindex="-1"></a>x_lstsq, <span>*</span>_ <span>=</span> np.linalg.lstsq(A, b, rcond<span>=</span><span>None</span>)</span>
<span id="cb754-2"><a href="#cb754-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;NumPy lstsq solution:&#34;</span>, x_lstsq)</span></code></pre></div></div>
<div>
<pre><code>NumPy lstsq solution: [ 8. -3.]</code></pre>
</div>
</div>
<p>The solutions should match (within numerical precision).</p>
<ol start="5" type="1">
<li>Geometric picture</li>
</ol>
<ul>
<li><span>\(b\)</span> is a point in <span>\(\mathbb{R}^3\)</span>.</li>
<li><span>\(Ax\)</span> is restricted to lie in the 2D column space of <span>\(A\)</span>.</li>
<li>The least-squares solution picks the <span>\(Ax\)</span> closest to <span>\(b\)</span>.</li>
<li>The error vector <span>\(r = b - Ax^*\)</span> is orthogonal to the subspace.</li>
</ul>
<div id="66ac1fcd" data-execution_count="434">
<div><div id="cb756"><pre><code><span id="cb756-1"><a href="#cb756-1" aria-hidden="true" tabindex="-1"></a>proj <span>=</span> A <span>@</span> x_star</span>
<span id="cb756-2"><a href="#cb756-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Projection of b onto Col(A):&#34;</span>, proj)</span></code></pre></div></div>
<div>
<pre><code>Projection of b onto Col(A): [ 5.  2. -1.]</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-74">
<h4 data-anchor-id="try-it-yourself-74">Try It Yourself</h4>
<ol type="1">
<li>Change <span>\(b\)</span> to <span>\([1,1,1]\)</span>. Solve again and check the residual.</li>
<li>Use a random tall <span>\(A\)</span> (say 6×2) and verify that the residual is always orthogonal.</li>
<li>Compute <span>\(\|r\|\)</span> and see how it changes when you change <span>\(b\)</span>.</li>
</ol>
</section>
<section id="the-takeaway-58">
<h4 data-anchor-id="the-takeaway-58">The Takeaway</h4>
<ul>
<li>Least squares works by making the residual orthogonal to the column space.</li>
<li>Normal equations are the algebraic way to encode this condition.</li>
<li>This orthogonality principle is the geometric heart of least-squares fitting.</li>
</ul>
</section>
</section>
<section id="qr-factorization-stable-least-squares-via-orthogonality">
<h3 data-anchor-id="qr-factorization-stable-least-squares-via-orthogonality">77. QR Factorization (Stable Least Squares via Orthogonality)</h3>
<p>While normal equations solve least squares, they can be numerically unstable if <span>\(A^T A\)</span> is ill-conditioned. A more stable method uses QR factorization:</p>
<p><span>\[
A = Q R
\]</span></p>
<ul>
<li><span>\(Q\)</span>: matrix with orthonormal columns</li>
<li><span>\(R\)</span>: upper triangular matrix</li>
</ul>
<p>Then the least-squares problem reduces to solving:</p>
<p><span>\[
Rx = Q^T b
\]</span></p>
<section id="set-up-your-lab-76">
<h4 data-anchor-id="set-up-your-lab-76">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-76">
<h4 data-anchor-id="step-by-step-code-walkthrough-76">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Overdetermined system</li>
</ol>
<div id="7c13ca99" data-execution_count="436">
<div><div id="cb759"><pre><code><span id="cb759-1"><a href="#cb759-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>1</span>,<span>1</span>],</span>
<span id="cb759-2"><a href="#cb759-2" aria-hidden="true" tabindex="-1"></a>              [<span>1</span>,<span>2</span>],</span>
<span id="cb759-3"><a href="#cb759-3" aria-hidden="true" tabindex="-1"></a>              [<span>1</span>,<span>3</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb759-4"><a href="#cb759-4" aria-hidden="true" tabindex="-1"></a>b <span>=</span> np.array([<span>6</span>, <span>0</span>, <span>0</span>], dtype<span>=</span><span>float</span>)</span></code></pre></div></div>
</div>
<ol start="2" type="1">
<li>QR factorization</li>
</ol>
<div id="a6360d97" data-execution_count="437">
<div><div id="cb760"><pre><code><span id="cb760-1"><a href="#cb760-1" aria-hidden="true" tabindex="-1"></a>Q, R <span>=</span> np.linalg.qr(A)</span>
<span id="cb760-2"><a href="#cb760-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Q (orthonormal basis):</span><span>\n</span><span>&#34;</span>, Q)</span>
<span id="cb760-3"><a href="#cb760-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;R (upper triangular):</span><span>\n</span><span>&#34;</span>, R)</span></code></pre></div></div>
<div>
<pre><code>Q (orthonormal basis):
 [[-5.77350269e-01  7.07106781e-01]
 [-5.77350269e-01 -1.73054947e-16]
 [-5.77350269e-01 -7.07106781e-01]]
R (upper triangular):
 [[-1.73205081 -3.46410162]
 [ 0.         -1.41421356]]</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Solve least squares using QR</li>
</ol>
<div id="d5f20abe" data-execution_count="438">
<div><div id="cb762"><pre><code><span id="cb762-1"><a href="#cb762-1" aria-hidden="true" tabindex="-1"></a>y <span>=</span> Q.T <span>@</span> b</span>
<span id="cb762-2"><a href="#cb762-2" aria-hidden="true" tabindex="-1"></a>x_star <span>=</span> np.linalg.solve(R[:<span>2</span>,:], y[:<span>2</span>])  <span># only top rows matter</span></span>
<span id="cb762-3"><a href="#cb762-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Least squares solution via QR:&#34;</span>, x_star)</span></code></pre></div></div>
<div>
<pre><code>Least squares solution via QR: [ 8. -3.]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Compare with NumPy’s <code>lstsq</code></li>
</ol>
<div id="2d1664dd" data-execution_count="439">
<div><div id="cb764"><pre><code><span id="cb764-1"><a href="#cb764-1" aria-hidden="true" tabindex="-1"></a>x_lstsq, <span>*</span>_ <span>=</span> np.linalg.lstsq(A, b, rcond<span>=</span><span>None</span>)</span>
<span id="cb764-2"><a href="#cb764-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;NumPy lstsq:&#34;</span>, x_lstsq)</span></code></pre></div></div>

</div>
<p>The answers should match closely.</p>
<ol start="5" type="1">
<li>Residual check</li>
</ol>
<div id="a3f1538a" data-execution_count="440">
<div><div id="cb766"><pre><code><span id="cb766-1"><a href="#cb766-1" aria-hidden="true" tabindex="-1"></a>residual <span>=</span> b <span>-</span> A <span>@</span> x_star</span>
<span id="cb766-2"><a href="#cb766-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Residual vector:&#34;</span>, residual)</span>
<span id="cb766-3"><a href="#cb766-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Check orthogonality (Q^T r):&#34;</span>, Q.T <span>@</span> residual)</span></code></pre></div></div>
<div>
<pre><code>Residual vector: [ 1. -2.  1.]
Check orthogonality (Q^T r): [0.00000000e+00 3.46109895e-16]</code></pre>
</div>
</div>
<p>Residual is orthogonal to the column space, confirming correctness.</p>
</section>
<section id="try-it-yourself-75">
<h4 data-anchor-id="try-it-yourself-75">Try It Yourself</h4>
<ol type="1">
<li>Solve least squares for a 5×2 random matrix using both normal equations and QR. Compare results.</li>
<li>Check stability by making columns of <span>\(A\)</span> nearly dependent - see if QR behaves better than normal equations.</li>
<li>Compute projection of <span>\(b\)</span> using <span>\(Q Q^T b\)</span> and confirm it equals <span>\(A x^*\)</span>.</li>
</ol>
</section>
<section id="the-takeaway-59">
<h4 data-anchor-id="the-takeaway-59">The Takeaway</h4>
<ul>
<li>QR factorization provides a numerically stable way to solve least squares.</li>
<li>It avoids the instability of normal equations.</li>
<li>In practice, modern solvers (like NumPy’s <code>lstsq</code>) rely on QR or SVD under the hood.</li>
</ul>
</section>
</section>
<section id="orthogonal-matrices-length-preserving-transforms">
<h3 data-anchor-id="orthogonal-matrices-length-preserving-transforms">78. Orthogonal Matrices (Length-Preserving Transforms)</h3>
<p>An orthogonal matrix <span>\(Q\)</span> is a square matrix whose columns (and rows) are orthonormal vectors. Formally:</p>
<p><span>\[
Q^T Q = Q Q^T = I
\]</span></p>
<p>Key properties:</p>
<ul>
<li>Preserves lengths: <span>\(\|Qx\| = \|x\|\)</span></li>
<li>Preserves dot products: <span>\(\langle Qx, Qy \rangle = \langle x, y \rangle\)</span></li>
<li>Determinant is either <span>\(+1\)</span> (rotation) or <span>\(-1\)</span> (reflection)</li>
</ul>
<section id="set-up-your-lab-77">
<h4 data-anchor-id="set-up-your-lab-77">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-77">
<h4 data-anchor-id="step-by-step-code-walkthrough-77">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Construct a simple orthogonal matrix</li>
</ol>
<p>90° rotation in 2D:</p>
<div id="9253571b" data-execution_count="442">
<div><div id="cb769"><pre><code><span id="cb769-1"><a href="#cb769-1" aria-hidden="true" tabindex="-1"></a>Q <span>=</span> np.array([[<span>0</span>, <span>-</span><span>1</span>],</span>
<span id="cb769-2"><a href="#cb769-2" aria-hidden="true" tabindex="-1"></a>              [<span>1</span>,  <span>0</span>]])</span>
<span id="cb769-3"><a href="#cb769-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb769-4"><a href="#cb769-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Q^T Q =</span><span>\n</span><span>&#34;</span>, Q.T <span>@</span> Q)</span></code></pre></div></div>

</div>
<p>Result = identity → confirms orthogonality.</p>
<ol start="2" type="1">
<li>Check length preservation</li>
</ol>
<div id="85611c49" data-execution_count="443">
<div><div id="cb771"><pre><code><span id="cb771-1"><a href="#cb771-1" aria-hidden="true" tabindex="-1"></a>x <span>=</span> np.array([<span>3</span>,<span>4</span>])</span>
<span id="cb771-2"><a href="#cb771-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Original length:&#34;</span>, np.linalg.norm(x))</span>
<span id="cb771-3"><a href="#cb771-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Transformed length:&#34;</span>, np.linalg.norm(Q <span>@</span> x))</span></code></pre></div></div>
<div>
<pre><code>Original length: 5.0
Transformed length: 5.0</code></pre>
</div>
</div>
<p>Both lengths match.</p>
<ol start="3" type="1">
<li>Check dot product preservation</li>
</ol>
<div id="89b4257e" data-execution_count="444">
<div><div id="cb773"><pre><code><span id="cb773-1"><a href="#cb773-1" aria-hidden="true" tabindex="-1"></a>u <span>=</span> np.array([<span>1</span>,<span>0</span>])</span>
<span id="cb773-2"><a href="#cb773-2" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.array([<span>0</span>,<span>1</span>])</span>
<span id="cb773-3"><a href="#cb773-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb773-4"><a href="#cb773-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Dot(u,v):&#34;</span>, np.dot(u,v))</span>
<span id="cb773-5"><a href="#cb773-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Dot(Q u, Q v):&#34;</span>, np.dot(Q <span>@</span> u, Q <span>@</span> v))</span></code></pre></div></div>
<div>
<pre><code>Dot(u,v): 0
Dot(Q u, Q v): 0</code></pre>
</div>
</div>
<p>Dot product is preserved.</p>
<ol start="4" type="1">
<li>Reflection matrix</li>
</ol>
<p>Reflection about the x-axis:</p>
<div id="c9fd18c3" data-execution_count="445">
<div><div id="cb775"><pre><code><span id="cb775-1"><a href="#cb775-1" aria-hidden="true" tabindex="-1"></a>R <span>=</span> np.array([[<span>1</span>,<span>0</span>],</span>
<span id="cb775-2"><a href="#cb775-2" aria-hidden="true" tabindex="-1"></a>              [<span>0</span>,<span>-</span><span>1</span>]])</span>
<span id="cb775-3"><a href="#cb775-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb775-4"><a href="#cb775-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;R^T R =</span><span>\n</span><span>&#34;</span>, R.T <span>@</span> R)</span>
<span id="cb775-5"><a href="#cb775-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Determinant of R:&#34;</span>, np.linalg.det(R))</span></code></pre></div></div>
<div>
<pre><code>R^T R =
 [[1 0]
 [0 1]]
Determinant of R: -1.0</code></pre>
</div>
</div>
<p>Determinant = -1 → reflection.</p>
<ol start="5" type="1">
<li>Random orthogonal matrix via QR</li>
</ol>
<div id="6438b79c" data-execution_count="446">
<div><div id="cb777"><pre><code><span id="cb777-1"><a href="#cb777-1" aria-hidden="true" tabindex="-1"></a>M <span>=</span> np.random.rand(<span>3</span>,<span>3</span>)</span>
<span id="cb777-2"><a href="#cb777-2" aria-hidden="true" tabindex="-1"></a>Q, _ <span>=</span> np.linalg.qr(M)</span>
<span id="cb777-3"><a href="#cb777-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Q (random orthogonal):</span><span>\n</span><span>&#34;</span>, Q)</span>
<span id="cb777-4"><a href="#cb777-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Check Q^T Q ≈ I:</span><span>\n</span><span>&#34;</span>, np.<span>round</span>(Q.T <span>@</span> Q, <span>6</span>))</span></code></pre></div></div>
<div>
<pre><code>Q (random orthogonal):
 [[-0.59472353  0.03725157 -0.80306677]
 [-0.61109913 -0.67000966  0.42147943]
 [-0.52236172  0.74141714  0.42123492]]
Check Q^T Q ≈ I:
 [[ 1.  0. -0.]
 [ 0.  1. -0.]
 [-0. -0.  1.]]</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-76">
<h4 data-anchor-id="try-it-yourself-76">Try It Yourself</h4>
<ol type="1">
<li>Build a 2D rotation matrix for 45°. Verify it’s orthogonal.</li>
<li>Check whether scaling matrices (e.g., <span>\(\text{diag}(2,1)\)</span>) are orthogonal. Why or why not?</li>
<li>Generate a random orthogonal matrix with <code>np.linalg.qr</code> and test its determinant.</li>
</ol>
</section>
<section id="the-takeaway-60">
<h4 data-anchor-id="the-takeaway-60">The Takeaway</h4>
<ul>
<li>Orthogonal matrices are rigid motions: they rotate or reflect without distorting lengths or angles.</li>
<li>They play a key role in numerical stability, geometry, and physics.</li>
<li>Every orthonormal basis corresponds to an orthogonal matrix.</li>
</ul>
</section>
</section>
<section id="fourier-viewpoint-expanding-in-orthogonal-waves">
<h3 data-anchor-id="fourier-viewpoint-expanding-in-orthogonal-waves">79. Fourier Viewpoint (Expanding in Orthogonal Waves)</h3>
<p>The Fourier viewpoint treats functions or signals as combinations of orthogonal waves (sines and cosines). This is just linear algebra: sine and cosine functions form an orthogonal basis, and any signal can be expressed as a linear combination of them.</p>
<section id="formula-recap-1">
<h4 data-anchor-id="formula-recap-1">Formula Recap</h4>
<p>For a discrete signal <span>\(x\)</span>, the Discrete Fourier Transform (DFT) is:</p>
<p><span>\[
X_k = \sum_{n=0}^{N-1} x_n e^{-2\pi i kn / N}, \quad k=0,\dots,N-1
\]</span></p>
<p>The inverse DFT reconstructs the signal. Orthogonality of complex exponentials makes this work.</p>
</section>
<section id="set-up-your-lab-78">
<h4 data-anchor-id="set-up-your-lab-78">Set Up Your Lab</h4>
<div id="e77c2a30" data-execution_count="447">
<div><div id="cb779"><pre><code><span id="cb779-1"><a href="#cb779-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb779-2"><a href="#cb779-2" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-78">
<h4 data-anchor-id="step-by-step-code-walkthrough-78">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Build a simple signal</li>
</ol>
<div id="51fb4f0e" data-execution_count="448">
<div><div id="cb780"><pre><code><span id="cb780-1"><a href="#cb780-1" aria-hidden="true" tabindex="-1"></a>t <span>=</span> np.linspace(<span>0</span>, <span>1</span>, <span>100</span>, endpoint<span>=</span><span>False</span>)</span>
<span id="cb780-2"><a href="#cb780-2" aria-hidden="true" tabindex="-1"></a>signal <span>=</span> np.sin(<span>2</span><span>*</span>np.pi<span>*</span><span>3</span><span>*</span>t) <span>+</span> <span>0.5</span><span>*</span>np.sin(<span>2</span><span>*</span>np.pi<span>*</span><span>5</span><span>*</span>t)</span>
<span id="cb780-3"><a href="#cb780-3" aria-hidden="true" tabindex="-1"></a>plt.plot(t, signal)</span>
<span id="cb780-4"><a href="#cb780-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span>&#34;Signal = sin(3Hz) + 0.5 sin(5Hz)&#34;</span>)</span>
<span id="cb780-5"><a href="#cb780-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span>&#34;Time&#34;</span>)</span>
<span id="cb780-6"><a href="#cb780-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span>&#34;Amplitude&#34;</span>)</span>
<span id="cb780-7"><a href="#cb780-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-449-output-1.png" width="600" height="449"/></p>
</figure>
</div>
</div>
</div>
<ol start="2" type="1">
<li>Compute Fourier transform (DFT)</li>
</ol>
<div id="07c06869" data-execution_count="449">
<div><div id="cb781"><pre><code><span id="cb781-1"><a href="#cb781-1" aria-hidden="true" tabindex="-1"></a>X <span>=</span> np.fft.fft(signal)</span>
<span id="cb781-2"><a href="#cb781-2" aria-hidden="true" tabindex="-1"></a>freqs <span>=</span> np.fft.fftfreq(<span>len</span>(t), d<span>=</span><span>1</span><span>/</span><span>100</span>)  <span># sampling rate = 100Hz</span></span>
<span id="cb781-3"><a href="#cb781-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb781-4"><a href="#cb781-4" aria-hidden="true" tabindex="-1"></a>plt.stem(freqs[:<span>50</span>], np.<span>abs</span>(X[:<span>50</span>]), basefmt<span>=</span><span>&#34; &#34;</span>)</span>
<span id="cb781-5"><a href="#cb781-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span>&#34;Fourier spectrum&#34;</span>)</span>
<span id="cb781-6"><a href="#cb781-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span>&#34;Frequency (Hz)&#34;</span>)</span>
<span id="cb781-7"><a href="#cb781-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span>&#34;Magnitude&#34;</span>)</span>
<span id="cb781-8"><a href="#cb781-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-450-output-1.png" width="585" height="449"/></p>
</figure>
</div>
</div>
</div>
<p>Peaks appear at 3Hz and 5Hz → the frequencies of the original signal.</p>
<ol start="3" type="1">
<li>Reconstruct signal using inverse FFT</li>
</ol>
<div id="e4642538" data-execution_count="450">
<div><div id="cb782"><pre><code><span id="cb782-1"><a href="#cb782-1" aria-hidden="true" tabindex="-1"></a>signal_reconstructed <span>=</span> np.fft.ifft(X).real</span>
<span id="cb782-2"><a href="#cb782-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Reconstruction error:&#34;</span>, np.linalg.norm(signal <span>-</span> signal_reconstructed))</span></code></pre></div></div>
<div>
<pre><code>Reconstruction error: 1.4664679821708477e-15</code></pre>
</div>
</div>
<p>Error is near zero → perfect reconstruction.</p>
<ol start="4" type="1">
<li>Orthogonality check of sinusoids</li>
</ol>
<div id="285e0b74" data-execution_count="451">
<div><div id="cb784"><pre><code><span id="cb784-1"><a href="#cb784-1" aria-hidden="true" tabindex="-1"></a>u <span>=</span> np.sin(<span>2</span><span>*</span>np.pi<span>*</span><span>3</span><span>*</span>t)</span>
<span id="cb784-2"><a href="#cb784-2" aria-hidden="true" tabindex="-1"></a>v <span>=</span> np.sin(<span>2</span><span>*</span>np.pi<span>*</span><span>5</span><span>*</span>t)</span>
<span id="cb784-3"><a href="#cb784-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb784-4"><a href="#cb784-4" aria-hidden="true" tabindex="-1"></a>inner <span>=</span> np.dot(u, v)</span>
<span id="cb784-5"><a href="#cb784-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Inner product of 3Hz and 5Hz sinusoids:&#34;</span>, inner)</span></code></pre></div></div>
<div>
<pre><code>Inner product of 3Hz and 5Hz sinusoids: 1.2961853812498703e-14</code></pre>
</div>
</div>
<p>The result is ≈ 0 → confirms orthogonality.</p>
</section>
<section id="try-it-yourself-77">
<h4 data-anchor-id="try-it-yourself-77">Try It Yourself</h4>
<ol type="1">
<li>Change the frequencies to 7Hz and 9Hz. Do the Fourier peaks move accordingly?</li>
<li>Mix in some noise and check how the spectrum looks.</li>
<li>Try cosine signals instead of sine. Do you still see orthogonality?</li>
</ol>
</section>
<section id="the-takeaway-61">
<h4 data-anchor-id="the-takeaway-61">The Takeaway</h4>
<ul>
<li>Fourier analysis = linear algebra with orthogonal sinusoidal basis functions.</li>
<li>Any signal can be decomposed into orthogonal waves.</li>
<li>This orthogonal viewpoint powers audio, image compression, and signal processing.</li>
</ul>
</section>
</section>
<section id="polynomial-and-multifeature-least-squares-fitting-more-flexibly">
<h3 data-anchor-id="polynomial-and-multifeature-least-squares-fitting-more-flexibly">80. Polynomial and Multifeature Least Squares (Fitting More Flexibly)</h3>
<p>Least squares isn’t limited to straight lines. By adding polynomial or multiple features, we can fit curves and capture more complex relationships. This is the foundation of regression models in data science.</p>
<section id="formula-recap-2">
<h4 data-anchor-id="formula-recap-2">Formula Recap</h4>
<p>Given data <span>\((x_i, y_i)\)</span>, we build a design matrix <span>\(A\)</span>:</p>
<ul>
<li>For polynomial fit of degree <span>\(d\)</span>:</li>
</ul>
<p><span>\[
A = \begin{bmatrix}
1 &amp; x_1 &amp; x_1^2 &amp; \dots &amp; x_1^d \\
1 &amp; x_2 &amp; x_2^2 &amp; \dots &amp; x_2^d \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; x_n &amp; x_n^2 &amp; \dots &amp; x_n^d
\end{bmatrix}
\]</span></p>
<p>Then solve least squares:</p>
<p><span>\[
\hat{c} = \arg\min_c \|Ac - y\|^2
\]</span></p>
</section>
<section id="set-up-your-lab-79">
<h4 data-anchor-id="set-up-your-lab-79">Set Up Your Lab</h4>
<div id="36db2604" data-execution_count="452">
<div><div id="cb786"><pre><code><span id="cb786-1"><a href="#cb786-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb786-2"><a href="#cb786-2" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-79">
<h4 data-anchor-id="step-by-step-code-walkthrough-79">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Generate noisy quadratic data</li>
</ol>
<div id="195fc36b" data-execution_count="453">
<div><div id="cb787"><pre><code><span id="cb787-1"><a href="#cb787-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span>0</span>)</span>
<span id="cb787-2"><a href="#cb787-2" aria-hidden="true" tabindex="-1"></a>x <span>=</span> np.linspace(<span>-</span><span>3</span>, <span>3</span>, <span>30</span>)</span>
<span id="cb787-3"><a href="#cb787-3" aria-hidden="true" tabindex="-1"></a>y_true <span>=</span> <span>1</span> <span>-</span> <span>2</span><span>*</span>x <span>+</span> <span>0.5</span><span>*</span>x<span>**</span><span>2</span></span>
<span id="cb787-4"><a href="#cb787-4" aria-hidden="true" tabindex="-1"></a>y_noisy <span>=</span> y_true <span>+</span> np.random.normal(scale<span>=</span><span>2.0</span>, size<span>=</span>x.shape)</span>
<span id="cb787-5"><a href="#cb787-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb787-6"><a href="#cb787-6" aria-hidden="true" tabindex="-1"></a>plt.scatter(x, y_noisy, label<span>=</span><span>&#34;Noisy data&#34;</span>)</span>
<span id="cb787-7"><a href="#cb787-7" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y_true, <span>&#34;g--&#34;</span>, label<span>=</span><span>&#34;True curve&#34;</span>)</span>
<span id="cb787-8"><a href="#cb787-8" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb787-9"><a href="#cb787-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-454-output-1.png" width="569" height="411"/></p>
</figure>
</div>
</div>
</div>
<ol start="2" type="1">
<li>Build polynomial design matrix (degree 2)</li>
</ol>
<div id="13c91ce0" data-execution_count="454">
<div><div id="cb788"><pre><code><span id="cb788-1"><a href="#cb788-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.column_stack([np.ones_like(x), x, x<span>**</span><span>2</span>])</span>
<span id="cb788-2"><a href="#cb788-2" aria-hidden="true" tabindex="-1"></a>coeffs, <span>*</span>_ <span>=</span> np.linalg.lstsq(A, y_noisy, rcond<span>=</span><span>None</span>)</span>
<span id="cb788-3"><a href="#cb788-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Fitted coefficients:&#34;</span>, coeffs)</span></code></pre></div></div>
<div>
<pre><code>Fitted coefficients: [ 1.15666306 -2.25753954  0.72733812]</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Plot fitted polynomial</li>
</ol>
<div id="44256511" data-execution_count="455">
<div><div id="cb790"><pre><code><span id="cb790-1"><a href="#cb790-1" aria-hidden="true" tabindex="-1"></a>y_fit <span>=</span> A <span>@</span> coeffs</span>
<span id="cb790-2"><a href="#cb790-2" aria-hidden="true" tabindex="-1"></a>plt.scatter(x, y_noisy, label<span>=</span><span>&#34;Noisy data&#34;</span>)</span>
<span id="cb790-3"><a href="#cb790-3" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y_fit, <span>&#34;r-&#34;</span>, label<span>=</span><span>&#34;Fitted quadratic&#34;</span>)</span>
<span id="cb790-4"><a href="#cb790-4" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb790-5"><a href="#cb790-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-456-output-1.png" width="569" height="411"/></p>
</figure>
</div>
</div>
</div>
<ol start="4" type="1">
<li>Higher-degree fit (overfitting demonstration)</li>
</ol>
<div id="bdc0c108" data-execution_count="456">
<div><div id="cb791"><pre><code><span id="cb791-1"><a href="#cb791-1" aria-hidden="true" tabindex="-1"></a>A_high <span>=</span> np.column_stack([x<span>**</span>i <span>for</span> i <span>in</span> <span>range</span>(<span>6</span>)])  <span># degree 5</span></span>
<span id="cb791-2"><a href="#cb791-2" aria-hidden="true" tabindex="-1"></a>coeffs_high, <span>*</span>_ <span>=</span> np.linalg.lstsq(A_high, y_noisy, rcond<span>=</span><span>None</span>)</span>
<span id="cb791-3"><a href="#cb791-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb791-4"><a href="#cb791-4" aria-hidden="true" tabindex="-1"></a>y_fit_high <span>=</span> A_high <span>@</span> coeffs_high</span>
<span id="cb791-5"><a href="#cb791-5" aria-hidden="true" tabindex="-1"></a>plt.scatter(x, y_noisy, label<span>=</span><span>&#34;Noisy data&#34;</span>)</span>
<span id="cb791-6"><a href="#cb791-6" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y_fit_high, <span>&#34;r-&#34;</span>, label<span>=</span><span>&#34;Degree 5 polynomial&#34;</span>)</span>
<span id="cb791-7"><a href="#cb791-7" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y_true, <span>&#34;g--&#34;</span>, label<span>=</span><span>&#34;True curve&#34;</span>)</span>
<span id="cb791-8"><a href="#cb791-8" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb791-9"><a href="#cb791-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-457-output-1.png" width="569" height="411"/></p>
</figure>
</div>
</div>
</div>
<ol start="5" type="1">
<li>Multifeature regression example</li>
</ol>
<p>Suppose we predict <span>\(y\)</span> from features <span>\([x, x^2, \sin(x)]\)</span>:</p>
<div id="3a6a7b6a" data-execution_count="457">
<div><div id="cb792"><pre><code><span id="cb792-1"><a href="#cb792-1" aria-hidden="true" tabindex="-1"></a>A_multi <span>=</span> np.column_stack([np.ones_like(x), x, x<span>**</span><span>2</span>, np.sin(x)])</span>
<span id="cb792-2"><a href="#cb792-2" aria-hidden="true" tabindex="-1"></a>coeffs_multi, <span>*</span>_ <span>=</span> np.linalg.lstsq(A_multi, y_noisy, rcond<span>=</span><span>None</span>)</span>
<span id="cb792-3"><a href="#cb792-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Multi-feature coefficients:&#34;</span>, coeffs_multi)</span></code></pre></div></div>
<div>
<pre><code>Multi-feature coefficients: [ 1.15666306 -2.0492999   0.72733812 -0.65902274]</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-78">
<h4 data-anchor-id="try-it-yourself-78">Try It Yourself</h4>
<ol type="1">
<li>Fit degree 3, 4, 5 polynomials to the same data. Watch how the curve changes.</li>
<li>Add features like <span>\(\cos(x)\)</span> or <span>\(\exp(x)\)</span> - does the fit improve?</li>
<li>Compare training error (fit to noisy data) vs error on new test points.</li>
</ol>
</section>
<section id="the-takeaway-62">
<h4 data-anchor-id="the-takeaway-62">The Takeaway</h4>
<ul>
<li>Least squares can fit polynomials and arbitrary feature combinations.</li>
<li>The design matrix encodes how input variables transform into features.</li>
<li>This is the basis of regression, curve fitting, and many machine learning models.</li>
</ul>
</section>
</section>
</section>
<section id="chapter-9.-svd-pca-and-conditioning">
<h2 data-anchor-id="chapter-9.-svd-pca-and-conditioning">Chapter 9. SVD, PCA, and Conditioning</h2>
<section id="singular-values-and-svd-universal-factorization">
<h3 data-anchor-id="singular-values-and-svd-universal-factorization">81. Singular Values and SVD (Universal Factorization)</h3>
<p>The Singular Value Decomposition (SVD) is one of the most powerful results in linear algebra. It says any <span>\(m \times n\)</span> matrix <span>\(A\)</span> can be factored as:</p>
<p><span>\[
A = U \Sigma V^T
\]</span></p>
<ul>
<li><span>\(U\)</span>: orthogonal <span>\(m \times m\)</span> matrix (left singular vectors)</li>
<li><span>\(\Sigma\)</span>: diagonal <span>\(m \times n\)</span> matrix with nonnegative numbers (singular values)</li>
<li><span>\(V\)</span>: orthogonal <span>\(n \times n\)</span> matrix (right singular vectors)</li>
</ul>
<p>Singular values are always nonnegative and sorted <span>\(\sigma_1 \geq \sigma_2 \geq \dots\)</span>.</p>
<section id="set-up-your-lab-80">
<h4 data-anchor-id="set-up-your-lab-80">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-80">
<h4 data-anchor-id="step-by-step-code-walkthrough-80">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Compute SVD of a matrix</li>
</ol>
<div id="58314483" data-execution_count="459">
<div><div id="cb795"><pre><code><span id="cb795-1"><a href="#cb795-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>3</span>,<span>1</span>,<span>1</span>],</span>
<span id="cb795-2"><a href="#cb795-2" aria-hidden="true" tabindex="-1"></a>              [<span>-</span><span>1</span>,<span>3</span>,<span>1</span>]])</span>
<span id="cb795-3"><a href="#cb795-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb795-4"><a href="#cb795-4" aria-hidden="true" tabindex="-1"></a>U, S, Vt <span>=</span> np.linalg.svd(A, full_matrices<span>=</span><span>True</span>)</span>
<span id="cb795-5"><a href="#cb795-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb795-6"><a href="#cb795-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;U:</span><span>\n</span><span>&#34;</span>, U)</span>
<span id="cb795-7"><a href="#cb795-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Singular values:&#34;</span>, S)</span>
<span id="cb795-8"><a href="#cb795-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;V^T:</span><span>\n</span><span>&#34;</span>, Vt)</span></code></pre></div></div>
<div>
<pre><code>U:
 [[-0.70710678 -0.70710678]
 [-0.70710678  0.70710678]]
Singular values: [3.46410162 3.16227766]
V^T:
 [[-4.08248290e-01 -8.16496581e-01 -4.08248290e-01]
 [-8.94427191e-01  4.47213595e-01  5.27355937e-16]
 [-1.82574186e-01 -3.65148372e-01  9.12870929e-01]]</code></pre>
</div>
</div>
<ul>
<li><code>U</code>: orthogonal basis in input space.</li>
<li><code>S</code>: singular values (as a 1D array).</li>
<li><code>V^T</code>: orthogonal basis in output space.</li>
</ul>
<ol start="2" type="1">
<li>Reconstruct <span>\(A\)</span> from decomposition</li>
</ol>
<div id="d62b5722" data-execution_count="460">
<div><div id="cb797"><pre><code><span id="cb797-1"><a href="#cb797-1" aria-hidden="true" tabindex="-1"></a>Sigma <span>=</span> np.zeros((U.shape[<span>1</span>], Vt.shape[<span>0</span>]))</span>
<span id="cb797-2"><a href="#cb797-2" aria-hidden="true" tabindex="-1"></a>Sigma[:<span>len</span>(S), :<span>len</span>(S)] <span>=</span> np.diag(S)</span>
<span id="cb797-3"><a href="#cb797-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb797-4"><a href="#cb797-4" aria-hidden="true" tabindex="-1"></a>A_reconstructed <span>=</span> U <span>@</span> Sigma <span>@</span> Vt</span>
<span id="cb797-5"><a href="#cb797-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Reconstruction error:&#34;</span>, np.linalg.norm(A <span>-</span> A_reconstructed))</span></code></pre></div></div>
<div>
<pre><code>Reconstruction error: 1.5895974606912448e-15</code></pre>
</div>
</div>
<p>The error should be near zero.</p>
<ol start="3" type="1">
<li>Rank from SVD</li>
</ol>
<p>Number of nonzero singular values = rank of <span>\(A\)</span>.</p>
<div id="8bf70821" data-execution_count="461">
<div><div id="cb799"><pre><code><span id="cb799-1"><a href="#cb799-1" aria-hidden="true" tabindex="-1"></a>rank <span>=</span> np.<span>sum</span>(S <span>&gt;</span> <span>1e-10</span>)</span>
<span id="cb799-2"><a href="#cb799-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rank of A:&#34;</span>, rank)</span></code></pre></div></div>

</div>
<ol start="4" type="1">
<li>Geometry: effect of <span>\(A\)</span></li>
</ol>
<p>SVD says:</p>
<ol type="1">
<li><span>\(V\)</span> rotates input space.</li>
<li><span>\(\Sigma\)</span> scales along orthogonal directions (by singular values).</li>
<li><span>\(U\)</span> rotates to output space.</li>
</ol>
<p>This explains why SVD works for any matrix (not just square ones).</p>
<ol start="5" type="1">
<li>Low-rank approximation preview</li>
</ol>
<p>Keep only the top singular value(s) → best approximation of <span>\(A\)</span>.</p>
<div id="b1c4c20b" data-execution_count="462">
<div><div id="cb801"><pre><code><span id="cb801-1"><a href="#cb801-1" aria-hidden="true" tabindex="-1"></a>k <span>=</span> <span>1</span></span>
<span id="cb801-2"><a href="#cb801-2" aria-hidden="true" tabindex="-1"></a>A_approx <span>=</span> np.outer(U[:,<span>0</span>], Vt[<span>0</span>]) <span>*</span> S[<span>0</span>]</span>
<span id="cb801-3"><a href="#cb801-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rank-1 approximation:</span><span>\n</span><span>&#34;</span>, A_approx)</span></code></pre></div></div>
<div>
<pre><code>Rank-1 approximation:
 [[1. 2. 1.]
 [1. 2. 1.]]</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-79">
<h4 data-anchor-id="try-it-yourself-79">Try It Yourself</h4>
<ol type="1">
<li>Compute SVD for a random 5×3 matrix. Check if <span>\(U\)</span> and <span>\(V\)</span> are orthogonal.</li>
<li>Compare singular values of a diagonal matrix vs a rotation matrix.</li>
<li>Zero out small singular values and see how much of <span>\(A\)</span> is preserved.</li>
</ol>
</section>
<section id="the-takeaway-63">
<h4 data-anchor-id="the-takeaway-63">The Takeaway</h4>
<ul>
<li>SVD factorizes any matrix into rotations and scalings.</li>
<li>Singular values reveal rank and strength of directions.</li>
<li>It’s the universal tool of numerical linear algebra: the backbone of PCA, compression, and stability analysis.</li>
</ul>
</section>
</section>
<section id="geometry-of-svd-rotations-stretching">
<h3 data-anchor-id="geometry-of-svd-rotations-stretching">82. Geometry of SVD (Rotations + Stretching)</h3>
<p>The Singular Value Decomposition (SVD) has a beautiful geometric interpretation: every matrix is just a combination of two rotations (or reflections) and a stretching.</p>
<p>For <span>\(A = U \Sigma V^T\)</span>:</p>
<ol type="1">
<li><span>\(V^T\)</span>: rotates (or reflects) the input space.</li>
<li><span>\(\Sigma\)</span>: stretches space along orthogonal axes by singular values <span>\(\sigma_i\)</span>.</li>
<li><span>\(U\)</span>: rotates (or reflects) the result into the output space.</li>
</ol>
<p>This turns any linear transformation into a rotation → stretching → rotation pipeline.</p>
<section id="set-up-your-lab-81">
<h4 data-anchor-id="set-up-your-lab-81">Set Up Your Lab</h4>
<div id="25cbcce5" data-execution_count="463">
<div><div id="cb803"><pre><code><span id="cb803-1"><a href="#cb803-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb803-2"><a href="#cb803-2" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-81">
<h4 data-anchor-id="step-by-step-code-walkthrough-81">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Make a 2D matrix</li>
</ol>
<div id="6fca29ba" data-execution_count="464">
<div><div id="cb804"><pre><code><span id="cb804-1"><a href="#cb804-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>2</span>, <span>1</span>],</span>
<span id="cb804-2"><a href="#cb804-2" aria-hidden="true" tabindex="-1"></a>              [<span>1</span>, <span>3</span>]])</span></code></pre></div></div>
</div>
<ol start="2" type="1">
<li>Apply SVD</li>
</ol>
<div id="56205e0f" data-execution_count="465">
<div><div id="cb805"><pre><code><span id="cb805-1"><a href="#cb805-1" aria-hidden="true" tabindex="-1"></a>U, S, Vt <span>=</span> np.linalg.svd(A)</span>
<span id="cb805-2"><a href="#cb805-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb805-3"><a href="#cb805-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;U:</span><span>\n</span><span>&#34;</span>, U)</span>
<span id="cb805-4"><a href="#cb805-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Singular values:&#34;</span>, S)</span>
<span id="cb805-5"><a href="#cb805-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;V^T:</span><span>\n</span><span>&#34;</span>, Vt)</span></code></pre></div></div>
<div>
<pre><code>U:
 [[-0.52573111 -0.85065081]
 [-0.85065081  0.52573111]]
Singular values: [3.61803399 1.38196601]
V^T:
 [[-0.52573111 -0.85065081]
 [-0.85065081  0.52573111]]</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Visualize effect on the unit circle</li>
</ol>
<p>The unit circle is often used to visualize linear transformations.</p>
<div id="66031540" data-execution_count="466">
<div><div id="cb807"><pre><code><span id="cb807-1"><a href="#cb807-1" aria-hidden="true" tabindex="-1"></a>theta <span>=</span> np.linspace(<span>0</span>, <span>2</span><span>*</span>np.pi, <span>200</span>)</span>
<span id="cb807-2"><a href="#cb807-2" aria-hidden="true" tabindex="-1"></a>circle <span>=</span> np.vstack((np.cos(theta), np.sin(theta)))</span>
<span id="cb807-3"><a href="#cb807-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb807-4"><a href="#cb807-4" aria-hidden="true" tabindex="-1"></a>transformed <span>=</span> A <span>@</span> circle</span>
<span id="cb807-5"><a href="#cb807-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb807-6"><a href="#cb807-6" aria-hidden="true" tabindex="-1"></a>plt.plot(circle[<span>0</span>], circle[<span>1</span>], <span>&#39;b--&#39;</span>, label<span>=</span><span>&#34;Unit circle&#34;</span>)</span>
<span id="cb807-7"><a href="#cb807-7" aria-hidden="true" tabindex="-1"></a>plt.plot(transformed[<span>0</span>], transformed[<span>1</span>], <span>&#39;r-&#39;</span>, label<span>=</span><span>&#34;Transformed&#34;</span>)</span>
<span id="cb807-8"><a href="#cb807-8" aria-hidden="true" tabindex="-1"></a>plt.axis(<span>&#34;equal&#34;</span>)</span>
<span id="cb807-9"><a href="#cb807-9" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb807-10"><a href="#cb807-10" aria-hidden="true" tabindex="-1"></a>plt.title(<span>&#34;Action of A on the unit circle&#34;</span>)</span>
<span id="cb807-11"><a href="#cb807-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-467-output-1.png" width="569" height="431"/></p>
</figure>
</div>
</div>
</div>
<p>The circle becomes an ellipse. Its axes align with the singular vectors, and its radii are the singular values.</p>
<ol start="4" type="1">
<li>Compare with decomposition steps</li>
</ol>
<div id="6a855ce5" data-execution_count="467">
<div><div id="cb808"><pre><code><span id="cb808-1"><a href="#cb808-1" aria-hidden="true" tabindex="-1"></a><span># Apply V^T</span></span>
<span id="cb808-2"><a href="#cb808-2" aria-hidden="true" tabindex="-1"></a>step1 <span>=</span> Vt <span>@</span> circle</span>
<span id="cb808-3"><a href="#cb808-3" aria-hidden="true" tabindex="-1"></a><span># Apply Σ</span></span>
<span id="cb808-4"><a href="#cb808-4" aria-hidden="true" tabindex="-1"></a>Sigma <span>=</span> np.diag(S)</span>
<span id="cb808-5"><a href="#cb808-5" aria-hidden="true" tabindex="-1"></a>step2 <span>=</span> Sigma <span>@</span> step1</span>
<span id="cb808-6"><a href="#cb808-6" aria-hidden="true" tabindex="-1"></a><span># Apply U</span></span>
<span id="cb808-7"><a href="#cb808-7" aria-hidden="true" tabindex="-1"></a>step3 <span>=</span> U <span>@</span> step2</span>
<span id="cb808-8"><a href="#cb808-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb808-9"><a href="#cb808-9" aria-hidden="true" tabindex="-1"></a>plt.plot(circle[<span>0</span>], circle[<span>1</span>], <span>&#39;b--&#39;</span>, label<span>=</span><span>&#34;Unit circle&#34;</span>)</span>
<span id="cb808-10"><a href="#cb808-10" aria-hidden="true" tabindex="-1"></a>plt.plot(step3[<span>0</span>], step3[<span>1</span>], <span>&#39;g-&#39;</span>, label<span>=</span><span>&#34;U Σ V^T circle&#34;</span>)</span>
<span id="cb808-11"><a href="#cb808-11" aria-hidden="true" tabindex="-1"></a>plt.axis(<span>&#34;equal&#34;</span>)</span>
<span id="cb808-12"><a href="#cb808-12" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb808-13"><a href="#cb808-13" aria-hidden="true" tabindex="-1"></a>plt.title(<span>&#34;SVD decomposition of transformation&#34;</span>)</span>
<span id="cb808-14"><a href="#cb808-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-468-output-1.png" width="569" height="431"/></p>
</figure>
</div>
</div>
</div>
<p>Both transformed shapes match → confirms SVD’s geometric picture.</p>
</section>
<section id="try-it-yourself-80">
<h4 data-anchor-id="try-it-yourself-80">Try It Yourself</h4>
<ol type="1">
<li>Change <span>\(A\)</span> to a pure shear, like <code>[[1,2],[0,1]]</code>. How does the ellipse look?</li>
<li>Try a diagonal matrix, like <code>[[3,0],[0,1]]</code>. Do the singular vectors match the coordinate axes?</li>
<li>Scale the input circle to a square and see if geometry still works.</li>
</ol>
</section>
<section id="the-takeaway-64">
<h4 data-anchor-id="the-takeaway-64">The Takeaway</h4>
<ul>
<li>SVD = rotate → stretch → rotate.</li>
<li>The unit circle becomes an ellipse: axes = singular vectors, radii = singular values.</li>
<li>This geometric lens makes SVD intuitive and explains why it’s so widely used in data, graphics, and signal processing.</li>
</ul>
</section>
</section>
<section id="relation-to-eigen-decompositions-ata-and-aat">
<h3 data-anchor-id="relation-to-eigen-decompositions-ata-and-aat">83. Relation to Eigen-Decompositions (ATA and AAT)</h3>
<p>Singular values and eigenvalues are closely connected. While eigen-decomposition applies only to square matrices, SVD works for any rectangular matrix. The bridge between them is:</p>
<p><span>\[
A^T A v = \sigma^2 v \quad \text{and} \quad A A^T u = \sigma^2 u
\]</span></p>
<ul>
<li><span>\(v\)</span>: right singular vector (from eigenvectors of <span>\(A^T A\)</span>)</li>
<li><span>\(u\)</span>: left singular vector (from eigenvectors of <span>\(A A^T\)</span>)</li>
<li><span>\(\sigma\)</span>: singular values (square roots of eigenvalues of <span>\(A^T A\)</span> or <span>\(A A^T\)</span>)</li>
</ul>
<section id="set-up-your-lab-82">
<h4 data-anchor-id="set-up-your-lab-82">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-82">
<h4 data-anchor-id="step-by-step-code-walkthrough-82">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Define a rectangular matrix</li>
</ol>
<div id="0da0b156" data-execution_count="469">
<div><div id="cb810"><pre><code><span id="cb810-1"><a href="#cb810-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>2</span>, <span>0</span>],</span>
<span id="cb810-2"><a href="#cb810-2" aria-hidden="true" tabindex="-1"></a>              [<span>1</span>, <span>1</span>],</span>
<span id="cb810-3"><a href="#cb810-3" aria-hidden="true" tabindex="-1"></a>              [<span>0</span>, <span>1</span>]])  <span># shape 3x2</span></span></code></pre></div></div>
</div>
<ol start="2" type="1">
<li>Compute SVD directly</li>
</ol>
<div id="48165fb1" data-execution_count="470">
<div><div id="cb811"><pre><code><span id="cb811-1"><a href="#cb811-1" aria-hidden="true" tabindex="-1"></a>U, S, Vt <span>=</span> np.linalg.svd(A)</span>
<span id="cb811-2"><a href="#cb811-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Singular values:&#34;</span>, S)</span></code></pre></div></div>
<div>
<pre><code>Singular values: [2.30277564 1.30277564]</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Compare with eigenvalues of <span>\(A^T A\)</span></li>
</ol>
<div id="d1a65d7c" data-execution_count="471">
<div><div id="cb813"><pre><code><span id="cb813-1"><a href="#cb813-1" aria-hidden="true" tabindex="-1"></a>ATA <span>=</span> A.T <span>@</span> A</span>
<span id="cb813-2"><a href="#cb813-2" aria-hidden="true" tabindex="-1"></a>eigvals, eigvecs <span>=</span> np.linalg.eig(ATA)</span>
<span id="cb813-3"><a href="#cb813-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb813-4"><a href="#cb813-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvalues of A^T A:&#34;</span>, eigvals)</span>
<span id="cb813-5"><a href="#cb813-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Square roots (sorted):&#34;</span>, np.sqrt(np.sort(eigvals)[::<span>-</span><span>1</span>]))</span></code></pre></div></div>
<div>
<pre><code>Eigenvalues of A^T A: [5.30277564 1.69722436]
Square roots (sorted): [2.30277564 1.30277564]</code></pre>
</div>
</div>
<p>Notice: singular values from SVD = square roots of eigenvalues of <span>\(A^T A\)</span>.</p>
<ol start="4" type="1">
<li>Compare with eigenvalues of <span>\(A A^T\)</span></li>
</ol>
<div id="1e2e32d6" data-execution_count="472">
<div><div id="cb815"><pre><code><span id="cb815-1"><a href="#cb815-1" aria-hidden="true" tabindex="-1"></a>AAT <span>=</span> A <span>@</span> A.T</span>
<span id="cb815-2"><a href="#cb815-2" aria-hidden="true" tabindex="-1"></a>eigvals2, eigvecs2 <span>=</span> np.linalg.eig(AAT)</span>
<span id="cb815-3"><a href="#cb815-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb815-4"><a href="#cb815-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvalues of A A^T:&#34;</span>, eigvals2)</span>
<span id="cb815-5"><a href="#cb815-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Square roots:&#34;</span>, np.sqrt(np.sort(eigvals2)[::<span>-</span><span>1</span>]))</span></code></pre></div></div>
<div>
<pre><code>Eigenvalues of A A^T: [ 5.30277564e+00  1.69722436e+00 -2.01266546e-17]
Square roots: [2.30277564 1.30277564        nan]</code></pre>
</div>
<div>
<pre><code>/var/folders/_g/lq_pglm508df70x751kkxrl80000gp/T/ipykernel_31637/436251338.py:5: RuntimeWarning: invalid value encountered in sqrt
  print(&#34;Square roots:&#34;, np.sqrt(np.sort(eigvals2)[::-1]))</code></pre>
</div>
</div>
<p>They match too → confirming the relationship.</p>
<ol start="5" type="1">
<li>Verify singular vectors</li>
</ol>
<ul>
<li>Right singular vectors (<span>\(V\)</span>) = eigenvectors of <span>\(A^T A\)</span>.</li>
<li>Left singular vectors (<span>\(U\)</span>) = eigenvectors of <span>\(A A^T\)</span>.</li>
</ul>
<div id="6dcf66b9" data-execution_count="473">
<div><div id="cb818"><pre><code><span id="cb818-1"><a href="#cb818-1" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Right singular vectors (V):</span><span>\n</span><span>&#34;</span>, Vt.T)</span>
<span id="cb818-2"><a href="#cb818-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvectors of A^T A:</span><span>\n</span><span>&#34;</span>, eigvecs)</span>
<span id="cb818-3"><a href="#cb818-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb818-4"><a href="#cb818-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Left singular vectors (U):</span><span>\n</span><span>&#34;</span>, U)</span>
<span id="cb818-5"><a href="#cb818-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvectors of A A^T:</span><span>\n</span><span>&#34;</span>, eigvecs2)</span></code></pre></div></div>
<div>
<pre><code>Right singular vectors (V):
 [[-0.95709203  0.28978415]
 [-0.28978415 -0.95709203]]
Eigenvectors of A^T A:
 [[ 0.95709203 -0.28978415]
 [ 0.28978415  0.95709203]]
Left singular vectors (U):
 [[-0.83125078  0.44487192  0.33333333]
 [-0.54146663 -0.51222011 -0.66666667]
 [-0.12584124 -0.73465607  0.66666667]]
Eigenvectors of A A^T:
 [[-0.83125078  0.44487192  0.33333333]
 [-0.54146663 -0.51222011 -0.66666667]
 [-0.12584124 -0.73465607  0.66666667]]</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-81">
<h4 data-anchor-id="try-it-yourself-81">Try It Yourself</h4>
<ol type="1">
<li>Try a square symmetric matrix and compare SVD with eigen-decomposition. Do they match?</li>
<li>For a tall vs wide rectangular matrix, check whether <span>\(U\)</span> and <span>\(V\)</span> differ.</li>
<li>Compute eigenvalues manually with <code>np.linalg.eig</code> for a random <span>\(A\)</span> and confirm singular values.</li>
</ol>
</section>
<section id="the-takeaway-65">
<h4 data-anchor-id="the-takeaway-65">The Takeaway</h4>
<ul>
<li>Singular values = square roots of eigenvalues of <span>\(A^T A\)</span> (or <span>\(A A^T\)</span>).</li>
<li>Right singular vectors = eigenvectors of <span>\(A^T A\)</span>.</li>
<li>Left singular vectors = eigenvectors of <span>\(A A^T\)</span>.</li>
<li>SVD generalizes eigen-decomposition to all matrices, rectangular or square.</li>
</ul>
</section>
</section>
<section id="low-rank-approximation-best-small-models">
<h3 data-anchor-id="low-rank-approximation-best-small-models">84. Low-Rank Approximation (Best Small Models)</h3>
<p>One of the most useful applications of SVD is low-rank approximation: compressing a large matrix into a smaller one while keeping most of the important information.</p>
<p>The Eckart–Young theorem says: If <span>\(A = U \Sigma V^T\)</span>, then the best rank-<span>\(k\)</span> approximation (in least-squares sense) is:</p>
<p><span>\[
A_k = U_k \Sigma_k V_k^T
\]</span></p>
<p>where we keep only the top <span>\(k\)</span> singular values (and corresponding vectors).</p>
<section id="set-up-your-lab-83">
<h4 data-anchor-id="set-up-your-lab-83">Set Up Your Lab</h4>
<div id="1a39b776" data-execution_count="474">
<div><div id="cb820"><pre><code><span id="cb820-1"><a href="#cb820-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb820-2"><a href="#cb820-2" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-83">
<h4 data-anchor-id="step-by-step-code-walkthrough-83">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Create a matrix with hidden low-rank structure</li>
</ol>
<div id="724a09b4" data-execution_count="475">
<div><div id="cb821"><pre><code><span id="cb821-1"><a href="#cb821-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span>0</span>)</span>
<span id="cb821-2"><a href="#cb821-2" aria-hidden="true" tabindex="-1"></a>U <span>=</span> np.random.randn(<span>50</span>, <span>5</span>)   <span># 50 x 5</span></span>
<span id="cb821-3"><a href="#cb821-3" aria-hidden="true" tabindex="-1"></a>V <span>=</span> np.random.randn(<span>5</span>, <span>30</span>)   <span># 5 x 30</span></span>
<span id="cb821-4"><a href="#cb821-4" aria-hidden="true" tabindex="-1"></a>A <span>=</span> U <span>@</span> V  <span># true rank ≤ 5</span></span></code></pre></div></div>
</div>
<ol start="2" type="1">
<li>Full SVD</li>
</ol>
<div id="f5740317" data-execution_count="476">
<div><div id="cb822"><pre><code><span id="cb822-1"><a href="#cb822-1" aria-hidden="true" tabindex="-1"></a>U, S, Vt <span>=</span> np.linalg.svd(A, full_matrices<span>=</span><span>False</span>)</span>
<span id="cb822-2"><a href="#cb822-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Singular values:&#34;</span>, S[:<span>10</span>])</span></code></pre></div></div>
<div>
<pre><code>Singular values: [4.90672194e+01 4.05935057e+01 3.39228766e+01 3.07883338e+01
 2.29261740e+01 3.97150036e-15 3.97150036e-15 3.97150036e-15
 3.97150036e-15 3.97150036e-15]</code></pre>
</div>
</div>
<p>Only the first ~5 should be large; the rest close to zero.</p>
<ol start="3" type="1">
<li>Build rank-1 approximation</li>
</ol>
<div id="5176ac50" data-execution_count="477">
<div><div id="cb824"><pre><code><span id="cb824-1"><a href="#cb824-1" aria-hidden="true" tabindex="-1"></a>k <span>=</span> <span>1</span></span>
<span id="cb824-2"><a href="#cb824-2" aria-hidden="true" tabindex="-1"></a>A1 <span>=</span> U[:, :k] <span>@</span> np.diag(S[:k]) <span>@</span> Vt[:k, :]</span>
<span id="cb824-3"><a href="#cb824-3" aria-hidden="true" tabindex="-1"></a>error1 <span>=</span> np.linalg.norm(A <span>-</span> A1)</span>
<span id="cb824-4"><a href="#cb824-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rank-1 approximation error:&#34;</span>, error1)</span></code></pre></div></div>
<div>
<pre><code>Rank-1 approximation error: 65.36149641872869</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Rank-5 approximation (should be almost exact)</li>
</ol>
<div id="2853198b" data-execution_count="478">
<div><div id="cb826"><pre><code><span id="cb826-1"><a href="#cb826-1" aria-hidden="true" tabindex="-1"></a>k <span>=</span> <span>5</span></span>
<span id="cb826-2"><a href="#cb826-2" aria-hidden="true" tabindex="-1"></a>A5 <span>=</span> U[:, :k] <span>@</span> np.diag(S[:k]) <span>@</span> Vt[:k, :]</span>
<span id="cb826-3"><a href="#cb826-3" aria-hidden="true" tabindex="-1"></a>error5 <span>=</span> np.linalg.norm(A <span>-</span> A5)</span>
<span id="cb826-4"><a href="#cb826-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rank-5 approximation error:&#34;</span>, error5)</span></code></pre></div></div>
<div>
<pre><code>Rank-5 approximation error: 5.756573247253659e-14</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Visual comparison (image compression demo)</li>
</ol>
<p>Let’s see it on an image.</p>
<div id="a6058968" data-execution_count="479">
<div><div id="cb828"><pre><code><span id="cb828-1"><a href="#cb828-1" aria-hidden="true" tabindex="-1"></a><span>from</span> sklearn.datasets <span>import</span> load_digits</span>
<span id="cb828-2"><a href="#cb828-2" aria-hidden="true" tabindex="-1"></a>digits <span>=</span> load_digits()</span>
<span id="cb828-3"><a href="#cb828-3" aria-hidden="true" tabindex="-1"></a>img <span>=</span> digits.images[<span>0</span>]  <span># 8x8 grayscale digit</span></span>
<span id="cb828-4"><a href="#cb828-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb828-5"><a href="#cb828-5" aria-hidden="true" tabindex="-1"></a>U, S, Vt <span>=</span> np.linalg.svd(img, full_matrices<span>=</span><span>False</span>)</span>
<span id="cb828-6"><a href="#cb828-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb828-7"><a href="#cb828-7" aria-hidden="true" tabindex="-1"></a><span># Keep only top 2 singular values</span></span>
<span id="cb828-8"><a href="#cb828-8" aria-hidden="true" tabindex="-1"></a>k <span>=</span> <span>2</span></span>
<span id="cb828-9"><a href="#cb828-9" aria-hidden="true" tabindex="-1"></a>img2 <span>=</span> U[:, :k] <span>@</span> np.diag(S[:k]) <span>@</span> Vt[:k, :]</span>
<span id="cb828-10"><a href="#cb828-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb828-11"><a href="#cb828-11" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span>1</span>,<span>2</span>,<span>1</span>)</span>
<span id="cb828-12"><a href="#cb828-12" aria-hidden="true" tabindex="-1"></a>plt.imshow(img, cmap<span>=</span><span>&#34;gray&#34;</span>)</span>
<span id="cb828-13"><a href="#cb828-13" aria-hidden="true" tabindex="-1"></a>plt.title(<span>&#34;Original&#34;</span>)</span>
<span id="cb828-14"><a href="#cb828-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb828-15"><a href="#cb828-15" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span>1</span>,<span>2</span>,<span>2</span>)</span>
<span id="cb828-16"><a href="#cb828-16" aria-hidden="true" tabindex="-1"></a>plt.imshow(img2, cmap<span>=</span><span>&#34;gray&#34;</span>)</span>
<span id="cb828-17"><a href="#cb828-17" aria-hidden="true" tabindex="-1"></a>plt.title(<span>&#34;Rank-2 Approximation&#34;</span>)</span>
<span id="cb828-18"><a href="#cb828-18" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-480-output-1.png" width="558" height="298"/></p>
</figure>
</div>
</div>
</div>
<p>Even with just 2 singular values, the digit shape is recognizable.</p>
</section>
<section id="try-it-yourself-82">
<h4 data-anchor-id="try-it-yourself-82">Try It Yourself</h4>
<ol type="1">
<li>Vary <span>\(k\)</span> in the image example (1, 2, 5, 10). How much detail do you keep?</li>
<li>Compare the approximation error <span>\(\|A - A_k\|\)</span> as <span>\(k\)</span> increases.</li>
<li>Apply low-rank approximation to random noisy data. Does it denoise?</li>
</ol>
</section>
<section id="the-takeaway-66">
<h4 data-anchor-id="the-takeaway-66">The Takeaway</h4>
<ul>
<li>SVD gives the best possible low-rank approximation in terms of error.</li>
<li>By truncating singular values, you compress data while keeping its essential structure.</li>
<li>This is the backbone of image compression, recommender systems, and dimensionality reduction.</li>
</ul>
</section>
</section>
<section id="principal-component-analysis-variance-and-directions">
<h3 data-anchor-id="principal-component-analysis-variance-and-directions">85. Principal Component Analysis (Variance and Directions)</h3>
<p>Principal Component Analysis (PCA) is one of the most important applications of SVD. It finds the directions (principal components) where data varies the most, and projects the data onto them to reduce dimensionality while preserving as much information as possible.</p>
<p>Mathematically:</p>
<ol type="1">
<li>Center the data (subtract the mean).</li>
<li>Compute covariance matrix <span>\(C = \frac{1}{n} X^T X\)</span>.</li>
<li>Eigenvectors of <span>\(C\)</span> = principal directions.</li>
<li>Eigenvalues = variance explained.</li>
<li>Equivalently: PCA = SVD of centered data matrix.</li>
</ol>
<section id="set-up-your-lab-84">
<h4 data-anchor-id="set-up-your-lab-84">Set Up Your Lab</h4>
<div id="a86d5597" data-execution_count="480">
<div><div id="cb829"><pre><code><span id="cb829-1"><a href="#cb829-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb829-2"><a href="#cb829-2" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span>
<span id="cb829-3"><a href="#cb829-3" aria-hidden="true" tabindex="-1"></a><span>from</span> sklearn.datasets <span>import</span> load_digits</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-84">
<h4 data-anchor-id="step-by-step-code-walkthrough-84">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Generate synthetic 2D data</li>
</ol>
<div id="17aaa593" data-execution_count="481">
<div><div id="cb830"><pre><code><span id="cb830-1"><a href="#cb830-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span>0</span>)</span>
<span id="cb830-2"><a href="#cb830-2" aria-hidden="true" tabindex="-1"></a>X <span>=</span> np.random.randn(<span>200</span>, <span>2</span>) <span>@</span> np.array([[<span>3</span>,<span>1</span>],[<span>1</span>,<span>0.5</span>]])  <span># stretched cloud</span></span>
<span id="cb830-3"><a href="#cb830-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb830-4"><a href="#cb830-4" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:,<span>0</span>], X[:,<span>1</span>], alpha<span>=</span><span>0.3</span>)</span>
<span id="cb830-5"><a href="#cb830-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span>&#34;Original data&#34;</span>)</span>
<span id="cb830-6"><a href="#cb830-6" aria-hidden="true" tabindex="-1"></a>plt.axis(<span>&#34;equal&#34;</span>)</span>
<span id="cb830-7"><a href="#cb830-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-482-output-1.png" width="569" height="431"/></p>
</figure>
</div>
</div>
</div>
<ol start="2" type="1">
<li>Center the data</li>
</ol>
<div id="f3675632" data-execution_count="482">
<div><div id="cb831"><pre><code><span id="cb831-1"><a href="#cb831-1" aria-hidden="true" tabindex="-1"></a>X_centered <span>=</span> X <span>-</span> X.mean(axis<span>=</span><span>0</span>)</span></code></pre></div></div>
</div>
<ol start="3" type="1">
<li>Compute SVD</li>
</ol>
<div id="831e9685" data-execution_count="483">
<div><div id="cb832"><pre><code><span id="cb832-1"><a href="#cb832-1" aria-hidden="true" tabindex="-1"></a>U, S, Vt <span>=</span> np.linalg.svd(X_centered, full_matrices<span>=</span><span>False</span>)</span>
<span id="cb832-2"><a href="#cb832-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Principal directions (V):</span><span>\n</span><span>&#34;</span>, Vt)</span></code></pre></div></div>
<div>
<pre><code>Principal directions (V):
 [[-0.94430098 -0.32908307]
 [ 0.32908307 -0.94430098]]</code></pre>
</div>
</div>
<p>Rows of <code>Vt</code> are the principal components.</p>
<ol start="4" type="1">
<li>Project data onto first component</li>
</ol>
<div id="c305b208" data-execution_count="484">
<div><div id="cb834"><pre><code><span id="cb834-1"><a href="#cb834-1" aria-hidden="true" tabindex="-1"></a>X_pca1 <span>=</span> X_centered <span>@</span> Vt.T[:,<span>0</span>]</span>
<span id="cb834-2"><a href="#cb834-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb834-3"><a href="#cb834-3" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_pca1, np.zeros_like(X_pca1), alpha<span>=</span><span>0.3</span>)</span>
<span id="cb834-4"><a href="#cb834-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span>&#34;Data projected on first principal component&#34;</span>)</span>
<span id="cb834-5"><a href="#cb834-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-485-output-1.png" width="590" height="431"/></p>
</figure>
</div>
</div>
</div>
<p>This collapses data into 1D, keeping the most variance.</p>
<ol start="5" type="1">
<li>Visualize principal axes</li>
</ol>
<div id="fd596086" data-execution_count="485">
<div><div id="cb835"><pre><code><span id="cb835-1"><a href="#cb835-1" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_centered[:,<span>0</span>], X_centered[:,<span>1</span>], alpha<span>=</span><span>0.3</span>)</span>
<span id="cb835-2"><a href="#cb835-2" aria-hidden="true" tabindex="-1"></a><span>for</span> length, vector <span>in</span> <span>zip</span>(S, Vt):</span>
<span id="cb835-3"><a href="#cb835-3" aria-hidden="true" tabindex="-1"></a>    plt.plot([<span>0</span>, vector[<span>0</span>]<span>*</span>length], [<span>0</span>, vector[<span>1</span>]<span>*</span>length], <span>&#39;r-&#39;</span>, linewidth<span>=</span><span>3</span>)</span>
<span id="cb835-4"><a href="#cb835-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span>&#34;Principal components (directions of max variance)&#34;</span>)</span>
<span id="cb835-5"><a href="#cb835-5" aria-hidden="true" tabindex="-1"></a>plt.axis(<span>&#34;equal&#34;</span>)</span>
<span id="cb835-6"><a href="#cb835-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-486-output-1.png" width="578" height="431"/></p>
</figure>
</div>
</div>
</div>
<p>The red arrows show where the data spreads most.</p>
<ol start="6" type="1">
<li>PCA on real data (digits)</li>
</ol>
<div id="b6906858" data-execution_count="486">
<div><div id="cb836"><pre><code><span id="cb836-1"><a href="#cb836-1" aria-hidden="true" tabindex="-1"></a>digits <span>=</span> load_digits()</span>
<span id="cb836-2"><a href="#cb836-2" aria-hidden="true" tabindex="-1"></a>X <span>=</span> digits.data  <span># 1797 samples, 64 features</span></span>
<span id="cb836-3"><a href="#cb836-3" aria-hidden="true" tabindex="-1"></a>X_centered <span>=</span> X <span>-</span> X.mean(axis<span>=</span><span>0</span>)</span>
<span id="cb836-4"><a href="#cb836-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb836-5"><a href="#cb836-5" aria-hidden="true" tabindex="-1"></a>U, S, Vt <span>=</span> np.linalg.svd(X_centered, full_matrices<span>=</span><span>False</span>)</span>
<span id="cb836-6"><a href="#cb836-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb836-7"><a href="#cb836-7" aria-hidden="true" tabindex="-1"></a>explained_variance <span>=</span> (S<span>**</span><span>2</span>) <span>/</span> np.<span>sum</span>(S<span>**</span><span>2</span>)</span>
<span id="cb836-8"><a href="#cb836-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Explained variance ratio (first 5):&#34;</span>, explained_variance[:<span>5</span>])</span></code></pre></div></div>
<div>
<pre><code>Explained variance ratio (first 5): [0.14890594 0.13618771 0.11794594 0.08409979 0.05782415]</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-83">
<h4 data-anchor-id="try-it-yourself-83">Try It Yourself</h4>
<ol type="1">
<li>Reduce digits dataset to 2D using the top 2 components and plot. Do digit clusters separate?</li>
<li>Compare explained variance ratio for top 10 components.</li>
<li>Add noise to data and check if PCA filters it out when projecting to fewer dimensions.</li>
</ol>
</section>
<section id="the-takeaway-67">
<h4 data-anchor-id="the-takeaway-67">The Takeaway</h4>
<ul>
<li>PCA finds directions of maximum variance using SVD.</li>
<li>By projecting onto top components, you compress data with minimal information loss.</li>
<li>PCA is the backbone of dimensionality reduction, visualization, and preprocessing in machine learning.</li>
</ul>
</section>
</section>
<section id="pseudoinverse-moorepenrose-and-solving-ill-posed-systems">
<h3 data-anchor-id="pseudoinverse-moorepenrose-and-solving-ill-posed-systems">86. Pseudoinverse (Moore–Penrose) and Solving Ill-Posed Systems</h3>
<p>The Moore–Penrose pseudoinverse <span>\(A^+\)</span> generalizes the inverse of a matrix. It allows solving systems <span>\(Ax = b\)</span> even when:</p>
<ul>
<li><span>\(A\)</span> is not square, or</li>
<li><span>\(A\)</span> is singular (non-invertible).</li>
</ul>
<p>The solution given by the pseudoinverse is the least-squares solution with minimum norm:</p>
<p><span>\[
x = A^+ b
\]</span></p>
<p>If <span>\(A = U \Sigma V^T\)</span>, then:</p>
<p><span>\[
A^+ = V \Sigma^+ U^T
\]</span></p>
<p>where <span>\(\Sigma^+\)</span> is obtained by taking reciprocals of nonzero singular values.</p>
<section id="set-up-your-lab-85">
<h4 data-anchor-id="set-up-your-lab-85">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-85">
<h4 data-anchor-id="step-by-step-code-walkthrough-85">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Solve an overdetermined system (more equations than unknowns)</li>
</ol>
<div id="440b7514" data-execution_count="488">
<div><div id="cb839"><pre><code><span id="cb839-1"><a href="#cb839-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>1</span>,<span>1</span>],</span>
<span id="cb839-2"><a href="#cb839-2" aria-hidden="true" tabindex="-1"></a>              [<span>1</span>,<span>2</span>],</span>
<span id="cb839-3"><a href="#cb839-3" aria-hidden="true" tabindex="-1"></a>              [<span>1</span>,<span>3</span>]])  <span># 3x2 system</span></span>
<span id="cb839-4"><a href="#cb839-4" aria-hidden="true" tabindex="-1"></a>b <span>=</span> np.array([<span>1</span>,<span>2</span>,<span>2</span>])</span>
<span id="cb839-5"><a href="#cb839-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb839-6"><a href="#cb839-6" aria-hidden="true" tabindex="-1"></a>x_ls, <span>*</span>_ <span>=</span> np.linalg.lstsq(A, b, rcond<span>=</span><span>None</span>)</span>
<span id="cb839-7"><a href="#cb839-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Least-squares solution:&#34;</span>, x_ls)</span></code></pre></div></div>
<div>
<pre><code>Least-squares solution: [0.66666667 0.5       ]</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Compute with pseudoinverse</li>
</ol>
<div id="658fedc6" data-execution_count="489">
<div><div id="cb841"><pre><code><span id="cb841-1"><a href="#cb841-1" aria-hidden="true" tabindex="-1"></a>A_pinv <span>=</span> np.linalg.pinv(A)</span>
<span id="cb841-2"><a href="#cb841-2" aria-hidden="true" tabindex="-1"></a>x_pinv <span>=</span> A_pinv <span>@</span> b</span>
<span id="cb841-3"><a href="#cb841-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Pseudoinverse solution:&#34;</span>, x_pinv)</span></code></pre></div></div>
<div>
<pre><code>Pseudoinverse solution: [0.66666667 0.5       ]</code></pre>
</div>
</div>
<p>Both match → pseudoinverse gives least-squares solution.</p>
<ol start="3" type="1">
<li>Solve an underdetermined system (fewer equations than unknowns)</li>
</ol>
<div id="7e95044d" data-execution_count="490">
<div><div id="cb843"><pre><code><span id="cb843-1"><a href="#cb843-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>1</span>,<span>2</span>,<span>3</span>]])  <span># 1x3</span></span>
<span id="cb843-2"><a href="#cb843-2" aria-hidden="true" tabindex="-1"></a>b <span>=</span> np.array([<span>1</span>])</span>
<span id="cb843-3"><a href="#cb843-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb843-4"><a href="#cb843-4" aria-hidden="true" tabindex="-1"></a>x_pinv <span>=</span> np.linalg.pinv(A) <span>@</span> b</span>
<span id="cb843-5"><a href="#cb843-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Minimum norm solution:&#34;</span>, x_pinv)</span></code></pre></div></div>
<div>
<pre><code>Minimum norm solution: [0.07142857 0.14285714 0.21428571]</code></pre>
</div>
</div>
<p>Here, infinitely many solutions exist. The pseudoinverse picks the one with smallest norm.</p>
<ol start="4" type="1">
<li>Compare with singular matrix</li>
</ol>
<div id="e452326e" data-execution_count="491">
<div><div id="cb845"><pre><code><span id="cb845-1"><a href="#cb845-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>1</span>,<span>2</span>],</span>
<span id="cb845-2"><a href="#cb845-2" aria-hidden="true" tabindex="-1"></a>              [<span>2</span>,<span>4</span>]])  <span># rank deficient</span></span>
<span id="cb845-3"><a href="#cb845-3" aria-hidden="true" tabindex="-1"></a>b <span>=</span> np.array([<span>1</span>,<span>2</span>])</span>
<span id="cb845-4"><a href="#cb845-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb845-5"><a href="#cb845-5" aria-hidden="true" tabindex="-1"></a>x_pinv <span>=</span> np.linalg.pinv(A) <span>@</span> b</span>
<span id="cb845-6"><a href="#cb845-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Solution with pseudoinverse:&#34;</span>, x_pinv)</span></code></pre></div></div>
<div>
<pre><code>Solution with pseudoinverse: [0.2 0.4]</code></pre>
</div>
</div>
<p>Even when <span>\(A\)</span> is singular, pseudoinverse provides a solution.</p>
<ol start="5" type="1">
<li>Manual pseudoinverse via SVD</li>
</ol>
<div id="987376d8" data-execution_count="492">
<div><div id="cb847"><pre><code><span id="cb847-1"><a href="#cb847-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>1</span>,<span>2</span>],</span>
<span id="cb847-2"><a href="#cb847-2" aria-hidden="true" tabindex="-1"></a>              [<span>3</span>,<span>4</span>]])</span>
<span id="cb847-3"><a href="#cb847-3" aria-hidden="true" tabindex="-1"></a>U, S, Vt <span>=</span> np.linalg.svd(A)</span>
<span id="cb847-4"><a href="#cb847-4" aria-hidden="true" tabindex="-1"></a>S_inv <span>=</span> np.zeros((Vt.shape[<span>0</span>], U.shape[<span>0</span>]))</span>
<span id="cb847-5"><a href="#cb847-5" aria-hidden="true" tabindex="-1"></a><span>for</span> i <span>in</span> <span>range</span>(<span>len</span>(S)):</span>
<span id="cb847-6"><a href="#cb847-6" aria-hidden="true" tabindex="-1"></a>    <span>if</span> S[i] <span>&gt;</span> <span>1e-10</span>:</span>
<span id="cb847-7"><a href="#cb847-7" aria-hidden="true" tabindex="-1"></a>        S_inv[i,i] <span>=</span> <span>1</span><span>/</span>S[i]</span>
<span id="cb847-8"><a href="#cb847-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb847-9"><a href="#cb847-9" aria-hidden="true" tabindex="-1"></a>A_pinv_manual <span>=</span> Vt.T <span>@</span> S_inv <span>@</span> U.T</span>
<span id="cb847-10"><a href="#cb847-10" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Manual pseudoinverse:</span><span>\n</span><span>&#34;</span>, A_pinv_manual)</span>
<span id="cb847-11"><a href="#cb847-11" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;NumPy pseudoinverse:</span><span>\n</span><span>&#34;</span>, np.linalg.pinv(A))</span></code></pre></div></div>
<div>
<pre><code>Manual pseudoinverse:
 [[-2.   1. ]
 [ 1.5 -0.5]]
NumPy pseudoinverse:
 [[-2.   1. ]
 [ 1.5 -0.5]]</code></pre>
</div>
</div>
<p>They match.</p>
</section>
<section id="try-it-yourself-84">
<h4 data-anchor-id="try-it-yourself-84">Try It Yourself</h4>
<ol type="1">
<li>Create an overdetermined system with noise and see how pseudoinverse smooths the solution.</li>
<li>Compare pseudoinverse with direct inverse (<code>np.linalg.inv</code>) on a square nonsingular matrix.</li>
<li>Zero out small singular values manually and see how solution changes.</li>
</ol>
</section>
<section id="the-takeaway-68">
<h4 data-anchor-id="the-takeaway-68">The Takeaway</h4>
<ul>
<li>The pseudoinverse solves any linear system, square or not.</li>
<li>It provides the least-squares solution in overdetermined cases and the minimum-norm solution in underdetermined cases.</li>
<li>Built on SVD, it is a cornerstone of regression, optimization, and numerical methods.</li>
</ul>
</section>
</section>
<section id="conditioning-and-sensitivity-how-errors-amplify">
<h3 data-anchor-id="conditioning-and-sensitivity-how-errors-amplify">87. Conditioning and Sensitivity (How Errors Amplify)</h3>
<p>Conditioning tells us how sensitive a system is to small changes. For a linear system <span>\(Ax = b\)</span>:</p>
<ul>
<li>If <span>\(A\)</span> is well-conditioned, small changes in <span>\(b\)</span> or <span>\(A\)</span> → small changes in <span>\(x\)</span>.</li>
<li>If <span>\(A\)</span> is ill-conditioned, tiny changes can cause huge swings in <span>\(x\)</span>.</li>
</ul>
<p>The condition number is defined as:</p>
<p><span>\[
\kappa(A) = \|A\| \cdot \|A^{-1}\|
\]</span></p>
<p>For SVD:</p>
<p><span>\[
\kappa(A) = \frac{\sigma_{\max}}{\sigma_{\min}}
\]</span></p>
<p>where <span>\(\sigma_{\max}\)</span> and <span>\(\sigma_{\min}\)</span> are the largest and smallest singular values.</p>
<ul>
<li>Large <span>\(\kappa(A)\)</span> → unstable system.</li>
<li>Small <span>\(\kappa(A)\)</span> → stable system.</li>
</ul>
<section id="set-up-your-lab-86">
<h4 data-anchor-id="set-up-your-lab-86">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-86">
<h4 data-anchor-id="step-by-step-code-walkthrough-86">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Well-conditioned system</li>
</ol>
<div id="4128faff" data-execution_count="494">
<div><div id="cb850"><pre><code><span id="cb850-1"><a href="#cb850-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>2</span>,<span>0</span>],</span>
<span id="cb850-2"><a href="#cb850-2" aria-hidden="true" tabindex="-1"></a>              [<span>0</span>,<span>1</span>]])</span>
<span id="cb850-3"><a href="#cb850-3" aria-hidden="true" tabindex="-1"></a>b <span>=</span> np.array([<span>1</span>,<span>1</span>])</span>
<span id="cb850-4"><a href="#cb850-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb850-5"><a href="#cb850-5" aria-hidden="true" tabindex="-1"></a>x <span>=</span> np.linalg.solve(A, b)</span>
<span id="cb850-6"><a href="#cb850-6" aria-hidden="true" tabindex="-1"></a>cond <span>=</span> np.linalg.cond(A)</span>
<span id="cb850-7"><a href="#cb850-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Solution:&#34;</span>, x)</span>
<span id="cb850-8"><a href="#cb850-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Condition number:&#34;</span>, cond)</span></code></pre></div></div>
<div>
<pre><code>Solution: [0.5 1. ]
Condition number: 2.0</code></pre>
</div>
</div>
<p>Condition number = ratio of singular values → moderate size.</p>
<ol start="2" type="1">
<li>Ill-conditioned system</li>
</ol>
<div id="e826c157" data-execution_count="495">
<div><div id="cb852"><pre><code><span id="cb852-1"><a href="#cb852-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>1</span>, <span>1.0001</span>],</span>
<span id="cb852-2"><a href="#cb852-2" aria-hidden="true" tabindex="-1"></a>              [<span>1</span>, <span>1.0000</span>]])</span>
<span id="cb852-3"><a href="#cb852-3" aria-hidden="true" tabindex="-1"></a>b <span>=</span> np.array([<span>2</span>,<span>2</span>])</span>
<span id="cb852-4"><a href="#cb852-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb852-5"><a href="#cb852-5" aria-hidden="true" tabindex="-1"></a>x <span>=</span> np.linalg.lstsq(A, b, rcond<span>=</span><span>None</span>)[<span>0</span>]</span>
<span id="cb852-6"><a href="#cb852-6" aria-hidden="true" tabindex="-1"></a>cond <span>=</span> np.linalg.cond(A)</span>
<span id="cb852-7"><a href="#cb852-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Solution:&#34;</span>, x)</span>
<span id="cb852-8"><a href="#cb852-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Condition number:&#34;</span>, cond)</span></code></pre></div></div>
<div>
<pre><code>Solution: [ 2.00000000e+00 -5.73526099e-13]
Condition number: 40002.000075017124</code></pre>
</div>
</div>
<p>Condition number is very large → instability.</p>
<ol start="3" type="1">
<li>Perturb the right-hand side</li>
</ol>
<div id="22ddf53f" data-execution_count="496">
<div><div id="cb854"><pre><code><span id="cb854-1"><a href="#cb854-1" aria-hidden="true" tabindex="-1"></a>b2 <span>=</span> np.array([<span>2</span>, <span>2.001</span>])  <span># tiny change</span></span>
<span id="cb854-2"><a href="#cb854-2" aria-hidden="true" tabindex="-1"></a>x2 <span>=</span> np.linalg.lstsq(A, b2, rcond<span>=</span><span>None</span>)[<span>0</span>]</span>
<span id="cb854-3"><a href="#cb854-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Solution after tiny change:&#34;</span>, x2)</span></code></pre></div></div>
<div>
<pre><code>Solution after tiny change: [ 12.001 -10.   ]</code></pre>
</div>
</div>
<p>The solution changes drastically → shows sensitivity.</p>
<ol start="4" type="1">
<li>Relation to singular values</li>
</ol>
<div id="112eeb2a" data-execution_count="497">
<div><div id="cb856"><pre><code><span id="cb856-1"><a href="#cb856-1" aria-hidden="true" tabindex="-1"></a>U, S, Vt <span>=</span> np.linalg.svd(A)</span>
<span id="cb856-2"><a href="#cb856-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Singular values:&#34;</span>, S)</span>
<span id="cb856-3"><a href="#cb856-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Condition number (SVD):&#34;</span>, S[<span>0</span>]<span>/</span>S[<span>-</span><span>1</span>])</span></code></pre></div></div>
<div>
<pre><code>Singular values: [2.000050e+00 4.999875e-05]
Condition number (SVD): 40002.00007501713</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Scaling experiment</li>
</ol>
<div id="1c3cf41d" data-execution_count="498">
<div><div id="cb858"><pre><code><span id="cb858-1"><a href="#cb858-1" aria-hidden="true" tabindex="-1"></a><span>for</span> scale <span>in</span> [<span>1</span>,<span>1e-2</span>,<span>1e-4</span>,<span>1e-6</span>]:</span>
<span id="cb858-2"><a href="#cb858-2" aria-hidden="true" tabindex="-1"></a>    A <span>=</span> np.array([[<span>1</span>,<span>0</span>],[<span>0</span>,scale]])</span>
<span id="cb858-3"><a href="#cb858-3" aria-hidden="true" tabindex="-1"></a>    <span>print</span>(<span>f&#34;Scale=</span><span>{</span>scale<span>}</span><span>, condition number=</span><span>{</span>np<span>.</span>linalg<span>.</span>cond(A)<span>}</span><span>&#34;</span>)</span></code></pre></div></div>
<div>
<pre><code>Scale=1, condition number=1.0
Scale=0.01, condition number=100.0
Scale=0.0001, condition number=10000.0
Scale=1e-06, condition number=1000000.0</code></pre>
</div>
</div>
<p>As scale shrinks, condition number explodes.</p>
</section>
<section id="try-it-yourself-85">
<h4 data-anchor-id="try-it-yourself-85">Try It Yourself</h4>
<ol type="1">
<li>Generate random matrices and compute their condition numbers. Which are stable?</li>
<li>Compare condition numbers of Hilbert matrices (notoriously ill-conditioned).</li>
<li>Explore how rounding errors grow with high condition numbers.</li>
</ol>
</section>
<section id="the-takeaway-69">
<h4 data-anchor-id="the-takeaway-69">The Takeaway</h4>
<ul>
<li>Condition number = measure of problem sensitivity.</li>
<li><span>\(\kappa(A) = \sigma_{\max}/\sigma_{\min}\)</span>.</li>
<li>Ill-conditioned problems amplify errors and are numerically unstable → why scaling, regularization, and good formulations matter.</li>
</ul>
</section>
</section>
<section id="matrix-norms-and-singular-values-measuring-size-properly">
<h3 data-anchor-id="matrix-norms-and-singular-values-measuring-size-properly">88. Matrix Norms and Singular Values (Measuring Size Properly)</h3>
<p>Matrix norms measure the size or strength of a matrix. They extend the idea of vector length to matrices. Norms are crucial for analyzing stability, error growth, and performance of algorithms.</p>
<p>Some important norms:</p>
<ul>
<li>Frobenius norm:</li>
</ul>
<p><span>\[
\|A\|_F = \sqrt{\sum_{i,j} |a_{ij}|^2}
\]</span></p>
<p>Equivalent to treating the matrix as a big vector.</p>
<ul>
<li>Spectral norm (operator 2-norm):</li>
</ul>
<p><span>\[
\|A\|_2 = \sigma_{\max}
\]</span></p>
<p>The largest singular value - tells how much <span>\(A\)</span> can stretch a vector.</p>
<ul>
<li>1-norm: maximum absolute column sum.</li>
<li>∞-norm: maximum absolute row sum.</li>
</ul>
<section id="set-up-your-lab-87">
<h4 data-anchor-id="set-up-your-lab-87">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-87">
<h4 data-anchor-id="step-by-step-code-walkthrough-87">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Build a test matrix</li>
</ol>
<div id="235cb189" data-execution_count="500">
<div><div id="cb861"><pre><code><span id="cb861-1"><a href="#cb861-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>1</span>, <span>-</span><span>2</span>, <span>3</span>],</span>
<span id="cb861-2"><a href="#cb861-2" aria-hidden="true" tabindex="-1"></a>              [<span>0</span>,  <span>4</span>, <span>5</span>],</span>
<span id="cb861-3"><a href="#cb861-3" aria-hidden="true" tabindex="-1"></a>              [<span>-</span><span>1</span>, <span>2</span>, <span>1</span>]])</span></code></pre></div></div>
</div>
<ol start="2" type="1">
<li>Compute different norms</li>
</ol>
<div id="6649f4bd" data-execution_count="501">
<div><div id="cb862"><pre><code><span id="cb862-1"><a href="#cb862-1" aria-hidden="true" tabindex="-1"></a>fro <span>=</span> np.linalg.norm(A, <span>&#39;fro&#39;</span>)</span>
<span id="cb862-2"><a href="#cb862-2" aria-hidden="true" tabindex="-1"></a>spec <span>=</span> np.linalg.norm(A, <span>2</span>)</span>
<span id="cb862-3"><a href="#cb862-3" aria-hidden="true" tabindex="-1"></a>one_norm <span>=</span> np.linalg.norm(A, <span>1</span>)</span>
<span id="cb862-4"><a href="#cb862-4" aria-hidden="true" tabindex="-1"></a>inf_norm <span>=</span> np.linalg.norm(A, np.inf)</span>
<span id="cb862-5"><a href="#cb862-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb862-6"><a href="#cb862-6" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Frobenius norm:&#34;</span>, fro)</span>
<span id="cb862-7"><a href="#cb862-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Spectral norm:&#34;</span>, spec)</span>
<span id="cb862-8"><a href="#cb862-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;1-norm:&#34;</span>, one_norm)</span>
<span id="cb862-9"><a href="#cb862-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Infinity norm:&#34;</span>, inf_norm)</span></code></pre></div></div>
<div>
<pre><code>Frobenius norm: 7.810249675906654
Spectral norm: 6.813953458914004
1-norm: 9.0
Infinity norm: 9.0</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Compare spectral norm with largest singular value</li>
</ol>
<div id="4863af1b" data-execution_count="502">
<div><div id="cb864"><pre><code><span id="cb864-1"><a href="#cb864-1" aria-hidden="true" tabindex="-1"></a>U, S, Vt <span>=</span> np.linalg.svd(A)</span>
<span id="cb864-2"><a href="#cb864-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Largest singular value:&#34;</span>, S[<span>0</span>])</span>
<span id="cb864-3"><a href="#cb864-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Spectral norm:&#34;</span>, spec)</span></code></pre></div></div>
<div>
<pre><code>Largest singular value: 6.813953458914004
Spectral norm: 6.813953458914004</code></pre>
</div>
</div>
<p>They match → spectral norm = largest singular value.</p>
<ol start="4" type="1">
<li>Frobenius norm from singular values</li>
</ol>
<p><span>\[
\|A\|_F = \sqrt{\sigma_1^2 + \sigma_2^2 + \dots}
\]</span></p>
<div id="669c5bb0" data-execution_count="503">
<div><div id="cb866"><pre><code><span id="cb866-1"><a href="#cb866-1" aria-hidden="true" tabindex="-1"></a>fro_from_svd <span>=</span> np.sqrt(np.<span>sum</span>(S<span>**</span><span>2</span>))</span>
<span id="cb866-2"><a href="#cb866-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Frobenius norm (from SVD):&#34;</span>, fro_from_svd)</span></code></pre></div></div>
<div>
<pre><code>Frobenius norm (from SVD): 7.810249675906654</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Stretching effect demonstration</li>
</ol>
<p>Pick a random vector and see how much it grows:</p>
<div id="22f1ca82" data-execution_count="504">
<div><div id="cb868"><pre><code><span id="cb868-1"><a href="#cb868-1" aria-hidden="true" tabindex="-1"></a>x <span>=</span> np.random.randn(<span>3</span>)</span>
<span id="cb868-2"><a href="#cb868-2" aria-hidden="true" tabindex="-1"></a>stretch <span>=</span> np.linalg.norm(A <span>@</span> x) <span>/</span> np.linalg.norm(x)</span>
<span id="cb868-3"><a href="#cb868-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Stretch factor:&#34;</span>, stretch)</span>
<span id="cb868-4"><a href="#cb868-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Spectral norm (max possible stretch):&#34;</span>, spec)</span></code></pre></div></div>
<div>
<pre><code>Stretch factor: 2.7537463268177698
Spectral norm (max possible stretch): 6.813953458914004</code></pre>
</div>
</div>
<p>The stretch ≤ spectral norm, always.</p>
</section>
<section id="try-it-yourself-86">
<h4 data-anchor-id="try-it-yourself-86">Try It Yourself</h4>
<ol type="1">
<li>Compare norms for diagonal matrices - do they match the largest diagonal entry?</li>
<li>Generate random matrices and see how norms differ.</li>
<li>Compute Frobenius vs spectral norm for a rank-1 matrix.</li>
</ol>
</section>
<section id="the-takeaway-70">
<h4 data-anchor-id="the-takeaway-70">The Takeaway</h4>
<ul>
<li>Frobenius norm = overall energy of the matrix.</li>
<li>Spectral norm = maximum stretching power (largest singular value).</li>
<li>Other norms (1-norm, ∞-norm) capture row/column dominance.</li>
<li>Singular values unify all these views of “matrix size.”</li>
</ul>
</section>
</section>
<section id="regularization-ridgetikhonov-to-tame-instability">
<h3 data-anchor-id="regularization-ridgetikhonov-to-tame-instability">89. Regularization (Ridge/Tikhonov to Tame Instability)</h3>
<p>When solving <span>\(Ax = b\)</span>, if <span>\(A\)</span> is ill-conditioned (large condition number), small errors in data can cause huge errors in the solution. Regularization stabilizes the problem by adding a penalty term that discourages extreme solutions.</p>
<p>The most common form: ridge regression (a.k.a. Tikhonov regularization):</p>
<p><span>\[
x_\lambda = \arg\min_x \|Ax - b\|^2 + \lambda \|x\|^2
\]</span></p>
<p>Closed form:</p>
<p><span>\[
x_\lambda = (A^T A + \lambda I)^{-1} A^T b
\]</span></p>
<p>Here <span>\(\lambda &gt; 0\)</span> controls the amount of regularization:</p>
<ul>
<li>Small <span>\(\lambda\)</span>: solution close to least-squares.</li>
<li>Large <span>\(\lambda\)</span>: smaller coefficients, more stability.</li>
</ul>
<section id="set-up-your-lab-88">
<h4 data-anchor-id="set-up-your-lab-88">Set Up Your Lab</h4>
<div id="9049a165" data-execution_count="505">
<div><div id="cb870"><pre><code><span id="cb870-1"><a href="#cb870-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb870-2"><a href="#cb870-2" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-88">
<h4 data-anchor-id="step-by-step-code-walkthrough-88">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Build an ill-conditioned system</li>
</ol>
<div id="acc8155f" data-execution_count="506">
<div><div id="cb871"><pre><code><span id="cb871-1"><a href="#cb871-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>1</span>, <span>1.001</span>],</span>
<span id="cb871-2"><a href="#cb871-2" aria-hidden="true" tabindex="-1"></a>              [<span>1</span>, <span>0.999</span>]])</span>
<span id="cb871-3"><a href="#cb871-3" aria-hidden="true" tabindex="-1"></a>b <span>=</span> np.array([<span>2</span>, <span>2</span>])</span></code></pre></div></div>
</div>
<ol start="2" type="1">
<li>Solve without regularization</li>
</ol>
<div id="36b4f819" data-execution_count="507">
<div><div id="cb872"><pre><code><span id="cb872-1"><a href="#cb872-1" aria-hidden="true" tabindex="-1"></a>x_ls, <span>*</span>_ <span>=</span> np.linalg.lstsq(A, b, rcond<span>=</span><span>None</span>)</span>
<span id="cb872-2"><a href="#cb872-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Least squares solution:&#34;</span>, x_ls)</span></code></pre></div></div>
<div>
<pre><code>Least squares solution: [ 2.00000000e+00 -2.84186735e-14]</code></pre>
</div>
</div>
<p>The result may be unstable.</p>
<ol start="3" type="1">
<li>Apply ridge regularization</li>
</ol>
<div id="d838a6e0" data-execution_count="508">
<div><div id="cb874"><pre><code><span id="cb874-1"><a href="#cb874-1" aria-hidden="true" tabindex="-1"></a>lam <span>=</span> <span>0.1</span></span>
<span id="cb874-2"><a href="#cb874-2" aria-hidden="true" tabindex="-1"></a>x_ridge <span>=</span> np.linalg.inv(A.T <span>@</span> A <span>+</span> lam<span>*</span>np.eye(<span>2</span>)) <span>@</span> A.T <span>@</span> b</span>
<span id="cb874-3"><a href="#cb874-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Ridge solution (λ=0.1):&#34;</span>, x_ridge)</span></code></pre></div></div>
<div>
<pre><code>Ridge solution (λ=0.1): [0.97561927 0.97559976]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Compare effect of different λ</li>
</ol>
<div id="b2f022a5" data-execution_count="509">
<div><div id="cb876"><pre><code><span id="cb876-1"><a href="#cb876-1" aria-hidden="true" tabindex="-1"></a>lambdas <span>=</span> np.logspace(<span>-</span><span>4</span>, <span>2</span>, <span>20</span>)</span>
<span id="cb876-2"><a href="#cb876-2" aria-hidden="true" tabindex="-1"></a>solutions <span>=</span> []</span>
<span id="cb876-3"><a href="#cb876-3" aria-hidden="true" tabindex="-1"></a><span>for</span> lam <span>in</span> lambdas:</span>
<span id="cb876-4"><a href="#cb876-4" aria-hidden="true" tabindex="-1"></a>    x_reg <span>=</span> np.linalg.inv(A.T <span>@</span> A <span>+</span> lam<span>*</span>np.eye(<span>2</span>)) <span>@</span> A.T <span>@</span> b</span>
<span id="cb876-5"><a href="#cb876-5" aria-hidden="true" tabindex="-1"></a>    solutions.append(np.linalg.norm(x_reg))</span>
<span id="cb876-6"><a href="#cb876-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb876-7"><a href="#cb876-7" aria-hidden="true" tabindex="-1"></a>plt.semilogx(lambdas, solutions, <span>&#39;o-&#39;</span>)</span>
<span id="cb876-8"><a href="#cb876-8" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span>&#34;λ (regularization strength)&#34;</span>)</span>
<span id="cb876-9"><a href="#cb876-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span>&#34;Solution norm&#34;</span>)</span>
<span id="cb876-10"><a href="#cb876-10" aria-hidden="true" tabindex="-1"></a>plt.title(<span>&#34;Effect of ridge regularization&#34;</span>)</span>
<span id="cb876-11"><a href="#cb876-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-510-output-1.png" width="589" height="451"/></p>
</figure>
</div>
</div>
</div>
<p>As <span>\(\lambda\)</span> increases, the solution becomes smaller and more stable.</p>
<ol start="5" type="1">
<li>Connection to SVD</li>
</ol>
<p>If <span>\(A = U \Sigma V^T\)</span>:</p>
<p><span>\[
x_\lambda = \sum_i \frac{\sigma_i}{\sigma_i^2 + \lambda} (u_i^T b) v_i
\]</span></p>
<p>Small singular values (causing instability) get damped by <span>\(\frac{\sigma_i}{\sigma_i^2 + \lambda}\)</span>.</p>
</section>
<section id="try-it-yourself-87">
<h4 data-anchor-id="try-it-yourself-87">Try It Yourself</h4>
<ol type="1">
<li>Experiment with larger and smaller <span>\(\lambda\)</span>. What happens to the solution?</li>
<li>Add random noise to <span>\(b\)</span>. Compare least-squares vs ridge stability.</li>
<li>Plot how each coefficient changes with λ.</li>
</ol>
</section>
<section id="the-takeaway-71">
<h4 data-anchor-id="the-takeaway-71">The Takeaway</h4>
<ul>
<li>Regularization controls instability in ill-conditioned problems.</li>
<li>Ridge regression balances fit vs. stability using λ.</li>
<li>In SVD terms, regularization damps small singular values that cause wild solutions.</li>
</ul>
</section>
</section>
<section id="rank-revealing-qr-and-practical-diagnostics-what-rank-really-is">
<h3 data-anchor-id="rank-revealing-qr-and-practical-diagnostics-what-rank-really-is">90. Rank-Revealing QR and Practical Diagnostics (What Rank Really Is)</h3>
<p>In practice, we often need to determine the numerical rank of a matrix - not just the theoretical rank, but how many directions carry meaningful information beyond round-off errors or noise. A useful tool for this is the Rank-Revealing QR (RRQR) factorization.</p>
<p>For a matrix <span>\(A\)</span>:</p>
<p><span>\[
A P = Q R
\]</span></p>
<ul>
<li><span>\(Q\)</span>: orthogonal matrix</li>
<li><span>\(R\)</span>: upper triangular matrix</li>
<li><span>\(P\)</span>: column permutation matrix</li>
</ul>
<p>By reordering columns smartly, the diagonal of <span>\(R\)</span> reveals which directions are significant.</p>
<section id="set-up-your-lab-89">
<h4 data-anchor-id="set-up-your-lab-89">Set Up Your Lab</h4>
<div id="8ba6e248" data-execution_count="510">
<div><div id="cb877"><pre><code><span id="cb877-1"><a href="#cb877-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb877-2"><a href="#cb877-2" aria-hidden="true" tabindex="-1"></a><span>from</span> scipy.linalg <span>import</span> qr</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-89">
<h4 data-anchor-id="step-by-step-code-walkthrough-89">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Build a nearly rank-deficient matrix</li>
</ol>
<div id="e3d441f0" data-execution_count="511">
<div><div id="cb878"><pre><code><span id="cb878-1"><a href="#cb878-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>1</span>, <span>2</span>, <span>3</span>],</span>
<span id="cb878-2"><a href="#cb878-2" aria-hidden="true" tabindex="-1"></a>              [<span>2</span>, <span>4.001</span>, <span>6</span>],</span>
<span id="cb878-3"><a href="#cb878-3" aria-hidden="true" tabindex="-1"></a>              [<span>3</span>, <span>6</span>, <span>9.001</span>]])</span>
<span id="cb878-4"><a href="#cb878-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rank (theoretical):&#34;</span>, np.linalg.matrix_rank(A))</span></code></pre></div></div>

</div>
<p>This matrix is almost rank 2 but with small perturbations.</p>
<ol start="2" type="1">
<li>QR with column pivoting</li>
</ol>
<div id="436f11a4" data-execution_count="512">
<div><div id="cb880"><pre><code><span id="cb880-1"><a href="#cb880-1" aria-hidden="true" tabindex="-1"></a>Q, R, P <span>=</span> qr(A, pivoting<span>=</span><span>True</span>)</span>
<span id="cb880-2"><a href="#cb880-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;R:</span><span>\n</span><span>&#34;</span>, R)</span>
<span id="cb880-3"><a href="#cb880-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Column permutation:&#34;</span>, P)</span></code></pre></div></div>
<div>
<pre><code>R:
 [[-1.12257740e+01 -7.48384925e+00 -3.74165738e+00]
 [ 0.00000000e+00 -1.20185042e-03 -1.84886859e-04]
 [ 0.00000000e+00  0.00000000e+00 -7.41196374e-05]]
Column permutation: [2 1 0]</code></pre>
</div>
</div>
<p>The diagonal entries of <span>\(R\)</span> decrease rapidly → numerical rank is determined where they become tiny.</p>
<ol start="3" type="1">
<li>Compare with SVD</li>
</ol>
<div id="036777be" data-execution_count="513">
<div><div id="cb882"><pre><code><span id="cb882-1"><a href="#cb882-1" aria-hidden="true" tabindex="-1"></a>U, S, Vt <span>=</span> np.linalg.svd(A)</span>
<span id="cb882-2"><a href="#cb882-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Singular values:&#34;</span>, S)</span></code></pre></div></div>
<div>
<pre><code>Singular values: [1.40009286e+01 1.00000000e-03 7.14238341e-05]</code></pre>
</div>
</div>
<p>The singular values tell the same story: one is very small → effective rank ≈ 2.</p>
<ol start="4" type="1">
<li>Thresholding for rank</li>
</ol>
<div id="ba3b4c4e" data-execution_count="514">
<div><div id="cb884"><pre><code><span id="cb884-1"><a href="#cb884-1" aria-hidden="true" tabindex="-1"></a>tol <span>=</span> <span>1e-3</span></span>
<span id="cb884-2"><a href="#cb884-2" aria-hidden="true" tabindex="-1"></a>rank_est <span>=</span> np.<span>sum</span>(S <span>&gt;</span> tol)</span>
<span id="cb884-3"><a href="#cb884-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Estimated rank:&#34;</span>, rank_est)</span></code></pre></div></div>

</div>
<ol start="5" type="1">
<li>Diagnostics on a noisy matrix</li>
</ol>
<div id="f410e44c" data-execution_count="515">
<div><div id="cb886"><pre><code><span id="cb886-1"><a href="#cb886-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span>0</span>)</span>
<span id="cb886-2"><a href="#cb886-2" aria-hidden="true" tabindex="-1"></a>B <span>=</span> np.random.randn(<span>50</span>, <span>10</span>) <span>@</span> np.random.randn(<span>10</span>, <span>10</span>)  <span># rank ≤ 10</span></span>
<span id="cb886-3"><a href="#cb886-3" aria-hidden="true" tabindex="-1"></a>B[:, <span>-</span><span>1</span>] <span>+=</span> <span>1e-6</span> <span>*</span> np.random.randn(<span>50</span>)  <span># tiny noise</span></span>
<span id="cb886-4"><a href="#cb886-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb886-5"><a href="#cb886-5" aria-hidden="true" tabindex="-1"></a>U, S, Vt <span>=</span> np.linalg.svd(B)</span>
<span id="cb886-6"><a href="#cb886-6" aria-hidden="true" tabindex="-1"></a>plt.semilogy(S, <span>&#39;o-&#39;</span>)</span>
<span id="cb886-7"><a href="#cb886-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span>&#34;Singular values (log scale)&#34;</span>)</span>
<span id="cb886-8"><a href="#cb886-8" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span>&#34;Index&#34;</span>)</span>
<span id="cb886-9"><a href="#cb886-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span>&#34;Value&#34;</span>)</span>
<span id="cb886-10"><a href="#cb886-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-516-output-1.png" width="591" height="449"/></p>
</figure>
</div>
</div>
</div>
<p>The drop in singular values shows effective rank.</p>
</section>
<section id="try-it-yourself-88">
<h4 data-anchor-id="try-it-yourself-88">Try It Yourself</h4>
<ol type="1">
<li>Change the perturbation in <span>\(A\)</span> from 0.001 to 0.000001. Does the numerical rank change?</li>
<li>Test QR with pivoting on random rectangular matrices.</li>
<li>Compare rank estimates from QR vs SVD for large noisy matrices.</li>
</ol>
</section>
<section id="the-takeaway-72">
<h4 data-anchor-id="the-takeaway-72">The Takeaway</h4>
<ul>
<li>Rank-revealing QR is a practical tool to detect effective rank in real-world data.</li>
<li>SVD gives the most precise picture (singular values), but QR with pivoting is faster.</li>
<li>Understanding numerical rank is crucial for diagnostics, stability, and model complexity control.</li>
</ul>
</section>
</section>
</section>
<section id="chapter-10.-applications-and-computation">
<h2 data-anchor-id="chapter-10.-applications-and-computation">Chapter 10. Applications and computation</h2>
<section id="d3d-geometry-pipelines-cameras-rotations-and-transforms">
<h3 data-anchor-id="d3d-geometry-pipelines-cameras-rotations-and-transforms">91. 2D/3D Geometry Pipelines (Cameras, Rotations, and Transforms)</h3>
<p>Linear algebra powers the geometry pipelines in computer graphics and robotics.</p>
<ul>
<li>2D transforms: rotation, scaling, translation.</li>
<li>3D transforms: same ideas, but with an extra dimension.</li>
<li>Homogeneous coordinates let us unify all transforms (even translations) into matrix multiplications.</li>
</ul>
<section id="set-up-your-lab-90">
<h4 data-anchor-id="set-up-your-lab-90">Set Up Your Lab</h4>
<div id="df7c2c91" data-execution_count="516">
<div><div id="cb887"><pre><code><span id="cb887-1"><a href="#cb887-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb887-2"><a href="#cb887-2" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-90">
<h4 data-anchor-id="step-by-step-code-walkthrough-90">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Rotation in 2D</li>
</ol>
<p><span>\[
R(\theta) =
\begin{bmatrix}
\cos\theta &amp; -\sin\theta \\
\sin\theta &amp; \cos\theta
\end{bmatrix}
\]</span></p>
<div id="e8f530c7" data-execution_count="517">
<div><div id="cb888"><pre><code><span id="cb888-1"><a href="#cb888-1" aria-hidden="true" tabindex="-1"></a>theta <span>=</span> np.pi<span>/</span><span>4</span>  <span># 45 degrees</span></span>
<span id="cb888-2"><a href="#cb888-2" aria-hidden="true" tabindex="-1"></a>R <span>=</span> np.array([[np.cos(theta), <span>-</span>np.sin(theta)],</span>
<span id="cb888-3"><a href="#cb888-3" aria-hidden="true" tabindex="-1"></a>              [np.sin(theta),  np.cos(theta)]])</span>
<span id="cb888-4"><a href="#cb888-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb888-5"><a href="#cb888-5" aria-hidden="true" tabindex="-1"></a>point <span>=</span> np.array([<span>1</span>, <span>0</span>])</span>
<span id="cb888-6"><a href="#cb888-6" aria-hidden="true" tabindex="-1"></a>rotated <span>=</span> R <span>@</span> point</span>
<span id="cb888-7"><a href="#cb888-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb888-8"><a href="#cb888-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Original:&#34;</span>, point)</span>
<span id="cb888-9"><a href="#cb888-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Rotated:&#34;</span>, rotated)</span></code></pre></div></div>
<div>
<pre><code>Original: [1 0]
Rotated: [0.70710678 0.70710678]</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Translation using homogeneous coordinates</li>
</ol>
<p>In 2D:</p>
<p><span>\[
T(dx, dy) =
\begin{bmatrix}
1 &amp; 0 &amp; dx \\
0 &amp; 1 &amp; dy \\
0 &amp; 0 &amp; 1
\end{bmatrix}
\]</span></p>
<div id="539612c3" data-execution_count="518">
<div><div id="cb890"><pre><code><span id="cb890-1"><a href="#cb890-1" aria-hidden="true" tabindex="-1"></a>T <span>=</span> np.array([[<span>1</span>,<span>0</span>,<span>2</span>],</span>
<span id="cb890-2"><a href="#cb890-2" aria-hidden="true" tabindex="-1"></a>              [<span>0</span>,<span>1</span>,<span>1</span>],</span>
<span id="cb890-3"><a href="#cb890-3" aria-hidden="true" tabindex="-1"></a>              [<span>0</span>,<span>0</span>,<span>1</span>]])</span>
<span id="cb890-4"><a href="#cb890-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb890-5"><a href="#cb890-5" aria-hidden="true" tabindex="-1"></a>p_h <span>=</span> np.array([<span>1</span>,<span>1</span>,<span>1</span>])  <span># homogeneous (x=1,y=1)</span></span>
<span id="cb890-6"><a href="#cb890-6" aria-hidden="true" tabindex="-1"></a>translated <span>=</span> T <span>@</span> p_h</span>
<span id="cb890-7"><a href="#cb890-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Translated point:&#34;</span>, translated)</span></code></pre></div></div>
<div>
<pre><code>Translated point: [3 2 1]</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Combine rotation + translation</li>
</ol>
<p>Transformations compose by multiplying matrices.</p>
<div id="83b56b24" data-execution_count="519">
<div><div id="cb892"><pre><code><span id="cb892-1"><a href="#cb892-1" aria-hidden="true" tabindex="-1"></a>M <span>=</span> T <span>@</span> np.block([[R, np.zeros((<span>2</span>,<span>1</span>))],</span>
<span id="cb892-2"><a href="#cb892-2" aria-hidden="true" tabindex="-1"></a>                  [np.zeros((<span>1</span>,<span>2</span>)), <span>1</span>]])</span>
<span id="cb892-3"><a href="#cb892-3" aria-hidden="true" tabindex="-1"></a>combined <span>=</span> M <span>@</span> p_h</span>
<span id="cb892-4"><a href="#cb892-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Combined transform (rotation+translation):&#34;</span>, combined)</span></code></pre></div></div>
<div>
<pre><code>Combined transform (rotation+translation): [2.         2.41421356 1.        ]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>3D rotation (around z-axis)</li>
</ol>
<p><span>\[
R_z(\theta) =
\begin{bmatrix}
\cos\theta &amp; -\sin\theta &amp; 0 \\
\sin\theta &amp;  \cos\theta &amp; 0 \\
0          &amp; 0           &amp; 1
\end{bmatrix}
\]</span></p>
<div id="e2a3a04f" data-execution_count="520">
<div><div id="cb894"><pre><code><span id="cb894-1"><a href="#cb894-1" aria-hidden="true" tabindex="-1"></a>theta <span>=</span> np.pi<span>/</span><span>3</span></span>
<span id="cb894-2"><a href="#cb894-2" aria-hidden="true" tabindex="-1"></a>Rz <span>=</span> np.array([[np.cos(theta), <span>-</span>np.sin(theta), <span>0</span>],</span>
<span id="cb894-3"><a href="#cb894-3" aria-hidden="true" tabindex="-1"></a>               [np.sin(theta),  np.cos(theta), <span>0</span>],</span>
<span id="cb894-4"><a href="#cb894-4" aria-hidden="true" tabindex="-1"></a>               [<span>0</span>,              <span>0</span>,             <span>1</span>]])</span>
<span id="cb894-5"><a href="#cb894-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb894-6"><a href="#cb894-6" aria-hidden="true" tabindex="-1"></a>point3d <span>=</span> np.array([<span>1</span>,<span>0</span>,<span>0</span>])</span>
<span id="cb894-7"><a href="#cb894-7" aria-hidden="true" tabindex="-1"></a>rotated3d <span>=</span> Rz <span>@</span> point3d</span>
<span id="cb894-8"><a href="#cb894-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;3D rotated point:&#34;</span>, rotated3d)</span></code></pre></div></div>
<div>
<pre><code>3D rotated point: [0.5       0.8660254 0.       ]</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Camera projection (3D → 2D)</li>
</ol>
<p>Simple pinhole model:</p>
<p><span>\[
\begin{bmatrix}
x&#39; \\
y&#39;
\end{bmatrix}
=
\begin{bmatrix}
f \cdot x / z \\
f \cdot y / z
\end{bmatrix}
\]</span></p>
<div id="d8f93d16" data-execution_count="521">
<div><div id="cb896"><pre><code><span id="cb896-1"><a href="#cb896-1" aria-hidden="true" tabindex="-1"></a>f <span>=</span> <span>1.0</span>  <span># focal length</span></span>
<span id="cb896-2"><a href="#cb896-2" aria-hidden="true" tabindex="-1"></a>P <span>=</span> np.array([[f,<span>0</span>,<span>0</span>],</span>
<span id="cb896-3"><a href="#cb896-3" aria-hidden="true" tabindex="-1"></a>              [<span>0</span>,f,<span>0</span>],</span>
<span id="cb896-4"><a href="#cb896-4" aria-hidden="true" tabindex="-1"></a>              [<span>0</span>,<span>0</span>,<span>1</span>]])  <span># projection matrix</span></span>
<span id="cb896-5"><a href="#cb896-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb896-6"><a href="#cb896-6" aria-hidden="true" tabindex="-1"></a>point3d <span>=</span> np.array([<span>2</span>,<span>3</span>,<span>5</span>])</span>
<span id="cb896-7"><a href="#cb896-7" aria-hidden="true" tabindex="-1"></a>p_proj <span>=</span> P <span>@</span> point3d</span>
<span id="cb896-8"><a href="#cb896-8" aria-hidden="true" tabindex="-1"></a>p_proj <span>=</span> p_proj[:<span>2</span>] <span>/</span> p_proj[<span>2</span>]  <span># divide by z</span></span>
<span id="cb896-9"><a href="#cb896-9" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Projected 2D point:&#34;</span>, p_proj)</span></code></pre></div></div>
<div>
<pre><code>Projected 2D point: [0.4 0.6]</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-89">
<h4 data-anchor-id="try-it-yourself-89">Try It Yourself</h4>
<ol type="1">
<li>Rotate a square in 2D, then translate it. Plot before/after.</li>
<li>Rotate a 3D point cloud around x, y, and z axes.</li>
<li>Project a cube into 2D using the pinhole camera model.</li>
</ol>
</section>
<section id="the-takeaway-73">
<h4 data-anchor-id="the-takeaway-73">The Takeaway</h4>
<ul>
<li>Geometry pipelines = sequences of linear transforms.</li>
<li>Homogeneous coordinates unify rotation, scaling, and translation.</li>
<li>Camera projection links 3D world to 2D images - a cornerstone of graphics and vision.</li>
</ul>
</section>
</section>
<section id="computer-graphics-and-robotics-homogeneous-tricks-in-action">
<h3 data-anchor-id="computer-graphics-and-robotics-homogeneous-tricks-in-action">92. Computer Graphics and Robotics (Homogeneous Tricks in Action)</h3>
<p>Computer graphics and robotics both rely on homogeneous coordinates to unify rotations, translations, scalings, and projections into a single framework. With <span>\(4 \times 4\)</span> matrices in 3D, entire transformation pipelines can be built as matrix products.</p>
<section id="set-up-your-lab-91">
<h4 data-anchor-id="set-up-your-lab-91">Set Up Your Lab</h4>
<div id="80c11f86" data-execution_count="522">
<div><div id="cb898"><pre><code><span id="cb898-1"><a href="#cb898-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb898-2"><a href="#cb898-2" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-91">
<h4 data-anchor-id="step-by-step-code-walkthrough-91">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Homogeneous representation of a point</li>
</ol>
<p>In 3D:</p>
<p><span>\[
(x, y, z) \mapsto (x, y, z, 1)
\]</span></p>
<div id="22c8a6b5" data-execution_count="523">
<div><div id="cb899"><pre><code><span id="cb899-1"><a href="#cb899-1" aria-hidden="true" tabindex="-1"></a>p <span>=</span> np.array([<span>1</span>,<span>2</span>,<span>3</span>,<span>1</span>])  <span># homogeneous point</span></span></code></pre></div></div>
</div>
<ol start="2" type="1">
<li>Define translation, rotation, and scaling matrices</li>
</ol>
<ul>
<li>Translation by <span>\((dx,dy,dz)\)</span>:</li>
</ul>
<div id="9dbaf916" data-execution_count="524">
<div><div id="cb900"><pre><code><span id="cb900-1"><a href="#cb900-1" aria-hidden="true" tabindex="-1"></a>T <span>=</span> np.array([[<span>1</span>,<span>0</span>,<span>0</span>,<span>2</span>],</span>
<span id="cb900-2"><a href="#cb900-2" aria-hidden="true" tabindex="-1"></a>              [<span>0</span>,<span>1</span>,<span>0</span>,<span>1</span>],</span>
<span id="cb900-3"><a href="#cb900-3" aria-hidden="true" tabindex="-1"></a>              [<span>0</span>,<span>0</span>,<span>1</span>,<span>3</span>],</span>
<span id="cb900-4"><a href="#cb900-4" aria-hidden="true" tabindex="-1"></a>              [<span>0</span>,<span>0</span>,<span>0</span>,<span>1</span>]])</span></code></pre></div></div>
</div>
<ul>
<li>Scaling by factors <span>\((sx, sy, sz)\)</span>:</li>
</ul>
<div id="48d5939f" data-execution_count="525">
<div><div id="cb901"><pre><code><span id="cb901-1"><a href="#cb901-1" aria-hidden="true" tabindex="-1"></a>S <span>=</span> np.diag([<span>2</span>, <span>0.5</span>, <span>1.5</span>, <span>1</span>])</span></code></pre></div></div>
</div>
<ul>
<li>Rotation about z-axis (<span>\(\theta = 90^\circ\)</span>):</li>
</ul>
<div id="35ea8743" data-execution_count="526">
<div><div id="cb902"><pre><code><span id="cb902-1"><a href="#cb902-1" aria-hidden="true" tabindex="-1"></a>theta <span>=</span> np.pi<span>/</span><span>2</span></span>
<span id="cb902-2"><a href="#cb902-2" aria-hidden="true" tabindex="-1"></a>Rz <span>=</span> np.array([[np.cos(theta), <span>-</span>np.sin(theta), <span>0</span>, <span>0</span>],</span>
<span id="cb902-3"><a href="#cb902-3" aria-hidden="true" tabindex="-1"></a>               [np.sin(theta),  np.cos(theta), <span>0</span>, <span>0</span>],</span>
<span id="cb902-4"><a href="#cb902-4" aria-hidden="true" tabindex="-1"></a>               [<span>0</span>,              <span>0</span>,             <span>1</span>, <span>0</span>],</span>
<span id="cb902-5"><a href="#cb902-5" aria-hidden="true" tabindex="-1"></a>               [<span>0</span>,              <span>0</span>,             <span>0</span>, <span>1</span>]])</span></code></pre></div></div>
</div>
<ol start="3" type="1">
<li>Combine transforms into a pipeline</li>
</ol>
<div id="f413813e" data-execution_count="527">
<div><div id="cb903"><pre><code><span id="cb903-1"><a href="#cb903-1" aria-hidden="true" tabindex="-1"></a>M <span>=</span> T <span>@</span> Rz <span>@</span> S  <span># first scale, then rotate, then translate</span></span>
<span id="cb903-2"><a href="#cb903-2" aria-hidden="true" tabindex="-1"></a>p_transformed <span>=</span> M <span>@</span> p</span>
<span id="cb903-3"><a href="#cb903-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Transformed point:&#34;</span>, p_transformed)</span></code></pre></div></div>
<div>
<pre><code>Transformed point: [1.  3.  7.5 1. ]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Robotics: forward kinematics of a 2-link arm</li>
</ol>
<p>Each joint is a rotation + translation.</p>
<div id="a322ad7c" data-execution_count="528">
<div><div id="cb905"><pre><code><span id="cb905-1"><a href="#cb905-1" aria-hidden="true" tabindex="-1"></a><span>def</span> link(theta, length):</span>
<span id="cb905-2"><a href="#cb905-2" aria-hidden="true" tabindex="-1"></a>    <span>return</span> np.array([[np.cos(theta), <span>-</span>np.sin(theta), <span>0</span>, length<span>*</span>np.cos(theta)],</span>
<span id="cb905-3"><a href="#cb905-3" aria-hidden="true" tabindex="-1"></a>                     [np.sin(theta),  np.cos(theta), <span>0</span>, length<span>*</span>np.sin(theta)],</span>
<span id="cb905-4"><a href="#cb905-4" aria-hidden="true" tabindex="-1"></a>                     [<span>0</span>,              <span>0</span>,             <span>1</span>, <span>0</span>],</span>
<span id="cb905-5"><a href="#cb905-5" aria-hidden="true" tabindex="-1"></a>                     [<span>0</span>,              <span>0</span>,             <span>0</span>, <span>1</span>]])</span>
<span id="cb905-6"><a href="#cb905-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb905-7"><a href="#cb905-7" aria-hidden="true" tabindex="-1"></a>theta1, theta2 <span>=</span> np.pi<span>/</span><span>4</span>, np.pi<span>/</span><span>6</span></span>
<span id="cb905-8"><a href="#cb905-8" aria-hidden="true" tabindex="-1"></a>L1, L2 <span>=</span> <span>2</span>, <span>1.5</span></span>
<span id="cb905-9"><a href="#cb905-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb905-10"><a href="#cb905-10" aria-hidden="true" tabindex="-1"></a>M1 <span>=</span> link(theta1, L1)</span>
<span id="cb905-11"><a href="#cb905-11" aria-hidden="true" tabindex="-1"></a>M2 <span>=</span> link(theta2, L2)</span>
<span id="cb905-12"><a href="#cb905-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb905-13"><a href="#cb905-13" aria-hidden="true" tabindex="-1"></a>end_effector <span>=</span> M1 <span>@</span> M2 <span>@</span> np.array([<span>0</span>,<span>0</span>,<span>0</span>,<span>1</span>])</span>
<span id="cb905-14"><a href="#cb905-14" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;End effector position:&#34;</span>, end_effector[:<span>3</span>])</span></code></pre></div></div>
<div>
<pre><code>End effector position: [1.80244213 2.8631023  0.        ]</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Graphics: simple 3D camera projection</li>
</ol>
<div id="532ad0dc" data-execution_count="529">
<div><div id="cb907"><pre><code><span id="cb907-1"><a href="#cb907-1" aria-hidden="true" tabindex="-1"></a>f <span>=</span> <span>2.0</span></span>
<span id="cb907-2"><a href="#cb907-2" aria-hidden="true" tabindex="-1"></a>P <span>=</span> np.array([[f,<span>0</span>,<span>0</span>,<span>0</span>],</span>
<span id="cb907-3"><a href="#cb907-3" aria-hidden="true" tabindex="-1"></a>              [<span>0</span>,f,<span>0</span>,<span>0</span>],</span>
<span id="cb907-4"><a href="#cb907-4" aria-hidden="true" tabindex="-1"></a>              [<span>0</span>,<span>0</span>,<span>1</span>,<span>0</span>]])</span>
<span id="cb907-5"><a href="#cb907-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb907-6"><a href="#cb907-6" aria-hidden="true" tabindex="-1"></a>cube <span>=</span> np.array([[x,y,z,<span>1</span>] <span>for</span> x <span>in</span> [<span>0</span>,<span>1</span>] <span>for</span> y <span>in</span> [<span>0</span>,<span>1</span>] <span>for</span> z <span>in</span> [<span>0</span>,<span>1</span>]])</span>
<span id="cb907-7"><a href="#cb907-7" aria-hidden="true" tabindex="-1"></a>proj <span>=</span> (P <span>@</span> cube.T).T</span>
<span id="cb907-8"><a href="#cb907-8" aria-hidden="true" tabindex="-1"></a>proj2d <span>=</span> proj[:,:<span>2</span>] <span>/</span> proj[:,<span>2</span>:<span>3</span>]</span>
<span id="cb907-9"><a href="#cb907-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb907-10"><a href="#cb907-10" aria-hidden="true" tabindex="-1"></a>plt.scatter(proj2d[:,<span>0</span>], proj2d[:,<span>1</span>])</span>
<span id="cb907-11"><a href="#cb907-11" aria-hidden="true" tabindex="-1"></a>plt.title(<span>&#34;Projected cube&#34;</span>)</span>
<span id="cb907-12"><a href="#cb907-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<pre><code>/var/folders/_g/lq_pglm508df70x751kkxrl80000gp/T/ipykernel_31637/2038614107.py:8: RuntimeWarning: divide by zero encountered in divide
  proj2d = proj[:,:2] / proj[:,2:3]
/var/folders/_g/lq_pglm508df70x751kkxrl80000gp/T/ipykernel_31637/2038614107.py:8: RuntimeWarning: invalid value encountered in divide
  proj2d = proj[:,:2] / proj[:,2:3]</code></pre>
</div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-530-output-2.png" width="579" height="431"/></p>
</figure>
</div>
</div>
</div>
</section>
<section id="try-it-yourself-90">
<h4 data-anchor-id="try-it-yourself-90">Try It Yourself</h4>
<ol type="1">
<li>Change order of transforms (<code>Rz @ S @ T</code>). How does the result differ?</li>
<li>Add a third joint to the robotic arm and compute new end-effector position.</li>
<li>Project the cube with different focal lengths <span>\(f\)</span>.</li>
</ol>
</section>
<section id="the-takeaway-74">
<h4 data-anchor-id="the-takeaway-74">The Takeaway</h4>
<ul>
<li>Homogeneous coordinates unify all transformations.</li>
<li>Robotics uses this framework for forward kinematics.</li>
<li>Graphics uses it for camera and projection pipelines.</li>
<li>Both fields rely on the same linear algebra tricks - just applied differently.</li>
</ul>
</section>
</section>
<section id="graphs-adjacency-and-laplacians-networks-via-matrices">
<h3 data-anchor-id="graphs-adjacency-and-laplacians-networks-via-matrices">93. Graphs, Adjacency, and Laplacians (Networks via Matrices)</h3>
<p>Graphs can be studied with linear algebra by encoding them into matrices. Two of the most important:</p>
<ul>
<li><p>Adjacency matrix <span>\(A\)</span>:</p>
<p><span>\[
A_{ij} =
\begin{cases}
1 &amp; \text{if edge between i and j exists} \\
0 &amp; \text{otherwise}
\end{cases}
\]</span></p></li>
<li><p>Graph Laplacian <span>\(L\)</span>:</p>
<p><span>\[
L = D - A
\]</span></p>
<p>where <span>\(D\)</span> is the degree matrix ($D_{ii} = $ number of neighbors of node <span>\(i\)</span>).</p></li>
</ul>
<p>These matrices let us analyze connectivity, diffusion, and clustering.</p>
<section id="set-up-your-lab-92">
<h4 data-anchor-id="set-up-your-lab-92">Set Up Your Lab</h4>
<div id="cdd35208" data-execution_count="530">
<div><div id="cb909"><pre><code><span id="cb909-1"><a href="#cb909-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb909-2"><a href="#cb909-2" aria-hidden="true" tabindex="-1"></a><span>import</span> networkx <span>as</span> nx</span>
<span id="cb909-3"><a href="#cb909-3" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-92">
<h4 data-anchor-id="step-by-step-code-walkthrough-92">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Build a simple graph</li>
</ol>
<div id="84a2eebf" data-execution_count="531">
<div><div id="cb910"><pre><code><span id="cb910-1"><a href="#cb910-1" aria-hidden="true" tabindex="-1"></a>G <span>=</span> nx.Graph()</span>
<span id="cb910-2"><a href="#cb910-2" aria-hidden="true" tabindex="-1"></a>G.add_edges_from([(<span>0</span>,<span>1</span>), (<span>1</span>,<span>2</span>), (<span>2</span>,<span>3</span>), (<span>3</span>,<span>0</span>), (<span>0</span>,<span>2</span>)])  <span># square with diagonal</span></span>
<span id="cb910-3"><a href="#cb910-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb910-4"><a href="#cb910-4" aria-hidden="true" tabindex="-1"></a>nx.draw(G, with_labels<span>=</span><span>True</span>, node_color<span>=</span><span>&#34;lightblue&#34;</span>, node_size<span>=</span><span>800</span>)</span>
<span id="cb910-5"><a href="#cb910-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-532-output-1.png" width="691" height="499"/></p>
</figure>
</div>
</div>
</div>
<ol start="2" type="1">
<li>Adjacency matrix</li>
</ol>
<div id="501a09ad" data-execution_count="532">
<div><div id="cb911"><pre><code><span id="cb911-1"><a href="#cb911-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> nx.to_numpy_array(G)</span>
<span id="cb911-2"><a href="#cb911-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Adjacency matrix:</span><span>\n</span><span>&#34;</span>, A)</span></code></pre></div></div>
<div>
<pre><code>Adjacency matrix:
 [[0. 1. 1. 1.]
 [1. 0. 1. 0.]
 [1. 1. 0. 1.]
 [1. 0. 1. 0.]]</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Degree and Laplacian matrices</li>
</ol>
<div id="21c0739e" data-execution_count="533">
<div><div id="cb913"><pre><code><span id="cb913-1"><a href="#cb913-1" aria-hidden="true" tabindex="-1"></a>D <span>=</span> np.diag(A.<span>sum</span>(axis<span>=</span><span>1</span>))</span>
<span id="cb913-2"><a href="#cb913-2" aria-hidden="true" tabindex="-1"></a>L <span>=</span> D <span>-</span> A</span>
<span id="cb913-3"><a href="#cb913-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Degree matrix:</span><span>\n</span><span>&#34;</span>, D)</span>
<span id="cb913-4"><a href="#cb913-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Graph Laplacian:</span><span>\n</span><span>&#34;</span>, L)</span></code></pre></div></div>
<div>
<pre><code>Degree matrix:
 [[3. 0. 0. 0.]
 [0. 2. 0. 0.]
 [0. 0. 3. 0.]
 [0. 0. 0. 2.]]
Graph Laplacian:
 [[ 3. -1. -1. -1.]
 [-1.  2. -1.  0.]
 [-1. -1.  3. -1.]
 [-1.  0. -1.  2.]]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Eigenvalues of Laplacian (connectivity check)</li>
</ol>
<div id="725c9809" data-execution_count="534">
<div><div id="cb915"><pre><code><span id="cb915-1"><a href="#cb915-1" aria-hidden="true" tabindex="-1"></a>eigvals, eigvecs <span>=</span> np.linalg.eigh(L)</span>
<span id="cb915-2"><a href="#cb915-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Laplacian eigenvalues:&#34;</span>, eigvals)</span></code></pre></div></div>
<div>
<pre><code>Laplacian eigenvalues: [1.11022302e-16 2.00000000e+00 4.00000000e+00 4.00000000e+00]</code></pre>
</div>
</div>
<ul>
<li>The number of zero eigenvalues = number of connected components.</li>
</ul>
<ol start="5" type="1">
<li>Spectral embedding (clustering)</li>
</ol>
<p>Use Laplacian eigenvectors to embed nodes in low dimensions.</p>
<div id="51f42d71" data-execution_count="535">
<div><div id="cb917"><pre><code><span id="cb917-1"><a href="#cb917-1" aria-hidden="true" tabindex="-1"></a>coords <span>=</span> eigvecs[:,<span>1</span>:<span>3</span>]  <span># skip the trivial first eigenvector</span></span>
<span id="cb917-2"><a href="#cb917-2" aria-hidden="true" tabindex="-1"></a>plt.scatter(coords[:,<span>0</span>], coords[:,<span>1</span>], c<span>=</span><span>range</span>(<span>len</span>(coords)), cmap<span>=</span><span>&#34;tab10&#34;</span>, s<span>=</span><span>200</span>)</span>
<span id="cb917-3"><a href="#cb917-3" aria-hidden="true" tabindex="-1"></a><span>for</span> i, (x,y) <span>in</span> <span>enumerate</span>(coords):</span>
<span id="cb917-4"><a href="#cb917-4" aria-hidden="true" tabindex="-1"></a>    plt.text(x, y, <span>str</span>(i), fontsize<span>=</span><span>12</span>, ha<span>=</span><span>&#34;center&#34;</span>, va<span>=</span><span>&#34;center&#34;</span>, color<span>=</span><span>&#34;white&#34;</span>)</span>
<span id="cb917-5"><a href="#cb917-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span>&#34;Spectral embedding of graph&#34;</span>)</span>
<span id="cb917-6"><a href="#cb917-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-536-output-1.png" width="582" height="431"/></p>
</figure>
</div>
</div>
</div>
</section>
<section id="try-it-yourself-91">
<h4 data-anchor-id="try-it-yourself-91">Try It Yourself</h4>
<ol type="1">
<li>Remove one edge from the graph and see how Laplacian eigenvalues change.</li>
<li>Add a disconnected node - does an extra zero eigenvalue appear?</li>
<li>Try a random graph and compare adjacency vs Laplacian spectra.</li>
</ol>
</section>
<section id="the-takeaway-75">
<h4 data-anchor-id="the-takeaway-75">The Takeaway</h4>
<ul>
<li>Adjacency matrices describe direct graph structure.</li>
<li>Laplacians capture connectivity and diffusion.</li>
<li>Eigenvalues of <span>\(L\)</span> reveal graph properties like connectedness and clustering - bridging networks with linear algebra.</li>
</ul>
</section>
</section>
<section id="data-preprocessing-as-linear-ops-centering-whitening-scaling">
<h3 data-anchor-id="data-preprocessing-as-linear-ops-centering-whitening-scaling">94. Data Preprocessing as Linear Ops (Centering, Whitening, Scaling)</h3>
<p>Many machine learning and data analysis workflows begin with preprocessing, and linear algebra provides the tools.</p>
<ul>
<li>Centering: subtract the mean → move data to origin.</li>
<li>Scaling: divide by standard deviation → normalize feature ranges.</li>
<li>Whitening: decorrelate features → make covariance matrix the identity.</li>
</ul>
<p>Each step can be written as a matrix operation.</p>
<section id="set-up-your-lab-93">
<h4 data-anchor-id="set-up-your-lab-93">Set Up Your Lab</h4>
<div id="792dc3fc" data-execution_count="536">
<div><div id="cb918"><pre><code><span id="cb918-1"><a href="#cb918-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb918-2"><a href="#cb918-2" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-93">
<h4 data-anchor-id="step-by-step-code-walkthrough-93">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Generate correlated data</li>
</ol>
<div id="45baa38e" data-execution_count="537">
<div><div id="cb919"><pre><code><span id="cb919-1"><a href="#cb919-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span>0</span>)</span>
<span id="cb919-2"><a href="#cb919-2" aria-hidden="true" tabindex="-1"></a>X <span>=</span> np.random.randn(<span>200</span>, <span>2</span>) <span>@</span> np.array([[<span>3</span>,<span>1</span>],[<span>1</span>,<span>0.5</span>]])</span>
<span id="cb919-3"><a href="#cb919-3" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:,<span>0</span>], X[:,<span>1</span>], alpha<span>=</span><span>0.4</span>)</span>
<span id="cb919-4"><a href="#cb919-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span>&#34;Original correlated data&#34;</span>)</span>
<span id="cb919-5"><a href="#cb919-5" aria-hidden="true" tabindex="-1"></a>plt.axis(<span>&#34;equal&#34;</span>)</span>
<span id="cb919-6"><a href="#cb919-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-538-output-1.png" width="569" height="431"/></p>
</figure>
</div>
</div>
</div>
<ol start="2" type="1">
<li>Centering (subtract mean)</li>
</ol>
<div id="ac953bbe" data-execution_count="538">
<div><div id="cb920"><pre><code><span id="cb920-1"><a href="#cb920-1" aria-hidden="true" tabindex="-1"></a>X_centered <span>=</span> X <span>-</span> X.mean(axis<span>=</span><span>0</span>)</span>
<span id="cb920-2"><a href="#cb920-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Mean after centering:&#34;</span>, X_centered.mean(axis<span>=</span><span>0</span>))</span></code></pre></div></div>
<div>
<pre><code>Mean after centering: [ 8.88178420e-18 -1.22124533e-17]</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Scaling (normalize features)</li>
</ol>
<div id="5446af64" data-execution_count="539">
<div><div id="cb922"><pre><code><span id="cb922-1"><a href="#cb922-1" aria-hidden="true" tabindex="-1"></a>X_scaled <span>=</span> X_centered <span>/</span> X_centered.std(axis<span>=</span><span>0</span>)</span>
<span id="cb922-2"><a href="#cb922-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Std after scaling:&#34;</span>, X_scaled.std(axis<span>=</span><span>0</span>))</span></code></pre></div></div>
<div>
<pre><code>Std after scaling: [1. 1.]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Whitening via eigen-decomposition</li>
</ol>
<p>Covariance of centered data:</p>
<div id="b3f5265b" data-execution_count="540">
<div><div id="cb924"><pre><code><span id="cb924-1"><a href="#cb924-1" aria-hidden="true" tabindex="-1"></a>C <span>=</span> np.cov(X_centered.T)</span>
<span id="cb924-2"><a href="#cb924-2" aria-hidden="true" tabindex="-1"></a>eigvals, eigvecs <span>=</span> np.linalg.eigh(C)</span>
<span id="cb924-3"><a href="#cb924-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb924-4"><a href="#cb924-4" aria-hidden="true" tabindex="-1"></a>W <span>=</span> eigvecs <span>@</span> np.diag(<span>1</span><span>/</span>np.sqrt(eigvals)) <span>@</span> eigvecs.T</span>
<span id="cb924-5"><a href="#cb924-5" aria-hidden="true" tabindex="-1"></a>X_white <span>=</span> X_centered <span>@</span> W</span></code></pre></div></div>
</div>
<p>Check covariance:</p>
<div id="31827587" data-execution_count="541">
<div><div id="cb925"><pre><code><span id="cb925-1"><a href="#cb925-1" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Whitened covariance:</span><span>\n</span><span>&#34;</span>, np.cov(X_white.T))</span></code></pre></div></div>
<div>
<pre><code>Whitened covariance:
 [[1.00000000e+00 2.54402864e-15]
 [2.54402864e-15 1.00000000e+00]]</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Compare scatter plots</li>
</ol>
<div id="60d5737a" data-execution_count="542">
<div><div id="cb927"><pre><code><span id="cb927-1"><a href="#cb927-1" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span>1</span>,<span>3</span>,<span>1</span>)</span>
<span id="cb927-2"><a href="#cb927-2" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:,<span>0</span>], X[:,<span>1</span>], alpha<span>=</span><span>0.4</span>)</span>
<span id="cb927-3"><a href="#cb927-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span>&#34;Original&#34;</span>)</span>
<span id="cb927-4"><a href="#cb927-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb927-5"><a href="#cb927-5" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span>1</span>,<span>3</span>,<span>2</span>)</span>
<span id="cb927-6"><a href="#cb927-6" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_scaled[:,<span>0</span>], X_scaled[:,<span>1</span>], alpha<span>=</span><span>0.4</span>)</span>
<span id="cb927-7"><a href="#cb927-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span>&#34;Scaled&#34;</span>)</span>
<span id="cb927-8"><a href="#cb927-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb927-9"><a href="#cb927-9" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span>1</span>,<span>3</span>,<span>3</span>)</span>
<span id="cb927-10"><a href="#cb927-10" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_white[:,<span>0</span>], X_white[:,<span>1</span>], alpha<span>=</span><span>0.4</span>)</span>
<span id="cb927-11"><a href="#cb927-11" aria-hidden="true" tabindex="-1"></a>plt.title(<span>&#34;Whitened&#34;</span>)</span>
<span id="cb927-12"><a href="#cb927-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb927-13"><a href="#cb927-13" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb927-14"><a href="#cb927-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-543-output-1.png" width="662" height="470"/></p>
</figure>
</div>
</div>
</div>
<ul>
<li>Original: elongated ellipse.</li>
<li>Scaled: axis-aligned ellipse.</li>
<li>Whitened: circular cloud (uncorrelated, unit variance).</li>
</ul>
</section>
<section id="try-it-yourself-92">
<h4 data-anchor-id="try-it-yourself-92">Try It Yourself</h4>
<ol type="1">
<li>Add a third feature and apply centering, scaling, whitening.</li>
<li>Compare whitening with PCA - they use the same eigen-decomposition.</li>
<li>Test what happens if you skip centering before whitening.</li>
</ol>
</section>
<section id="the-takeaway-76">
<h4 data-anchor-id="the-takeaway-76">The Takeaway</h4>
<ul>
<li>Centering → mean zero.</li>
<li>Scaling → unit variance.</li>
<li>Whitening → features uncorrelated, variance = 1. Linear algebra provides the exact matrix operations to make preprocessing systematic and reliable.</li>
</ul>
</section>
</section>
<section id="linear-regression-and-classification-from-model-to-matrix">
<h3 data-anchor-id="linear-regression-and-classification-from-model-to-matrix">95. Linear Regression and Classification (From Model to Matrix)</h3>
<p>Linear regression and classification problems can be written neatly in matrix form. This unifies data, models, and solutions under the framework of least squares and linear decision boundaries.</p>
<section id="linear-regression-model">
<h4 data-anchor-id="linear-regression-model">Linear Regression Model</h4>
<p>For data <span>\((x_i, y_i)\)</span>:</p>
<p><span>\[
y \approx X \beta
\]</span></p>
<ul>
<li><span>\(X\)</span>: design matrix (rows = samples, columns = features).</li>
<li><span>\(\beta\)</span>: coefficients to solve for.</li>
<li>Solution (least squares):</li>
</ul>
<p><span>\[
\hat{\beta} = (X^T X)^{-1} X^T y
\]</span></p>
</section>
<section id="set-up-your-lab-94">
<h4 data-anchor-id="set-up-your-lab-94">Set Up Your Lab</h4>
<div id="e8b6b4df" data-execution_count="543">
<div><div id="cb928"><pre><code><span id="cb928-1"><a href="#cb928-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb928-2"><a href="#cb928-2" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span>
<span id="cb928-3"><a href="#cb928-3" aria-hidden="true" tabindex="-1"></a><span>from</span> sklearn.datasets <span>import</span> make_classification</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-94">
<h4 data-anchor-id="step-by-step-code-walkthrough-94">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Linear regression example</li>
</ol>
<div id="5194d201" data-execution_count="544">
<div><div id="cb929"><pre><code><span id="cb929-1"><a href="#cb929-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span>0</span>)</span>
<span id="cb929-2"><a href="#cb929-2" aria-hidden="true" tabindex="-1"></a>X <span>=</span> np.linspace(<span>0</span>, <span>10</span>, <span>30</span>).reshape(<span>-</span><span>1</span>,<span>1</span>)</span>
<span id="cb929-3"><a href="#cb929-3" aria-hidden="true" tabindex="-1"></a>y <span>=</span> <span>3</span><span>*</span>X.squeeze() <span>+</span> <span>5</span> <span>+</span> np.random.randn(<span>30</span>)<span>*</span><span>2</span></span></code></pre></div></div>
</div>
<p>Construct design matrix with bias term:</p>
<div id="fabf5d4e" data-execution_count="545">
<div><div id="cb930"><pre><code><span id="cb930-1"><a href="#cb930-1" aria-hidden="true" tabindex="-1"></a>X_design <span>=</span> np.column_stack([np.ones_like(X), X])</span>
<span id="cb930-2"><a href="#cb930-2" aria-hidden="true" tabindex="-1"></a>beta_hat, <span>*</span>_ <span>=</span> np.linalg.lstsq(X_design, y, rcond<span>=</span><span>None</span>)</span>
<span id="cb930-3"><a href="#cb930-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Fitted coefficients:&#34;</span>, beta_hat)</span></code></pre></div></div>
<div>
<pre><code>Fitted coefficients: [6.65833151 2.84547628]</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Visualize regression line</li>
</ol>
<div id="0dcf8422" data-execution_count="546">
<div><div id="cb932"><pre><code><span id="cb932-1"><a href="#cb932-1" aria-hidden="true" tabindex="-1"></a>y_pred <span>=</span> X_design <span>@</span> beta_hat</span>
<span id="cb932-2"><a href="#cb932-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb932-3"><a href="#cb932-3" aria-hidden="true" tabindex="-1"></a>plt.scatter(X, y, label<span>=</span><span>&#34;Data&#34;</span>)</span>
<span id="cb932-4"><a href="#cb932-4" aria-hidden="true" tabindex="-1"></a>plt.plot(X, y_pred, <span>&#39;r-&#39;</span>, label<span>=</span><span>&#34;Fitted line&#34;</span>)</span>
<span id="cb932-5"><a href="#cb932-5" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb932-6"><a href="#cb932-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-547-output-1.png" width="566" height="411"/></p>
</figure>
</div>
</div>
</div>
<ol start="3" type="1">
<li>Logistic classification with linear decision boundary</li>
</ol>
<div id="691d7b6a" data-execution_count="547">
<div><div id="cb933"><pre><code><span id="cb933-1"><a href="#cb933-1" aria-hidden="true" tabindex="-1"></a>Xc, yc <span>=</span> make_classification(n_features<span>=</span><span>2</span>, n_redundant<span>=</span><span>0</span>, n_informative<span>=</span><span>2</span>,</span>
<span id="cb933-2"><a href="#cb933-2" aria-hidden="true" tabindex="-1"></a>                             n_clusters_per_class<span>=</span><span>1</span>, n_samples<span>=</span><span>100</span>, random_state<span>=</span><span>0</span>)</span>
<span id="cb933-3"><a href="#cb933-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb933-4"><a href="#cb933-4" aria-hidden="true" tabindex="-1"></a>plt.scatter(Xc[:,<span>0</span>], Xc[:,<span>1</span>], c<span>=</span>yc, cmap<span>=</span><span>&#34;bwr&#34;</span>, alpha<span>=</span><span>0.7</span>)</span>
<span id="cb933-5"><a href="#cb933-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span>&#34;Classification data&#34;</span>)</span>
<span id="cb933-6"><a href="#cb933-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-548-output-1.png" width="569" height="431"/></p>
</figure>
</div>
</div>
</div>
<ol start="4" type="1">
<li>Logistic regression via gradient descent</li>
</ol>
<div id="9af00f2e" data-execution_count="548">
<div><div id="cb934"><pre><code><span id="cb934-1"><a href="#cb934-1" aria-hidden="true" tabindex="-1"></a><span>def</span> sigmoid(z):</span>
<span id="cb934-2"><a href="#cb934-2" aria-hidden="true" tabindex="-1"></a>    <span>return</span> <span>1</span><span>/</span>(<span>1</span><span>+</span>np.exp(<span>-</span>z))</span>
<span id="cb934-3"><a href="#cb934-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb934-4"><a href="#cb934-4" aria-hidden="true" tabindex="-1"></a>X_design <span>=</span> np.column_stack([np.ones(<span>len</span>(Xc)), Xc])</span>
<span id="cb934-5"><a href="#cb934-5" aria-hidden="true" tabindex="-1"></a>y <span>=</span> yc</span>
<span id="cb934-6"><a href="#cb934-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb934-7"><a href="#cb934-7" aria-hidden="true" tabindex="-1"></a>w <span>=</span> np.zeros(X_design.shape[<span>1</span>])</span>
<span id="cb934-8"><a href="#cb934-8" aria-hidden="true" tabindex="-1"></a>lr <span>=</span> <span>0.1</span></span>
<span id="cb934-9"><a href="#cb934-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb934-10"><a href="#cb934-10" aria-hidden="true" tabindex="-1"></a><span>for</span> _ <span>in</span> <span>range</span>(<span>2000</span>):</span>
<span id="cb934-11"><a href="#cb934-11" aria-hidden="true" tabindex="-1"></a>    preds <span>=</span> sigmoid(X_design <span>@</span> w)</span>
<span id="cb934-12"><a href="#cb934-12" aria-hidden="true" tabindex="-1"></a>    grad <span>=</span> X_design.T <span>@</span> (preds <span>-</span> y) <span>/</span> <span>len</span>(y)</span>
<span id="cb934-13"><a href="#cb934-13" aria-hidden="true" tabindex="-1"></a>    w <span>-=</span> lr <span>*</span> grad</span>
<span id="cb934-14"><a href="#cb934-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb934-15"><a href="#cb934-15" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Learned weights:&#34;</span>, w)</span></code></pre></div></div>
<div>
<pre><code>Learned weights: [-2.10451116  0.70752542  4.13295129]</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Plot decision boundary</li>
</ol>
<div id="1085020b" data-execution_count="549">
<div><div id="cb936"><pre><code><span id="cb936-1"><a href="#cb936-1" aria-hidden="true" tabindex="-1"></a>xx, yy <span>=</span> np.meshgrid(np.linspace(Xc[:,<span>0</span>].<span>min</span>()<span>-</span><span>1</span>, Xc[:,<span>0</span>].<span>max</span>()<span>+</span><span>1</span>, <span>200</span>),</span>
<span id="cb936-2"><a href="#cb936-2" aria-hidden="true" tabindex="-1"></a>                     np.linspace(Xc[:,<span>1</span>].<span>min</span>()<span>-</span><span>1</span>, Xc[:,<span>1</span>].<span>max</span>()<span>+</span><span>1</span>, <span>200</span>))</span>
<span id="cb936-3"><a href="#cb936-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb936-4"><a href="#cb936-4" aria-hidden="true" tabindex="-1"></a>grid <span>=</span> np.c_[np.ones(xx.size), xx.ravel(), yy.ravel()]</span>
<span id="cb936-5"><a href="#cb936-5" aria-hidden="true" tabindex="-1"></a>probs <span>=</span> sigmoid(grid <span>@</span> w).reshape(xx.shape)</span>
<span id="cb936-6"><a href="#cb936-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb936-7"><a href="#cb936-7" aria-hidden="true" tabindex="-1"></a>plt.contourf(xx, yy, probs, levels<span>=</span>[<span>0</span>,<span>0.5</span>,<span>1</span>], alpha<span>=</span><span>0.3</span>, cmap<span>=</span><span>&#34;bwr&#34;</span>)</span>
<span id="cb936-8"><a href="#cb936-8" aria-hidden="true" tabindex="-1"></a>plt.scatter(Xc[:,<span>0</span>], Xc[:,<span>1</span>], c<span>=</span>yc, cmap<span>=</span><span>&#34;bwr&#34;</span>, edgecolor<span>=</span><span>&#34;k&#34;</span>)</span>
<span id="cb936-9"><a href="#cb936-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span>&#34;Linear decision boundary&#34;</span>)</span>
<span id="cb936-10"><a href="#cb936-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-550-output-1.png" width="569" height="431"/></p>
</figure>
</div>
</div>
</div>
</section>
<section id="try-it-yourself-93">
<h4 data-anchor-id="try-it-yourself-93">Try It Yourself</h4>
<ol type="1">
<li>Add polynomial features to regression and refit. Does the line bend into a curve?</li>
<li>Change learning rate in logistic regression - what happens?</li>
<li>Generate data that is not linearly separable. Can a linear model still classify well?</li>
</ol>
</section>
<section id="the-takeaway-77">
<h4 data-anchor-id="the-takeaway-77">The Takeaway</h4>
<ul>
<li>Regression and classification fit naturally into linear algebra with matrix formulations.</li>
<li>Least squares solves regression directly; logistic regression requires optimization.</li>
<li>Linear models are simple, interpretable, and still form the foundation of modern machine learning.</li>
</ul>
</section>
</section>
<section id="pca-in-practice-dimensionality-reduction-workflow">
<h3 data-anchor-id="pca-in-practice-dimensionality-reduction-workflow">96. PCA in Practice (Dimensionality Reduction Workflow)</h3>
<p>Principal Component Analysis (PCA) is widely used to reduce dimensions, compress data, and visualize high-dimensional datasets. Here, we’ll walk through a full PCA workflow: centering, computing components, projecting, and visualizing.</p>
<section id="set-up-your-lab-95">
<h4 data-anchor-id="set-up-your-lab-95">Set Up Your Lab</h4>
<div id="371a6791" data-execution_count="550">
<div><div id="cb937"><pre><code><span id="cb937-1"><a href="#cb937-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb937-2"><a href="#cb937-2" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span>
<span id="cb937-3"><a href="#cb937-3" aria-hidden="true" tabindex="-1"></a><span>from</span> sklearn.datasets <span>import</span> load_digits</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-95">
<h4 data-anchor-id="step-by-step-code-walkthrough-95">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Load dataset (digits)</li>
</ol>
<div id="bdc48c42" data-execution_count="551">
<div><div id="cb938"><pre><code><span id="cb938-1"><a href="#cb938-1" aria-hidden="true" tabindex="-1"></a>digits <span>=</span> load_digits()</span>
<span id="cb938-2"><a href="#cb938-2" aria-hidden="true" tabindex="-1"></a>X <span>=</span> digits.data  <span># shape (1797, 64)</span></span>
<span id="cb938-3"><a href="#cb938-3" aria-hidden="true" tabindex="-1"></a>y <span>=</span> digits.target</span>
<span id="cb938-4"><a href="#cb938-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Data shape:&#34;</span>, X.shape)</span></code></pre></div></div>

</div>
<p>Each sample is an 8×8 grayscale image flattened into 64 features.</p>
<ol start="2" type="1">
<li>Center the data</li>
</ol>
<div id="815b2e39" data-execution_count="552">
<div><div id="cb940"><pre><code><span id="cb940-1"><a href="#cb940-1" aria-hidden="true" tabindex="-1"></a>X_centered <span>=</span> X <span>-</span> X.mean(axis<span>=</span><span>0</span>)</span></code></pre></div></div>
</div>
<ol start="3" type="1">
<li>Compute PCA via SVD</li>
</ol>
<div id="e4b8509b" data-execution_count="553">
<div><div id="cb941"><pre><code><span id="cb941-1"><a href="#cb941-1" aria-hidden="true" tabindex="-1"></a>U, S, Vt <span>=</span> np.linalg.svd(X_centered, full_matrices<span>=</span><span>False</span>)</span>
<span id="cb941-2"><a href="#cb941-2" aria-hidden="true" tabindex="-1"></a>explained_variance <span>=</span> (S<span>**</span><span>2</span>) <span>/</span> (<span>len</span>(X) <span>-</span> <span>1</span>)</span>
<span id="cb941-3"><a href="#cb941-3" aria-hidden="true" tabindex="-1"></a>explained_ratio <span>=</span> explained_variance <span>/</span> explained_variance.<span>sum</span>()</span></code></pre></div></div>
</div>
<ol start="4" type="1">
<li>Plot explained variance ratio</li>
</ol>
<div id="6546ba1f" data-execution_count="554">
<div><div id="cb942"><pre><code><span id="cb942-1"><a href="#cb942-1" aria-hidden="true" tabindex="-1"></a>plt.plot(np.cumsum(explained_ratio[:<span>30</span>]), <span>&#39;o-&#39;</span>)</span>
<span id="cb942-2"><a href="#cb942-2" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span>&#34;Number of components&#34;</span>)</span>
<span id="cb942-3"><a href="#cb942-3" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span>&#34;Cumulative explained variance&#34;</span>)</span>
<span id="cb942-4"><a href="#cb942-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span>&#34;PCA explained variance&#34;</span>)</span>
<span id="cb942-5"><a href="#cb942-5" aria-hidden="true" tabindex="-1"></a>plt.grid(<span>True</span>)</span>
<span id="cb942-6"><a href="#cb942-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-555-output-1.png" width="590" height="449"/></p>
</figure>
</div>
</div>
</div>
<p>This shows how many components are needed to capture most variance.</p>
<ol start="5" type="1">
<li>Project onto top 2 components for visualization</li>
</ol>
<div id="1064dba5" data-execution_count="555">
<div><div id="cb943"><pre><code><span id="cb943-1"><a href="#cb943-1" aria-hidden="true" tabindex="-1"></a>X_pca2 <span>=</span> X_centered <span>@</span> Vt[:<span>2</span>].T</span>
<span id="cb943-2"><a href="#cb943-2" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_pca2[:,<span>0</span>], X_pca2[:,<span>1</span>], c<span>=</span>y, cmap<span>=</span><span>&#34;tab10&#34;</span>, alpha<span>=</span><span>0.6</span>, s<span>=</span><span>15</span>)</span>
<span id="cb943-3"><a href="#cb943-3" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb943-4"><a href="#cb943-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span>&#34;Digits dataset (PCA 2D projection)&#34;</span>)</span>
<span id="cb943-5"><a href="#cb943-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-556-output-1.png" width="536" height="431"/></p>
</figure>
</div>
</div>
</div>
<ol start="6" type="1">
<li>Reconstruct images from reduced dimensions</li>
</ol>
<div id="4adf8ea5" data-execution_count="556">
<div><div id="cb944"><pre><code><span id="cb944-1"><a href="#cb944-1" aria-hidden="true" tabindex="-1"></a>k <span>=</span> <span>20</span></span>
<span id="cb944-2"><a href="#cb944-2" aria-hidden="true" tabindex="-1"></a>X_pca20 <span>=</span> X_centered <span>@</span> Vt[:k].T</span>
<span id="cb944-3"><a href="#cb944-3" aria-hidden="true" tabindex="-1"></a>X_reconstructed <span>=</span> X_pca20 <span>@</span> Vt[:k]</span>
<span id="cb944-4"><a href="#cb944-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb944-5"><a href="#cb944-5" aria-hidden="true" tabindex="-1"></a>fig, axes <span>=</span> plt.subplots(<span>2</span>, <span>10</span>, figsize<span>=</span>(<span>10</span>,<span>2</span>))</span>
<span id="cb944-6"><a href="#cb944-6" aria-hidden="true" tabindex="-1"></a><span>for</span> i <span>in</span> <span>range</span>(<span>10</span>):</span>
<span id="cb944-7"><a href="#cb944-7" aria-hidden="true" tabindex="-1"></a>    axes[<span>0</span>,i].imshow(X[i].reshape(<span>8</span>,<span>8</span>), cmap<span>=</span><span>&#34;gray&#34;</span>)</span>
<span id="cb944-8"><a href="#cb944-8" aria-hidden="true" tabindex="-1"></a>    axes[<span>0</span>,i].axis(<span>&#34;off&#34;</span>)</span>
<span id="cb944-9"><a href="#cb944-9" aria-hidden="true" tabindex="-1"></a>    axes[<span>1</span>,i].imshow(X_reconstructed[i].reshape(<span>8</span>,<span>8</span>), cmap<span>=</span><span>&#34;gray&#34;</span>)</span>
<span id="cb944-10"><a href="#cb944-10" aria-hidden="true" tabindex="-1"></a>    axes[<span>1</span>,i].axis(<span>&#34;off&#34;</span>)</span>
<span id="cb944-11"><a href="#cb944-11" aria-hidden="true" tabindex="-1"></a>plt.suptitle(<span>&#34;Original (top) vs PCA reconstruction (bottom, 20 comps)&#34;</span>)</span>
<span id="cb944-12"><a href="#cb944-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-557-output-1.png" width="763" height="184"/></p>
</figure>
</div>
</div>
</div>
<p>Even with only 20/64 components, the digits remain recognizable.</p>
</section>
<section id="try-it-yourself-94">
<h4 data-anchor-id="try-it-yourself-94">Try It Yourself</h4>
<ol type="1">
<li>Change <span>\(k\)</span> to 5, 10, 30 - how do reconstructions change?</li>
<li>Use top 2 PCA components to classify digits with k-NN. How does accuracy compare to full 64 features?</li>
<li>Try PCA on your own dataset (images, tabular data).</li>
</ol>
</section>
<section id="the-takeaway-78">
<h4 data-anchor-id="the-takeaway-78">The Takeaway</h4>
<ul>
<li>PCA reduces dimensions while keeping maximum variance.</li>
<li>In practice: center → decompose → select top components → project/reconstruct.</li>
<li>PCA enables visualization, compression, and denoising in real-world workflows.</li>
</ul>
</section>
</section>
<section id="recommender-systems-and-low-rank-models-fill-the-missing-entries">
<h3 data-anchor-id="recommender-systems-and-low-rank-models-fill-the-missing-entries">97. Recommender Systems and Low-Rank Models (Fill the Missing Entries)</h3>
<p>Recommender systems often deal with incomplete matrices - rows are users, columns are items, entries are ratings. Most entries are missing, but the matrix is usually close to low-rank (because user preferences depend on only a few hidden factors). SVD and low-rank approximations are powerful tools to fill in these missing values.</p>
<section id="set-up-your-lab-96">
<h4 data-anchor-id="set-up-your-lab-96">Set Up Your Lab</h4>
<div id="97ca48fe" data-execution_count="557">
<div><div id="cb945"><pre><code><span id="cb945-1"><a href="#cb945-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb945-2"><a href="#cb945-2" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-96">
<h4 data-anchor-id="step-by-step-code-walkthrough-96">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Simulate a user–item rating matrix</li>
</ol>
<div id="b02cb992" data-execution_count="558">
<div><div id="cb946"><pre><code><span id="cb946-1"><a href="#cb946-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span>0</span>)</span>
<span id="cb946-2"><a href="#cb946-2" aria-hidden="true" tabindex="-1"></a>true_users <span>=</span> np.random.randn(<span>10</span>, <span>3</span>)   <span># 10 users, 3 latent features</span></span>
<span id="cb946-3"><a href="#cb946-3" aria-hidden="true" tabindex="-1"></a>true_items <span>=</span> np.random.randn(<span>3</span>, <span>8</span>)    <span># 8 items</span></span>
<span id="cb946-4"><a href="#cb946-4" aria-hidden="true" tabindex="-1"></a>R_full <span>=</span> true_users <span>@</span> true_items      <span># true low-rank ratings</span></span></code></pre></div></div>
</div>
<ol start="2" type="1">
<li>Hide some ratings (simulate missing data)</li>
</ol>
<div id="a149f707" data-execution_count="559">
<div><div id="cb947"><pre><code><span id="cb947-1"><a href="#cb947-1" aria-hidden="true" tabindex="-1"></a>mask <span>=</span> np.random.rand(<span>*</span>R_full.shape) <span>&gt;</span> <span>0.3</span>  <span># keep 70% of entries</span></span>
<span id="cb947-2"><a href="#cb947-2" aria-hidden="true" tabindex="-1"></a>R_obs <span>=</span> np.where(mask, R_full, np.nan)</span>
<span id="cb947-3"><a href="#cb947-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb947-4"><a href="#cb947-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Observed ratings:</span><span>\n</span><span>&#34;</span>, R_obs)</span></code></pre></div></div>
<div>
<pre><code>Observed ratings:
 [[-1.10781465         nan -3.56526968         nan -2.1729387   1.43510077
   1.46641178  0.79023284]
 [ 0.84819453         nan         nan         nan         nan         nan
   2.30434358  3.03008138]
 [        nan  0.32479187 -0.51818422         nan  0.02013802         nan
   1.29874918  1.33053637]
 [-1.81407786  1.24241182         nan -1.32723907         nan         nan
  -0.31110699         nan]
 [-0.48527696         nan -1.51957106         nan -0.86984941  0.52807989
          nan  0.33771451]
 [-0.26997359 -0.48498966         nan -2.73891459 -2.48167957  2.88740609
  -0.24614835         nan]
 [ 3.57769701 -1.608339    4.73789234  1.13583164  3.63451505 -2.60495928
   2.12453635  3.76472563]
 [ 0.69623809 -0.59117353 -0.28890188 -2.36431192         nan  1.50136796
   0.74268078         nan]
 [ 0.85768141  1.33357168         nan         nan  1.65089037 -2.46456289
   3.51030491  3.31220347]
 [-2.463496    0.60826298 -3.81241599 -2.11839267 -3.86597359  3.52934055
  -1.76203083 -2.63130953]]</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Simple mean imputation (baseline)</li>
</ol>
<div id="208ac586" data-execution_count="560">
<div><div id="cb949"><pre><code><span id="cb949-1"><a href="#cb949-1" aria-hidden="true" tabindex="-1"></a>R_mean <span>=</span> np.where(np.isnan(R_obs), np.nanmean(R_obs), R_obs)</span></code></pre></div></div>
</div>
<ol start="4" type="1">
<li>Apply SVD for low-rank approximation</li>
</ol>
<div id="c39b13e2" data-execution_count="561">
<div><div id="cb950"><pre><code><span id="cb950-1"><a href="#cb950-1" aria-hidden="true" tabindex="-1"></a><span># Replace NaNs with zeros for SVD step</span></span>
<span id="cb950-2"><a href="#cb950-2" aria-hidden="true" tabindex="-1"></a>R_filled <span>=</span> np.nan_to_num(R_obs, nan<span>=</span><span>0.0</span>)</span>
<span id="cb950-3"><a href="#cb950-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb950-4"><a href="#cb950-4" aria-hidden="true" tabindex="-1"></a>U, S, Vt <span>=</span> np.linalg.svd(R_filled, full_matrices<span>=</span><span>False</span>)</span>
<span id="cb950-5"><a href="#cb950-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb950-6"><a href="#cb950-6" aria-hidden="true" tabindex="-1"></a>k <span>=</span> <span>3</span>  <span># latent dimension</span></span>
<span id="cb950-7"><a href="#cb950-7" aria-hidden="true" tabindex="-1"></a>R_approx <span>=</span> U[:, :k] <span>@</span> np.diag(S[:k]) <span>@</span> Vt[:k, :]</span></code></pre></div></div>
</div>
<ol start="5" type="1">
<li>Compare filled matrix with ground truth</li>
</ol>
<div id="67904f97" data-execution_count="562">
<div><div id="cb951"><pre><code><span id="cb951-1"><a href="#cb951-1" aria-hidden="true" tabindex="-1"></a>error <span>=</span> np.nanmean((R_full <span>-</span> R_approx)<span>**</span><span>2</span>)</span>
<span id="cb951-2"><a href="#cb951-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Approximation error (MSE):&#34;</span>, error)</span></code></pre></div></div>
<div>
<pre><code>Approximation error (MSE): 1.4862378490976202</code></pre>
</div>
</div>
<ol start="6" type="1">
<li>Visualize original vs reconstructed</li>
</ol>
<div id="6e6cf3f3" data-execution_count="563">
<div><div id="cb953"><pre><code><span id="cb953-1"><a href="#cb953-1" aria-hidden="true" tabindex="-1"></a>fig, axes <span>=</span> plt.subplots(<span>1</span>, <span>2</span>, figsize<span>=</span>(<span>8</span>,<span>4</span>))</span>
<span id="cb953-2"><a href="#cb953-2" aria-hidden="true" tabindex="-1"></a>axes[<span>0</span>].imshow(R_full, cmap<span>=</span><span>&#34;viridis&#34;</span>)</span>
<span id="cb953-3"><a href="#cb953-3" aria-hidden="true" tabindex="-1"></a>axes[<span>0</span>].set_title(<span>&#34;True ratings&#34;</span>)</span>
<span id="cb953-4"><a href="#cb953-4" aria-hidden="true" tabindex="-1"></a>axes[<span>1</span>].imshow(R_approx, cmap<span>=</span><span>&#34;viridis&#34;</span>)</span>
<span id="cb953-5"><a href="#cb953-5" aria-hidden="true" tabindex="-1"></a>axes[<span>1</span>].set_title(<span>&#34;Low-rank approximation&#34;</span>)</span>
<span id="cb953-6"><a href="#cb953-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-564-output-1.png" width="598" height="357"/></p>
</figure>
</div>
</div>
</div>
</section>
<section id="try-it-yourself-95">
<h4 data-anchor-id="try-it-yourself-95">Try It Yourself</h4>
<ol type="1">
<li>Vary <span>\(k\)</span> (2, 3, 5). Does error go down?</li>
<li>Mask more entries (50%, 80%) - how does SVD reconstruction perform?</li>
<li>Use iterative imputation: alternate filling missing entries with low-rank approximations.</li>
</ol>
</section>
<section id="the-takeaway-79">
<h4 data-anchor-id="the-takeaway-79">The Takeaway</h4>
<ul>
<li>Recommender systems rely on low-rank structure of user–item matrices.</li>
<li>SVD provides a natural way to approximate and fill missing ratings.</li>
<li>This low-rank modeling idea underpins modern collaborative filtering systems like Netflix and Spotify recommenders.</li>
</ul>
</section>
</section>
<section id="pagerank-and-random-walks-ranking-with-eigenvectors">
<h3 data-anchor-id="pagerank-and-random-walks-ranking-with-eigenvectors">98. PageRank and Random Walks (Ranking with Eigenvectors)</h3>
<p>The PageRank algorithm, made famous by Google, uses linear algebra and random walks on graphs to rank nodes (webpages, people, items). The idea: importance flows through links - being linked by important nodes makes you important.</p>

<section id="set-up-your-lab-97">
<h4 data-anchor-id="set-up-your-lab-97">Set Up Your Lab</h4>
<div id="a01f2545" data-execution_count="564">
<div><div id="cb954"><pre><code><span id="cb954-1"><a href="#cb954-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb954-2"><a href="#cb954-2" aria-hidden="true" tabindex="-1"></a><span>import</span> networkx <span>as</span> nx</span>
<span id="cb954-3"><a href="#cb954-3" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div></div>
</div>
</section>
<section id="step-by-step-code-walkthrough-97">
<h4 data-anchor-id="step-by-step-code-walkthrough-97">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Build a small directed graph</li>
</ol>
<div id="b6e2fe96" data-execution_count="565">
<div><div id="cb955"><pre><code><span id="cb955-1"><a href="#cb955-1" aria-hidden="true" tabindex="-1"></a>G <span>=</span> nx.DiGraph()</span>
<span id="cb955-2"><a href="#cb955-2" aria-hidden="true" tabindex="-1"></a>G.add_edges_from([</span>
<span id="cb955-3"><a href="#cb955-3" aria-hidden="true" tabindex="-1"></a>    (<span>0</span>,<span>1</span>), (<span>1</span>,<span>2</span>), (<span>2</span>,<span>0</span>),  <span># cycle among 0–1–2</span></span>
<span id="cb955-4"><a href="#cb955-4" aria-hidden="true" tabindex="-1"></a>    (<span>2</span>,<span>3</span>), (<span>3</span>,<span>2</span>),         <span># back-and-forth 2–3</span></span>
<span id="cb955-5"><a href="#cb955-5" aria-hidden="true" tabindex="-1"></a>    (<span>1</span>,<span>3</span>), (<span>3</span>,<span>4</span>), (<span>4</span>,<span>1</span>)   <span># small loop with 1–3–4</span></span>
<span id="cb955-6"><a href="#cb955-6" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb955-7"><a href="#cb955-7" aria-hidden="true" tabindex="-1"></a>nx.draw_circular(G, with_labels<span>=</span><span>True</span>, node_color<span>=</span><span>&#34;lightblue&#34;</span>, node_size<span>=</span><span>800</span>, arrowsize<span>=</span><span>15</span>)</span>
<span id="cb955-8"><a href="#cb955-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-566-output-1.png" width="691" height="499"/></p>
</figure>
</div>
</div>
</div>
<ol start="2" type="1">
<li>Build adjacency and transition matrix</li>
</ol>
<div id="65893da4" data-execution_count="566">
<div><div id="cb956"><pre><code><span id="cb956-1"><a href="#cb956-1" aria-hidden="true" tabindex="-1"></a>n <span>=</span> G.number_of_nodes()</span>
<span id="cb956-2"><a href="#cb956-2" aria-hidden="true" tabindex="-1"></a>A <span>=</span> nx.to_numpy_array(G, nodelist<span>=</span><span>range</span>(n))</span>
<span id="cb956-3"><a href="#cb956-3" aria-hidden="true" tabindex="-1"></a>P <span>=</span> A <span>/</span> A.<span>sum</span>(axis<span>=</span><span>1</span>, keepdims<span>=</span><span>True</span>)  <span># row-stochastic transition matrix</span></span></code></pre></div></div>
</div>
<ol start="3" type="1">
<li>Add teleportation (Google matrix)</li>
</ol>
<div id="d7e0f57a" data-execution_count="567">
<div><div id="cb957"><pre><code><span id="cb957-1"><a href="#cb957-1" aria-hidden="true" tabindex="-1"></a>alpha <span>=</span> <span>0.85</span>  <span># damping factor</span></span>
<span id="cb957-2"><a href="#cb957-2" aria-hidden="true" tabindex="-1"></a>G_matrix <span>=</span> alpha <span>*</span> P <span>+</span> (<span>1</span> <span>-</span> alpha) <span>*</span> np.ones((n,n)) <span>/</span> n</span></code></pre></div></div>
</div>
<ol start="4" type="1">
<li>Power iteration to compute PageRank</li>
</ol>
<div id="13505b0b" data-execution_count="568">
<div><div id="cb958"><pre><code><span id="cb958-1"><a href="#cb958-1" aria-hidden="true" tabindex="-1"></a>r <span>=</span> np.ones(n) <span>/</span> n  <span># start uniform</span></span>
<span id="cb958-2"><a href="#cb958-2" aria-hidden="true" tabindex="-1"></a><span>for</span> _ <span>in</span> <span>range</span>(<span>100</span>):</span>
<span id="cb958-3"><a href="#cb958-3" aria-hidden="true" tabindex="-1"></a>    r <span>=</span> r <span>@</span> G_matrix</span>
<span id="cb958-4"><a href="#cb958-4" aria-hidden="true" tabindex="-1"></a>r <span>/=</span> r.<span>sum</span>()</span>
<span id="cb958-5"><a href="#cb958-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;PageRank vector:&#34;</span>, r)</span></code></pre></div></div>
<div>
<pre><code>PageRank vector: [0.13219034 0.25472358 0.24044787 0.24044787 0.13219034]</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Compare with NetworkX built-in</li>
</ol>
<div id="d71f3c41" data-execution_count="569">
<div><div id="cb960"><pre><code><span id="cb960-1"><a href="#cb960-1" aria-hidden="true" tabindex="-1"></a>pr <span>=</span> nx.pagerank(G, alpha<span>=</span>alpha)</span>
<span id="cb960-2"><a href="#cb960-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;NetworkX PageRank:&#34;</span>, pr)</span></code></pre></div></div>
<div>
<pre><code>NetworkX PageRank: {0: 0.13219008157546333, 1: 0.2547244023837789, 2: 0.24044771723264727, 3: 0.24044771723264727, 4: 0.13219008157546333}</code></pre>
</div>
</div>
<ol start="6" type="1">
<li>Visualize node importance</li>
</ol>
<div id="0bced649" data-execution_count="570">
<div><div id="cb962"><pre><code><span id="cb962-1"><a href="#cb962-1" aria-hidden="true" tabindex="-1"></a>sizes <span>=</span> [<span>5000</span> <span>*</span> r_i <span>for</span> r_i <span>in</span> r]</span>
<span id="cb962-2"><a href="#cb962-2" aria-hidden="true" tabindex="-1"></a>nx.draw_circular(G, with_labels<span>=</span><span>True</span>, node_size<span>=</span>sizes, node_color<span>=</span><span>&#34;lightblue&#34;</span>, arrowsize<span>=</span><span>15</span>)</span>
<span id="cb962-3"><a href="#cb962-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span>&#34;PageRank visualization (node size ~ importance)&#34;</span>)</span>
<span id="cb962-4"><a href="#cb962-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-571-output-1.png" width="691" height="519"/></p>
</figure>
</div>
</div>
</div>
</section>
<section id="try-it-yourself-96">
<h4 data-anchor-id="try-it-yourself-96">Try It Yourself</h4>
<ol type="1">
<li>Change <span>\(\alpha\)</span> (e.g., 0.6 vs 0.95). Does ranking change?</li>
<li>Add a “dangling node” with no outlinks. How does teleportation handle it?</li>
<li>Try PageRank on a larger graph (like a random graph with 50 nodes).</li>
</ol>
</section>
<section id="the-takeaway-80">
<h4 data-anchor-id="the-takeaway-80">The Takeaway</h4>
<ul>
<li>PageRank is a random-walk steady state problem.</li>
<li>It reduces to finding the dominant eigenvector of the Google matrix.</li>
<li>This method generalizes beyond webpages - to influence ranking, recommendation, and network analysis.</li>
</ul>
</section>
</section>
<section id="numerical-linear-algebra-essentials-floating-point-blaslapack">
<h3 data-anchor-id="numerical-linear-algebra-essentials-floating-point-blaslapack">99. Numerical Linear Algebra Essentials (Floating Point, BLAS/LAPACK)</h3>
<p>When working with linear algebra on computers, numbers are not exact. They live in floating-point arithmetic, and computations rely on highly optimized libraries like BLAS and LAPACK. Understanding these essentials is crucial to doing linear algebra at scale.</p>
<section id="floating-point-basics">
<h4 data-anchor-id="floating-point-basics">Floating Point Basics</h4>
<ul>
<li><p>Numbers are stored in base-2 scientific notation:</p>
<p><span>\[
x = \pm (1.b_1b_2b_3\ldots) \times 2^e
\]</span></p></li>
<li><p>Limited precision means rounding errors.</p></li>
<li><p>Two key constants:</p>
<ul>
<li>Machine epsilon ($<span>\(): smallest difference detectable (\)</span>^{-16}$ for double).</li>
<li>Overflow/underflow: too large or too small to represent.</li>
</ul></li>
</ul>
</section>
<section id="set-up-your-lab-98">
<h4 data-anchor-id="set-up-your-lab-98">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-98">
<h4 data-anchor-id="step-by-step-code-walkthrough-98">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Machine epsilon</li>
</ol>
<div id="0aad442b" data-execution_count="572">
<div><div id="cb964"><pre><code><span id="cb964-1"><a href="#cb964-1" aria-hidden="true" tabindex="-1"></a>eps <span>=</span> np.finfo(<span>float</span>).eps</span>
<span id="cb964-2"><a href="#cb964-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Machine epsilon:&#34;</span>, eps)</span></code></pre></div></div>
<div>
<pre><code>Machine epsilon: 2.220446049250313e-16</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Round-off error demo</li>
</ol>
<div id="15e367d8" data-execution_count="573">
<div><div id="cb966"><pre><code><span id="cb966-1"><a href="#cb966-1" aria-hidden="true" tabindex="-1"></a>a <span>=</span> <span>1e16</span></span>
<span id="cb966-2"><a href="#cb966-2" aria-hidden="true" tabindex="-1"></a>b <span>=</span> <span>1.0</span></span>
<span id="cb966-3"><a href="#cb966-3" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;a + b - a:&#34;</span>, (a <span>+</span> b) <span>-</span> a)  <span># may lose b due to precision limits</span></span></code></pre></div></div>

</div>
<ol start="3" type="1">
<li>Stability of matrix inversion</li>
</ol>
<div id="5bbb754b" data-execution_count="574">
<div><div id="cb968"><pre><code><span id="cb968-1"><a href="#cb968-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>1</span>, <span>1.0001</span>], [<span>1.0001</span>, <span>1</span>]])</span>
<span id="cb968-2"><a href="#cb968-2" aria-hidden="true" tabindex="-1"></a>b <span>=</span> np.array([<span>2</span>, <span>2.0001</span>])</span>
<span id="cb968-3"><a href="#cb968-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb968-4"><a href="#cb968-4" aria-hidden="true" tabindex="-1"></a>x_direct <span>=</span> np.linalg.solve(A, b)</span>
<span id="cb968-5"><a href="#cb968-5" aria-hidden="true" tabindex="-1"></a>x_via_inv <span>=</span> np.linalg.inv(A) <span>@</span> b</span>
<span id="cb968-6"><a href="#cb968-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb968-7"><a href="#cb968-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Solve:&#34;</span>, x_direct)</span>
<span id="cb968-8"><a href="#cb968-8" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Inverse method:&#34;</span>, x_via_inv)</span></code></pre></div></div>
<div>
<pre><code>Solve: [1.499975 0.499975]
Inverse method: [1.499975 0.499975]</code></pre>
</div>
</div>
<p>Notice: using <code>np.linalg.inv</code> can be less stable - better to solve directly.</p>
<ol start="4" type="1">
<li>Conditioning of a matrix</li>
</ol>
<div id="484c8a2c" data-execution_count="575">
<div><div id="cb970"><pre><code><span id="cb970-1"><a href="#cb970-1" aria-hidden="true" tabindex="-1"></a>cond <span>=</span> np.linalg.cond(A)</span>
<span id="cb970-2"><a href="#cb970-2" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Condition number:&#34;</span>, cond)</span></code></pre></div></div>
<div>
<pre><code>Condition number: 20001.00000000417</code></pre>
</div>
</div>
<ul>
<li>Large condition number → small input changes cause big output changes.</li>
</ul>
<ol start="5" type="1">
<li>BLAS/LAPACK under the hood</li>
</ol>
<div id="d05bdc3f" data-execution_count="576">
<div><div id="cb972"><pre><code><span id="cb972-1"><a href="#cb972-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.random.randn(<span>500</span>, <span>500</span>)</span>
<span id="cb972-2"><a href="#cb972-2" aria-hidden="true" tabindex="-1"></a>B <span>=</span> np.random.randn(<span>500</span>, <span>500</span>)</span>
<span id="cb972-3"><a href="#cb972-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb972-4"><a href="#cb972-4" aria-hidden="true" tabindex="-1"></a><span># Matrix multiplication (calls optimized BLAS under the hood)</span></span>
<span id="cb972-5"><a href="#cb972-5" aria-hidden="true" tabindex="-1"></a>C <span>=</span> A <span>@</span> B</span></code></pre></div></div>
</div>
<p>This <code>@</code> operator is not a naive loop - it calls a highly optimized C/Fortran routine.</p>
</section>
<section id="try-it-yourself-97">
<h4 data-anchor-id="try-it-yourself-97">Try It Yourself</h4>
<ol type="1">
<li>Compare solving <code>Ax = b</code> with <code>np.linalg.solve</code> vs <code>np.linalg.inv(A) @ b</code> for larger, ill-conditioned systems.</li>
<li>Use <code>np.linalg.svd</code> on a nearly singular matrix. How stable are the singular values?</li>
<li>Check performance: time <code>A @ B</code> for sizes 100, 500, 1000.</li>
</ol>
</section>
<section id="the-takeaway-81">
<h4 data-anchor-id="the-takeaway-81">The Takeaway</h4>
<ul>
<li>Numerical linear algebra = math + floating-point reality.</li>
<li>Always prefer stable algorithms (<code>solve</code>, <code>qr</code>, <code>svd</code>) over naive inversion.</li>
<li>Libraries like BLAS/LAPACK make large computations fast, but understanding precision and conditioning prevents nasty surprises.</li>
</ul>
</section>
</section>
<section id="capstone-problem-sets-and-next-steps-a-roadmap-to-mastery">
<h3 data-anchor-id="capstone-problem-sets-and-next-steps-a-roadmap-to-mastery">100. Capstone Problem Sets and Next Steps (A Roadmap to Mastery)</h3>
<p>This final section ties everything together. Instead of introducing a new topic, it provides capstone labs that combine multiple ideas from the book. Working through them will give you confidence that you can apply linear algebra to real problems.</p>
<section id="problem-set-1---image-compression-with-svd">
<h4 data-anchor-id="problem-set-1---image-compression-with-svd">Problem Set 1 - Image Compression with SVD</h4>
<p>Take an image, treat it as a matrix, and approximate it with low-rank SVD.</p>
<div id="23b4a029" data-execution_count="577">
<div><div id="cb973"><pre><code><span id="cb973-1"><a href="#cb973-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb973-2"><a href="#cb973-2" aria-hidden="true" tabindex="-1"></a><span>import</span> matplotlib.pyplot <span>as</span> plt</span>
<span id="cb973-3"><a href="#cb973-3" aria-hidden="true" tabindex="-1"></a><span>from</span> skimage <span>import</span> data, color</span>
<span id="cb973-4"><a href="#cb973-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb973-5"><a href="#cb973-5" aria-hidden="true" tabindex="-1"></a><span># Load grayscale image</span></span>
<span id="cb973-6"><a href="#cb973-6" aria-hidden="true" tabindex="-1"></a>img <span>=</span> color.rgb2gray(data.astronaut())</span>
<span id="cb973-7"><a href="#cb973-7" aria-hidden="true" tabindex="-1"></a>U, S, Vt <span>=</span> np.linalg.svd(img, full_matrices<span>=</span><span>False</span>)</span>
<span id="cb973-8"><a href="#cb973-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb973-9"><a href="#cb973-9" aria-hidden="true" tabindex="-1"></a><span># Approximate with rank-k</span></span>
<span id="cb973-10"><a href="#cb973-10" aria-hidden="true" tabindex="-1"></a>k <span>=</span> <span>50</span></span>
<span id="cb973-11"><a href="#cb973-11" aria-hidden="true" tabindex="-1"></a>img_approx <span>=</span> U[:, :k] <span>@</span> np.diag(S[:k]) <span>@</span> Vt[:k, :]</span>
<span id="cb973-12"><a href="#cb973-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb973-13"><a href="#cb973-13" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span>1</span>,<span>2</span>,<span>1</span>)</span>
<span id="cb973-14"><a href="#cb973-14" aria-hidden="true" tabindex="-1"></a>plt.imshow(img, cmap<span>=</span><span>&#34;gray&#34;</span>)</span>
<span id="cb973-15"><a href="#cb973-15" aria-hidden="true" tabindex="-1"></a>plt.title(<span>&#34;Original&#34;</span>)</span>
<span id="cb973-16"><a href="#cb973-16" aria-hidden="true" tabindex="-1"></a>plt.axis(<span>&#34;off&#34;</span>)</span>
<span id="cb973-17"><a href="#cb973-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb973-18"><a href="#cb973-18" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span>1</span>,<span>2</span>,<span>2</span>)</span>
<span id="cb973-19"><a href="#cb973-19" aria-hidden="true" tabindex="-1"></a>plt.imshow(img_approx, cmap<span>=</span><span>&#34;gray&#34;</span>)</span>
<span id="cb973-20"><a href="#cb973-20" aria-hidden="true" tabindex="-1"></a>plt.title(<span>f&#34;Rank-</span><span>{</span>k<span>}</span><span> Approximation&#34;</span>)</span>
<span id="cb973-21"><a href="#cb973-21" aria-hidden="true" tabindex="-1"></a>plt.axis(<span>&#34;off&#34;</span>)</span>
<span id="cb973-22"><a href="#cb973-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb973-23"><a href="#cb973-23" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></div>
<div>
<div>
<figure>
<p><img src="https://blog.veitheller.de/lab_files/figure-html/cell-578-output-1.png" width="540" height="276"/></p>
</figure>
</div>
</div>
</div>
<p>Try different <span>\(k\)</span> values (5, 20, 100). How does quality vs. compression trade off?</p>
</section>
<section id="problem-set-2---predictive-modeling-with-pca-regression">
<h4 data-anchor-id="problem-set-2---predictive-modeling-with-pca-regression">Problem Set 2 - Predictive Modeling with PCA + Regression</h4>
<p>Combine PCA for dimensionality reduction with linear regression for prediction.</p>
<div id="77a0cb02" data-execution_count="578">
<div><div id="cb974"><pre><code><span id="cb974-1"><a href="#cb974-1" aria-hidden="true" tabindex="-1"></a><span>from</span> sklearn.datasets <span>import</span> load_diabetes</span>
<span id="cb974-2"><a href="#cb974-2" aria-hidden="true" tabindex="-1"></a><span>from</span> sklearn.model_selection <span>import</span> train_test_split</span>
<span id="cb974-3"><a href="#cb974-3" aria-hidden="true" tabindex="-1"></a><span>from</span> sklearn.linear_model <span>import</span> LinearRegression</span>
<span id="cb974-4"><a href="#cb974-4" aria-hidden="true" tabindex="-1"></a><span>from</span> sklearn.decomposition <span>import</span> PCA</span>
<span id="cb974-5"><a href="#cb974-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb974-6"><a href="#cb974-6" aria-hidden="true" tabindex="-1"></a><span># Load dataset</span></span>
<span id="cb974-7"><a href="#cb974-7" aria-hidden="true" tabindex="-1"></a>X, y <span>=</span> load_diabetes(return_X_y<span>=</span><span>True</span>)</span>
<span id="cb974-8"><a href="#cb974-8" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span>=</span> train_test_split(X, y, random_state<span>=</span><span>0</span>)</span>
<span id="cb974-9"><a href="#cb974-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb974-10"><a href="#cb974-10" aria-hidden="true" tabindex="-1"></a><span># PCA reduce features</span></span>
<span id="cb974-11"><a href="#cb974-11" aria-hidden="true" tabindex="-1"></a>pca <span>=</span> PCA(n_components<span>=</span><span>5</span>)</span>
<span id="cb974-12"><a href="#cb974-12" aria-hidden="true" tabindex="-1"></a>X_train_pca <span>=</span> pca.fit_transform(X_train)</span>
<span id="cb974-13"><a href="#cb974-13" aria-hidden="true" tabindex="-1"></a>X_test_pca <span>=</span> pca.transform(X_test)</span>
<span id="cb974-14"><a href="#cb974-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb974-15"><a href="#cb974-15" aria-hidden="true" tabindex="-1"></a><span># Regression on reduced space</span></span>
<span id="cb974-16"><a href="#cb974-16" aria-hidden="true" tabindex="-1"></a>model <span>=</span> LinearRegression().fit(X_train_pca, y_train)</span>
<span id="cb974-17"><a href="#cb974-17" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;R^2 on test set:&#34;</span>, model.score(X_test_pca, y_test))</span></code></pre></div></div>
<div>
<pre><code>R^2 on test set: 0.3691398497153573</code></pre>
</div>
</div>
<p>Does reducing dimensions improve or hurt accuracy?</p>
</section>

<section id="problem-set-4---solving-differential-equations-with-eigen-decomposition">
<h4 data-anchor-id="problem-set-4---solving-differential-equations-with-eigen-decomposition">Problem Set 4 - Solving Differential Equations with Eigen Decomposition</h4>
<p>Use eigenvalues/eigenvectors to solve a linear dynamical system.</p>
<div id="d2ae3bd6" data-execution_count="580">
<div><div id="cb977"><pre><code><span id="cb977-1"><a href="#cb977-1" aria-hidden="true" tabindex="-1"></a>A <span>=</span> np.array([[<span>0</span>,<span>1</span>],[<span>-</span><span>2</span>,<span>-</span><span>3</span>]])</span>
<span id="cb977-2"><a href="#cb977-2" aria-hidden="true" tabindex="-1"></a>eigvals, eigvecs <span>=</span> np.linalg.eig(A)</span>
<span id="cb977-3"><a href="#cb977-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb977-4"><a href="#cb977-4" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvalues:&#34;</span>, eigvals)</span>
<span id="cb977-5"><a href="#cb977-5" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Eigenvectors:</span><span>\n</span><span>&#34;</span>, eigvecs)</span></code></pre></div></div>
<div>
<pre><code>Eigenvalues: [-1. -2.]
Eigenvectors:
 [[ 0.70710678 -0.4472136 ]
 [-0.70710678  0.89442719]]</code></pre>
</div>
</div>
<p>Predict long-term behavior: will the system decay, oscillate, or grow?</p>
</section>
<section id="problem-set-5---least-squares-for-overdetermined-systems">
<h4 data-anchor-id="problem-set-5---least-squares-for-overdetermined-systems">Problem Set 5 - Least Squares for Overdetermined Systems</h4>
<div id="7ed4e2da" data-execution_count="581">
<div><div id="cb979"><pre><code><span id="cb979-1"><a href="#cb979-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span>0</span>)</span>
<span id="cb979-2"><a href="#cb979-2" aria-hidden="true" tabindex="-1"></a>X <span>=</span> np.random.randn(<span>100</span>, <span>3</span>)</span>
<span id="cb979-3"><a href="#cb979-3" aria-hidden="true" tabindex="-1"></a>beta_true <span>=</span> np.array([<span>2</span>, <span>-</span><span>1</span>, <span>0.5</span>])</span>
<span id="cb979-4"><a href="#cb979-4" aria-hidden="true" tabindex="-1"></a>y <span>=</span> X <span>@</span> beta_true <span>+</span> np.random.randn(<span>100</span>)<span>*</span><span>0.1</span></span>
<span id="cb979-5"><a href="#cb979-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb979-6"><a href="#cb979-6" aria-hidden="true" tabindex="-1"></a>beta_hat, <span>*</span>_ <span>=</span> np.linalg.lstsq(X, y, rcond<span>=</span><span>None</span>)</span>
<span id="cb979-7"><a href="#cb979-7" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>&#34;Estimated coefficients:&#34;</span>, beta_hat)</span></code></pre></div></div>
<div>
<pre><code>Estimated coefficients: [ 1.99371939 -1.00708947  0.50661857]</code></pre>
</div>
</div>
<p>Compare estimated vs. true coefficients. How close are they?</p>
</section>
<section id="try-it-yourself-98">
<h4 data-anchor-id="try-it-yourself-98">Try It Yourself</h4>
<ol type="1">
<li>Combine SVD and recommender systems - build a movie recommender with synthetic data.</li>
<li>Implement Gram–Schmidt by hand and test it against <code>np.linalg.qr</code>.</li>
<li>Write a mini “linear algebra toolkit” with your favorite helper functions.</li>
</ol>
</section>
<section id="the-takeaway-82">
<h4 data-anchor-id="the-takeaway-82">The Takeaway</h4>
<ul>
<li>You’ve practiced vectors, matrices, systems, eigenvalues, SVD, PCA, PageRank, and more.</li>
<li>Real problems often combine multiple concepts - the labs show how everything fits together.</li>
<li>Next steps: dive deeper into numerical linear algebra, explore machine learning applications, or study advanced matrix factorizations (Jordan form, tensor decompositions).</li>
</ul>
<p>This concludes the hands-on journey. By now, you don’t just know the theory - you can use linear algebra as a working tool in Python for data, science, and engineering.</p>


</section>
</section>
</section>

</div></div>
  </body>
</html>
