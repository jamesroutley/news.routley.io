<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lwn.net/Articles/1030818/">Original</a>
    <h1>Rethinking the Linux cloud stack for confidential VMs</h1>
    
    <div id="readability-page-1" class="page"><div>

<p>
There is an inherent limit to the privacy of the <em>public</em>
cloud. While Linux can isolate virtual machines (VMs) from each other,
nothing in the system&#39;s memory is ultimately out of reach for the host cloud
provider. To accommodate the most privacy-conscious clients, <a href="https://en.wikipedia.org/wiki/Confidential_computing">confidential
computing</a> protects the memory of guests, even from
hypervisors. But the Linux cloud stack needs to be rethought in order to host
confidential VMs, juggling two goals that are often at odds: performance
and security.
</p>

<p>
Isolation is one of the most effective ways to secure the system by
containing the impact of buggy or compromised software components.
That&#39;s good news for the cloud, which is built around
virtualization — a design that fundamentally isolates resources within
virtual machines. This is achieved through a combination of
hardware-assisted virtualization, system-level orchestration (like KVM, the
hypervisor integrated into the kernel), and higher-level user-space
encapsulation.
</p>

<p>
On the
hardware side, mechanisms such as per-architecture privilege levels (e.g.,
rings 0-3 in x86_64 or Exception Levels on ARM) and the <a href="https://en.wikipedia.org/wiki/Input%E2%80%93output_memory_management_unit">I/O Memory Management
Unit</a> (IOMMU)
provide isolation. Hypervisors extend
this by handling the execution context of VMs to enforce separation even on
shared physical resources. At the user-space level, control groups limit the
resources (CPU, memory, I/O) available to processes, while namespaces
isolate different aspects of the system, such as the process tree,
network stack, mount points, MAC addresses, etc. Confidential computing
adds a new layer of isolation, protecting guests even from potentially
compromised hosts.
</p>

<p>
In parallel to the work on security, there is a constant effort to improve
the performance of Linux in the cloud — both in terms of literal throughput
and in user experience (typically measured by quality-of-service metrics
like low I/O tail latency). With the knowledge that there is room to
improve, the cloud providers increasingly turn to I/O passthrough to speed up Linux:
bypassing the host kernel (and sometimes the guest kernel) to expose
physical devices directly to guest VMs.  This can be done with user-space
libraries like the <a href="https://www.dpdk.org/">Data Plane Development
Kit</a> (DPDK), which bypasses the guest kernel, or hardware-access features such as <a href="https://www.redhat.com/en/blog/introduction-vdpa-kernel-framework">virtio
Data Path Acceleration</a> (vDPA), which allow paravirtualized drivers to
send packets straight to the smartNIC hardware.
</p>

<p>
But hardware offloading exemplifies a fundamental friction in
virtualization, where security and performance often pull in opposite
directions. While it is true that offloading provides a faster path for network
traffic, it has some downsides, such as limiting
visibility and auditing, increasing reliance on hardware and firmware, and
circumventing OS-based security checks of flows and data. The uncomfortable
reality is that it&#39;s tricky for Linux to provide fast access to resources
while concurrently enforcing the strict separation required to secure
workloads. As it happens, the strongest isolation isn&#39;t the most
performant.
</p>

<p>
A potential solution to this tension is extending confidential computing to
the devices themselves by making them part of the VM&#39;s circle of trust.
Hardware technologies like AMD&#39;s <a href="https://www.amd.com/content/dam/amd/en/documents/developer/sev-tio-whitepaper.pdf">SEV Trusted I/O</a> (SEV-TIO)
allow a confidential VM to cryptographically verify (and attest to) a device&#39;s
identity and configuration. Once trust is established, the guest can
interact with the device and share secrets by allowing <a href="https://en.wikipedia.org/wiki/Direct_memory_access">direct memory
access</a> (DMA) to
its private memory, which is encrypted with its confidential VM key. This
avoids bounce buffers — temporary memory copies used when devices, like
GPUs when they are used to train AI models, need access to plaintext data — which significantly
slow down I/O operations.
</p>

<p>
The <a href="https://pcisig.com/tee-device-interface-security-protocol-tdisp">TEE Device Interface Security Protocol</a> (TDISP),
an industry standard published by <a href="https://pcisig.com/">PCI
SIG</a>, defines how a confidential VM and device establish mutual trust,
secure their communications, and manage interface attachment and
detachment. A common way to implement TDISP is using a device with <a href="https://www.kernel.org/doc/html/latest/PCI/pci-iov-howto.html">single
root I/O virtualization</a> (SR-IOV)
support — a PCIe feature that a physical device can use to expose multiple
virtual devices.
</p>

<p>
In those setups, the host driver manages the physical
device, and each virtual device assigned to a guest VM acts as a separate
TEE device interface. Unfortunately, TDISP requires changes in the entire
software stack, including the device&#39;s firmware and hardware, host CPU, and
the hypervisor. TDISP also faces headwinds because not all of the vendors
are on board. Interestingly, NVIDIA, one of the biggest players in the
GPU arena, sells GPUs with its own non-TDISP architecture.
</p>

<h4>Secure Boot</h4>

<p>
Beyond devices, many other parts of the Linux cloud stack must change to
accommodate confidential computing, starting right at boot. To understand
how, we need to look at Secure Boot. A typical sequence is shown in the
area outlined in red
in the figure below. First, the firmware verifies the <a href="https://github.com/rhboot/shim#shim-a-first-stage-uefi-bootloader">shim</a>
pre-bootloader using a cryptographic key embedded in
the firmware&#39;s non-volatile memory by the OEM, along with a database of
valid signatures (DB) and a revocation list (DBX) to reject known-bad
binaries, such as a first-stage bootloader, and revoked certificates. Once verified, shim is loaded into system memory and
execution jumps to it.
</p>

<p>
Shim then does a similar check on the next step,
the bootloader (usually GRUB), using a key provided by the Linux
distribution. Finally, the bootloader verifies and loads the kernel inside
the guest VM. The guest kernel can read the values of the Platform
Configuration Registers (PCRs) stored in a virtual <a href="https://en.wikipedia.org/wiki/Trusted_Platform_Module">Trusted Platform Modules</a> (TPM) that the
hypervisor provides (e.g. using <a href="https://github.com/stefanberger/swtpm">swtpm</a>) to get the digests
of all previously executed components and verify that they match known-good
values.
</p>

<blockquote>
<a href="https://lwn.net/Articles/1031006/">
<img src="https://static.lwn.net/images/2025/coco-boot-sm.png" alt="[Secure Boot]"/>
</a>
</blockquote>

<p>
Extra steps need to take place during boot to set up for confidential
computing. In the figure above, a <a href="https://lwn.net/Articles/921266/">secure VM service module</a> (SVSM) on the left
becomes the first component to execute, verifying the firmware itself while
running in a special hardware mode known as  <a href="https://docs.enclaive.cloud/confidential-cloud/technology-in-depth/amd-sev/technology/fundamentals/features/virtual-machine-privilege-levels">VMPL0</a>
(Intel&#39;s equivalent is
<a href="https://learn.microsoft.com/en-us/virtualization/hyper-v-on-windows/tlfs/vsm">VTL0</a>). But how can a
confidential VM trust that the platform it runs on hasn&#39;t been tampered
with? In traditional Secure Boot, the chain of trust relies on a virtual
TPM (vTPM)
provided by the host.  However, the hypervisor itself is now untrusted, so
the guest cannot rely on a TPM controlled by it. Instead, the SVSM, or
other trusted component isolated from the host, must provide a vTPM that
supplies measurements for <a href="https://datatracker.ietf.org/wg/rats/about/">remote
attestation</a>. This allows the guest OS to verify the integrity of the
platform and decide whether it is safe to run.
</p>

<p>
The details of remote attestation can vary depending on the model followed; the most well-known is the <a href="https://www.redhat.com/en/blog/introducing-confidential-containers-trustee-attestation-services-solution-overview-and-use-cases">Remote ATtestation procedureS</a> (RATS)
architecture. In this model, three actors play a role:
</p>

<ul>
<li> <strong>Attester</strong>: Dedicated hardware like AMD&#39;s Platform
  Security Processor (PSP) that generates evidence about its current state
  (e.g., firmware version) by signing measurements with a private key
  stored within it. </li>

<li> <strong>Verifier</strong>: A remote entity that evaluates the
  evidence&#39;s integrity and trustworthiness. To do so, it consults an
  endorser to validate that the signing key and reported measurements
  (digests) are legitimate. The verifier can also be configured to enforce
  appraisal policies — for example, rejecting systems with outdated
  firmware versions from receiving secrets.</li>

<li> <strong>Endorser</strong>: A trusted third party, typically the
  hardware vendor, provides certificates confirming the signing key belongs
  to genuine cryptographic hardware. The endorser also supplies reference
  measurement values used by the verifier for validation.</li>
</ul>

<p>
The final product is an <a href="https://confidentialcomputing.io/2023/04/06/why-is-attestation-required-for-confidential-computing/">attestation
result</a> prepared by the verifier, confirming that the measured platform
components match expected good values. A Linux confidential VM can use this
report — including a <a href="https://lwn.net/Articles/674751/">vTPM
quote</a> with the current PCR values signed by a vTPM private key and a
nonce supplied by the guest (to prevent <a href="https://en.wikipedia.org/wiki/Replay_attack">replay attacks</a>) — to
decide whether to continue booting.
</p>

<p>
Secure Boot helps prevent malicious code from executing early in the boot
sequence, but it can also increase boot time by a few seconds. Adding
confidential computing to the equation slows down things even more. For
most Linux users, the slight delay of Secure Boot is negligible and well
worth the security benefits. But, in cloud environments, even a few extra
seconds for guest boot can be consequential — small delays quickly add up at
fleet scale. That&#39;s why, since the cloud runs on Linux, it&#39;s important for
cloud providers to focus on optimizing this process within it.
</p>

<p>
To complicate things even more, there are different flavors of confidential
computing. For example, instead of using an SVSM, Microsoft&#39;s <a href="https://github.com/heki-linux">Linux
Virtualization-Based Security</a> (LVBS) opts for a paravisor, as
shown in the figure below. In LVBS, the paravisor is a small Linux kernel
that runs in a special hardware mode (e.g. VTL0) after the bootloader. This
design has the advantage of being vendor-neutral, but also has
drawbacks, such as a significantly larger attack surface than the
SVSM. Even though there are many ways to implement confidential VMs in
Linux, we still lack a clear, shared understanding of the trade-offs
between them.
</p>

<blockquote>
<a href="https://lwn.net/Articles/1031007/">
<img src="https://static.lwn.net/images/2025/coco-boot2-sm.png" alt="[LVBS boot]"/>
</a>
</blockquote>

<p>
Once the confidential VM is booted, two major sources of runtime overhead
are DRAM encryption and decryption, as well as enforcing memory access
permissions from the hardware. That said, because this happens inline
within the memory controller, the delay is usually small; this impact can
vary depending on the workload, particularly for cache-sensitive
applications.
</p>

<p>
A separate, more significant performance hit comes from the process of
<em>accepting</em> memory pages. Before a confidential VM can access DRAM,
each page must be explicitly accepted by the guest. This step binds the
guest physical address (gPA) of the page to a system physical address
(sPA), preventing remapping — that is, once validated, the hardware
enforces this mapping, and any attempt by the hypervisor to remap the gPA
to a different sPA via nested page tables will trigger a page fault
(#PF). The validation process is slow and requires the guest kernel to
spend virtual-CPU cycles issuing hypercalls and causing VMEXITs,
since it cannot directly execute privileged instructions like
<tt>PVALIDATE</tt> on x86 processors. Only components running in special hardware
modes — such as the SVSM at VMPL0 — can call them directly. To avoid this
overhead cost at runtime, the SVSM (or whatever component is used)
should pre-accept all memory pages early during the boot process.
</p>

<h4>Scaling</h4>

<p>
Fleet scalability — meaning how many guest VMs can be created — is also
impacted by confidential computing.  The most significant hardware limitations come from
architectural constraints: for example, the number of available
address-space identifiers (ASIDs). Each confidential VM requires a unique
ASID in order to be tagged and isolated; without a unique ASID, the
hardware cannot differentiate between encrypted memory regions belonging to
different VMs.  The maximum number of ASIDs that Linux can use is typically
capped by the BIOS and limited to a few hundred. That might seem enough,
but modern multicore processors can have hundreds of cores, each hosting
one or even two virtual CPUs with simultaneous multithreading. As Moore&#39;s Law
slows (or dies) and processor performance gains become harder to achieve,
the hardware industry is likely to continue scaling core counts
instead. Thus, without scalable support in Linux for confidential VMs, the
cloud risks underutilizing cores.
</p>

<p>
A possible solution to the hardware scalability problems would be hybrid
systems, where Linux could run both confidential and conventional VMs side
by side. Today, kernel-configuration options enforce an all-or-nothing
approach — either the system hosts only encrypted VMs or it hosts no
encrypted VMs. Unfortunately, this limitation may be beyond the Linux
kernel&#39;s control and come from microarchitectural constraints in current
hardware generations.
</p>

<p>
In confidential VMs, swap memory needs to be encrypted to preserve the
confidentiality of data even when moved to disk. Likewise, when the VMs
communicate over the network — particularly through host-managed NICs —
they must establish secure end-to-end sessions to maintain data integrity
and confidentiality across untrusted host networks. Given the added
overhead of these security measures, it&#39;s possible that future users of
confidential computing won&#39;t be traditional, low-latency cloud applications
like client-server workloads, but rather high-performance computing or
scientific workloads. While these batch-oriented applications may still
experience some performance impact, they generally have a higher tolerance
for latency — not because they are inherently less sensitive to it, but
because they lack realtime human interaction (e.g., there are no users
sitting in front of a browser waiting for a reply).
</p>

<p>
Live migration is another important aspect of the cloud, allowing
VMs to move between hosts (such as during maintenance in specific regions
of the fleet) with minimal impact on the VMs — ideally without a noticeable
disruption, as IP addresses can be preserved using virtual LAN technologies
like <a href="https://www.juniper.net/us/en/research-topics/what-is-vxlan.html">VXLAN</a>.
However, after migration, the attestation process must be repeated on the
destination node. While pre-attesting a destination node (as a plan B
option) can help reduce overhead, unexpected emergencies in the fleet may
force the VM to migrate again shortly after arrival. Worse still, because
the guest VM no longer implicitly trusts the host, it must also verify that
its memory and execution context were correctly preserved during migration,
and that any changes were properly tracked throughout the live
migration. To facilitate all of this, a <a href="https://docs.enclaive.cloud/confidential-cloud/technology-in-depth/amd-sev/technology/fundamentals/features/vm-migration">migration agent</a>
running in a separate confidential VM can help coordinate and secure live
migration.
</p>

<h4>In conclusion</h4>

<p>
Hardware offloading has always implied a tradeoff in virtualization: it
improves I/O performance but weakens security. Thanks to confidential
computing, Linux can now achieve the former without sacrificing the latter.
That said, one thing is still true for hardware offloading — and more
broadly, for Linux in the cloud — it deepens Linux&#39;s
reliance on firmware and hardware. In that sense, trust doesn&#39;t grow or
shrink, it simply shifts. In this case, it shifts toward OEMs (hardware and
device manufacturers).
</p>

<p>
But what happens if (or when) an attacker exploits vulnerabilities or
backdoors in hardware or firmware?  Unlike software, hardware is difficult
to verify, leaving open the risk of hidden compromises that can undermine
the entire security model. Open architectures like <a href="https://riscv.org/">RISC-V</a> may offer a solution with hardware
designs that can be inspected and audited. This speaks to the security
value of transparency and openness — ultimately the only way to eliminate
the need to trust third parties.
</p>

<p>
Cloud providers are already expected to respect user privacy, but
confidential computing turns that promise into more than just a leap of
faith taken in someone else&#39;s computer. That shift puts the guest Linux
kernel in an awkward spot. Cooperation with the host can be genuinely
useful — say, synchronizing schedulers to make the most of NUMA layouts, or
avoiding guest deadlocks. But the host is also, unavoidably, untrusted.
</p>

<p>
This means that
Linux finds itself trying to work with something it&#39;s supposed to be
protected from. As a consequence, a lot has to change in the Linux cloud
stack to truly accommodate cloud confidential computing. Is this a
worthwhile investment for the overall kernel community? As the foundation
of the modern public cloud, Linux is in a good position to explore the
potential of confidential VMs.
</p></div></div>
  </body>
</html>
