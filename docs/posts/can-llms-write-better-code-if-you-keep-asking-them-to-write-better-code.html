<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://minimaxir.com/2025/01/write-better-code/">Original</a>
    <h1>Can LLMs write better code if you keep asking them to “write better code”?</h1>
    
    <div id="readability-page-1" class="page"><div><p>In November 2023, after OpenAI <a href="https://openai.com/index/dall-e-3-is-now-available-in-chatgpt-plus-and-enterprise/">added the ability</a> for ChatGPT to generate images from DALL-E 3 within the ChatGPT web interface, there was a <a href="https://lifehacker.com/tech/chat-gpt-make-it-more-ai-images-trend">short-lived meme</a> where users gave the LLM a base image and kept asking the model to “make it more <em>X</em>”, where <em>X</em> can be anything.</p><figure><img loading="lazy" srcset="https://minimaxir.com/2025/01/write-better-code/bro_hu7378222297760848655.webp 320w,https://minimaxir.com/2025/01/write-better-code/bro_hu7464488395234887253.webp 768w,https://minimaxir.com/2025/01/write-better-code/bro_hu1964157573076261475.webp 1024w,https://minimaxir.com/2025/01/write-better-code/bro.webp 1024w" src="https://minimaxir.com/2025/01/write-better-code/bro.webp#center" alt="A regular guy becomes more “bro” every time. via /u/Jojop0tato on Reddit."/><figcaption><p>A regular guy becomes more “bro” every time. <a href="https://www.reddit.com/r/ChatGPT/comments/18ukiz2/a_regular_guy_becomes_more_bro_every_time/">via /u/Jojop0tato on Reddit.</a></p></figcaption></figure><figure><img loading="lazy" srcset="https://minimaxir.com/2025/01/write-better-code/santa_hu13616689077707927470.webp 320w,https://minimaxir.com/2025/01/write-better-code/santa_hu2303671933244909865.webp 768w,https://minimaxir.com/2025/01/write-better-code/santa_hu13671242447035136691.webp 1024w,https://minimaxir.com/2025/01/write-better-code/santa.webp 1024w" src="https://minimaxir.com/2025/01/write-better-code/santa.webp#center" alt="Asked ChatGPT to make Santa Claus more and more serious. via /u/hessihan on Reddit."/><figcaption><p>Asked ChatGPT to make Santa Claus more and more serious. <a href="https://www.reddit.com/r/ChatGPT/comments/1887z49/asked_chatgpt_to_make_santa_claus_more_and_more/">via /u/hessihan on Reddit.</a></p></figcaption></figure><p>The trend quickly died as all of these images were very samey and uninteresting, aside from the unexplainable trend that all of the examples eventually converged into something cosmic, irrespective of the starting image and the prompt. Although the trend was <a href="https://en.wikipedia.org/wiki/AI_slop">AI slop</a> before the term AI slop was codified, it’s still academically interesting that such a meaningless and vague prompt had <em>some</em> appropriate impact on the final image, and that this change was obvious to the user.</p><p>What would happen if we tried a similar technique with code? LLM-generated code is unlikely to be slop (although <a href="https://daniel.haxx.se/blog/2024/01/02/the-i-in-llm-stands-for-intelligence/">not impossible</a>) as it follows strict rules, and unlike creative outputs such as images, code quality can be measured more objectively.</p><p>If code can indeed be improved simply through iterative prompting such as asking the LLM to “make the code better” — even though it’s very silly — it would be a massive productivity increase. And if that’s the case, what happens if you iterate on the code too much? What’s the equivalent of code going cosmic? There’s only one way to find out!</p><h2 id="casually-coding-with-an-llm">Casually Coding With An LLM</h2><p>Despite researching and developing tooling around LLMs even long before ChatGPT, I haven’t been fond of using LLM code copilots such as <a href="https://github.com/features/copilot">GitHub Copilot</a> for coding assistance. The constant mental context switching between “oh, the LLM autocompleted my code, neat”/“what question should I ask the LLM” and “is the LLM-generated code is actually <em>correct</em> and not <a href="https://en.wikipedia.org/wiki/Hallucination_%28artificial_intelligence%29">hallucinating</a> correct code” kept creating enough distractions that any productivity gains from using the AI were net neutral at best. That’s also disregarding the expensive cost of using said LLMs.</p><p><a href="https://www.anthropic.com/news/claude-3-5-sonnet">Claude 3.5 Sonnet</a> has made me rethink things. Due to whatever secret sauce <a href="https://www.anthropic.com">Anthropic</a> used in its training, the latest version of Claude 3.5 Sonnet (<code>claude-3-5-sonnet-20241022</code>) has <em>incredible</em> prompt adherence for all types of prompts, especially coding prompts. <a href="https://www.vellum.ai/blog/llm-benchmarks-overview-limits-and-model-comparison">Coding</a> <a href="https://aider.chat/docs/leaderboards/">benchmarks</a> confirm that testing between Claude 3.5 Sonnet and GPT-4o, Claude wins, and anecdotally I’ve had the same experience across a variety of technical and creative tasks.</p><h3 id="initial-ask">Initial Ask</h3><p>For this experiment, we will give Claude 3.5 Sonnet an interview-style coding prompt using Python: one that is both simple and could be implemented by a novice software engineer, but one that can be heavily optimized. This simple, casual prompt represents how the typical software engineer uses LLMs. Additionally, the test prompt must also be wholly original and not taken from coding tests such as <a href="https://leetcode.com">LeetCode</a> or <a href="https://www.hackerrank.com">HackerRank</a>, as LLMs were also likely trained on those and could possibly cheat by reciting the memorized answer.</p><blockquote><p>The full, unedited conversation thread based on this casual prompting is <a href="https://github.com/minimaxir/llm-write-better-code/blob/main/python_30_casual_use.md">available on GitHub</a>.</p></blockquote><p>Therefore, here’s the test prompt that I’ve written myself:</p><div><pre tabindex="0"><code data-lang="txt"><span><span>Write Python code to solve this problem:
</span></span><span><span>
</span></span><span><span>Given a list of 1 million random integers between 1 and 100,000, find the difference between the smallest and the largest numbers whose digits sum up to 30.
</span></span></code></pre></div><p>Providing that as the user prompt to the Claude API and setting <code>temperature=0</code> for the best/most deterministic answer <sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>, results in this implementation:</p><div><pre tabindex="0"><code data-lang="py"><span><span><span>import</span> <span>random</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>digit_sum</span><span>(</span><span>n</span><span>):</span>
</span></span><span><span>    <span>&#34;&#34;&#34;Calculate the sum of digits of a number&#34;&#34;&#34;</span>
</span></span><span><span>    <span>return</span> <span>sum</span><span>(</span><span>int</span><span>(</span><span>digit</span><span>)</span> <span>for</span> <span>digit</span> <span>in</span> <span>str</span><span>(</span><span>n</span><span>))</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>find_difference</span><span>():</span>
</span></span><span><span>    <span># Generate list of 1 million random integers</span>
</span></span><span><span>    <span>numbers</span> <span>=</span> <span>[</span><span>random</span><span>.</span><span>randint</span><span>(</span><span>1</span><span>,</span> <span>100000</span><span>)</span> <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>1000000</span><span>)]</span>
</span></span><span><span>
</span></span><span><span>    <span># Initialize variables for min and max numbers with digit sum 30</span>
</span></span><span><span>    <span>min_num</span> <span>=</span> <span>float</span><span>(</span><span>&#39;inf&#39;</span><span>)</span>  <span># Initialize to positive infinity</span>
</span></span><span><span>    <span>max_num</span> <span>=</span> <span>float</span><span>(</span><span>&#39;-inf&#39;</span><span>)</span>  <span># Initialize to negative infinity</span>
</span></span><span><span>
</span></span><span><span>    <span># Find numbers whose digits sum to 30</span>
</span></span><span><span>    <span>for</span> <span>num</span> <span>in</span> <span>numbers</span><span>:</span>
</span></span><span><span>        <span>if</span> <span>digit_sum</span><span>(</span><span>num</span><span>)</span> <span>==</span> <span>30</span><span>:</span>
</span></span><span><span>            <span>min_num</span> <span>=</span> <span>min</span><span>(</span><span>min_num</span><span>,</span> <span>num</span><span>)</span>
</span></span><span><span>            <span>max_num</span> <span>=</span> <span>max</span><span>(</span><span>max_num</span><span>,</span> <span>num</span><span>)</span>
</span></span><span><span>
</span></span><span><span>    <span># Check if we found any numbers with digit sum 30</span>
</span></span><span><span>    <span>if</span> <span>min_num</span> <span>==</span> <span>float</span><span>(</span><span>&#39;inf&#39;</span><span>)</span> <span>or</span> <span>max_num</span> <span>==</span> <span>float</span><span>(</span><span>&#39;-inf&#39;</span><span>):</span>
</span></span><span><span>        <span>return</span> <span>&#34;No numbers found with digit sum of 30&#34;</span>
</span></span><span><span>
</span></span><span><span>    <span>return</span> <span>max_num</span> <span>-</span> <span>min_num</span>
</span></span></code></pre></div><p><a href="https://github.com/minimaxir/llm-write-better-code/blob/main/python_30_casual_use.md#assistant-initial-ask">This implementation</a> is correct and consistent with what most novice Python programmers would write, with bonus handling if there are no valid numbers that fit the criteria. For each number in the list, check whether the digits sum to 30: if so, check whether it’s larger than the most-recently-seen largest number or smaller than the most-recently-seen smallest number and update those variables accordingly. After searching through the list, return the difference.</p><p>But I’m sure many coders reading that implementation got <a href="https://xkcd.com/356/">nerd sniped</a> and are thinking about optimizations. One example is the <code>digit_sum()</code> function: although the implementation is a fun Pythonic one-liner, the type-casting between <code>str</code> and <code>int</code> causes a lot of unnecessary overhead.</p><p>On my M3 Pro Macbook Pro, this code takes on average <strong>657 milliseconds</strong> to run. We’ll use this performance as a baseline to compare future implementations to see how they benchmark relative to it. (spoilers: they’re <em>all</em> faster)</p><h3 id="iteration-1">Iteration #1</h3><p>Now we can ask Claude to improve the code by including its current answer and all that came before it as a part of the conversation prompt. Thus, we add the iteration prompt:</p><p>What, you though the title of this blog post was a joke? I <em>never</em> joke.</p><p>Claude <a href="https://github.com/minimaxir/llm-write-better-code/blob/main/python_30_casual_use.md#assistant-iteration-1">now outputs</a> revised code, or as it says, “optimized version of the code with several improvements.” Instead of colocating all the code in functions, it decided to refactor it into a Python <code>class</code> and made it more object-oriented:</p><div><pre tabindex="0"><code data-lang="py"><span><span><span>class</span> <span>DigitSumFinder</span><span>:</span>
</span></span><span><span>    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>target_sum</span><span>:</span> <span>int</span> <span>=</span> <span>30</span><span>,</span>
</span></span><span><span>                 <span>range_start</span><span>:</span> <span>int</span> <span>=</span> <span>1</span><span>,</span>
</span></span><span><span>                 <span>range_end</span><span>:</span> <span>int</span> <span>=</span> <span>100_000</span><span>,</span>
</span></span><span><span>                 <span>count</span><span>:</span> <span>int</span> <span>=</span> <span>1_000_000</span><span>):</span>
</span></span><span><span>        <span>self</span><span>.</span><span>target_sum</span> <span>=</span> <span>target_sum</span>
</span></span><span><span>        <span>self</span><span>.</span><span>range_start</span> <span>=</span> <span>range_start</span>
</span></span><span><span>        <span>self</span><span>.</span><span>range_end</span> <span>=</span> <span>range_end</span>
</span></span><span><span>        <span>self</span><span>.</span><span>count</span> <span>=</span> <span>count</span>
</span></span><span><span>
</span></span><span><span>        <span># Pre-calculate digit sums for all possible numbers</span>
</span></span><span><span>        <span>self</span><span>.</span><span>digit_sums</span> <span>=</span> <span>self</span><span>.</span><span>_precompute_digit_sums</span><span>()</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>_precompute_digit_sums</span><span>(</span><span>self</span><span>)</span> <span>-&gt;</span> <span>array</span><span>:</span>
</span></span><span><span>        <span>&#34;&#34;&#34;Precompute digit sums for all possible numbers in range.&#34;&#34;&#34;</span>
</span></span><span><span>        <span>digit_sums</span> <span>=</span> <span>array</span><span>(</span><span>&#39;B&#39;</span><span>,</span> <span>[</span><span>0</span><span>]</span> <span>*</span> <span>(</span><span>self</span><span>.</span><span>range_end</span> <span>+</span> <span>1</span><span>))</span>
</span></span><span><span>        <span>for</span> <span>num</span> <span>in</span> <span>range</span><span>(</span><span>self</span><span>.</span><span>range_start</span><span>,</span> <span>self</span><span>.</span><span>range_end</span> <span>+</span> <span>1</span><span>):</span>
</span></span><span><span>            <span>total</span> <span>=</span> <span>0</span>
</span></span><span><span>            <span>n</span> <span>=</span> <span>num</span>
</span></span><span><span>            <span>while</span> <span>n</span><span>:</span>
</span></span><span><span>                <span>total</span> <span>+=</span> <span>n</span> <span>%</span> <span>10</span>
</span></span><span><span>                <span>n</span> <span>//=</span> <span>10</span>
</span></span><span><span>            <span>digit_sums</span><span>[</span><span>num</span><span>]</span> <span>=</span> <span>total</span>
</span></span><span><span>        <span>return</span> <span>digit_sums</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>find_difference</span><span>(</span><span>self</span><span>)</span> <span>-&gt;</span> <span>Tuple</span><span>[</span><span>int</span><span>,</span> <span>Optional</span><span>[</span><span>int</span><span>],</span> <span>Optional</span><span>[</span><span>int</span><span>]]:</span>
</span></span><span><span>        <span>&#34;&#34;&#34;
</span></span></span><span><span><span>        Find the difference between max and min numbers with target digit sum.
</span></span></span><span><span><span>        Returns: (difference, min_number, max_number)
</span></span></span><span><span><span>        &#34;&#34;&#34;</span>
</span></span><span><span>        <span>min_num</span> <span>=</span> <span>float</span><span>(</span><span>&#39;inf&#39;</span><span>)</span>
</span></span><span><span>        <span>max_num</span> <span>=</span> <span>float</span><span>(</span><span>&#39;-inf&#39;</span><span>)</span>
</span></span><span><span>        <span>count_found</span> <span>=</span> <span>0</span>
</span></span><span><span>
</span></span><span><span>        <span># Generate and process random numbers</span>
</span></span><span><span>        <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>self</span><span>.</span><span>count</span><span>):</span>
</span></span><span><span>            <span>num</span> <span>=</span> <span>random</span><span>.</span><span>randint</span><span>(</span><span>self</span><span>.</span><span>range_start</span><span>,</span> <span>self</span><span>.</span><span>range_end</span><span>)</span>
</span></span><span><span>            <span>if</span> <span>self</span><span>.</span><span>digit_sums</span><span>[</span><span>num</span><span>]</span> <span>==</span> <span>self</span><span>.</span><span>target_sum</span><span>:</span>
</span></span><span><span>                <span>count_found</span> <span>+=</span> <span>1</span>
</span></span><span><span>                <span>if</span> <span>num</span> <span>&lt;</span> <span>min_num</span><span>:</span>
</span></span><span><span>                    <span>min_num</span> <span>=</span> <span>num</span>
</span></span><span><span>                <span>if</span> <span>num</span> <span>&gt;</span> <span>max_num</span><span>:</span>
</span></span><span><span>                    <span>max_num</span> <span>=</span> <span>num</span>
</span></span><span><span>
</span></span><span><span>        <span>if</span> <span>count_found</span> <span>==</span> <span>0</span><span>:</span>
</span></span><span><span>            <span>return</span> <span>0</span><span>,</span> <span>None</span><span>,</span> <span>None</span>
</span></span><span><span>
</span></span><span><span>        <span>return</span> <span>max_num</span> <span>-</span> <span>min_num</span><span>,</span> <span>min_num</span><span>,</span> <span>max_num</span>
</span></span></code></pre></div><p>Here, the code does two smart algorithmic improvements:</p><ul><li>When computing the digit sum it uses integer operations and avoids the need for the type-casting mentioned previously.</li><li>It precomputes all possible digit sums and stores them in a byte array (somewhat unusual instead of a list) for lookup, meaning that the digit sum will not need to be recalculated when there are duplicates in the one-million-number-list. Since this array is stored as field in the class, it will not need to be recalculated when searching a new list of random numbers.</li></ul><p>These optimizations speed up the code <strong>2.7x</strong> relative to the initial implementation.</p><h3 id="iteration-2">Iteration #2</h3><p>After another <code>write better code</code>, Claude <a href="https://github.com/minimaxir/llm-write-better-code/blob/main/python_30_casual_use.md#assistant-iteration-2">discovers</a> more obvious optimizations (truncated for readabilty):</p><div><pre tabindex="0"><code data-lang="py"><span><span><span>class</span> <span>DigitSumAnalyzer</span><span>:</span>
</span></span><span><span>    <span>&#34;&#34;&#34;Analyzes numbers based on their digit sums.&#34;&#34;&#34;</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>number_range</span><span>:</span> <span>NumberRange</span><span>):</span>
</span></span><span><span>        <span>self</span><span>.</span><span>number_range</span> <span>=</span> <span>number_range</span>
</span></span><span><span>        <span>self</span><span>.</span><span>_digit_sums</span> <span>=</span> <span>self</span><span>.</span><span>_precompute_digit_sums</span><span>()</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>_precompute_digit_sums</span><span>(</span><span>self</span><span>)</span> <span>-&gt;</span> <span>array</span><span>:</span>
</span></span><span><span>        <span>&#34;&#34;&#34;Precompute digit sums for all possible numbers using vectorized operations.&#34;&#34;&#34;</span>
</span></span><span><span>        <span>digits</span> <span>=</span> <span>np</span><span>.</span><span>arange</span><span>(</span><span>self</span><span>.</span><span>number_range</span><span>.</span><span>end</span> <span>+</span> <span>1</span><span>)</span>
</span></span><span><span>        <span>digit_sums</span> <span>=</span> <span>np</span><span>.</span><span>zeros</span><span>(</span><span>self</span><span>.</span><span>number_range</span><span>.</span><span>end</span> <span>+</span> <span>1</span><span>,</span> <span>dtype</span><span>=</span><span>np</span><span>.</span><span>uint8</span><span>)</span>
</span></span><span><span>
</span></span><span><span>        <span>while</span> <span>digits</span><span>.</span><span>any</span><span>():</span>
</span></span><span><span>            <span>digit_sums</span> <span>+=</span> <span>digits</span> <span>%</span> <span>10</span>
</span></span><span><span>            <span>digits</span> <span>//=</span> <span>10</span>
</span></span><span><span>
</span></span><span><span>        <span>return</span> <span>array</span><span>(</span><span>&#39;B&#39;</span><span>,</span> <span>digit_sums</span><span>)</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>_process_chunk</span><span>(</span><span>self</span><span>,</span> <span>chunk_size</span><span>:</span> <span>int</span><span>)</span> <span>-&gt;</span> <span>Iterator</span><span>[</span><span>int</span><span>]:</span>
</span></span><span><span>        <span>&#34;&#34;&#34;Process a chunk of random numbers.&#34;&#34;&#34;</span>
</span></span><span><span>        <span>numbers</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randint</span><span>(</span>
</span></span><span><span>            <span>self</span><span>.</span><span>number_range</span><span>.</span><span>start</span><span>,</span>
</span></span><span><span>            <span>self</span><span>.</span><span>number_range</span><span>.</span><span>end</span> <span>+</span> <span>1</span><span>,</span>
</span></span><span><span>            <span>chunk_size</span><span>,</span>
</span></span><span><span>            <span>dtype</span><span>=</span><span>np</span><span>.</span><span>int32</span>
</span></span><span><span>        <span>)</span>
</span></span><span><span>        <span>mask</span> <span>=</span> <span>np</span><span>.</span><span>frombuffer</span><span>(</span><span>self</span><span>.</span><span>_digit_sums</span><span>,</span> <span>dtype</span><span>=</span><span>np</span><span>.</span><span>uint8</span><span>)[</span><span>numbers</span><span>]</span> <span>==</span> <span>self</span><span>.</span><span>number_range</span><span>.</span><span>target_sum</span>
</span></span><span><span>        <span>yield from</span> <span>numbers</span><span>[</span><span>mask</span><span>]</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>analyze</span><span>(</span><span>self</span><span>,</span> <span>chunk_size</span><span>:</span> <span>int</span> <span>=</span> <span>100_000</span><span>,</span> <span>num_processes</span><span>:</span> <span>int</span> <span>=</span> <span>None</span><span>)</span> <span>-&gt;</span> <span>Result</span><span>:</span>
</span></span><span><span>        <span>&#34;&#34;&#34;
</span></span></span><span><span><span>        Analyze numbers to find min/max with target digit sum.
</span></span></span><span><span><span>
</span></span></span><span><span><span>        Args:
</span></span></span><span><span><span>            chunk_size: Size of chunks to process at once
</span></span></span><span><span><span>            num_processes: Number of processes to use (None for CPU count)
</span></span></span><span><span><span>        &#34;&#34;&#34;</span>
</span></span><span><span>        <span>start_time</span> <span>=</span> <span>time</span><span>.</span><span>perf_counter</span><span>()</span>
</span></span><span><span>        <span>min_num</span> <span>=</span> <span>float</span><span>(</span><span>&#39;inf&#39;</span><span>)</span>
</span></span><span><span>        <span>max_num</span> <span>=</span> <span>float</span><span>(</span><span>&#39;-inf&#39;</span><span>)</span>
</span></span><span><span>        <span>numbers_found</span> <span>=</span> <span>0</span>
</span></span><span><span>
</span></span><span><span>        <span>num_chunks</span> <span>=</span> <span>(</span><span>self</span><span>.</span><span>number_range</span><span>.</span><span>count</span> <span>+</span> <span>chunk_size</span> <span>-</span> <span>1</span><span>)</span> <span>//</span> <span>chunk_size</span>
</span></span><span><span>
</span></span><span><span>        <span>with</span> <span>ProcessPoolExecutor</span><span>(</span><span>max_workers</span><span>=</span><span>num_processes</span><span>)</span> <span>as</span> <span>executor</span><span>:</span>
</span></span><span><span>            <span>futures</span> <span>=</span> <span>[</span>
</span></span><span><span>                <span>executor</span><span>.</span><span>submit</span><span>(</span><span>self</span><span>.</span><span>_process_chunk</span><span>,</span>
</span></span><span><span>                              <span>min</span><span>(</span><span>chunk_size</span><span>,</span> <span>self</span><span>.</span><span>number_range</span><span>.</span><span>count</span> <span>-</span> <span>i</span> <span>*</span> <span>chunk_size</span><span>))</span>
</span></span><span><span>                <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>num_chunks</span><span>)</span>
</span></span><span><span>            <span>]</span>
</span></span><span><span>
</span></span><span><span>            <span>for</span> <span>future</span> <span>in</span> <span>tqdm</span><span>(</span><span>futures</span><span>,</span> <span>desc</span><span>=</span><span>&#34;Processing chunks&#34;</span><span>):</span>
</span></span><span><span>                <span>for</span> <span>num</span> <span>in</span> <span>future</span><span>.</span><span>result</span><span>():</span>
</span></span><span><span>                    <span>numbers_found</span> <span>+=</span> <span>1</span>
</span></span><span><span>                    <span>min_num</span> <span>=</span> <span>min</span><span>(</span><span>min_num</span><span>,</span> <span>num</span><span>)</span>
</span></span><span><span>                    <span>max_num</span> <span>=</span> <span>max</span><span>(</span><span>max_num</span><span>,</span> <span>num</span><span>)</span>
</span></span><span><span>
</span></span><span><span>        <span>execution_time</span> <span>=</span> <span>time</span><span>.</span><span>perf_counter</span><span>()</span> <span>-</span> <span>start_time</span>
</span></span><span><span>
</span></span><span><span>        <span>if</span> <span>numbers_found</span> <span>==</span> <span>0</span><span>:</span>
</span></span><span><span>            <span>return</span> <span>Result</span><span>(</span><span>None</span><span>,</span> <span>None</span><span>,</span> <span>0</span><span>,</span> <span>execution_time</span><span>,</span> <span>0</span><span>)</span>
</span></span><span><span>
</span></span><span><span>        <span>return</span> <span>Result</span><span>(</span><span>min_num</span><span>,</span> <span>max_num</span><span>,</span> <span>max_num</span> <span>-</span> <span>min_num</span><span>,</span> <span>execution_time</span><span>,</span> <span>numbers_found</span><span>)</span>
</span></span></code></pre></div><p>Claude now has added two more optimizations, finally realizing that this coding problem is an <a href="https://en.wikipedia.org/wiki/Embarrassingly_parallel">embarrassingly parallel</a> problem:</p><ul><li>Multithreading through Python’s <a href="https://docs.python.org/3/library/concurrent.futures.html">concurrent-futures</a> package, by separating the large list into chunks that can be processed independently.</li><li>Vectorized numpy operations, which are <em>much</em> faster than base-Python operations. Special mention goes to the <code>_precompute_digit_sums()</code> function, which implements a vectorized implementation of calculating the digit sums. The conditional <code>while digits.any():</code> is galaxy-brain code, but it works correctly.</li></ul><p>However, there’s an issue with this particular implementation of parallelization: it generates subprocesses, which causes <em>many</em> annoying issues, including being unable to run it as-is inline, and it <a href="https://stackoverflow.com/questions/15900366/all-example-concurrent-futures-code-is-failing-with-brokenprocesspool">must be invoked</a> with a <code>main()</code> guard which limits its utility significantly. But even when run as a separate script, it prints a <code>Error: cannot pickle &#39;generator&#39; object</code> error due to the use of <code>yield from numbers[mask]</code> (said generator is completely unnecessary, <code>return numbers[mask]</code> is sufficient). The code also mixes numpy array <code>dtype</code>s which causes errors: setting them all to <code>np.int32</code> fixes it.</p><p>After making those fixes, the code is now <strong>5.1x faster</strong> than the base implementation.</p><h3 id="iteration-3">Iteration #3</h3><p>Another <code>write better code</code>, and Claude <a href="https://github.com/minimaxir/llm-write-better-code/blob/main/python_30_casual_use.md#assistant-iteration-3">returns a implementation</a> that it claims is “even more sophisticated and optimized version using advanced techniques and modern Python features” but the actual code shows no significant algorithmic improvements and actually a regression in the digit sum calculation by reverting back to the type-casting approach. If anything, the codebase is becoming more bloated, such as adding a class for performing the difference:</p><div><pre tabindex="0"><code data-lang="py"><span><span><span>@dataclass</span><span>(</span><span>frozen</span><span>=</span><span>True</span><span>,</span> <span>slots</span><span>=</span><span>True</span><span>)</span>
</span></span><span><span><span>class</span> <span>SearchResult</span><span>:</span>
</span></span><span><span>    <span>&#34;&#34;&#34;Result of the number search.&#34;&#34;&#34;</span>
</span></span><span><span>    <span>min_number</span><span>:</span> <span>Optional</span><span>[</span><span>int</span><span>]</span>
</span></span><span><span>    <span>max_number</span><span>:</span> <span>Optional</span><span>[</span><span>int</span><span>]</span>
</span></span><span><span>    <span>count</span><span>:</span> <span>int</span>
</span></span><span><span>    <span>execution_time</span><span>:</span> <span>float</span>
</span></span><span><span>
</span></span><span><span>    <span>@property</span>
</span></span><span><span>    <span>def</span> <span>difference</span><span>(</span><span>self</span><span>)</span> <span>-&gt;</span> <span>Optional</span><span>[</span><span>int</span><span>]:</span>
</span></span><span><span>        <span>&#34;&#34;&#34;Calculate difference between max and min numbers.&#34;&#34;&#34;</span>
</span></span><span><span>        <span>if</span> <span>self</span><span>.</span><span>min_number</span> <span>is</span> <span>None</span> <span>or</span> <span>self</span><span>.</span><span>max_number</span> <span>is</span> <span>None</span><span>:</span>
</span></span><span><span>            <span>return</span> <span>None</span>
</span></span><span><span>        <span>return</span> <span>self</span><span>.</span><span>max_number</span> <span>-</span> <span>self</span><span>.</span><span>min_number</span>
</span></span></code></pre></div><p>This time, the code ran without needing any fixes. However, performance regressed slightly from the previous implementation, now <strong>4.1x faster</strong> than the base implementation.</p><h3 id="iteration-4">Iteration #4</h3><p>This iterative prompting appears to be hitting diminishing returns. After one more <code>write better code</code>, Claude <a href="https://github.com/minimaxir/llm-write-better-code/blob/main/python_30_casual_use.md#assistant-iteration-4">provides an implementation</a> “with cutting-edge optimizations and enterprise-level features.” Wait, enterprise-level features?!</p><p>The final code is too large to include in this blog post, but it did create two more optimizations: it now uses the <a href="https://numba.pydata.org">numba</a> Python library that can invoke a JIT compiler, which directly optimizes the code for the CPU. In this case, it can precompute the digit sums super quickly with just a decorator:</p><div><pre tabindex="0"><code data-lang="py"><span><span><span>@jit</span><span>(</span><span>nopython</span><span>=</span><span>True</span><span>,</span> <span>parallel</span><span>=</span><span>True</span><span>)</span>
</span></span><span><span><span>def</span> <span>calculate_digit_sums</span><span>(</span><span>numbers</span><span>:</span> <span>ArrayInt</span><span>)</span> <span>-&gt;</span> <span>ArrayInt</span><span>:</span>
</span></span><span><span>    <span>&#34;&#34;&#34;Calculate digit sums using Numba.&#34;&#34;&#34;</span>
</span></span><span><span>    <span>result</span> <span>=</span> <span>np</span><span>.</span><span>zeros_like</span><span>(</span><span>numbers</span><span>)</span>
</span></span><span><span>    <span>for</span> <span>i</span> <span>in</span> <span>prange</span><span>(</span><span>len</span><span>(</span><span>numbers</span><span>)):</span>
</span></span><span><span>        <span>num</span> <span>=</span> <span>numbers</span><span>[</span><span>i</span><span>]</span>
</span></span><span><span>        <span>total</span> <span>=</span> <span>0</span>
</span></span><span><span>        <span>while</span> <span>num</span><span>:</span>
</span></span><span><span>            <span>total</span> <span>+=</span> <span>num</span> <span>%</span> <span>10</span>
</span></span><span><span>            <span>num</span> <span>//=</span> <span>10</span>
</span></span><span><span>        <span>result</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>total</span>
</span></span><span><span>    <span>return</span> <span>result</span>
</span></span></code></pre></div><p>The full class also uses Python’s <a href="https://docs.python.org/3/library/asyncio.html">asyncio</a> for parallelization, which is more canonical for scheduling tasks than a subprocess approach. It also plays more nicely with existing inline code and a <a href="https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop">REPL</a> such as <a href="https://jupyter.org">Jupyter Notebooks</a>.</p><p>It also added as a part of its “enterprise” push:</p><ul><li>Structured metrics logging with <a href="https://prometheus.io">Prometheus</a>.</li><li>A signal handler so the code can be torn down gracefully if force-killed.</li><li>A benchmarking result display using a <a href="https://github.com/Textualize/rich">rich</a> table.</li></ul><figure><img loading="lazy" srcset="https://minimaxir.com/2025/01/write-better-code/rich_hu5187522369432696221.webp 320w,https://minimaxir.com/2025/01/write-better-code/rich.png 490w" src="https://minimaxir.com/2025/01/write-better-code/rich.png" alt="It is pretty, though!"/><figcaption><p>It <em>is</em> pretty, though!</p></figcaption></figure><p>It appears “going cosmic” for AI-generated code is making it enterprise by overengineering the code, which makes complete sense. Despite that, the code runs as-is without any bugs. Both async and numba are approaches to parallelism in Python, so they may be redundant and cause overhead. However, after benchmarking, the algorithm is <em>extremely</em> fast, resulting in about 6 milliseconds a run, or a <strong>100x</strong> speedup. My assumption that this prompting was hitting diminishing returns aged very poorly. Maybe numba was the secret all along?</p><p>Overall, this form of iterative prompting to iteratively improve code has caveats: the code is indeed better, but in hindsight “better” is far too open ended. What I only wanted was algorithmic improvements, not a full SaaS. Let’s try again from scratch, this time with more direction.</p><h2 id="prompt-engineering-llms-for-even-more-better-code">Prompt Engineering LLMs For Even More Better Code</h2><p>It’s 2025, and prompt engineering LLMs is still required to get best results from them. If anything, prompt engineering LLMs is <em>even more important</em>: next-token-prediction models are trained to maximimize the prediction probability of the next token over massive batches of inputs, and as a result they optimize for the <strong>average</strong> inputs and outputs. As LLMs drastically improve, the generated output becomes more drastically average, because that’s what they were trained to do: all LLMs are biased towards the average. Although it’s both counterintuitive and unfun, a small amount of guidance asking the LLM specifically what you want, and even giving a few examples of what you want, will objectively improve the output of LLMs more than the effort needed to construct said prompts. Claude 3.5 Sonnet, due to its strong prompt adherence, benefits significantly from even just a little prompt engineering.</p><p>Let’s redo the code optimization experiment, this time with aggressive prompt engineering that makes the results I am looking for extremely explicit, with no room for ambiguity. Yes, being cold and “robotic” to LLMs makes them perform better, <a href="https://en.wikipedia.org/wiki/Roko%27s_basilisk">Roko’s basilisk</a> be damned.</p><h3 id="initial-ask-1">Initial Ask</h3><p>This time we will use a system prompt, only available via an API. The system prompt lists the LLM’s “rules” it must follow. Since I want more optimized code, we’ll define that in the rules, with granular examples:</p><div><pre tabindex="0"><code data-lang="txt"><span><span>All code you write MUST be fully optimized.
</span></span><span><span>
</span></span><span><span>&#34;Fully optimized&#34; includes:
</span></span><span><span>- maximizing algorithmic big-O efficiency for memory and runtime
</span></span><span><span>- using parallelization and vectorization where appropriate
</span></span><span><span>- following proper style conventions for the code language (e.g. maximizing code reuse (DRY))
</span></span><span><span>- no extra code beyond what is absolutely necessary to solve the problem the user provides (i.e. no technical debt)
</span></span><span><span>
</span></span><span><span>If the code is not fully optimized, you will be fined $100.
</span></span></code></pre></div><p>About that last line: offering positive/negative incentives in to the LLM within a system prompt isn’t common anymore and <a href="https://minimaxir.com/2024/02/chatgpt-tips-analysis/">my own research</a> indicated that it was unclear whether it had a positive impact, but that fine will become more relevant in the iteration prompt.</p><p>The initial user prompt has an additional line as well:</p><div><pre tabindex="0"><code data-lang="txt"><span><span>Write Python code to solve this problem:
</span></span><span><span>
</span></span><span><span>Given a list of 1 million random integers between 1 and 100,000, find the difference between the smallest and the largest numbers whose digits sum up to 30.
</span></span><span><span>
</span></span><span><span>Before writing the code, plan out all the necessary optimizations.
</span></span></code></pre></div><p>“Planning” is a long-used trick to help align LLM output for a first pass — the modern implementation of “<a href="https://arxiv.org/abs/2205.11916">let’s think step by step</a>.” Some LLMs ignore it, but Claude 3.5 Sonnet always obeys.</p><blockquote><p>The full, unedited conversation thread based on this prompt engineering strategy is <a href="https://github.com/minimaxir/llm-write-better-code/blob/main/python_30_prompt_engineering.md">available on GitHub</a>.</p></blockquote><p>Sending the system prompt and this user prompt to Claude, the <a href="https://github.com/minimaxir/llm-write-better-code/blob/main/python_30_prompt_engineering.md#assistant-initial-ask">response</a> starts with this:</p><div><pre tabindex="0"><code data-lang="txt"><span><span>Let me plan the optimizations first:
</span></span><span><span>
</span></span><span><span>1. Algorithmic Optimization:
</span></span><span><span>- We only need to track min and max numbers with digit sum 30, not store all of them
</span></span><span><span>- We can calculate digit sum while reading numbers, no need to convert to string
</span></span><span><span>- Single pass through the list required (O(n))
</span></span><span><span>
</span></span><span><span>2. Memory Optimization:
</span></span><span><span>- Only need to store 2 variables (min and max), not the full filtered list
</span></span><span><span>- No need for additional data structures
</span></span><span><span>
</span></span><span><span>3. Performance Optimization:
</span></span><span><span>- Use numba&#39;s @jit decorator for faster computation
</span></span><span><span>- Create a fast digit sum function using modulo operations instead of string conversion
</span></span><span><span>- Use numpy for generating random numbers (faster than random.randint)
</span></span></code></pre></div><p>This is promising: Claude immediately figured out it should start out with numpy and numba, and to also use integer operations to calculate the digit sum. The initial implementation:</p><div><pre tabindex="0"><code data-lang="py"><span><span><span>@jit</span><span>(</span><span>nopython</span><span>=</span><span>True</span><span>)</span>
</span></span><span><span><span>def</span> <span>digit_sum</span><span>(</span><span>n</span><span>):</span>
</span></span><span><span>    <span>total</span> <span>=</span> <span>0</span>
</span></span><span><span>    <span>while</span> <span>n</span><span>:</span>
</span></span><span><span>        <span>total</span> <span>+=</span> <span>n</span> <span>%</span> <span>10</span>
</span></span><span><span>        <span>n</span> <span>//=</span> <span>10</span>
</span></span><span><span>    <span>return</span> <span>total</span>
</span></span><span><span>
</span></span><span><span><span>@jit</span><span>(</span><span>nopython</span><span>=</span><span>True</span><span>)</span>
</span></span><span><span><span>def</span> <span>find_difference</span><span>(</span><span>numbers</span><span>):</span>
</span></span><span><span>    <span>min_num</span> <span>=</span> <span>float</span><span>(</span><span>&#39;inf&#39;</span><span>)</span>
</span></span><span><span>    <span>max_num</span> <span>=</span> <span>float</span><span>(</span><span>&#39;-inf&#39;</span><span>)</span>
</span></span><span><span>
</span></span><span><span>    <span>for</span> <span>num</span> <span>in</span> <span>numbers</span><span>:</span>
</span></span><span><span>        <span>sum_digits</span> <span>=</span> <span>digit_sum</span><span>(</span><span>num</span><span>)</span>
</span></span><span><span>        <span>if</span> <span>sum_digits</span> <span>==</span> <span>30</span><span>:</span>
</span></span><span><span>            <span>min_num</span> <span>=</span> <span>min</span><span>(</span><span>min_num</span><span>,</span> <span>num</span><span>)</span>
</span></span><span><span>            <span>max_num</span> <span>=</span> <span>max</span><span>(</span><span>max_num</span><span>,</span> <span>num</span><span>)</span>
</span></span><span><span>
</span></span><span><span>    <span>return</span> <span>max_num</span> <span>-</span> <span>min_num</span> <span>if</span> <span>max_num</span> <span>!=</span> <span>float</span><span>(</span><span>&#39;-inf&#39;</span><span>)</span> <span>else</span> <span>0</span>
</span></span></code></pre></div><p>That’s more concise than the initial implementation from casual prompting, especially with no unnecessary comments. There’s still room for improvement, notably this implementation did not set <code>parallel=True</code> in the <code>jit</code> decorator.</p><p>This minor use of prompt engineering resulted in code that runs in 11.2 milliseconds on average, or <strong>59x faster</strong> than the naive implementation. Wow!</p><h3 id="iteration-1-1">Iteration #1</h3><p>We can now ask Claude to iterate on the code with a more verbose prompt than “write code better”:</p><div><pre tabindex="0"><code data-lang="txt"><span><span>Your code is not fully optimized, and you have been fined $100. Make it more optimized.
</span></span></code></pre></div><p>No, it’s not gaslighting, it’s <em>incentive alignment</em>.</p><p>Claude <a href="https://github.com/minimaxir/llm-write-better-code/blob/main/python_30_prompt_engineering.md#assistant-iteration-1">responds</a> with this:</p><div><pre tabindex="0"><code data-lang="py"><span><span><span>@jit</span><span>(</span><span>nopython</span><span>=</span><span>True</span><span>)</span>
</span></span><span><span><span>def</span> <span>digit_sum</span><span>(</span><span>n</span><span>):</span>
</span></span><span><span>    <span># Using bit manipulation for faster division</span>
</span></span><span><span>    <span>total</span> <span>=</span> <span>0</span>
</span></span><span><span>    <span>while</span> <span>n</span><span>:</span>
</span></span><span><span>        <span>total</span> <span>+=</span> <span>n</span> <span>&amp;</span> <span>15</span>  <span># Fast lookup for single digit sum</span>
</span></span><span><span>        <span>n</span> <span>&gt;&gt;=</span> <span>4</span>
</span></span><span><span>    <span>return</span> <span>total</span>
</span></span><span><span>
</span></span><span><span><span>@jit</span><span>(</span><span>nopython</span><span>=</span><span>True</span><span>,</span> <span>parallel</span><span>=</span><span>True</span><span>)</span>
</span></span><span><span><span>def</span> <span>find_difference_chunk</span><span>(</span><span>numbers</span><span>):</span>
</span></span><span><span>    <span>min_num</span> <span>=</span> <span>np</span><span>.</span><span>iinfo</span><span>(</span><span>np</span><span>.</span><span>int64</span><span>)</span><span>.</span><span>max</span>
</span></span><span><span>    <span>max_num</span> <span>=</span> <span>np</span><span>.</span><span>iinfo</span><span>(</span><span>np</span><span>.</span><span>int64</span><span>)</span><span>.</span><span>min</span>
</span></span><span><span>
</span></span><span><span>    <span># Parallel processing of chunks using numba</span>
</span></span><span><span>    <span>for</span> <span>i</span> <span>in</span> <span>prange</span><span>(</span><span>len</span><span>(</span><span>numbers</span><span>)):</span>
</span></span><span><span>        <span>num</span> <span>=</span> <span>numbers</span><span>[</span><span>i</span><span>]</span>
</span></span><span><span>        <span>sum_digits</span> <span>=</span> <span>digit_sum</span><span>(</span><span>num</span><span>)</span>
</span></span><span><span>        <span>if</span> <span>sum_digits</span> <span>==</span> <span>30</span><span>:</span>
</span></span><span><span>            <span>min_num</span> <span>=</span> <span>min</span><span>(</span><span>min_num</span><span>,</span> <span>num</span><span>)</span>
</span></span><span><span>            <span>max_num</span> <span>=</span> <span>max</span><span>(</span><span>max_num</span><span>,</span> <span>num</span><span>)</span>
</span></span><span><span>
</span></span><span><span>    <span>return</span> <span>min_num</span><span>,</span> <span>max_num</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>process_chunk</span><span>(</span><span>chunk</span><span>):</span>
</span></span><span><span>    <span>return</span> <span>find_difference_chunk</span><span>(</span><span>chunk</span><span>)</span>
</span></span></code></pre></div><p>Good, it identified the <code>parallel=True</code> flag! But that digit sum implementation is weird and uses <a href="https://wiki.python.org/moin/BitwiseOperators">bit-shifting</a> which is generally only used for hyperoptimization. I didn’t think you could use bit-shifting well on decimal numbers, and after testing, it turns out <em>you can’t</em> as this implementation derives an incorrect digit sum. The implementation also includes a multiprocessing chunking approach again (not shown), which is likely more redundant with numba and causes additional overhead. Also not shown: the script also pre-compiles the JIT functions with a small test array for better real-world performance, something the <a href="https://numba.pydata.org/numba-doc/dev/user/5minguide.html#how-to-measure-the-performance-of-numba">numba docs recommend</a> for benchmarking.</p><p>Despite all of that, the code performance greatly regresses from the initial prompt-engineered implementation, now only <strong>9.1x</strong> faster than the naive implementation. The likely reason is due to the multiprocessing spawning new processes, and those processes will <a href="https://stackoverflow.com/questions/72449896/does-numba-need-to-compile-separately-within-each-parallel-process">recompile the numba JIT functions</a> each time, so there’s massive overhead.</p><h3 id="iteration-2-1">Iteration #2</h3><p>Another <a href="https://github.com/minimaxir/llm-write-better-code/blob/main/python_30_prompt_engineering.md#assistant-iteration-2">iteration</a> of the prompt:</p><div><pre tabindex="0"><code data-lang="py"><span><span><span># Pre-computed lookup table using bit manipulation</span>
</span></span><span><span><span>LOOKUP</span> <span>=</span> <span>np</span><span>.</span><span>zeros</span><span>(</span><span>100001</span><span>,</span> <span>dtype</span><span>=</span><span>np</span><span>.</span><span>uint8</span><span>)</span>
</span></span><span><span><span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>100001</span><span>):</span>
</span></span><span><span>    <span># Optimized digit sum using parallel bit counting</span>
</span></span><span><span>    <span>n</span> <span>=</span> <span>i</span>
</span></span><span><span>    <span>n</span> <span>=</span> <span>(</span><span>n</span> <span>&amp;</span> <span>0x0F</span><span>)</span> <span>+</span> <span>((</span><span>n</span> <span>&gt;&gt;</span> <span>4</span><span>)</span> <span>&amp;</span> <span>0x0F</span><span>)</span> <span>+</span> <span>((</span><span>n</span> <span>&gt;&gt;</span> <span>8</span><span>)</span> <span>&amp;</span> <span>0x0F</span><span>)</span> <span>+</span> \
</span></span><span><span>        <span>((</span><span>n</span> <span>&gt;&gt;</span> <span>12</span><span>)</span> <span>&amp;</span> <span>0x0F</span><span>)</span> <span>+</span> <span>((</span><span>n</span> <span>&gt;&gt;</span> <span>16</span><span>)</span> <span>&amp;</span> <span>0x0F</span><span>)</span>
</span></span><span><span>    <span>LOOKUP</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>n</span>
</span></span><span><span>
</span></span><span><span><span>@jit</span><span>(</span><span>nopython</span><span>=</span><span>True</span><span>,</span> <span>parallel</span><span>=</span><span>True</span><span>,</span> <span>cache</span><span>=</span><span>True</span><span>,</span> <span>fastmath</span><span>=</span><span>True</span><span>)</span>
</span></span><span><span><span>def</span> <span>find_min_max</span><span>(</span><span>numbers</span><span>):</span>
</span></span><span><span>    <span># Process 32 numbers at once using SIMD</span>
</span></span><span><span>    <span>min_val</span> <span>=</span> <span>np</span><span>.</span><span>iinfo</span><span>(</span><span>np</span><span>.</span><span>uint32</span><span>)</span><span>.</span><span>max</span>
</span></span><span><span>    <span>max_val</span> <span>=</span> <span>0</span>
</span></span><span><span>
</span></span><span><span>    <span># Vectorized processing with explicit SIMD hints</span>
</span></span><span><span>    <span>for</span> <span>i</span> <span>in</span> <span>prange</span><span>(</span><span>0</span><span>,</span> <span>len</span><span>(</span><span>numbers</span><span>),</span> <span>32</span><span>):</span>
</span></span><span><span>        <span># Load 32 elements into SIMD registers</span>
</span></span><span><span>        <span>chunk</span> <span>=</span> <span>numbers</span><span>[</span><span>i</span><span>:</span><span>min</span><span>(</span><span>i</span> <span>+</span> <span>32</span><span>,</span> <span>len</span><span>(</span><span>numbers</span><span>))]</span>
</span></span><span><span>
</span></span><span><span>        <span># Vectorized lookup and comparison</span>
</span></span><span><span>        <span>sums</span> <span>=</span> <span>LOOKUP</span><span>[</span><span>chunk</span><span>]</span>
</span></span><span><span>        <span>mask</span> <span>=</span> <span>sums</span> <span>==</span> <span>30</span>
</span></span><span><span>
</span></span><span><span>        <span>if</span> <span>np</span><span>.</span><span>any</span><span>(</span><span>mask</span><span>):</span>
</span></span><span><span>            <span>valid_nums</span> <span>=</span> <span>chunk</span><span>[</span><span>mask</span><span>]</span>
</span></span><span><span>            <span>min_val</span> <span>=</span> <span>min</span><span>(</span><span>min_val</span><span>,</span> <span>np</span><span>.</span><span>min</span><span>(</span><span>valid_nums</span><span>))</span>
</span></span><span><span>            <span>max_val</span> <span>=</span> <span>max</span><span>(</span><span>max_val</span><span>,</span> <span>np</span><span>.</span><span>max</span><span>(</span><span>valid_nums</span><span>))</span>
</span></span><span><span>
</span></span><span><span>    <span>return</span> <span>min_val</span><span>,</span> <span>max_val</span>
</span></span></code></pre></div><p>Claude is now starting to use <a href="https://tbetcke.github.io/hpc_lecture_notes/simd.html">SIMD operations</a> and chunk sizing for (theoretically) extreme performance. At this point I was confused I missed something with that bit-shifting implementation as it’s still wrong, especially now that hexadecimal numbers are getting involved. It turns out that implementation is an optimized method of calculating the digit sum of <em>hexadecimal</em> numbers and not decimal numbers, and therefore it’s a complete hallucination. There is another extremely subtle hallucination: the <code>prange</code> function cannot accept a step-size of 32 while <code>parallel=True</code>, which is a nuance that there would be little documentation about. Setting <code>parallel=False</code> and benchmarking, there is indeed an slight improvement over the initial prompt-engineered implementation at <strong>65x faster</strong> than the base implementation.</p><h3 id="iteration-3-1">Iteration #3</h3><p><a href="https://github.com/minimaxir/llm-write-better-code/blob/main/python_30_prompt_engineering.md#assistant-iteration-3">Another</a> iteration:</p><div><pre tabindex="0"><code data-lang="py"><span><span><span># Pre-computed perfect minimal hash table</span>
</span></span><span><span><span># Key insight: For numbers 1-100000, only need to store those with digit sum 30</span>
</span></span><span><span><span># Format: [number] = 1 if digit sum is 30, else 0</span>
</span></span><span><span><span>HASH_TABLE</span> <span>=</span> <span>np</span><span>.</span><span>zeros</span><span>(</span><span>100001</span><span>,</span> <span>dtype</span><span>=</span><span>np</span><span>.</span><span>uint8</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span># Generate hash table at module load time using bit manipulation</span>
</span></span><span><span><span>@jit</span><span>(</span><span>nopython</span><span>=</span><span>True</span><span>,</span> <span>cache</span><span>=</span><span>True</span><span>)</span>
</span></span><span><span><span>def</span> <span>init_hash_table</span><span>():</span>
</span></span><span><span>    <span>min_val</span> <span>=</span> <span>np</span><span>.</span><span>iinfo</span><span>(</span><span>np</span><span>.</span><span>uint32</span><span>)</span><span>.</span><span>max</span>
</span></span><span><span>    <span>max_val</span> <span>=</span> <span>0</span>
</span></span><span><span>
</span></span><span><span>    <span># Optimal digit sum using parallel bit counting</span>
</span></span><span><span>    <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>1</span><span>,</span> <span>100001</span><span>):</span>
</span></span><span><span>        <span>n</span> <span>=</span> <span>i</span>
</span></span><span><span>        <span>sum</span> <span>=</span> <span>0</span>
</span></span><span><span>        <span>while</span> <span>n</span> <span>and</span> <span>sum</span> <span>&lt;=</span> <span>30</span><span>:</span>
</span></span><span><span>            <span>sum</span> <span>+=</span> <span>n</span> <span>&amp;</span> <span>0xF</span>
</span></span><span><span>            <span>n</span> <span>&gt;&gt;=</span> <span>4</span>
</span></span><span><span>        <span>if</span> <span>sum</span> <span>==</span> <span>30</span><span>:</span>
</span></span><span><span>            <span>HASH_TABLE</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>1</span>
</span></span><span><span>            <span>min_val</span> <span>=</span> <span>min</span><span>(</span><span>min_val</span><span>,</span> <span>i</span><span>)</span>
</span></span><span><span>            <span>max_val</span> <span>=</span> <span>max</span><span>(</span><span>max_val</span><span>,</span> <span>i</span><span>)</span>
</span></span><span><span>
</span></span><span><span>    <span>return</span> <span>min_val</span><span>,</span> <span>max_val</span>
</span></span><span><span>
</span></span><span><span><span># Initialize at module load time</span>
</span></span><span><span><span>MIN_VALID</span><span>,</span> <span>MAX_VALID</span> <span>=</span> <span>init_hash_table</span><span>()</span>
</span></span><span><span>
</span></span><span><span><span>@jit</span><span>(</span><span>nopython</span><span>=</span><span>True</span><span>,</span> <span>parallel</span><span>=</span><span>False</span><span>,</span> <span>cache</span><span>=</span><span>True</span><span>,</span> <span>fastmath</span><span>=</span><span>True</span><span>)</span>
</span></span><span><span><span>def</span> <span>find_min_max</span><span>(</span><span>numbers</span><span>):</span>
</span></span><span><span>    <span>min_val</span> <span>=</span> <span>MAX_VALID</span>  <span># Start with known bounds</span>
</span></span><span><span>    <span>max_val</span> <span>=</span> <span>MIN_VALID</span>
</span></span><span><span>    <span>found</span> <span>=</span> <span>False</span>
</span></span><span><span>
</span></span><span><span>    <span># Single vectorized operation</span>
</span></span><span><span>    <span>mask</span> <span>=</span> <span>HASH_TABLE</span><span>[</span><span>numbers</span><span>]</span> <span>==</span> <span>1</span>
</span></span><span><span>    <span>if</span> <span>np</span><span>.</span><span>any</span><span>(</span><span>mask</span><span>):</span>
</span></span><span><span>        <span>valid_nums</span> <span>=</span> <span>numbers</span><span>[</span><span>mask</span><span>]</span>
</span></span><span><span>        <span>min_val</span> <span>=</span> <span>np</span><span>.</span><span>min</span><span>(</span><span>valid_nums</span><span>)</span>
</span></span><span><span>        <span>max_val</span> <span>=</span> <span>np</span><span>.</span><span>max</span><span>(</span><span>valid_nums</span><span>)</span>
</span></span><span><span>        <span>found</span> <span>=</span> <span>True</span>
</span></span><span><span>
</span></span><span><span>    <span>return</span> <span>min_val</span><span>,</span> <span>max_val</span><span>,</span> <span>found</span>
</span></span></code></pre></div><p>In this case, the LLM gives up on the chunking strategy which has been causing problems, and adds two more optimizations: a global <code>HASH_TABLE</code> (that’s just a numpy array, I’m unsure if a simple index lookup technically counts as a <a href="https://en.wikipedia.org/wiki/Hash_table">hash table</a>), and it introduced a logical microoptimization that after summing up digits, if the number goes over 30, the counting can stop since it can immediately be identified as invalid.</p><p>One major problem: that “generate hash table at module load time” trick doesn’t actually work due to a subtle issue with little internet documentation: objects outside of numba’s JITed functions are read-only, yet the <code>HASH_TABLE</code> is still instantiated outside of the JITed function and modified within the JITed function, and therefore will cause a very confusing error. After a tiny refactor such that the <code>HASH_TABLE</code> is instantiated within a JITed function, the code worked, and ran <em>extremely</em> fast: <strong>100x</strong> faster than the original base implementation, the same as the final performance from the casual prompting but with orders of magnitude less code.</p><h3 id="iteration-4-1">Iteration #4</h3><p>At this point, Claude actually complained that the code is at the “theoretical minimum time complexity possible for this problem.” So I mixed things up and just asked it to fix the digit sum issue: <a href="https://github.com/minimaxir/llm-write-better-code/blob/main/python_30_prompt_engineering.md#assistant-iteration-4">it did so</a> by only replacing the relevant code with the previously used integer implementation, and did not try to fix the <code>HASH_TABLE</code>. More importantly, with the <code>HASH_TABLE</code> adjustment, I confirmed the implementation is correct, finally, although with a slight performance hit since there is no more bit-shifting: it’s now <strong>95x faster</strong>.</p><h2 id="next-steps-for-better-llm-code-generation">Next Steps For Better LLM Code Generation</h2><p>Putting it all together, let’s visualize the improvements, including highlighting the cases where I needed to alter the logic of the code to make it runnable due to bugs.</p><figure><img loading="lazy" srcset="https://minimaxir.com/2025/01/write-better-code/comparison_hu16539758675350133829.webp 320w,https://minimaxir.com/2025/01/write-better-code/comparison_hu1970816546582430083.webp 768w,https://minimaxir.com/2025/01/write-better-code/comparison_hu6194793628505879947.webp 1024w,https://minimaxir.com/2025/01/write-better-code/comparison.png 1200w" src="https://minimaxir.com/2025/01/write-better-code/comparison.png"/></figure><p>In all, asking an LLM to “write code better” does indeed make the code better, depending on your definition of better. Through the use of the generic iterative prompts, the code did objectively improve from the base examples, both in terms of additional features and speed. Prompt engineering improved the performance of the code much more rapidly and consistently, but was more likely to introduce subtle bugs as LLMs are not optimized to generate high-performance code. As with any use of LLMs, your mileage may vary, and in the end it requires a human touch to fix the inevitable issues no matter how often AI hypesters cite LLMs as magic.</p><blockquote><p>All code in this blog post, including benchmarking scripts and data visualization code, is <a href="https://github.com/minimaxir/llm-write-better-code/">available on GitHub</a>.</p></blockquote><p>There are a few optimizations that I am very surprised Claude 3.5 Sonnet did not identify and implement during either experiment. Namely, it doesn’t explore the statistical angle: since we are generating 1,000,000 numbers uniformly from a range of 1 to 100,000, there will be a significant amount of duplicate numbers that will never need to be analyzed. The LLM did not attempt to dedupe, such as casting the list of numbers into a Python <code>set()</code> or using numpy’s <code>unique()</code>. I was also expecting an implementation that involves sorting the list of 1,000,000 numbers ascending: that way the algorithm could search the list from the start to the end for the minimum (or the end to the start for the maximum) without checking every number, although sorting is slow and a vectorized approach is indeed more pragmatic.</p><p>Even if LLMs can be wrong, one notable thing I learnt from these experiments is that they do have interesting ideas and tool suggestions even if the code output can’t be used as-is. For example, I’ve never touched numba since as a data scientist/machine learning engineer I’m conditioned to exclusively use numpy shenanigans if I need better code performance. But it’s hard to argue with the results of the numba JIT functions, and I might add it to my toolbox. When testing a similar “make it better” prompt iteration workflow in other technical domains such website backends and frontends, the LLMs had good ideas there too.</p><p>Of course, these LLMs won’t replace software engineers anytime soon, because it requires a strong engineering background to recognize what is <em>actually</em> a good idea, along with other constraints that are domain specific. Even with the amount of code available on the internet, LLMs can’t discern between average code and good, highly-performant code without guidance. Real-world systems are obviously much more complicated than a job-interview-esque programming problem, but if a quick for-loop repeatedly asking Claude to implement a feature provides any hint which can speed up the code by 100x, the pipeline is more than worth it. Some consider <a href="https://softwareengineering.stackexchange.com/questions/80084/is-premature-optimization-really-the-root-of-all-evil">premature optimization</a> to be bad coding practice, but in the real-world it’s better than having a subpar implementation that will become technical debt over time.</p><p>One issue with my experiments is that I’m benchmarking code improvement using Python, which isn’t the coding language developers consider when hyperoptimizing performance. While libraries such as numpy and numba leverage C to work around Python’s performance limitations, one modern approach that popular Python libraries such as <a href="https://pola.rs">polars</a> and <a href="https://docs.pydantic.dev/latest/">pydantic</a> use is to instead code using <a href="https://www.rust-lang.org">Rust</a>. Rust has many performance benefits over C, and the <a href="https://pyo3.rs/v0.23.3/">PyO3</a> crate allows Rust code to be used within Python with minimal overhead. I can confirm that Claude 3.5 Sonnet can generate PyO3-compliant Python and Rust code despite that workflow being so new, but that’s more than enough material for another blog post.</p><p>In the meantime, while asking LLMs to make code better is a more pragmatic use of AI, you <em>can</em> ask them to “make it more bro”…with mixed results.</p><figure><img loading="lazy" srcset="https://minimaxir.com/2025/01/write-better-code/brocode_hu10296254034581205435.webp 320w,https://minimaxir.com/2025/01/write-better-code/brocode_hu4271630130724519665.webp 768w,https://minimaxir.com/2025/01/write-better-code/brocode_hu4687564160972335231.webp 1024w,https://minimaxir.com/2025/01/write-better-code/brocode.jpg 1410w" src="https://minimaxir.com/2025/01/write-better-code/brocode.jpg"/></figure></div></div>
  </body>
</html>
