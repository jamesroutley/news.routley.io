<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://airbyte.com/tutorials/chat-with-your-data-using-openai-pinecone-airbyte-and-langchain">Original</a>
    <h1>Show HN: Chat with your data using LangChain, Pinecone, and Airbyte</h1>
    
    <div id="readability-page-1" class="page"><div fs-toc-offsettop="8rem" fs-toc-element="contents" fs-codehighlight-theme="base16/harmonic16-dark" fs-richtext-element="rich-text"><p>In <a href="https://airbyte.com/tutorials/implement-ai-data-pipelines-with-langchain-airbyte-and-dagster" target="_blank">a previous article</a>, we explained how Dagster and Airbyte can be leveraged to power LLM-supported use cases. Our newly introduced <a href="https://docs.airbyte.com/integrations/destinations/langchain/" target="_blank">vector database destination</a> makes this even easier as it removes the need to orchestrate chunking and embedding manually - instead the sources can be directly connected to the vector database through an Airbyte connection.</p><p>This tutorial walks you through a real-world use case of how to leverage vector databases and LLMs to make sense out of your unstructured data. By the end of this, you will:</p><ul role="list"><li>Know how to extract unstructured data from a variety of sources using Airbyte</li><li>Know how to use Airbyte to efficiently load data into a vector database, preparing the data for LLM usage along the way</li><li>Know how to integrate a vector database into your LLM to ask questions about your proprietary data</li></ul><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/6064b31ff49a2d31e0493af1/64be8f601730eafec46db241_KR_NhEQHHKlCFQxGekCCZdyUKzTB8CusOu6_bCHvWP3Rv1q2oQdmZ6dh3iIw2lGTceGyt6J8MsHWrZRv5IbYgOv7rQeaD4q0Bb0IxfBfjgq3eB-CFdaWy0pzREjG7wtzBMyzpO3a9Zwfyn8H4r64hhc.png" alt=""/></p></figure><h2>What we will build</h2><p>To better illustrate how this can look in practice, let’s use something that’s relevant for Airbyte itself.</p><p>Airbyte is a highly extensible system that allows users to develop their own connectors to extract data from any API or internal systems. Helpful information for connector developers can be found in different places:</p><ul role="list"><li>The official connector development documentation website</li><li>Github issues documenting existing feature requests, known bugs and work in progress</li><li>The community Slack help channel</li></ul><p>This article describes how to tie together all of these diverse sources to offer a single chat interface to access information about connector development - a bot that can answer questions in plain english about the code base, documentation and reference previous conversations:</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/6064b31ff49a2d31e0493af1/64cf32950aef8ef8c796036f_V1GY2h5GAs1bIkKp2J43I-geDugBrCqpnpuAyJ7y9pxalBNnLRSsDoV5r5dTK5imoJI8bxuIVPW88msowFqys6qfkQQ43c_Z13EgR79BuH30JrHsXqhh8JomGA1Qq5cuc3kwYLTzgJbGbhH7cI4ijEI.png" alt=""/></p></figure><p>In these examples, information from the documentation website and existing Github issues is combined in a single answer.</p><h2>Prerequisites</h2><p>For following through the whole process, you will need the following accounts. However, you can also work with your own custom sources and use a local vector store to avoid all but the OpenAI account:</p><p><strong>1. Source-specific accounts</strong></p><ul role="list"><li>Apify account</li><li>Github account</li><li>Slack account</li></ul><p><strong>2. Destination-specific accounts</strong></p><ul role="list"><li>OpenAI account</li><li>Pinecone account</li></ul><p><strong>3. Airbyte instance (</strong><a href="https://docs.airbyte.com/deploying-airbyte/local-deployment/" target="_blank"><strong>local</strong></a><strong> or </strong><a href="https://cloud.airbyte.com/" target="_blank"><strong>cloud</strong></a><strong>)</strong></p><h2>Step 1 - Fetch Github issues</h2><p>Airbyte’s feature and bug tracking is handled by the <a href="https://github.com/airbytehq/airbyte/issues" target="_blank">Github issue tracker</a> of the Airbyte open source repository. These issues contain important information people need to look up regularly.</p><p>To fetch Github issues, create a new source using the Github connector.</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/6064b31ff49a2d31e0493af1/64cf3296e8c576bff0a4b227_cmEnI4qaIn3kA-E9M8Lqui-SeqPKNPrS5LMWOh9Y06VB7Eqy33TJS12o3W6bV6i3iYmhNGtkz_k1bO2OJOkZUpcKrmxyj-sszksKny5gLHNaKdp7RjS9i6MawIOzS4x4jU-8l2wMjPnZX6dzW-Gq2bE.png" alt=""/></p></figure><p>If you are using Airbyte Cloud , you can easily authenticate using the “Authenticate your GitHub account”, otherwise follow the instructions in the documentation on the right side of how to set up a <a href="https://github.com/settings/tokens" target="_blank">personal access token</a> in the Github UI.</p><p>Next, configure a cutoff date for issues and specify the repositories that should be synced. In this case I’m going with “2023-07-01T00:00:00Z” and “airbytehq/airbyte” to sync recent issues from the main Airbyte repository:</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/6064b31ff49a2d31e0493af1/64cf3294bd18871ab53b534b_b1SG5Nk65k4_adaHKv9Gyyp8ssHELtDdn07xdwwG4Wtd0ijLXTUuhCcjdRbwGQg3cmAKgCN-9fiYtcbY2qpCH32Mc8dZDPcXTHOMpXxy9Y4bL64_3Vb8Nk-i1SkPjWyojaJOrVjlQ1ofXBbFLzI4cxU.png" alt=""/></p></figure><h2>Step 2 - Load into vector database</h2><p>Now we have our first source ready, but Airbyte doesn’t know yet where to put the data. The next step is to configure the destination. To do so, pick the “Vector Database (powered by LangChain)”. There is some preprocessing that Airbyte is doing for you so that the data is vector ready:</p><ul role="list"><li>Separating text and metadata fields and splitting up records into multiple documents to keep each document focused on a single topic and to make sure the text fits into the context window of the LLM that’s going to be used for question answering</li></ul><ul role="list"><li>Embedding the text of every document using the configured embedding service, turning the text into a vector to do similarity search on</li><li>Indexing the documents into the vector database (uploading the vector from the embedding service along with the metadata object)</li></ul><p>The vector database destination currently supports two different vector databases (with more to come) - Pinecone, which is a hosted service with a free tier and Chroma which stores the vector database in a local file.</p><p>For using Pinecone, sign up for a <a href="https://app.pinecone.io/?sessionType=signup" target="_blank">free trial account</a> and create an index using a starter pod. Set the dimensions to <strong>1536</strong> as that’s the size of the OpenAI embeddings we will be using</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/6064b31ff49a2d31e0493af1/64cf329550014214567eb21e_XCRydu23sUl92Kv6lhY5yrm564yhw4T7hltxT1n8alhWxwonxvhgCYr_B1Ql6OIuqUtjrfyKbqFRBw1kPHYZhknXW1xkepJLPa8xgB2JdN9bI5X3Eyx6rsnBW7OQLfji-vjoThs8FagLaD70yZLxP0c.png" alt=""/></p></figure><p>Once the index is ready, configure the vector database destination in Airbyte:</p><ul role="list"><li>Set chunk size to <strong>1000</strong> (the size refers to number of tokens, not characters, so this is roughly 4KB of text. The best chunking is dependent on the data you are dealing with)</li><li>Configure the records fields to treat as text fields which will be embedded. All other fields will be handled as metadata. For now, set it “<strong>title</strong>” and “<strong>body</strong>” as these are the relevant feels in the issue stream of the Github source</li><li>Set your OpenAI api key for powering the embedding service. You can find your API key in the API keys section of the <a href="https://platform.openai.com/account" target="_blank">platform.openai.com/account</a> page</li><li>For the indexing step, copy over index, environment and api key from the Pinecone UI. You can find the API key and the environment in the “API Keys” section in the UI</li></ul><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/6064b31ff49a2d31e0493af1/64cf3296bd18871ab53b53dc_Pp3o4YoGrQu-DFt6b9dslx6hleweOuSfkF_azqQ767s8jun3ufqdUFvZQ8ec2k3jVqR9yVELuZKfZFwPTsXTnrfiusVsNJ4zaT80OHrs2yXQF4jtb6tUKtuKzi7BpR8oqrUYUCgeI1GTxb0QMuRnrFc.png" alt=""/></p></figure><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/6064b31ff49a2d31e0493af1/64cf329552b401ed4357cc88_hDduQ9SU74XXYofV6o_iUtSzSlPQdr_aPA4nwRq96nKLGdfcqopaHIepMsy0YTI4lY81UXxkpdI3yR6AgbUkGDJuvBXwCIFTIdbMjTHKHlYt-SNGM1w5BD4JX6S0eAstmuRG1IUns_QZewWiTLWW1bA.png" alt=""/></p></figure><h2>Step 3 - Create a connection</h2><p>Once the destination is set up successfully, set up a connection from the Github source to the vector database destination. In the configuration flow, pick the existing source and destination. When configuring the connection, make sure to only use the “issues” stream, as this is the one we are interested in.</p><p>Side note: Airbyte allows to make this sync more efficient in a production environment:</p><ul role="list"><li>To keep the metadata focused, you can click on the stream name to select the individual fields you want to sync. For example if the “assignee” or the “milestone” field is never relevant to you, you can uncheck it and it won’t be synced to the destination.</li><li>The sync mode can be used to sync issues incrementally while deduplicating the records in the vector database so no stale data will show up in searches</li></ul><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/6064b31ff49a2d31e0493af1/64cf3296d9d267abe7e5b794_tVVBMEXjO0Cvkb_BGnx3JF3X7fvQAyjcyrdqowx79Hkb9FVlWrnnt2Paxfsn3B2pvcF4ejK0kBohZLNszFL5dz8YHVb18DaFOMO3wU9PFkxT8s9uTnhO9SBLRxLsRVQ3-I5ffolcruddp8v-A2Xqcqw.png" alt=""/></p></figure><p>If everything went well, there should be a connection now syncing data from Github to Pinecone via the vector store destination. Give the sync a few minutes to run. Once the first run has completed, you can check the <a href="https://app.pinecone.io/" target="_blank">Pinecone index management page</a> to see a bunch of indexed vectors ready to be queried. </p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/6064b31ff49a2d31e0493af1/64cf3296b8a6b3413b184c06_L6D9vLZf8huWjjiswHbC6c6lwNY9x0lZ91htAtNG8rc5dQztZg-QPupkAIlzKkM5Gi4-Zbi1dfMkVzKQHQimaXUnU67QguAqRpAHlwWQr_XyktA96A2KgaGTE2y4ar9nn0sf8tII6no7hT3CrU_-Wf0.png" alt=""/></p></figure><p>Each vector is associated with a metadata object that’s filled with the fields that were not mentioned as “text fields” in the destination configuration. These fields will be retrieved along with the embedded text and can be leveraged by our chatbot in later sections. This is how a vector with metadata looks like when retrieved from Pinecone:</p><div><pre><code>{
      &#34;id&#34;: &#34;599d75c8-517c-4f37-88df-ff16576bd607&#34;,
      &#34;values&#34;: [0.0076571689, ..., 0.0138477711],
      &#34;metadata&#34;: {
        &#34;_airbyte_stream&#34;: &#34;issues&#34;,
        &#34;_record_id&#34;: 1556650122,
        &#34;author_association&#34;: &#34;CONTRIBUTOR&#34;,
        &#34;comments&#34;: 3,
        &#34;created_at&#34;: &#34;2023-01-25T13:21:50Z&#34;,
        // ...
        &#34;text&#34;: &#34;...The acceptance-test-config.yml file is in a legacy format. Please migrate to the latest format...&#34;,
        &#34;updated_at&#34;: &#34;2023-07-17T09:20:56Z&#34;,
      }
}
</code></pre></div><p>On subsequent runs, Airbyte will only re-embed and update the vectors for the issues that changed since the last sync - this will speed up subsequent runs while making sure your data is always up-to-date and available.</p><h2>Step 4 - Chat interface</h2><p>The data is ready, now let’s wire it up with our LLM to answer questions in natural language. As we already used OpenAI for the embedding, the easiest approach is to use it as well for the question answering.</p><p>We will use Langchain as an orchestration framework to tie all the bits together.</p><p>First, install a few pip packages locally:</p><div><pre><code>pip install pinecone-client langchain openai</code></pre></div><p>The basic functionality here works the following way:</p><ul role="list"><li>User asks a question</li><li>The question is embedded using the same model used for generating the vectors in the vector database (OpenAI in this case)</li><li>The question vector is sent to the vector database and documents with similar vectors are returned - as the vectors represent the meaning of the text, the question and the answer to the question will have very similar vectors and relevant documents will be returned</li><li>The text of all documents with the relevant metadata are put together into a single string and sent to the LLM together with the question the user asked and the instruction to answer the user’s question based on the provided context</li><li>The LLM answers the question based on the provided context</li><li>The answer is presented to the user</li></ul><p>This flow is often referred to as <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html" target="_blank">retrieval augmented generation</a>. The RetrievalQA class from the Langchain framework already implements the basic interaction. The simplest version of our question answering bot only has to provide the vector store and the used LLM:</p><div><pre><code># chatbot.py
import os
import pinecone
from langchain.chains import RetrievalQA
from langchain.embeddings import OpenAIEmbeddings
from langchain.llms import OpenAI
from langchain.vectorstores import Pinecone

embeddings = OpenAIEmbeddings()
pinecone.init(api_key=os.environ[&#34;PINECONE_KEY&#34;], environment=os.environ[&#34;PINECONE_ENV&#34;])
index = pinecone.Index(os.environ[&#34;PINECONE_INDEX&#34;])
vector_store = Pinecone(index, embeddings.embed_query, &#34;text&#34;)

qa = RetrievalQA.from_chain_type(llm=OpenAI(temperature=0), chain_type=&#34;stuff&#34;, retriever=vector_store.as_retriever())

print(&#34;Connector development help bot. What do you want to know?&#34;)
while True:
    query = input(&#34;&#34;)
    answer = qa.run(query)
    print(answer)
    print(&#34;\nWhat else can I help you with:&#34;)</code></pre></div><p>To run this script, you need to set OpenAI and Pinecone credentials as environment variables:</p><div><pre><code>export OPENAI_API_KEY=...
export PINECONE_KEY=...
export PINECONE_ENV=...
export PINECONE_INDEX=...
python chatbot.py</code></pre></div><p>This works in general, but it has some limitations. By default, only the text fields are passed into the prompt of the LLM, so it doesn’t know what the context of a text is and it also can’t give a reference back to where it found its information:</p><div><pre><code>Connector development help bot. What do you want to know?
&gt; Can you give me information about how to authenticate via a login endpoint that 
returns a session token?


Yes, the GenericSessionTokenAuthenticator should be supported in the UI[...]</code></pre></div><p>From here, there’s lots of fine tuning to do to optimize our chat bot. For example we can improve the prompt to contain more information based on the metadata fields and be more specific for our use case:</p><div><pre><code>prompt_template = &#34;&#34;&#34;You are a question-answering bot operating on Github issues and documentation pages for a product called connector builder. 
The documentation pages document what can be done, the issues document future plans and bugs. 
Use the following pieces of context to answer the question at the end. 
If you don&#39;t know the answer, just say that you don&#39;t know, don&#39;t try to make up an answer. 
Always state were you got this information from (and the github issue number if applicable).
If the answer is based on a Github issue that&#39;s not closed yet, add &#39;This issue is not closed yet - the feature might not be shipped yet&#39; to the answer.

{context}

Question: {question}
Helpful Answer:&#34;&#34;&#34;

prompt = PromptTemplate(
    template=prompt_template, input_variables=[&#34;context&#34;, &#34;question&#34;]
)

class ConnectorDevelopmentPrompt(PromptTemplate):
    def format_document(doc: Document, prompt: PromptTemplate) -&gt; str:
        if doc.metadata[&#34;_airbyte_stream&#34;] == &#34;issues&#34;:
            return f&#34;Excerpt from Github issue: {doc.page_content}, issue number: {doc.metadata[&#39;number&#39;]}, issue state: {doc.metadata[&#39;state&#39;]}&#34;
        else:
            return super().format_document(doc, prompt)

document_prompt = ConnectorDevelopmentPrompt(input_variables=[&#34;page_content&#34;], template=&#34;{page_content}&#34;)
qa = RetrievalQA.from_chain_type(llm=OpenAI(temperature=0), chain_type=&#34;stuff&#34;, retriever=vector_store.as_retriever(), chain_type_kwargs={&#34;prompt&#34;: prompt, &#34;document_prompt&#34;: document_prompt})</code></pre></div><p>The full script also be found <a href="https://github.com/airbytehq/tutorial-connector-dev-bot/blob/main/localbot_adapted.py">on Github</a></p><p>This revised version of the RetrievalQA chain customizes the prompts that are sent to the LLM after the context has been retrieved:</p><ul role="list"><li>The basic prompt template sets the broader context what this question is about (previously the LLM had to guess from the documents)</li><li>It also changes the way documents are added to the prompt - by default, only the text is added, but the ConnectorDevelopmentPrompt implementation sets the context where the data is coming from and also adds relevant metadata to the prompt so the LLM can base its answer on more than just the text</li></ul><div><pre><code>Connector development help bot. What do you want to know?
&gt; Can you give me information about how to authenticate via a login endpoint that 
returns a session token?

You can use the GenericSessionTokenAuthenticator to authenticate via a login endpoint 
that returns a session token. This is documented in the Connector Builder 
documentation with an example of how the request flow functions (e.g. metabase). This 
issue is not closed yet - the feature might not be shipped yet (Github issue #26341).</code></pre></div><h2>Step 5 - Put it on Slack</h2><p>So far this helper can only be used locally. However, using the python slack sdk it’s easy to turn this into a Slack bot itself.</p><p>To do so, we need to set up a Slack “App” first. Go to <a href="https://api.slack.com/apps" target="_blank">https://api.slack.com/apps</a> and create a new app based on the manifest <a href="https://raw.githubusercontent.com/airbytehq/tutorial-connector-dev-bot/main/slack_manifest.json" target="_blank">here</a> (this saves you some work configuring permissions by hand). After you set up your app, install it to the workspace you want to integrate with. This will generate a “Bot User OAuth Access Token” you need to note down. Afterwards, go to the “Basic information” page of your app, scroll down to “App-Level Tokens” and create a new token. Note down this “app level token” as well.</p><p>Within the regular Slack client, your app can be added to a slack channel by clicking the channel name and going to the “Integrations” tab:</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/6064b31ff49a2d31e0493af1/64cf32964c000a2e7a0f085e_7EqGxZDFTY9RU3DfPrg6bfuAfs4cpsRpo6vppyzyvshYrV8R1E8j5mKEEiTjiNsaYOJwrWBztzwxPKx1hznDHDnZeG9JZUbu-iStCdvnaKONR-MwtGDNlD4HUnlJATjKQO_EwCH1hcHozqJHeyVlCJs.png" alt=""/></p></figure><p>After this, your Slack app is ready to receive pings from users to answer questions - the next step is to call Slack from within python code, so we need to install the python client library:</p><p>Afterwards, we can extend our existing chatbot script with a Slack integration:</p><div><pre><code>from slack_sdk import WebClient
from slack_sdk.socket_mode import SocketModeClient
from slack_sdk.socket_mode.request import SocketModeRequest
from slack_sdk.socket_mode.response import SocketModeResponse

slack_web_client = WebClient(token=os.environ[&#34;SLACK_BOT_TOKEN&#34;])

handled_messages = {}

def process(client: SocketModeClient, socket_mode_request: SocketModeRequest):
    if socket_mode_request.type == &#34;events_api&#34;:
        event = socket_mode_request.payload.get(&#34;event&#34;, {})
        client_msg_id = event.get(&#34;client_msg_id&#34;)
        if event.get(&#34;type&#34;) == &#34;app_mention&#34; and not handled_messages.get(client_msg_id):
            handled_messages[client_msg_id] = True
            channel_id = event.get(&#34;channel&#34;)
            text = event.get(&#34;text&#34;)
            result = qa.answer(text)
            slack_web_client.chat_postMessage(channel=channel_id, text=result)
    
    return SocketModeResponse(envelope_id=socket_mode_request.envelope_id)

socket_mode_client = SocketModeClient(
    app_token=os.environ[&#34;SLACK_APP_TOKEN&#34;], 
    web_client=slack_web_client
)
socket_mode_client.socket_mode_request_listeners.append(process)

socket_mode_client.connect()
print(&#34;listening&#34;)
from threading import Event
Event().wait()
</code></pre></div><p>The full script also be found <a href="https://github.com/airbytehq/tutorial-connector-dev-bot/blob/main/slackbot.py" target="_blank">on Github</a></p><p>To run the script, the environment variables for the slack bot token and app token need to be added as environment variables as well:</p><div><pre><code>export SLACK_BOT_TOKEN=...
export SLACK_APP_TOKEN=...
python chatbot.py</code></pre></div><p>Running this, you should be able to ping the development bot application in the channel you added it to like a user and it will respond to questions by running the RetrievalQA chain that loads relevant context from the vector database and uses an LLM to formulate a nice answer:</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/6064b31ff49a2d31e0493af1/64cf32964c000a2e7a0f0881_vTyLpkj1MHbsiYFB2CICRWrozqvsiwvE4vdt_AFwqWpQgXbaMFQz4k2lslmJLVWaQVE9qNnTjiRCHEe_0D9I3CPyEeVEfGvkXoz9PIQwPne4DhxqDfRaUM5mBEW1XBstAr1bas4OAaKnxVYGBLYm40I.png" alt=""/></p></figure><p>All the code can also be found <a href="https://github.com/airbytehq/tutorial-connector-dev-bot" target="_blank">on Github</a></p><h2>Step 6 - Additional data source: Scrape documentation website</h2><p>Github issues are helpful, but there is more information we want our development bot to know.</p><p>The <a href="https://docs.airbyte.com/connector-development/connector-builder-ui/overview/" target="_blank">documentation page</a> for connector development is a very important source of information to answer questions, so it definitely needs to be included. The easiest way to make sure the bot has the same information as what’s published, is to scrape the website. For this case, we are going to use the <a href="https://docs.airbyte.com/integrations/destinations/langchain/" target="_blank">Apify</a> service to take care of the scraping and turning the website into a nicely structured dataset. This dataset can be extracted using the Airbyte Apify Dataset source connector.</p><p>First, log into Apify and navigate to the store. Choose the “Web Scraper” actor as a basis - it already implements most of the functionality we need</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/6064b31ff49a2d31e0493af1/64cf32960aef8ef8c796039d_6R0PBxp9FZSvldwNjMaZHEE3MNg8k4jVomPTX_cIDysWYsA4cW26mn6S0mTKGWNVGWUlYBGftbG_O8g-FHlqcXSqqSiOrG_dr3kwO-sKKf1NHpGqy5npmDiruMpj8G0ZZExt9y61r8uLSp9CExLB8mE.png" alt=""/></p></figure><p>Next, create a new task and configure it to scrape all pages of the documentation, extracting the page title and all of the content:</p><ul role="list"><li>Set Start URLs to <a href="https://docs.airbyte.com/connector-development/connector-builder-ui/overview/" target="_blank">https://docs.airbyte.com/connector-development/connector-builder-ui/overview/</a> , the intro page of the documentation linking to other pages</li><li>Set Link selector to a[href] to follow all links from every page</li><li>Set Glob Patterns to <a href="https://docs.airbyte.com/connector-development/connector-builder-ui/*" target="_blank">https://docs.airbyte.com/connector-development/connector-builder-ui/*</a> to limit the scraper to stick to the documentation and not crawl the whole internet</li><li>Configure the Page function to extract the page title and the content - in this case the content element can be found using the CSS class name</li></ul><div><pre><code>async function pageFunction(context) {
    const $ = context.jQuery;
    const pageTitle = $(&#39;title&#39;).first().text();
    const content = $(&#39;.markdown&#39;).first().text();

    return {
        url: context.request.url,
        pageTitle,
        content
    };
}
</code></pre></div><p>Running this actor will complete quickly and give us a nicely consumable dataset with a column for the page title and the content:</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/6064b31ff49a2d31e0493af1/64cf329850014214567eb2ed_5xcWYKrZInzlQJs6pFlTemphPLS6V0L9A8x504bIgDm8B9bcmVo1-eaxpfwxt79RkXQg-zP6agQkgryy6TrY5j4EN6QNDkiaL8DVyOe61EKVocCO6w2uGjrQSLAGmvziyZ8gMakiWV48sLdZdmQbm9U.png" alt=""/></p></figure><p>Now it’s time to connect Airbyte to the Apify data set - go to the Airbyte web UI and add your second Source - pick “Apify Dataset”</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/6064b31ff49a2d31e0493af1/64cf3297d40a4b1b7d0fe6b1_5nd8X3zhXv61VB0zU5hN6GtQsFErLtmdC_JMYaSrDqmpKi-gSmh8d2pxB_c0y6gmQUW1hj77bwAiGV9hTK21tI9MUaGuR92qZqttUAtuSVCzDHk2dQYn8O6ysKl-tKnkyGp4cueJ3Nf7-OTZhV3jVtk.png" alt=""/></p></figure><p>To set up the Source, you only need to copy the dataset ID that’s shown in the “Storage” tap of the “Run” in the Apify UI</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/6064b31ff49a2d31e0493af1/64cf329919c2fe31dee10424_Ujq9XkzeNwphZ2d5vxch_jM-lt1lcahHMWLPeywhrld5mzltPZ604mOv-ETKrvBCp-6YiY0j8lPpRy30Wc9BRjrsiGTmSRuJZZj-FMbnOhV0oSbsIZIA1eBervtu0XuqJyE8m27n_-29KJMUmI8RZOk.png" alt=""/></p></figure><figure class="w-richtext-align-center w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/6064b31ff49a2d31e0493af1/64cf32980aef8ef8c7960424_tILg5dMD_pXDT1-FAogb6jvfm90hUPR4vfANJOdgTMkOF2vllP0H0HqFt3XWBE1qUD9KYKFHniHsNs1n6lV9Yr_7e7mo5mR_fvQHOKicsr6pzZJFK7RWjV5j2ELORHBgngmPwEgLo46YOppqV4y3r_g.png" alt=""/></p></figure><p>Once the source is set up, follow the same steps as for the Github source to set up a connection moving data from the Apify dataset to the vector store. As the relevant text content is sitting in different fields, you also need to update the vector store destination - add <strong>data.pageTitle</strong> and <strong>data.content</strong> to the “text fields” of the destination and save.</p><h2>Step 7 - Additional data source: Fetch Slack messages</h2><p>Another valuable source of information relevant to connector development are Slack messages from the public help channel. These can be loaded in a very similar fashion. Create a new source using the Slack connector. When using cloud, you can authenticate using the “Authenticate your Slack account” button for simple setup, otherwise follow the instructions in the documentation on the right hand side how to create a Slack “App” with the required permissions and add it to your workspace. To avoid fetching messages from all channels, set the channel name filter to the correct channel.</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/6064b31ff49a2d31e0493af1/64cf329862f87a60330875d1_1gjkCtJ_kGBeQzUhxy_ONp-rgrdguY-SPL_bZmqQPVQKxYYlJh6aIgnQj-pafIQXFBUNo6_ZfI0aSvGTRYpEO-Jwg3vyMQjvOqLYrOjKRZg0EIZSrarZFj5uIzpQ0RM7CzCdmmgrY0Ds54eLx0Msa7Q.png" alt=""/></p></figure><p>As for Apify and Github, a new connection needs to be created to move data from Slack to Pinecone. Also add <strong>text</strong> to the “text fields” of the destination to make sure the relevant data gets embedded properly so similarity searches will yield the right results.</p><p>If everything went well, there should be three connections now, all syncing data from their respective sources to the centralized vector store destination using a Pinecone index.</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/6064b31ff49a2d31e0493af1/64cf329b52b401ed4357cf3d_foNAVHB4HLrDJ-9AtJQbxt7tWUSM_-j9CwYd5TMVUx8SGvAcoMkI9AjOufJTsZuyMf3Q4lKAisy4b1iXNtDUS6pkhK3J2_kGs4vCm6YRHWOA2ic_be_9rHk4jm-v36IILxcyJOJspDenqV8hAuPTC9w.png" alt=""/></p></figure><p>By adjusting the frequency of the connections, you can control how often Airbyte will rerun the connection to make sure the knowledge base of our chat bot stays up to date. As Github and Slack are frequently updated and support efficient incremental updates, it makes sense to set them to a daily frequency or higher. The documentation pages don’t change as often, so they can be kept at a lower frequency or even just be triggered on demand when there are changes.</p><p>As we have more sources now, let’s improve our prompt to make sure the LLM has all necessary information to formulate a good answer:</p><div><pre><code>class ConnectorDevelopmentPrompt(PromptTemplate):
    def format_document(doc: Document, prompt: PromptTemplate) -&gt; str:
        if doc.metadata[&#34;_airbyte_stream&#34;] == &#34;DatasetItems&#34;:
            return f&#34;Excerpt from documentation page: {doc.page_content}&#34;
        elif doc.metadata[&#34;_airbyte_stream&#34;] == &#34;issues&#34;:
            return f&#34;Excerpt from Github issue: {doc.page_content}, issue number: {doc.metadata[&#39;number&#39;]}, issue state: {doc.metadata[&#39;state&#39;]}&#34;
        elif doc.metadata[&#34;_airbyte_stream&#34;] == &#34;threads&#34; or doc.metadata[&#34;_airbyte_stream&#34;] == &#34;channel_messages&#34;:
            return f&#34;Excerpt from Slack thread: {doc.page_content}&#34;
        else:
            return super().format_document(doc, prompt)</code></pre></div><p>By default the RetrievalQA chain retrieves the top 5 matching documents, so if it’s applicable the answer will be based on multiple sources at the same time:</p><div><pre><code>Connector development help bot. What do you want to know?
&gt; What different authentication methods are supported by the builder? Can I 
authenticate a login endpoint that returns a session token?

The authentication methods supported by the builder are Basic HTTP, Bearer Token, API 
Key, and OAuth. The builder does not currently support authenticating a login endpoint 
that returns a session token, but this feature is planned and can be tracked in the 
Github issue #26341. This issue is not closed yet - the feature might not be shipped 
yet.</code></pre></div><p>The first sentence about Basic HTTP, Bearer Token, API Key and OAuth is retrieved from the <a href="https://docs.airbyte.com/connector-development/connector-builder-ui/authentication/">documentation page about authentication</a>, while the second sentence is referring to the same Github issue as before.</p><h2>Wrapping up</h2><p>We covered a lot of ground here - stepping back a bit, we accomplished the following parts:</p><ul role="list"><li>Set up a pipeline that loads unstructured data from multiple sources into a vector database</li><li>Implement an application that can answer plain text questions about the unstructured data in a general way</li><li>Expose this application as a Slack bot</li></ul><p>With data flowing through this system, Airbyte will make sure the data in your vector database will always be up-to-date while only syncing records that changed in the connected source, minimizing the load on embedding and vector database services while also providing an overview over the current state of running pipelines.</p><p>This setup isn’t using a single black box service that encapsulates all the details and leaves us with limited options for tweaking behavior and controlling data processing - instead it’s composed out of multiple components that be easily extended in various places:</p><ul role="list"><li>The large catalog of Airbyte sources and the connector builder for integrating specialized sources allow to easily load just about any data into a vector db using a single tool</li><li>Langchain is very extensible and allows you to leverage LLMs in different ways beyond this simple application, including enriching data from other sources, keeping a chat history to be able to have full conversations and more</li></ul><p><strong>Survey</strong></p><p>If you are interested in leveraging Airbyte to ship data to your LLM-based applications, take a moment to <a href="https://docs.google.com/forms/d/e/1FAIpQLSduobMZwbqiFlPxsWDG-hrBw6NLYMDu_7zRfo4j7AsaO1QtfQ/viewform?usp=sf_link">fill out our survey</a> so we can make sure to prioritize the most important features.</p></div></div>
  </body>
</html>
