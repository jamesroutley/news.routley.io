<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.usenix.org/publications/loginonline/evolution-sre-google">Original</a>
    <h1>The Evolution of SRE at Google</h1>
    
    <div id="readability-page-1" class="page"><div><div><div><p><span>An example of this phenomenon occurred at Google in 2021. We set and enforce resource quotas for some kinds of internal software running on our infrastructure. To maximize efficiency, we also monitor how much of its quota each software service uses. If a service consistently uses less resources than its quota, we automatically reduce the quota. In STPA terms, this quota rightsizer has a control action to reduce a service&#39;s quota. From a safety perspective, we then ask when this action would be unsafe. As one example, if the rightsizer  ever reduced a service&#39;s quota below the actual needs of that service, it would be unsafe—the service would be resource-starved. This is what STPA calls an </span><span>unsafe control action</span><span> (UCA).</span></p><p><span>STPA analyzes each interaction in a system to determine comprehensively how the interaction must be controlled in order for the system to be safe. Unsafe control actions lead to the system entering one or more hazard states. There are only four possible types of UCA:</span></p><ol><li><p><span>A required control action is not provided.</span></p></li><li><p><span>An incorrect or inadequate control action is provided.</span></p></li><li><p><span>A control action is provided at the wrong time or in the wrong sequence.</span></p></li><li><p><span>A control action is stopped too soon or applied for too long.</span></p></li></ol><p><span>This particular unsafe control action—reducing an assigned quota to be less than what the service requires—is an example of the second type of UCA.</span></p><p><span>Simply identifying this unsafe control action by itself is only partially useful. If &#34;quota rightsizer reduces the assigned quota under what the service requires&#34; is unsafe, then preventing that behavior is what the system must do, i.e. &#34;quota rightsizer must not reduce the assigned quota under what the service currently requires.&#34; This is a </span><span>safety requirement</span><span>. Safety requirements can be very useful for formulating future designs, elaborating testing plans, and helping people understand the system. And let’s be honest—even mature software systems can operate in ways that are undocumented, unclear, and surprising.</span></p><p><span>Nonetheless, what we really want is to anticipate all of the concrete scenarios that lead to a hazard state. Again, STPA has a simple and comprehensive way to structure an analysis to find all of the scenarios that could lead the quota rightsizer to violate this safety requirement.</span></p><p><span>So in the case of the rightsizer, there are four archetypal scenarios that we can investigate.</span></p><ol><li><p><span>Scenarios in which the rightsizer has incorrect behavior.</span></p></li><li><p><span>Scenarios in which the rightsizer gets incorrect feedback (or no feedback at all).</span></p></li><li><p><span>Scenarios in which the quota system never receives an action from the rightsizer (even though the rightsizer tried to send one).</span></p></li><li><p><span>Scenarios in which the quota system has incorrect behavior.</span></p></li></ol><p><span>One specific scenario quickly jumped out to us when analyzing the rightsizer. It gets feedback on the current resource usage from the quota service. As implemented, the calculation of current resource usage is complicated, involving different data collectors and some tricky aggregation logic. What if something went wrong with this complex calculation, resulting in a value that was too low? In short, the rightsizer would react exactly as designed and reliably shrink a service’s quota to the incorrect lower usage level. </span></p><p><span>Exactly the disaster we wanted to prevent. </span></p><p><span>Up to this point, lots of attention had been paid to getting the quota adjustment algorithm right and reliably producing the correct outputs, namely, the action to adjust a service’s quota. However, the feedback path—including the service’s current resource usage—had been less well understood. </span></p><p><span>This highlights a major advantage of STPA—by looking at the system level and by modeling the system in terms of control-feedback loops, we find issues both in the control path and the feedback path. As we run STPA on more and more systems, we see that the feedback path is often less well understood than the control path, but just as important from a system safety perspective.</span></p><p><span>As we dug into the feedback paths for the rightsizer, we saw many opportunities to improve them. None of these changes looked like a traditional reliability solution—it didn’t boil down to managing the rightsizer with a different SLO and error budget. Instead, the solutions showed up in other parts of the system and involved redesigning parts of the stack that had previously appeared to be unrelated–again, an advantage of STPA’s system theory approach.</span></p><p><span>In the 2021 incident, incorrect feedback about the resources used by a critical service in Google&#39;s infrastructure was sent to the rightsizer. The rightsizer calculated a new quota, allocating far fewer resources than the service was actually using. As a precautionary measure, this quota reduction was not immediately applied, but was held for several weeks to give time for someone to intervene in case the quota was wrong. </span></p><p><span>Of course, major incidents are never simple events—the next problem was that despite adding the delay as a safety feature, feedback about the pending change was never sent to anyone. The entire system was in a hazard state for weeks, but because we weren&#39;t looking for it, we missed our chance to prevent the loss that followed. After several weeks, the quota reduction was applied resulting in a significant outage. Using STPA, we have anticipated problems just like this one in many different systems across Google.</span></p><p><span><span>As Leveson writes in </span><span>Engineering a Safer World</span><span>: &#34;In [STAMP], understanding why an accident occurred requires determining why the control was ineffective. Preventing future accidents requires shifting from a focus on preventing failures to the broader goal of designing and implementing controls that will enforce the necessary constraints.&#34;</span><span> This shift in perspective - from trying to prove the absence of problems to effectively managing known and potential hazards - is a key principle in our system safety approach.</span></span></p></div></div></div></div>
  </body>
</html>
