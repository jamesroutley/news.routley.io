<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.cmu.edu/news/stories/archives/2023/july/researchers-discover-new-vulnerability-in-large-language-models">Original</a>
    <h1>Researchers Discover New Vulnerability in Large Language Models</h1>
    
    <div id="readability-page-1" class="page"><div id="main" tabindex="0" aria-label="main content">
  
<div id="block-cmu2019-content">
  
    
      
<div>
  <div>
    <p><em>
      July 28, 2023
    </em></p>
    
    


    

  
                  
      
    

  
          <details>
                            <summary>Media Inquiries</summary>
  
          <div>
                                                                                                                              <div>        


                  
                                                                                                                


                  
                                                                                                                        


                  
                                                                                                                  <address>
                      <strong>

  
                                            <p>Name</p>
            
                                                                                                                Ryan Noone
                                          
      </strong>
                        
                        

  
                  <div>
                                  <p>Title</p><p>
            
                                                                                                                  College of Engineering
                                          
      </p></div>
      
                        


                  
      
                        
            </address>

                                          
      
      
  
                                          
      
                                          
      
      
  </div>
                                                        
  </div>

      
    </details>
  
  </div>
</div>



                  
                                                                                                                


    <section>

        
        
    
                <div>
                          <p><span lang="EN" xml:lang="EN">Large language models (LLMs) use deep-learning techniques to process and generate human-like text. The models train on vast amounts of data from books, articles, websites and other sources to generate responses, translate languages, summarize text, answer questions and perform a wide range of natural language processing tasks.</span></p><p><span lang="EN" xml:lang="EN">This rapidly evolving artificial intelligence technology has led to the creation of both open- and closed-source tools, such as ChatGPT, Claude and Google Bard, enabling anyone to search and find answers to a seemingly endless range of queries. While these tools offer significant benefits, there is growing concern about their ability to generate objectionable content and the resulting consequences.</span></p><p><span lang="EN" xml:lang="EN">Researchers at </span><a href="https://www.cs.cmu.edu/" target="_blank"><span lang="EN" xml:lang="EN">Carnegie Mellon University’s School of Computer Science</span><span role="text">(opens in new window)</span></a><span lang="EN" xml:lang="EN"> (SCS), the </span><a href="https://www.cylab.cmu.edu/" target="_blank"><span lang="EN" xml:lang="EN">CyLab Security and Privacy Institute</span><span role="text">(opens in new window)</span></a><span lang="EN" xml:lang="EN">, and the </span><a href="https://www.safe.ai/" target="_blank"><span lang="EN" xml:lang="EN">Center for AI Safety in San Francisco</span><span role="text">(opens in new window)</span></a><span lang="EN" xml:lang="EN"> have uncovered a new vulnerability, proposing a simple and effective attack method that causes aligned language models to generate objectionable behaviors at a high success rate.</span></p><p><span lang="EN" xml:lang="EN">In their latest study, ‘</span><a href="https://llm-attacks.org/zou2023universal.pdf" target="_blank"><span lang="EN" xml:lang="EN">Universal and Transferable Adversarial Attacks on Aligned Language Models</span><span role="text">(opens in new window)</span></a><span lang="EN" xml:lang="EN">,’ CMU Associate Professors </span><a href="https://www.cs.cmu.edu/~mfredrik/" target="_blank"><span lang="EN" xml:lang="EN">Matt Fredrikson</span><span role="text">(opens in new window)</span></a><span lang="EN" xml:lang="EN"> and </span><a href="http://zicokolter.com/" target="_blank"><span lang="EN" xml:lang="EN">Zico Kolter</span><span role="text">(opens in new window)</span></a><span lang="EN" xml:lang="EN">, Ph.D. student </span><a href="https://csd.cmu.edu/people/doctoral-student/andy-zou" target="_blank"><span lang="EN" xml:lang="EN">Andy Zou</span><span role="text">(opens in new window)</span></a><span lang="EN" xml:lang="EN">, and alumnus Zifan Wang found a suffix that, when attached to a wide range of queries, significantly increases the likelihood that both open- and closed-source LLMs will produce affirmative responses to queries that they would otherwise refuse. Rather than relying on manual engineering, their approach automatically produces these adversarial suffixes through a combination of greedy and gradient-based search techniques.</span></p><p><span lang="EN" xml:lang="EN">&#34;At the moment, the direct harms to people that could be brought about by prompting a chatbot to produce objectionable or toxic content may not be especially severe,” said Fredrikson. “The concern is that these models will play a larger role in autonomous systems that operate without human supervision. As autonomous systems become more of a reality, it will be very important to ensure that we have a reliable way to stop them from being hijacked by attacks like these.”</span></p><p><span lang="EN" xml:lang="EN">In 2020, Fredrikson and fellow researchers from CyLab and the </span><a href="https://www.sei.cmu.edu/" target="_blank"><span lang="EN" xml:lang="EN">Software Engineering Institute</span><span role="text">(opens in new window)</span></a><span lang="EN" xml:lang="EN"> discovered vulnerabilities within image classifiers, AI-based deep-learning models that automatically identify the subject of photos. By making minor changes to the images, the researchers could alter how the classifiers viewed and labeled them.</span></p><figure role="group"><p>
    
    


                  
                                                                                                                    <img data-focal-point="center center" src="https://www.cmu.edu/news/sites/default/files/2023-07/badchat01.png" width="487" height="304" alt="Researchers&#39; adversarial prompts can elicit arbitrary harmful behaviors from state-of-the-art commercial LLMs with high probability, demonstrating potentials for misuse." loading="lazy"/></p>
<figcaption>Researchers&#39; adversarial prompts can elicit arbitrary harmful behaviors from state-of-the-art commercial LLMs with high probability, demonstrating potentials for misuse.</figcaption></figure><p><span lang="EN" xml:lang="EN">Using similar methods, Fredrikson, Kolter, Zou, and Wang successfully attacked Meta’s open-source chatbot, tricking the LLM into generating objectionable content. While discussing their finding, Wang decided to try the attack on ChatGPT, a much larger and more sophisticated LLM. To their surprise, it worked.</span></p><p><span lang="EN" xml:lang="EN">“We didn’t set out to attack proprietary large language models and chatbots,” Fredrikson said. “But our research shows that even if you have a big trillion parameter closed-source model, people can still attack it by looking at freely available, smaller and simpler open-sourced models and learning how to attack those.” </span></p><p><span lang="EN" xml:lang="EN">By training the attack suffix on multiple prompts and models, the researchers have also induced objectionable content in public interfaces like Google Bard and Claud and in open-source LLMs such as Llama 2 Chat, Pythia, Falcon and others.</span></p><p><span lang="EN" xml:lang="EN">“Right now, we simply don’t have a convincing way to stop this from happening, so the next step is to figure out how to fix these models,” Fredrikson said.</span></p><p><span lang="EN" xml:lang="EN">Similar attacks have existed for a decade on different types of machine learning classifiers, such as in computer vision. While these attacks still pose a challenge, many of the proposed defenses build directly on top of the attacks themselves.</span></p><p><span lang="EN" xml:lang="EN">“Understanding how to mount these attacks is often the first step in developing a strong defense,” he said.</span></p>
              </div>
    
                
    
  </section>


                                                                                                                                                


                  
                                                                                                                
  


  <section>

        
                  


                  
                                                                                                                


  



  



                                          
      
              
            </section>


                                          
      

                                                                                                                                            
  


  <section>

        
                  


                  
                                                                                                                


  



  



                                                                                                                                            


  



  



                                          
      
              
            </section>


                                          
      

  </div>


</div></div>
  </body>
</html>
