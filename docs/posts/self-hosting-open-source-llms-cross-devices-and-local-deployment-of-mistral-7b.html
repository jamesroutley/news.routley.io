<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.secondstate.io/articles/mistral-7b-instruct-v0.1/">Original</a>
    <h1>Self-Hosting Open Source LLMs: Cross Devices and Local Deployment of Mistral 7B</h1>
    
    <div id="readability-page-1" class="page"><article><p>The <a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1" target="_blank">mistral-7b-instruct-v0.1 model</a> is a 7B instruction-tuned LLM released by Mistral AI. It is a true open source model licensed under Apache 2.0. It has a context length of 8,000 tokens and performs on par with 13B llama2 models. It is great for generating prose, summarizing documents, and writing code.</p>
<p>In this article, we will cover</p>
<ul>
<li>How to run mistral-7b-instruct-v0.1 on your own device</li>
<li>How to create an OpenAI-compatible API service for mistral-7b-instruct-v0.1</li>
</ul>
<p><img src="https://www.secondstate.io/articles/mistral-7b-instruct-v0.1.png" alt=""/>
We will use the Rust + Wasm stack to develop and deploy applications for this model. There is no complex Python packages or C++ toolchains to install! <a href="https://www.secondstate.io/articles/fast-llm-inference/" target="_blank">See why we choose this tech stack</a>.</p>
<h2 id="run-the-model-on-your-own-device">Run the model on your own device</h2>
<p>Step 1: Install <a href="https://github.com/WasmEdge/WasmEdge" target="_blank">WasmEdge</a> via the following command line.</p>
<pre><code>curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install.sh | bash -s -- --plugin wasi_nn-ggml
</code></pre><p>Step 2: Download the <a href="https://huggingface.co/second-state/Mistral-7B-Instruct-v0.1-GGUF/blob/main/mistral-7b-instruct-v0.1.Q5_K_M.gguf" target="_blank">model GGUF file</a>. It may take a long time, since the size of the model is several GBs.</p>
<pre><code>https://huggingface.co/second-state/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q5_K_M.gguf
</code></pre><p>Step 3: Download a cross-platform portable Wasm file for the chat app. The application allows you to chat with the model on the command line. The Rust source code for the app is <a href="https://github.com/second-state/llama-utils/tree/main/chat" target="_blank">here</a>.</p>
<pre><code>curl -LO https://github.com/second-state/llama-utils/raw/main/chat/llama-chat.wasm
</code></pre><p>That&#39;s it. You can chat with the model in the terminal by entering the following command.</p>
<pre><code>wasmedge --dir .:. --nn-preload default:GGML:AUTO:mistral-7b-instruct-v0.1.Q5_K_M.gguf llama-chat.wasm -p mistral-instruct-v0.1
</code></pre><p>The portable Wasm app automatically takes advantage of the hardware accelerators (eg GPUs) I have on the device.</p>
<p>On my Mac M1 32G memory device, it clocks in at about 20.71 tokens per second.</p>
<pre><code>[USER]:
What is the capital of France?

[ASSITANT]:
The capital of France is Paris.

[USER]:
</code></pre><div>
<blockquote><p lang="en" dir="ltr">The Mistral 7B model is an <a href="https://twitter.com/hashtag/opensource?src=hash&amp;ref_src=twsrc%5Etfw">#opensource</a> <a href="https://twitter.com/hashtag/LLM?src=hash&amp;ref_src=twsrc%5Etfw">#LLM</a> licensed under Apache 2.0. It has a 8k context length and performs on par with many 13B models on a variety of tasks including writing code.</p>— wasmedge (@realwasmedge) <a href="https://twitter.com/realwasmedge/status/1725207312367853577?ref_src=twsrc%5Etfw">November 16, 2023</a></blockquote> 
</div>
<h2 id="create-an-openai-compatible-api-service">Create an OpenAI-compatible API service</h2>
<p>An OpenAI-compatible web API allows the model to work with a large ecosystem of LLM tools and agent frameworks such as <a href="https://flows.network/" target="_blank">flows.network</a>, LangChain, and LlamaIndex.</p>
<p>Download an API server app. It is also a cross-platform portable Wasm app that can run on many CPU and GPU devices.</p>
<pre><code>curl -LO https://github.com/second-state/llama-utils/raw/main/api-server/llama-api-server.wasm
</code></pre><p>Then, use the following command lines to start an API server for the model.</p>
<pre><code>wasmedge --dir .:. --nn-preload default:GGML:AUTO:mistral-7b-instruct-v0.1.Q5_K_M.gguf llama-api-server.wasm -p mistral-instruct-v0.1
</code></pre><p>From another terminal, you can interact with the API server using curl.</p>
<pre><code>curl -X POST http://0.0.0.0:8080/v1/chat/completions -H &#39;accept:application/json&#39; -H &#39;Content-Type: application/json&#39; -d &#39;{&#34;messages&#34;:[{&#34;role&#34;:&#34;system&#34;, &#34;content&#34;:&#34;You are a helpful AI assistant&#34;}, {&#34;role&#34;:&#34;user&#34;, &#34;content&#34;:&#34;What is the capital of France?&#34;}], &#34;model&#34;:&#34;mistral-7b-instruct-v0.1&#34;}&#39;
</code></pre><p>That’s all. WasmEdge is the <a href="https://www.secondstate.io/articles/fast-llm-inference/" target="_blank">easiest, fastest, and safest way to run LLM applications</a>. Give it a try!</p>
<p>Join the <a href="https://discord.com/invite/U4B5sFTkFc" target="_blank">WasmEdge discord</a> to ask questions or share insights.</p>
<p><a href="https://code.flows.network/webhook/vvAtEBUk6QMhVVLuw7IU" target="_blank">No time to DIY? Book a Demo with us to enjoy your own LLMs across devices!</a></p>
</article><section><img src="https://www.secondstate.io/assets/img/logo.png" alt=""/><p>A high-performance, extensible, and hardware optimized WebAssembly Virtual Machine for automotive, cloud, AI, and blockchain applications</p>
</section></div>
  </body>
</html>
