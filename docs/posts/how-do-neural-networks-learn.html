<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://phys.org/news/2024-03-neural-networks-mathematical-formula-relevant.html">Original</a>
    <h1>How do neural networks learn?</h1>
    
    <div id="readability-page-1" class="page"><div>
										
<div>
    <div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2024/how-do-neural-networks.jpg" data-src="https://scx2.b-cdn.net/gfx/news/2024/how-do-neural-networks.jpg" data-sub-html="Top eigenvector of AGOP of two separate models, MLPs and Laplace kernel machines, captured similar features (cosine similarity greater than .99) when trained on the same data from CelebA across various tasks. Credit: &lt;i&gt;Science&lt;/i&gt; (2024). DOI: 10.1126/science.adi5639">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2024/how-do-neural-networks.jpg" alt="How do neural networks learn? A mathematical formula explains how they detect relevant patterns" title="Top eigenvector of AGOP of two separate models, MLPs and Laplace kernel machines, captured similar features (cosine similarity greater than .99) when trained on the same data from CelebA across various tasks. Credit: Science (2024). DOI: 10.1126/science.adi5639" width="800" height="530"/>
             <figcaption>
                Top eigenvector of AGOP of two separate models, MLPs and Laplace kernel machines, captured similar features (cosine similarity greater than .99) when trained on the same data from CelebA across various tasks. Credit: <i>Science</i> (2024). DOI: 10.1126/science.adi5639
            </figcaption>        </figure>
    </div>
</div><p>Neural networks have been powering breakthroughs in artificial intelligence, including the large language models that are now being used in a wide range of applications, from finance, to human resources to health care. But these networks remain a black box whose inner workings engineers and scientists struggle to understand.</p>


										      <section>
         <!-- /4988204/Phys_Story_InText_Box -->
        
      </section>
																																	
<p>Now, a team led by data and computer scientists at the University of California San Diego has given neural networks the equivalent of an X-ray to uncover how they actually learn.</p>
<p>The researchers found that a formula used in <a href="https://phys.org/tags/statistical+analysis/" rel="tag">statistical analysis</a> provides a streamlined mathematical description of how neural networks, such as GPT-2, a precursor to ChatGPT, learn relevant patterns in data, known as features. This formula also explains how neural networks use these relevant patterns to make predictions.</p>
<p>&#34;We are trying to understand neural networks from first principles,&#34; said Daniel Beaglehole, a Ph.D. student in the UC San Diego Department of Computer Science and Engineering and co-first author of the study. &#34;With our formula, one can simply interpret which features the network is using to make predictions.&#34;</p>
<p>The team <a href="https://www.science.org/doi/10.1126/science.adi5639" target="_blank">present their findings</a> in the journal <i>Science</i>.</p>
<p>Why does this matter? AI-powered tools are now pervasive in everyday life. Banks use them to approve loans. Hospitals use them to analyze medical data, such as X-rays and MRIs. Companies use them to screen job applicants. But it&#39;s currently difficult to understand the mechanism neural networks use to make decisions and the biases in the training data that might impact this.</p>

																																						
																																			<p>&#34;If you don&#39;t understand how neural networks learn, it&#39;s very hard to establish whether neural networks produce reliable, accurate, and appropriate responses,&#34; said Mikhail Belkin, the paper&#39;s corresponding author and a professor at the UC San Diego Halicioglu Data Science Institute. &#34;This is particularly significant given the rapid recent growth of machine learning and neural net technology.&#34;</p>
<p>The study is part of a larger effort in Belkin&#39;s research group to develop a <a href="https://phys.org/tags/mathematical+theory/" rel="tag">mathematical theory</a> that explains how neural networks work. &#34;Technology has outpaced theory by a huge amount,&#34; he said. &#34;We need to catch up.&#34;</p>
<p>The team also showed that the statistical formula they used to understand how neural networks learn, known as Average Gradient Outer Product (AGOP), could be applied to improve performance and efficiency in other types of machine learning architectures that do not include neural networks.</p>
<p>&#34;If we understand the underlying mechanisms that drive neural networks, we should be able to build machine learning models that are simpler, more efficient and more interpretable,&#34; Belkin said. &#34;We hope this will help democratize AI.&#34;</p>
<p>The machine learning systems that Belkin envisions would need less <a href="https://phys.org/tags/computational+power/" rel="tag">computational power</a>, and therefore less power from the grid, to function. These systems also would be less complex and so easier to understand.</p>

																																			<h2>Illustrating the new findings with an example</h2>
<p>(Artificial) neural networks are computational tools to learn relationships between data characteristics (i.e. identifying specific objects or faces in an image). One example of a task is determining whether in a new image a person is wearing glasses or not. Machine learning approaches this problem by providing the neural network many example (training) images labeled as images of &#34;a person wearing glasses&#34; or &#34;a person not wearing glasses.&#34;</p>
<p>The neural network learns the relationship between images and their labels, and extracts data patterns, or features, that it needs to focus on to make a determination. One of the reasons AI systems are considered a <a href="https://phys.org/tags/black+box/" rel="tag">black box</a> is because it is often difficult to describe mathematically what criteria the systems are actually using to make their predictions, including potential biases. The new work provides a simple mathematical explanation for how the systems are learning these features.</p>
<p>Features are relevant patterns in the data. In the example above, there are a wide range of features that the neural networks learns, and then uses, to determine if in fact a person in a photograph is wearing glasses or not.</p>
<p>One feature it would need to pay attention to for this task is the upper part of the face. Other features could be the eye or the nose area where glasses often rest. The network selectively pays attention to the features that it learns are relevant and then discards the other parts of the image, such as the lower part of the face, the hair and so on.</p>

																																						
																																			<p>Feature learning is the ability to recognize relevant patterns in data and then use those patterns to make predictions. In the glasses example, the network learns to pay attention to the upper part of the face. In the new <i>Science</i> paper, the researchers identified a statistical formula that describes how the neural networks are learning features.</p>
<p>Alternative neural network architectures: The researchers went on to show that inserting this formula into computing systems that do not rely on <a href="https://phys.org/tags/neural+networks/" rel="tag">neural networks</a> allowed these systems to learn faster and more efficiently.</p>
<p>&#34;How do I ignore what&#39;s not necessary? Humans are good at this,&#34; said Belkin. &#34;Machines are doing the same thing. Large Language Models, for example, are implementing this &#39;selective paying attention&#39; and we haven&#39;t known how they do it. In our <i>Science</i> paper, we present a mechanism explaining at least some of how the neural nets are &#39;selectively paying attention.&#39;&#34;</p>


																																																					
																				<div>
																						<p><strong>More information:</strong>
												Adityanarayanan Radhakrishnan et al, Mechanism for feature learning in neural networks and backpropagation-free machine learning models, <i>Science</i> (2024). <a data-doi="1" href="https://dx.doi.org/10.1126/science.adi5639" target="_blank">DOI: 10.1126/science.adi5639</a>
																						
																						</p>
																					</div>
                               											
																					
                              										                                        
										<!-- print only -->
										<div>
											 <p><strong>Citation</strong>:
												How do neural networks learn? A mathematical formula explains how they detect relevant patterns (2024, March 12)
												retrieved 19 March 2024
												from https://phys.org/news/2024-03-neural-networks-mathematical-formula-relevant.html
											 </p>
											 <p>
											 This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
											 part may be reproduced without the written permission. The content is provided for information purposes only.
											 </p>
										</div>
                                        
									</div></div>
  </body>
</html>
