<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2211.00241">Original</a>
    <h1>Adversarial Policies Beat Professional-Level Go AIs</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    
      
    
  
    <p><a href="https://arxiv.org/pdf/2211.00241">Download PDF</a></p><blockquote>
      <span>Abstract:</span>  We attack the state-of-the-art Go-playing AI system, KataGo, by training an
adversarial policy that plays against a frozen KataGo victim. Our attack
achieves a &gt;99% win-rate against KataGo without search, and a &gt;50% win-rate
when KataGo uses enough search to be near-superhuman. To the best of our
knowledge, this is the first successful end-to-end attack against a Go AI
playing at the level of a top human professional. Notably, the adversary does
not win by learning to play Go better than KataGo -- in fact, the adversary is
easily beaten by human amateurs. Instead, the adversary wins by tricking KataGo
into ending the game prematurely at a point that is favorable to the adversary.
Our results demonstrate that even professional-level AI systems may harbor
surprising failure modes. See <a href="https://goattack.alignmentfund.org/" rel="external noopener nofollow">this https URL</a> for example
games.

    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Adam Gleave [<a href="https://arxiv.org/show-email/e0b2eec9/2211.00241">view email</a>]
      </p></div></div>
  </body>
</html>
