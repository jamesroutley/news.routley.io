<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://rocm.blogs.amd.com/software-tools-optimization/aiter:-ai-tensor-engine-for-rocm%e2%84%a2/README.html">Original</a>
    <h1>Aiter: AI Tensor Engine for ROCm</h1>
    
    <div id="readability-page-1" class="page"><article>
                   <!---
Copyright (c) 2025 Advanced Micro Devices, Inc. (AMD)

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
--->
<section id="aiter-ai-tensor-engine-for-rocm">



<p><img src="https://rocm.blogs.amd.com/_images/aiter_thumbnail.png" alt="AITER: AI Tensor Engine For ROCm" title="AITER: AI Tensor Engine For ROCm"/> 
</p>
<div>
    <div>
        
        <div>
            <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 16 16">
                <path d="M1 2.828c.885-.37 2.154-.769 3.388-.893 1.33-.134 2.458.063 3.112.752v9.746c-.935-.53-2.12-.603-3.213-.493-1.18.12-2.37.461-3.287.811zm7.5-.141c.654-.689 1.782-.886 3.112-.752 1.234.124 2.503.523 3.388.893v9.923c-.918-.35-2.107-.692-3.287-.81-1.094-.111-2.278-.039-3.213.492zM8 1.783C7.015.936 5.587.81 4.287.94c-1.514.153-3.042.672-3.994 1.105A.5.5 0 0 0 0 2.5v11a.5.5 0 0 0 .707.455c.882-.4 2.303-.881 3.68-1.02 1.409-.142 2.59.087 3.223.877a.5.5 0 0 0 .78 0c.633-.79 1.814-1.019 3.222-.877 1.378.139 2.8.62 3.681 1.02A.5.5 0 0 0 16 13.5v-11a.5.5 0 0 0-.293-.455c-.952-.433-2.48-.952-3.994-1.105C10.413.809 8.985.936 8 1.783"></path>
            </svg>
            <p><span>5 min read. | 1331 total words.</span>
        </p></div>
    </div>
    
</div>  



<!-- AITER: AI Tensor Engine For ROCm | ROCm Blogs -->
<!-- We introduce AMD's AI Tensor Engine for ROCm (AITER), our centralized high performance AI operators repository, designed to significantly accelerate AI workloads on AMD GPUs -->
<p>Performance optimization is critical when working with GPUs, especially for tasks involving artificial intelligence, which can be extremely demanding. To fully leverage the capabilities of advanced hardware, it’s essential to master optimization strategies and ensure every available resource is utilized efficiently. In this blog we will provide an overview of AMD’s AI Tensor Engine for ROCm (AITER) and show you how easy it is to integrate AITER kernels in basic LLM training and inference workload. AITER helps developers to focus on creating operators while allowing customers to seamlessly integrate this operator collection into their own private, public, or any custom framework.</p>
<section id="what-is-ai-tensor-engine-for-rocm-aiter">
<h2>What is AI Tensor Engine for ROCm (AITER)<a href="#what-is-ai-tensor-engine-for-rocm-aiter" title="Link to this heading">#</a></h2>
<p>AMD is introducing the AI Tensor Engine for ROCm (AITER), a centralized repository filled with high-performance AI operators <a href="#id7" id="id1" role="doc-noteref"><span>[</span>1<span>]</span></a> designed to accelerate various AI workloads. AITER serves as a unified platform where customers can easily find and integrate optimized operators into their existing frameworks—be it private, public, or custom-built as you can see in Figure 1 below. With AITER, AMD simplifies the complexity of optimization, enabling users to maximize performance while providing flexibility to meet diverse AI requirements.</p>
<figure id="id13">
<img alt="Scaling performance" src="https://rocm.blogs.amd.com/_images/image5.png"/>
<figcaption>
<p><span>Figure 1: Block level diagram of AITER</span><a href="#id13" title="Link to this image">#</a></p>
</figcaption>
</figure>
<section id="key-features">
<h3>Key Features<a href="#key-features" title="Link to this heading">#</a></h3>
<ul>
<li><p><strong>Versatile and User-Friendly Design:</strong> AITER’s architecture is carefully crafted for versatility and ease of use, allowing seamless integration into various workflows and systems.</p></li>
<li><p><strong>Dual Programming Interfaces:</strong> At the highest abstraction level, AITER supports two primary interfaces—C++ and Python (Torch API). This dual-interface approach makes AITER highly accessible, catering to developers with different programming preferences and skillsets.</p></li>
<li><p><strong>Robust Kernel Infrastructure:</strong> Underneath the user-level APIs, AITER employs a powerful and robust kernel infrastructure. This infrastructure is built upon a variety of underlying technologies, including Triton, CK (Compute Kernel), ASM (Assembly), and HIP (Heterogeneous Interface for Portability).</p></li>
<li><p><strong>Comprehensive Kernel Support:</strong> The AITER kernel ecosystem efficiently supports diverse computational tasks such as inference workloads, training kernels, GEMM (General Matrix Multiplication) operations, and communication kernels. Such comprehensive kernel support ensures that users can confidently handle complex and resource-intensive AI tasks.</p></li>
<li><p><strong>Customizable and Optimizable Kernel Ecosystem:</strong> With its rich kernel environment, AITER allows developers to perform customized optimizations tailored specifically to their applications. This flexibility helps developers to bypass or overcome architectural limitations, resulting in significantly enhanced performance and adaptability.</p></li>
<li><p><strong>Seamless Integration with AMD ROCm:</strong> At its core, AITER leverages AMD’s ROCm, ensuring efficient bridging between optimized kernels and AMD GPUs. This integration unlocks the full potential and peak performance of AMD GPUs, delivering optimal efficiency across a wide range of AI workloads.</p></li>
</ul>
<p>By combining user-friendly interfaces, extensive kernel capabilities, and robust GPU integration, AITER empowers developers to achieve maximum efficiency and performance in their AI applications.</p>
</section>
</section>
<section id="performance-gains-with-aiter">
<h2>Performance Gains with AITER<a href="#performance-gains-with-aiter" title="Link to this heading">#</a></h2>
<p>By leveraging AITER’s advanced optimizations, users can experience significant performance improvements across various AI operations:</p>
<ul>
<li><p><strong>AITER block-scale GEMM:</strong> Achieves up to <strong>2x performance boost</strong> <a href="#id8" id="id2" role="doc-noteref"><span>[</span>2<span>]</span></a>, substantially accelerating general matrix multiplication tasks.</p></li>
<li><p><strong>AITER block-scale fused MoE:</strong> Delivers up to <strong>3x performance boost</strong> <a href="#id9" id="id3" role="doc-noteref"><span>[</span>3<span>]</span></a>, optimizing the efficiency of Mixture of Experts (MoE) operations.</p></li>
<li><p><strong>AITER MLA for decode:</strong> Provides an impressive <strong>up to 17x performance boost</strong><a href="#id10" id="id4" role="doc-noteref"><span>[</span>4<span>]</span></a>, dramatically enhancing decoding efficiency.</p></li>
<li><p><strong>AITER MHA for prefill:</strong> Realizes up to <strong>14x performance boost</strong><a href="#id11" id="id5" role="doc-noteref"><span>[</span>5<span>]</span></a>, significantly improving Multi-Head Attention (MHA) performance during prefill stages.</p></li>
</ul>
</section>
<section id="aiter-s-integration-in-vllm-sglang-for-deepseek-v3-r1">
<h2>AITER’s Integration in vLLM/SGLang for DeepSeek V3/R1<a href="#aiter-s-integration-in-vllm-sglang-for-deepseek-v3-r1" title="Link to this heading">#</a></h2>
<p>The integration of AITER into vLLM/SGLang for the DeepSeek v3/r1 model has led to remarkable improvements in total token throughput (tokens per second, tok/s). Before AITER’s integration, the throughput stood at <strong>6484.76 tok/s</strong>. After incorporating AITER’s optimizations, throughput dramatically increased to <strong>13704.36 tok/s</strong>, marking more than a <strong>2x improvement</strong><a href="#id12" id="id6" role="doc-noteref"><span>[</span>6<span>]</span></a> in processing speed as shown in Figure 2 below.</p>
<figure id="id14">
<img alt="Scaling performance" src="https://rocm.blogs.amd.com/_images/Screenshot_2025-03-18_124343.png"/>
<figcaption>
<p><span>Figure 2.Throughput Comparison: Before and After Integrating AITER in SGLang on DeepSeek Models on AMD Instinct™ MI300X.</span><a href="#id14" title="Link to this image">#</a></p>
</figcaption>
</figure>
<section id="running-deepseek-with-aiter">
<h3>Running Deepseek with AITER<a href="#running-deepseek-with-aiter" title="Link to this heading">#</a></h3>
<p>using vLLM</p>
<div><div><pre><span></span><span>VLLM_SEED</span><span>=</span><span>42</span><span> </span><span>VLLM_MLA_DISABLE</span><span>=</span><span>0</span><span> </span><span>VLLM_USE_TRITON_FLASH_ATTN</span><span>=</span><span>0</span><span> </span><span>\ </span>
<span>VLLM_USE_ROCM_FP8_FLASH_ATTN</span><span>=</span><span>0</span><span> </span><span>VLLM_FP8_PADDING</span><span>=</span><span>1</span><span> </span><span>VLLM_USE_AITER_MOE</span><span>=</span><span>1</span><span> </span><span>\</span>
<span>VLLM_USE_AITER_BLOCK_GEMM</span><span>=</span><span>1</span><span> </span><span>VLLM_USE_AITER_MLA</span><span>=</span><span>0</span><span> </span>vllm<span> </span>serve<span> </span><span>\ </span>
/workspace/models/DeepSeek-R1<span> </span><span>\</span>
<span>  </span>--host<span> </span><span>0</span>.0.0.0<span> </span><span>\</span>
<span>  </span>--port<span> </span><span>8000</span><span> </span><span>\</span>
<span>  </span>--api-key<span> </span>abc-123<span> </span><span>\</span>
<span>  </span>--tensor-parallel-size<span> </span><span>8</span><span> </span><span>\</span>
<span>  </span>--trust-remote-code<span> </span><span>\</span>
<span>  </span>--seed<span> </span><span>42</span>
<span> </span>
</pre></div>
</div>
<p>using SGLang</p>
<div><div><pre><span></span><span>CK_BLOCK_GEMM</span><span>=</span><span>1</span><span> </span><span>SGLANG_ROCM_AITER_BLOCK_MOE</span><span>=</span><span>1</span><span> </span><span>RCCL_MSCCL_ENABLE</span><span>=</span><span>0</span><span> </span><span>\ </span>
<span>DEBUG_HIP_BLOCK_SYN</span><span>=</span><span>1024</span><span> </span><span>GPU_FORCE_BLIT_COPY_SIZE</span><span>=</span><span>64</span><span> </span><span>\</span>
python3<span> </span>-m<span> </span>sglang.launch_server<span>  </span>--model<span> </span>/models/DeepSeek-V3/<span> </span><span>\</span>
<span> </span>--tp<span> </span><span>8</span><span> </span>--trust-remote-code
</pre></div>
</div>
</section>
</section>
<section id="getting-started-with-aiter">
<h2>Getting Started with AITER<a href="#getting-started-with-aiter" title="Link to this heading">#</a></h2>
<p>To begin working with AITER, follow these simple installation steps:</p>
<ol>
<li><p>Clone the repository:</p></li>
</ol>
<div><div><pre><span></span>git<span> </span>clone<span> </span>https://github.com/ROCm/aiter.git
<span>cd</span><span> </span>AITER
</pre></div>
</div>
<ol start="2">
<li><p>Under the AITER root directory, run the following command to install the library in development mode:</p></li>
</ol>

</section>
<section id="implementing-a-simple-linear-layer-using-aiter">
<h2>Implementing a Simple Linear Layer Using AITER<a href="#implementing-a-simple-linear-layer-using-aiter" title="Link to this heading">#</a></h2>
<p>Let’s demonstrate how you can implement a simple replica of PyTorch’s linear layer using AITER’s <code><span>tgemm</span></code> function.</p>
<div><div><pre><span></span><span>from</span> <span>aiter.tuned_gemm</span> <span>import</span> <span>tgemm</span>
<span>import</span> <span>torch</span>

<span>class</span> <span>LinearLayer</span><span>(</span><span>torch</span><span>.</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
	<span>def</span> <span>**</span><span>init</span><span>**</span><span>(</span><span>self</span><span>,</span> <span>in_features</span><span>,</span> <span>out_features</span><span>):</span>
		<span>super</span><span>(</span><span>LinearLayer</span><span>,</span> <span>self</span><span>)</span><span>.**</span><span>init</span><span>**</span><span>()</span>
		<span>self</span><span>.</span><span>weight</span> <span>=</span> <span>torch</span><span>.</span><span>nn</span><span>.</span><span>Parameter</span><span>(</span><span>torch</span><span>.</span><span>randn</span><span>(</span><span>out_features</span><span>,</span> <span>in_features</span><span>)</span><span>.</span><span>cuda</span><span>())</span>
		<span>self</span><span>.</span><span>bias</span> <span>=</span> <span>torch</span><span>.</span><span>nn</span><span>.</span><span>Parameter</span><span>(</span><span>torch</span><span>.</span><span>randn</span><span>(</span><span>out_features</span><span>)</span><span>.</span><span>cuda</span><span>())</span>

	<span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>input</span><span>):</span>
		<span>input</span> <span>=</span> <span>input</span><span>.</span><span>cuda</span><span>()</span>
		<span>return</span> <span>[</span><span>tgemm</span><span>.</span><span>mm</span><span>](</span><span>http</span><span>:</span><span>//</span><span>tgemm</span><span>.</span><span>mm</span><span>/</span><span>)(</span><span>input</span><span>,</span> <span>self</span><span>.</span><span>weight</span><span>,</span> <span>self</span><span>.</span><span>bias</span><span>,</span> <span>None</span><span>,</span> <span>None</span><span>)</span>

<span># Define input size and layer size</span>
<span>in_features</span> <span>=</span> <span>128</span>
<span>out_features</span> <span>=</span> <span>64</span>
<span>batch_size</span> <span>=</span> <span>32</span>

<span># Create custom AITER linear layer</span>
<span>layer</span> <span>=</span> <span>LinearLayer</span><span>(</span><span>in_features</span><span>,</span> <span>out_features</span><span>)</span><span>.</span><span>cuda</span><span>()</span>
<span>input_tensor</span> <span>=</span> <span>torch</span><span>.</span><span>randn</span><span>(</span><span>batch_size</span><span>,</span> <span>in_features</span><span>)</span><span>.</span><span>cuda</span><span>()</span>

<span># Get output from AITER linear layer</span>
<span>output_aiter</span> <span>=</span> <span>layer</span><span>(</span><span>input_tensor</span><span>)</span>

<span># Create PyTorch linear layer with same weights and bias</span>
<span>pytorch_layer</span> <span>=</span> <span>torch</span><span>.</span><span>nn</span><span>.</span><span>Linear</span><span>(</span><span>in_features</span><span>,</span> <span>out_features</span><span>)</span><span>.</span><span>cuda</span><span>()</span>
<span>pytorch_layer</span><span>.</span><span>weight</span> <span>=</span> <span>torch</span><span>.</span><span>nn</span><span>.</span><span>Parameter</span><span>(</span><span>layer</span><span>.</span><span>weight</span><span>.</span><span>clone</span><span>())</span>
<span>pytorch_layer</span><span>.</span><span>bias</span> <span>=</span> <span>torch</span><span>.</span><span>nn</span><span>.</span><span>Parameter</span><span>(</span><span>layer</span><span>.</span><span>bias</span><span>.</span><span>clone</span><span>())</span>

<span># Get output from PyTorch linear layer</span>
<span>output_pytorch</span> <span>=</span> <span>pytorch_layer</span><span>(</span><span>input_tensor</span><span>)</span>

<span># Compare outputs</span>
<span>print</span><span>(</span><span>&#34;Output difference (max absolute error):&#34;</span><span>,</span> <span>torch</span><span>.</span><span>max</span><span>(</span><span>torch</span><span>.</span><span>abs</span><span>(</span><span>output_aiter</span> <span>-</span> <span>output_pytorch</span><span>)))</span>
<span>print</span><span>(</span><span>&#34;Output difference (mean absolute error):&#34;</span><span>,</span> <span>torch</span><span>.</span><span>mean</span><span>(</span><span>torch</span><span>.</span><span>abs</span><span>(</span><span>output_aiter</span> <span>-</span> <span>output_pytorch</span><span>)))</span>
</pre></div>
</div>
<p>It can be very simple using AITER in daily workload, some of the other low level kernel APIs are mentioned as below which can be used to integrate in your architecture.</p>
<table>
<thead>
<tr><th><p><strong>Kernel</strong></p></th>
<th><p><strong>API</strong></p></th>
</tr>
</thead>
<tbody>
<tr><td><p>MHA (Flash Attention)</p></td>
<td><p><code><span>aiter.flash_attn_func()</span></code></p></td>
</tr>
<tr><td><p>LayerNorm</p></td>
<td><p><code><span>aiter.layer_norm()</span></code></p></td>
</tr>
<tr><td><p>LayerNormFusedResidualAdd</p></td>
<td><p><code><span>aiter.layernorm2d_with_add_asm()</span></code></p></td>
</tr>
<tr><td><p>RoPE forward</p></td>
<td><p><code><span>aiter.rope_fwd()</span></code></p></td>
</tr>
<tr><td><p>RoPE backward</p></td>
<td><p><code><span>aiter.rope_bwd()</span></code></p></td>
</tr>
<tr><td><p>RMSNorm</p></td>
<td><p><code><span>aiter.rms_norm()</span></code></p></td>
</tr>
<tr><td><p>MLA Decode</p></td>
<td><p><code><span>aiter.ops.triton.mla_decode()</span></code></p></td>
</tr>
</tbody>
</table>
<p>AITER is not just limited to the above mentioned APIs, there are a lot of features available as mentioned in the below table and a lot are coming very soon.</p>
<table>
<thead>
<tr><th><p><strong>Feature</strong></p></th>
<th><p><strong>Type (F=Forward, B=Backward)</strong></p></th>
<th><p><strong>Details</strong></p></th>
</tr>
</thead>
<tbody>
<tr><td><p><strong>Prefill Attention</strong></p></td>
<td><p>F/B</p></td>
<td><p>Fav3 FWD FP16/BF16 </p></td>
</tr>
<tr><td><p><strong>Decode Attention</strong></p></td>
<td><p>F</p></td>
<td><p>Paged Attention FP16/BF16 </p></td>
</tr>
<tr><td><p><strong>Fused-Moe</strong></p></td>
<td><p>F</p></td>
<td><p>Moe-Sorting kernel and tiling solution </p></td>
</tr>
<tr><td><p><strong>Low Precision Gemm</strong></p></td>
<td><p>F</p></td>
<td><p>FP8 per-token/channel <strong>Gemm</strong> </p></td>
</tr>
<tr><td><p><strong>Distributed Gemm</strong></p></td>
<td><p>F/B</p></td>
<td><p>Distributed <strong>GEMM</strong></p></td>
</tr>
<tr><td><p><strong>Normalization and Fusion</strong></p></td>
<td><p>F</p></td>
<td><p><strong>Layernorm+quant/shortcut</strong> </p></td>
</tr>
<tr><td><p><strong>Custom Comm.</strong></p></td>
<td><p>F</p></td>
<td><p>AR/AG fused with normalization </p></td>
</tr>
<tr><td><p><strong>Conv2d/2d</strong></p></td>
<td><p>F/B</p></td>
<td><p>FP16/BF16 <strong>fwd/bwd/wrw</strong> </p></td>
</tr>
</tbody>
</table>
</section>
<section id="summary">
<h2>Summary<a href="#summary" title="Link to this heading">#</a></h2>
<p>In this blog we introduced AMD’s AI Tensor Engine for ROCm (AITER), our centralized high performance AI operators repository, designed to significantly accelerate AI workloads on AMD GPUs. AITER has already demonstrated its value by substantially accelerating AI workloads and significantly improving efficiency and performance. AMD remains committed to continuous innovation, with numerous further enhancements and optimization efforts currently underway. The roadmap includes even greater advancements, which promise to set new standards in AI computation. Stay tuned as AMD continues to push the boundaries of performance, ensuring machine learning engineers can consistently achieve faster, more efficient, and more powerful AI solutions.</p>
</section>
<section id="additional-resources">
<h2>Additional Resources<a href="#additional-resources" title="Link to this heading">#</a></h2>
<p>AITER Github: <a href="https://github.com/ROCm/aiter">ROCm/aiter</a></p>
</section>
<section id="disclaimers">
<h2>Disclaimers<a href="#disclaimers" title="Link to this heading">#</a></h2>
<p>Third-party content is licensed to you directly by the third party that owns the
content and is not licensed to you by AMD. ALL LINKED THIRD-PARTY CONTENT IS
PROVIDED “AS IS” WITHOUT A WARRANTY OF ANY KIND. USE OF SUCH THIRD-PARTY CONTENT
IS DONE AT YOUR SOLE DISCRETION AND UNDER NO CIRCUMSTANCES WILL AMD BE LIABLE TO
YOU FOR ANY THIRD-PARTY CONTENT. YOU ASSUME ALL RISK AND ARE SOLELY RESPONSIBLE
FOR ANY DAMAGES THAT MAY ARISE FROM YOUR USE OF THIRD-PARTY CONTENT.</p>



</section>
</section>



                </article></div>
  </body>
</html>
