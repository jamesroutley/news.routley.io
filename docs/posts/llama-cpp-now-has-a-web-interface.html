<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/ggerganov/llama.cpp/pull/1998">Original</a>
    <h1>LLama.cpp now has a web interface</h1>
    
    <div id="readability-page-1" class="page"><div>
        <div>
  
  <task-lists disabled="" sortable="">
    <div>
      <p dir="auto">I put together a simple web-chat that demonstrates how to use the SSE(ish) streaming in the server example. I also went ahead and served it from the root url, to make the server a bit more approachable.</p>
<p dir="auto">I tried to match the spirit of llama.cpp and used minimalistic js dependencies and went with the ozempic css style of ggml.ai.</p>
<p dir="auto">Initially I went for no-js dependencies but gave up and used a few minimal that i&#39;m importing from js cdns instead of adding them here. Let me know if you agree with this approach. I needed microsoft&#39;s fetch-event-source for using event-source over POST (super disappointed that browsers don&#39;t support that, actually) and preact+htm for keeping my sanity with all this state,. The upshot is that everything is in one small html file. Speaking of- there is probably a better (and less fragile) way to include the server.html in the cpp binary, but it&#39;s been 25 years since I worked with cpp tooling.</p>
<p dir="auto">(updated screenshot)</p>
    </div>
  </task-lists>
  
</div>

      </div></div>
  </body>
</html>
