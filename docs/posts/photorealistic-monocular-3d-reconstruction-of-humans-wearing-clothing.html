<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://phorhum.github.io/">Original</a>
    <h1>Photorealistic Monocular 3D Reconstruction of Humans Wearing Clothing</h1>
    
    <div id="readability-page-1" class="page">


<section>
  <div>
    <div>
      <div>
        <div>
          
          <div>
            <p>
            <span>
              <a href="https://scholar.google.com/citations?user=af68sKkAAAAJ" target="_blank">Mihai Zanfir</a>,
            </span>
            <span>
              <a href="https://scholar.google.com/citations?user=LHTI1W8AAAAJ" target="_blank">Cristian Sminchisescu</a>
            </span>
          </p></div>

          <p><span>Google Research</span>
          </p>

          <div>
            <div>
              <!-- PDF Link. -->
              <p><span>
                <a href="https://phorhum.github.io/static/assets/alldieck2022phorhum.pdf" target="_blank">
                  <span>
                      <i></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span>
                <a href="http://arxiv.org/abs/2204.08906" target="_blank">
                  <span>
                      <i></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span></p><!-- Video Link. -->
              <p><span>
                <a href="https://www.youtube.com/watch?v=mU16oKt_U_k" target="_blank">
                  <span>
                      <i></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
            </p></div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section>
  <div>
    <figure>
      <img src="https://phorhum.github.io/static/images/teaser.png"/>
    </figure>
    <p>Given a single image, we reconstruct the full 3D geometry
      – including self-occluded (or unseen) regions – of the photographed person, together with
      albedo and shaded surface color.
      Our end-to-end trainable pipeline requires no image matting and
      reconstructs all outputs in a single step.</p>
  </div>
</section>


<section>
  <div>
    <!-- Abstract. -->
    <div>
      <div>
        <h2>Abstract</h2>
        <p>
            We present PHORHUM, a novel, end-to-end trainable, deep neural network methodology for
            photorealistic 3D human reconstruction given just a monocular RGB image. Our
            pixel-aligned method estimates detailed 3D geometry and, for the first time, the
            unshaded surface color together with the scene illumination. Observing that 3D
            supervision alone is not sufficient for high fidelity color reconstruction, we introduce
            patch-based rendering losses that enable reliable color reconstruction on visible parts
            of the human, and detailed and plausible color estimation for the non-visible parts.
            Moreover, our method specifically addresses methodological and practical limitations of
            prior work in terms of representing geometry, albedo, and illumination effects, in an
            end-to-end model where factors can be effectively disentangled. In extensive
            experiments, we demonstrate the versatility and robustness of our approach. Our
            state-of-the-art results validate the method qualitatively and for different metrics,
            for both geometric and color reconstruction.
          </p>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section>
  <div>
    <h2>Method</h2>
    <figure>
      <div>
        <div>
          <p><img src="https://phorhum.github.io/static/images/method.png"/></p><p>Overview of our method. The feature extractor network
            \(G\) produces pixel-aligned features \(\boldsymbol{z}_{\boldsymbol{x}}\) from an input image
            \(\mathbf{I}\) for all points in space \(\boldsymbol{x}\). The implicit signed distance function network
            \(f\) computes the distance \(d\) to the closest surface given a point and its feature.
            Additionally \(f\) returns albedo colors \(\boldsymbol{a}\) defined for surface points. The
            shading network \(s\) predicts the shading for surface points given its surface
            normal \(\boldsymbol{n}_{\boldsymbol{x}}\), as well as illumination \(\boldsymbol{l}\).
            On the right we show the reconstruction of geometry and albedo colors, and the shaded 3D geometry.</p>
        </div>
      </div>
    </figure>
  </div>
</section>
<section>
  <div>
    <h2>Video</h2>
    <!-- Paper video. -->
    <div>
      <div>
        <p>
          <iframe src="https://www.youtube.com/embed/mU16oKt_U_k?rel=0&amp;showinfo=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>
        </p>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>
<section>
  <div>
    <h2>Results</h2>
    <div>
      <div id="results-carousel">
        <div>
          <figure>
            <img src="https://phorhum.github.io/static/images/video_sample_000.jpg"/>
          </figure>
        </div>
        
        
        <div>
          <figure>
            <img src="https://phorhum.github.io/static/images/video_sample_001.jpg"/>
          </figure>
        </div>
        
        
        <div>
          <figure>
            <img src="https://phorhum.github.io/static/images/video_sample_002.png"/>
          </figure>
        </div>
        
        
        <div>
          <figure>
            <img src="https://phorhum.github.io/static/images/video_sample_003.jpg"/>
          </figure>
        </div>
        
        
        <div>
          <figure>
            <img src="https://phorhum.github.io/static/images/video_sample_004.png"/>
          </figure>
        </div>
        
        
        <div>
          <figure>
            <img src="https://phorhum.github.io/static/images/video_sample_005.jpg"/>
          </figure>
        </div>
        
        
      </div>
    </div>
  </div>
  
</section>
<section>
  <div>
    <h2>3D Viewer</h2>
    <div>
      <div>
        <figure>
          <img src="https://phorhum.github.io/static/images/sample_00.jpg"/>
        </figure>
      </div>
      <div>
        
      </div>
      <div>
        <figure>
          <img src="https://phorhum.github.io/static/images/sample_01.jpg"/>
        </figure>
      </div>
      <div>
        
      </div>
    </div>
  </div>
</section>
<section id="BibTeX">
  <div>
    <h2>BibTeX</h2>
    <pre><code>@inproceedings{alldieck2022phorhum,
  title	  = {Photorealistic Monocular 3D Reconstruction of Humans Wearing Clothing},
  author  = {Thiemo Alldieck and Mihai Zanfir and Cristian Sminchisescu},
  year	  = {2022},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}
}</code></pre>
  </div>
</section>






</div>
  </body>
</html>
