<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://quickwit.io/blog/quickwit-binance-story">Original</a>
    <h1>Binance built a 100PB log service with Quickwit</h1>
    
    <div id="readability-page-1" class="page"><div><main><center><img loading="lazy" src="https://quickwit.io/img/blog/2024-06-20-scaling-log-search-to-100-pb.png" alt="How Binance built a 100 PB log service with Quickwit"/></center><p>Three years ago, we <a href="https://quickwit.io/blog/quickwit-first-release">open-sourced</a> Quickwit, a distributed search engine for large-scale datasets. Our goal was ambitious: to create a new breed of full-text search engine that is ten times more cost-efficient than Elasticsearch, significantly easier to configure and manage, and capable of scaling to petabytes of data <sup id="fnref-1-609909"><a href="#fn-1-609909">1</a></sup>.</p><p>While we knew the potential of Quickwit, our tests typically did not exceed 100 TB of data and 1 GB/s of indexing throughput. We lacked the real-world datasets and computing resources to test Quickwit at a multi-petabyte scale.</p><p>That is, until six months ago, when two engineers at Binance, the world&#39;s leading cryptocurrency exchange, discovered Quickwit and began experimenting with it. Within a few months, they achieved what we had only dreamed of: they successfully migrated multiple petabyte-scale Elasticsearch clusters to Quickwit, with remarkable achievements including:</p><ul><li>Scaling indexing to <strong>1.6 PB per day</strong>.</li><li>Operating a search cluster handling <strong>100 PB of logs</strong>.</li><li><strong>Saving millions of dollars</strong> annually by <strong>slashing compute costs by 80%</strong> and storage costs by 20x (for the same retention period).</li></ul><div><div><table><tbody><tr><th>Dataset</th><th></th></tr><tr><td>Uncompressed Size</td><td>100 PB</td></tr><tr><td>Size on S3 (compressed)</td><td>20 PB</td></tr><tr><td>Num documents</td><td>181 trillions</td></tr></tbody></table></div><div><table><tbody><tr><th>Indexing deployment</th><th></th></tr><tr><td>Indexing throughput</td><td>1.6 PB / day</td></tr><tr><td>Num pods</td><td>700</td></tr><tr><td>Num vCPUs</td><td><span>4000</span><span> 2800*</span></td></tr><tr><td>RAM</td><td><span>6</span><span> 5.6*</span> TB</td></tr></tbody></table></div></div><div><div><table><tbody><tr><th>Search deployment</th><th></th></tr><tr><td>Searchable dataset size</td><td>100 PB</td></tr><tr><td>Num pods</td><td>30</td></tr><tr><td>Num vCPUs</td><td>1200</td></tr><tr><td>RAM</td><td>3 TB</td></tr></tbody></table></div><div><table><tbody><tr><th>Infra costs vs. Elasticsearch</th><th></th></tr><tr><td>Compute costs</td><td>5x reduction</td></tr><tr><td>Storage costs </td><td>20x reduction</td></tr><tr></tr></tbody></table></div></div><small>*First figures initially published were wrong :/</small><p>In this blog post, I will share with you how Binance built a petabyte-scale log service and overcame the challenges of scaling Quickwit to multi-petabytes.</p><p>As the world&#39;s leading cryptocurrency exchange, Binance handles an enormous volume of transactions, each generating logs that are crucial for security, compliance, and operational insights. This results in processing roughly 21 million log lines per second, equivalent to 18.5 GB/s, or 1.6 PB per day.</p><p>To manage such a volume, Binance previously relied on 20 Elasticsearch clusters. Around 600 Vector pods were pulling logs from different Kafka topics and processing them before pushing them into Elasticsearch.</p><p><img loading="lazy" alt="Binance ES setup" src="https://quickwit.io/assets/images/2024-06-20-binance-setup-kafka-vector-es-bc7ee32a83e8aae14aa3ec088d918244.png" width="1889" height="856"/></p><p>However, this setup fell short of Binance&#39;s requirements in several critical areas:</p><ul><li><strong>Operational Complexity</strong>: Managing numerous Elasticsearch clusters was becoming increasingly challenging and time-consuming.</li><li><strong>Limited Retention</strong>: Binance was retaining most logs for only a few days. Their goal was to extend this to months, requiring the storage and management of 100 PB of logs, which was prohibitively expensive and complex with their Elasticsearch setup.</li><li><strong>Limited Reliability</strong>: Elasticsearch clusters with high ingestion throughput were configured without replication to limit infrastructure costs, compromising durability and availability.</li></ul><p>The team knew they needed a radical change to meet their growing needs for log management, retention, and analysis.</p><p>When Binance&#39;s engineers discovered Quickwit, they quickly realized it offered several key advantages over their existing setup:</p><ul><li><strong>Native Kafka integration</strong>: It allows ingesting logs directly from Kafka with exactly-once semantics, providing huge operational benefits. Concretely speaking, you can tear down your cluster, recreate it in a minute without losing any data, ready to ingest at 1.6 PB/day or search through petabytes, and scale up and down to handle temporary spikes.</li><li><strong>Built-in VRL transformations</strong> (Vector Remap Language): As Quickwit supports VRL, it eliminates the need for hundreds of Vector pods to handle log transformations.</li><li><strong>Object storage as the primary storage</strong>: All indexed data remains on object storage, removing the need for provisioning and managing storage on the cluster side.</li><li><strong>Better data compression</strong>: Quickwit typically achieves 2x better compression than Elasticsearch, further reducing the storage footprint of indexes.</li></ul><p>However, no users had scaled Quickwit to multi-petabytes, and any engineer knows that scaling a system by a factor of 10 or 100 can reveal unexpected issues. This did not stop them, and they were ready to take on the challenge!</p><center><img loading="lazy" src="https://quickwit.io/img/blog/2024-06-20-challenge-accepted.jpg" alt="Searching 100 PB, Challenge accepted" width="400"/></center><p>Binance rapidly scaled its indexing thanks to the Kafka datasource. One month into their Quickwit PoC, they were indexing at several GB/s.</p><p>This quick progress was largely due to how Quickwit works with Kafka: Quickwit uses Kafka&#39;s consumer groups to distribute the workload across multiple pods. Each pod indexes a subset of the Kafka partitions and updates the metastore with the latest offsets, ensuring exactly-once semantics. This setup makes Quickwit&#39;s indexers stateless: you can tear down your entire cluster and restart it, and the indexers will resume from where they left off as if nothing happened.</p><p>However, Binance&#39;s scale revealed two main issues:</p><ul><li><strong>Cluster Stability Issues</strong>: A few months ago, Quickwit’s gossip protocol (called <a href="https://github.com/quickwit-oss/chitchat" target="_blank" rel="noopener noreferrer">Chitchat</a>) struggled with hundreds of pods: some indexers would leave the cluster and rejoin, making the indexing throughput unstable.</li><li><strong>Uneven Workload Distribution</strong>: Binance uses several Quickwit indexes for their logs, with varying indexing throughputs. Some have a high throughput of several GB/s, others just a few MB/s. Quickwit&#39;s placement algorithm does not spread its workload evenly. This is a known <a href="https://github.com/quickwit-oss/quickwit/issues/4630" target="_blank" rel="noopener noreferrer">issue</a>, and we will work on this later this year.</li></ul><p>To work around these limitations, Binance deployed separate indexing clusters for each high-throughput topic, keeping one cluster for smaller topics. Isolating each high-throughput cluster did not impose an operational burden thanks to stateless indexers. Additionally, all Vector pods were removed as Binance used Vector transformation directly in Quickwit.</p><p><img loading="lazy" alt="Binance Quickwit setup" src="https://quickwit.io/assets/images/2024-06-20-binance-setup-kafka-quickwit-5326a36908ce42113ac90fccaf3799e6.png" width="2110" height="1183"/></p><p>After several months of migration and optimization, Binance finally achieved an indexing throughput of 1.6 PB with 10 Quickwit indexing clusters, 700 pods requesting around 2800 vCPU and 6 TB of memory, that&#39;s 6.6 MB/s per vCPU on average. On a given high-throughput Kafka topic, this figure goes up to 11 MB/s per vCPU.</p><p>Next challenge to come: scaling search!</p><p>With Quickwit now capable of efficiently indexing 1.6 PB daily, the challenge shifted to searching through petabytes of logs. With 10 clusters, Binance would normally need to deploy searcher pods for each cluster, undermining one of Quickwit’s strengths: pooling searcher resources to hit the object storage shared by all indexes.</p><p>To avoid this pitfall, Binance&#39;s engineers devised a clever workaround: they created a unified metastore by replicating all metadata from each indexing cluster metastore into one PostgreSQL database. This unified metastore enables the deployment of one unique centralized search cluster capable of searching through all indexes!</p><p><img loading="lazy" alt="Quickwit multi clusters setup" src="https://quickwit.io/assets/images/2024-06-20-quickwit-multi-indexers-clusters-setup-f83edb0ef91fcc35680e31469c451eb1.png" width="1966" height="1098"/></p><p>As we speak, Binance now manages a reasonably sized cluster of 30 searcher pods, each requesting 40 vCPU and 100GB memory. To give you an idea, you only need 5 searchers (8 vCPU, 6GB memory requests) to find the needle in the haystack in 400 TB of logs. Binance runs those types of queries on petabytes but also aggregation queries, hence the higher resource requests.</p><p>Overall, Binance&#39;s migration to Quickwit was a huge success and brought several substantial benefits:</p><ul><li>80% reduction in computing resources compared to Elasticsearch.</li><li>Storage costs reduced by a factor of 20 for the same retention period.</li><li>Economically viable solution for large-scale log management, both in terms of infrastructure costs and maintenance operations.</li><li>Minimal configuration tweaking, working efficiently once the right number of pods and resources were determined.</li><li>Increased log retention to one or several months, depending on the log type, improving internal troubleshooting capabilities.</li></ul><p>In conclusion, Binance’s migration from Elasticsearch to Quickwit has been an exciting 6-month experience between Binance and Quickwit engineers, and we are very proud of this collaboration. We have already planned improvements in data compression, multi-cluster support, and better workload spread with Kafka datasources.</p><p>Huge kudos to Binance&#39;s engineers for their work and insights throughout this migration &lt;3</p></main></div></div>
  </body>
</html>
