<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://xania.org/202506/how-compiler-explorer-works">Original</a>
    <h1>How Compiler Explorer Works in 2025</h1>
    
    <div id="readability-page-1" class="page"><div>
    <div>
        

        <p>Written with LLM assistance.</p>

<p>Ever wondered what happens when you hit that compile button on <a href="https://godbolt.org">Compiler Explorer</a>? Last week I was looking at our metrics dashboard, and then I realised: we now do <strong>92 million compilations a year</strong>. That’s 1.8 million a week…for a site that started as a hacky way to justify to my boss that it would be OK to turn on C++17 features<sup id="fnref:jord"><a href="#fn:jord">1</a></sup>, that’s not bad.</p>
<p>I thought it might be fun to share how this all works. Not just a high-level summary, but some of the details of keeping 3000+ different compiler versions<sup id="fnref:1"><a href="#fn:1">2</a></sup> running smoothly across 81 programming languages. It’s a lot more warty than you might imagine (and than I’d like).</p>
<h3>What actually happens when you stop typing</h3>
<p>When you type some code into Compiler Explorer, here’s what actually happens (for a simple x86-based compilation):</p>
<ol>
<li>You type into the <a href="https://github.com/microsoft/monaco-editor">Monaco editor</a> (the same one VS Code uses)</li>
<li>Your code wings its way via <a href="https://aws.amazon.com/cloudfront/">CloudFront</a><sup id="fnref:rate"><a href="#fn:rate">5</a></sup> and a load balancer</li>
<li>The load balancer determines which cluster this request is for, and picks a healthy instance to send it to<sup id="fnref:healthy"><a href="#fn:healthy">4</a></sup></li>
<li>That server queues your compilation request (currently up to 2 concurrent compilations per instance)</li>
<li>Here’s where it gets interesting: <strong>nsjail</strong> creates a tiny prison for your code</li>
<li>The compiler is run with appropriate language- and compiler-dependent flags in this sandbox, generates output</li>
<li>Results get filtered (removing the boring bits), source lines are attributed, and it’s all sent back as a JSON response<sup id="fnref:api"><a href="#fn:api">3</a></sup></li>
<li>Your browser renders the assembly, and you go “ooh, how clever is <em>this</em> compiler!”</li>
</ol>
<p>
<a href="https://xania.org/202506/compiler-explorer-architecture.svg">
<img src="https://xania.org/202506/compiler-explorer-architecture.svg" width="800" height="500" alt="Compiler Explorer architecture diagram showing request flow"/>
</a>
</p>

<p>As load comes and goes, we scale up the number of instances in each cluster: so if we get a flurry of Windows compiler requests, then within a few minutes our fleet should have scaled up to meet the demand. We keep it simple: we just try and keep the average CPU load below a threshold. We’ve kicked around more sophisticated ideas but this is simple and supported out-of-the-box in AWS.</p>
<h3>Not getting hacked by random people on the internet</h3>
<p>We let random people run arbitrary code on our machine, which seems like a terrible mistake. GitHub’s own “security analyser” keeps flagging this during PRs, in fact. That used to keep me up at night, and for good reason. We’ve had some proper “oh no” moments over the years.</p>
<p>Back when we only had basic <a href="https://www.docker.com/">Docker</a> isolation, someone found a way to crash Clang in just the right way that it left a built <code>.o</code> file in a temporary location - and helpfully printed out the path. A subsequent request could then load that file with <code>-fplugin=/tmp/that/place.o</code>, giving them arbitrary code execution in the compiler. Not ideal.</p>
<p>More recently, we had a clever attack involving <a href="https://cmake.org/">CMake</a> rules that would symbolically link outputs to sensitive files like <code>/etc/passwd</code> in the output directory as <code>example.s</code>. The CMake runs in our strict nsjail, BUT the code that processes compiler output runs under the web server’s account. So our web server would dutifully read the “file” (unaware it’s a symlink) and send back the contents. Oops<sup id="fnref:2"><a href="#fn:2">6</a></sup>. We now run everything on mounts with <a href="https://github.com/compiler-explorer/infra/blob/99eb606518cd11aafe9ef5936a44f841de93ebb0/start-support.sh#L106-L111"><code>nosymfollow</code></a> (no symbolic link following) which should protect us going forward.</p>
<p>Enter <a href="https://github.com/google/nsjail">nsjail</a>, Google’s lightweight process isolation tool that’s basically a paranoid security guard for processes.</p>
<p>We configure nsjail with two personalities:</p>
<ul>
<li><code>etc/nsjail/execute.cfg</code> - for actually running your compiled programs</li>
<li><code>etc/nsjail/sandbox.cfg</code> - for the compilation process itself</li>
</ul>
<p>It gives us:</p>
<ul>
<li><a href="https://man7.org/linux/man-pages/man7/namespaces.7.html">Linux namespaces</a> (all of them - UTS, MOUNT, PID, IPC, NET, USER, CGROUPS)<sup id="fnref:namespaces"><a href="#fn:namespaces">7</a></sup></li>
<li>Resource limits (file handles, memory, and a 20-second timeout because infinite loops are only fun in theory)<sup id="fnref:timeout"><a href="#fn:timeout">8</a></sup></li>
<li>Filesystem isolation (your code can’t read sensitive files, sorry attackers)</li>
</ul>
<p>This paranoid approach means we can now actually <em>run</em> your programs, not just compile them. That’s a massive improvement from the early days when we’d just show you assembly and hope you could imagine what it did<sup id="fnref:3"><a href="#fn:3">9</a></sup>.</p>
<h3>Yes, we really do have 4TB of compilers</h3>
<p>How do you manage nearly 4TB of compilers? Very carefully, and with a lot of Python and shell scripts. The requests we get for new compilers range from sensible to wonderfully niche. One that sticks out is supporting the oldest compilable C version: GCC 1.27 (yes, from 1987). Another ongoing quest is adding the “cfront” compiler, specifically the one used on early Acorn computers for the then-nascent ARM architecture<sup id="fnref:4"><a href="#fn:4">10</a></sup>.</p>
<p>One of our core principles is that we never retire compiler versions. Once a compiler goes up on Compiler Explorer, it stays there forever. This might seem excessive, but it means that URLs linking to specific compiler versions will always work. Ten years from now, that Stack Overflow answer showing a GCC 4.8 bug will still compile on CE exactly as it did when posted. It’s our small contribution to fighting link rot<sup id="fnref:rot"><a href="#fn:rot">11</a></sup>, even if it means hoarding terabytes of compilers.</p>
<p>We’ve got two main tools for managing this madness:</p>
<ul>
<li><strong>bin/ce_install</strong> - This installs compilers and libraries:<ul>
<li>Installs compilers to <code>/opt/compiler-explorer</code> from a variety of sources (mostly our own builds, stored on S3)</li>
<li>Handles everything from bleeding-edge trunk builds to that one version of GCC from 2006 someone definitely needs</li>
<li>Builds squashfs images for stable compilers</li>
</ul>
</li>
<li><strong>bin/ce</strong> - This handles deployments:<ul>
<li>Sets which versions are deployed to which environment (x86, arm, windows production, staging, beta)</li>
<li>Manages environment lifecycles</li>
<li>Handles rolling updates</li>
</ul>
</li>
</ul>
<h3>Why squashfs saved our bacon</h3>
<p>One of the issues with having so many compilers is that we can’t install them individually on each machine: the VMs don’t work well with such huge disk images. Fairly early on we had to accept that the compilers needed to live on some kind of shared storage, and Amazon has the <a href="https://aws.amazon.com/efs/">Elastic File System (EFS)</a> which is their “infinitely sized” NFS system:</p>
<div><pre><span></span><code>admin-node~<span> </span>$<span> </span>df<span> </span>-h
Filesystem<span>           </span>Size<span>  </span>Used<span> </span>Avail<span> </span>Use%<span> </span>Mounted<span> </span>on
/dev/nvme0n1p1<span>        </span>24G<span>   </span>18G<span>  </span><span>6</span>.0G<span>  </span><span>75</span>%<span> </span>/
tmpfs<span>                </span>969M<span>     </span><span>0</span><span>  </span>969M<span>   </span><span>0</span>%<span> </span>/dev/shm
tmpfs<span>                </span><span>5</span>.0M<span>     </span><span>0</span><span>  </span><span>5</span>.0M<span>   </span><span>0</span>%<span> </span>/run/lock
efs.amazonaws.com:/<span>  </span><span>8</span>.0E<span>  </span><span>3</span>.9T<span>  </span><span>8</span>.0E<span>   </span><span>1</span>%<span> </span>/efs
</code></pre></div>

<p>OK, apparently not infinite, but 8 exabytes ought to be enough for anyone, right?</p>
<p>The issue with any network file system is latency. And that latency adds up quickly with compilation: C-like languages love to include tons of tiny little files. In fact, we used to have issues compiling <em>any</em> <a href="https://www.boost.org/">Boost</a> code as we would time out while the preprocessor was still running. Our initial solution was to have Boost <code>rsync</code> onto the machine locally at boot-up, but as we supported more and more compilers and libraries, that wouldn’t scale.</p>
<p>So in 2020, we came up with a decent(ish) hack: <a href="https://github.com/compiler-explorer/infra/issues/445">we built squashfs</a> images for all our major compilers, and then mounted them “over the top” of NFS. The images themselves we <em>also</em> stored on NFS, so this seems like a pointless thing to do but it works pretty well!<sup id="fnref:squashfs"><a href="#fn:squashfs">12</a></sup></p>
<h3>Building fresh compilers every night</h3>
<p>Perhaps a surprising thing we do: we build and install many compilers every single day. We use the excellent <a href="https://github.com/github-aws-runners/terraform-aws-github-runner">terraform-aws-github-runner</a> project to configure our AWS-based GitHub Actions runners<sup id="fnref:ghr"><a href="#fn:ghr">13</a></sup>, but the Docker infrastructure and compiler orchestration is all our own creation built on top.</p>
<p>The magic happens across several GitHub repos:</p>
<ul>
<li><strong><a href="https://github.com/compiler-explorer/compiler-workflows">compiler-workflows</a></strong> - The orchestration system with a <code>compilers.yaml</code> file that drives the daily builds</li>
<li><strong><a href="https://github.com/compiler-explorer/gcc-builder">gcc-builder</a></strong> - Docker image to build GCC variants including trunk and experimental branches</li>
<li><strong><a href="https://github.com/compiler-explorer/clang-builder">clang-builder</a></strong> - Similarly, for clang</li>
<li><strong><a href="https://github.com/compiler-explorer/misc-builder">misc-builder</a></strong> - For the weird and wonderful compilers that don’t fit elsewhere</li>
<li>We have some other <a href="https://github.com/compiler-explorer/?q=builder&amp;type=all&amp;language=&amp;sort=">”*-builder”s</a> for things like COBOL and .NET too</li>
</ul>
<p>Every night, our GitHub Actions spin up and build:</p>
<ul>
<li>GCC trunk</li>
<li>Clang trunk</li>
<li>Many experimental branches (reflections, contracts, coroutines - all the fun stuff)</li>
<li>Some other languages’ nightly builds</li>
</ul>
<p>The timing of all this is… well, let’s call it “organic”. GitHub Actions are scheduled to build at midnight UTC, but they queue up on our limited build infrastructure. There’s a separate workflow that does the install at 5:30am. There’s currently zero synchronisation between these two systems, which means sometimes we’re installing yesterday’s builds, sometimes today’s, and occasionally we get lucky and everything lines up. It’s on the TODO list to fix, right after the other 900+ items.</p>
<p>It’s like Christmas every morning, except instead of presents, we get fresh compiler builds. You can see our build status on <a href="https://github.com/compiler-explorer/compiler-workflows/blob/main/build-status.md">GitHub</a>.</p>
<p>
<a href="https://xania.org/202506/compiler-wall-dynamic.svg">
<img src="https://xania.org/202506/compiler-wall-dynamic.svg" width="800" height="600" alt="Visualisation showing 4,724 compilers across 81 languages"/>
</a>
</p>

<h3>Making it work on Windows, ARM, and GPUs too</h3>
<p>Gone are the days when “Linux x86-64” was good enough for everyone. Now we support:</p>
<ul>
<li><strong>Windows</strong>: At least 2 spot instances running MSVC and friends. Getting Windows compilers to play nicely with our Linux-centric infrastructure has been an adventure. We’re <a href="https://xania.org/202407/msvc-on-ce">still learning</a> how best to integrate Windows - none of the core team are Windows security experts, but we’re getting there.</li>
<li><strong>ARM machines</strong>: Native ARM64 execution for the growing number of ARM-based systems out there.</li>
<li><strong>GPU Instances</strong>: 2 instances with actual NVIDIA GPUs. We worked with our friends at NVIDIA (now a corporate sponsor - thank you NVIDIA!) to get drivers and toolchains working properly.<sup id="fnref:5"><a href="#fn:5">14</a></sup></li>
</ul>
<p>Everything runs from AWS’s us-east-1 region. Yes, that means if you’re compiling from Australia or Europe, your code takes a scenic route across the Pacific or Atlantic. We’ve thought about multi-region deployment, but the complexity of keeping that amount of compilers in sync across the globe makes my head hurt. For now, we just let CloudFront’s edge caching handle the static content and apologise to our friends in far-flung places for the extra latency.</p>
<h3>Keeping an eye on things</h3>
<p>Some statistics about our current setup:</p>
<ul>
<li><strong>3.9 terabytes</strong> of compilers, libraries, and tools</li>
<li><strong>Up to 30+ EC2 instances</strong> (EC2 instances are virtual machines)</li>
<li><strong>4,724 compiler versions</strong></li>
<li><strong>1,982,662</strong> short links saved (and as of recently, ~14k ex-goo.gl links)</li>
<li><strong>1.8 million compilations per week</strong></li>
</ul>
<p>That’s around 90 million compilations a year, so we keep an eye on things with:</p>
<ul>
<li><strong><a href="https://grafana.com/">Grafana agents</a></strong> on every instance</li>
<li><strong><a href="https://prometheus.io/">Prometheus</a></strong> for metrics collection</li>
<li><strong><a href="https://grafana.com/oss/loki/">Loki</a></strong> for log aggregation</li>
<li><strong><a href="https://aws.amazon.com/cloudwatch/">CloudWatch</a></strong> for AWS metrics and auto-scaling triggers</li>
</ul>
<p>We have <a href="https://stats.compiler-explorer.com/">public dashboards</a> so you can see our metrics in real-time.</p>
<p>
<a href="https://xania.org/202506/ce-dashboard.png">
<img src="https://xania.org/202506/ce-dashboard-thumb.png" width="461" height="300" alt="Screenshot of Compiler Explorer&#39;s public Grafana dashboard"/>
</a>
</p>

<p>Keeping costs down is tricky. We’re behind on this front, choosing to spend our limited volunteer time on adding features and compilers rather than reducing costs. We use spot instances where we can, and implement caching to avoid redundant compilations: caching in the browser, in each instance in an LRU cache, and then also on S3 using a daily-expiring content-addressable approach.</p>
<p>Our <a href="https://patreon.com/mattgodbolt">Patreon supporters</a>, GitHub sponsors and <a href="https://godbolt.org/#sponsors">commercial sponsors</a> cover the bills with enough slack to make this sustainable. Right now, Compiler Explorer costs around $3,000<sup id="fnref:costs"><a href="#fn:costs">15</a></sup> a month (including AWS, monitoring, Sentry for errors, Grafana, and other expenses). I’m hoping to find a way to be more transparent about the costs (if you’re a Patron, then you know I usually do a post about this once a year).</p>
<p>It all works pretty well these days. We haven’t had a major outage from traffic spikes in years - the auto-scaling just quietly does its job.</p>
<p>
<a href="https://xania.org/202506/traffic-patterns.png">
<img src="https://xania.org/202506/traffic-patterns-thumb.png" width="600" height="429" alt="Compiler Explorer traffic patterns showing steady growth with occasional spikes"/>
</a>
</p>

<p>These days, our traffic is a bit more predictable with regular daily and weekly patterns rather than the wild viral spikes of the early days.</p>
<h3>What’s Next?</h3>
<p>What we have today started from that first hacky prototype. Every decision was made to solve a real problem:</p>
<ul>
<li><code>nsjail</code> came about because people kept trying to break things</li>
<li>Daily builds started because manually updating compilers was painful</li>
<li>We added multi-architecture support because users kept asking</li>
<li>We moved to Typescript from pure Javascript because we like types<sup id="fnref:types"><a href="#fn:types">16</a></sup></li>
</ul>
<p>Looking forward, I’m currently working on an opt-in AI explanation tool panel. If I can get it working well enough, it should be deployed in the next few weeks. You can read my thoughts on <a href="https://xania.org/202504/ai-in-coding">AI in coding</a> if you’re curious about the approach.</p>
<p>Eventually I’d like to add:</p>
<ul>
<li>User accounts for managing short links (at least, if I can be sure enough of the privacy implications and regulatory burden)</li>
<li>More architectures (particularly RISC-V)</li>
<li>CPU performance analysis visualisation (been wanting this for 6+ years)</li>
</ul>
<p>Some things I wish I’d done differently:</p>
<p><strong>Early decisions that haunt us</strong>: Our save format is tightly coupled to the <a href="https://golden-layout.com/">GoldenLayout</a> library we use for the UI. This means we have to be super careful upgrading GoldenLayout to ensure we can still load older links. When you promise “URLs that last forever,” even your JavaScript library choices become permanent fixtures.</p>
<p><strong>The link shortener mess</strong>: A long time ago we used short links that looked like <code>godbolt.org/g/abc123</code>. They were actually wrappers around Google’s goo.gl shortener. When Google <a href="https://xania.org/202505/compiler-explorer-urls-forever">announced they were killing it</a>, we had to scramble to preserve 12,000+ legacy links. Never trust a third-party service with your core infrastructure<sup id="fnref:aws"><a href="#fn:aws">17</a></sup> - lesson learned the hard way.</p>
<p><strong>Infrastructure cruft</strong>: I wish I’d laid out our NFS directory structure better from the start. I wish we had a better story for configuration management across thousands of compilers. Multi-cluster support and service discovery remain ongoing challenges. Oh, and deployment? It’s completely ad hoc right now - we deploy updates to each cluster manually and separately. We’re working on blue/green deployment to make deploys less problematic, and ideally we’ll automate more of this process. At least when things do break, the auto-scaling group replaces the dead instances and I get a friendly 3am text message.</p>
<p>The fact that this whole thing works at all still amazes me sometimes. From a weekend project to infrastructure that serves thousands of developers<sup id="fnref:how"><a href="#fn:how">18</a></sup> - it’s been quite the journey.</p>
<h3>Thanks</h3>
<p>None of this would work without the amazing team of contributors and administrators who keep the lights on. Huge thanks to:</p>
<p><strong><a href="https://bsky.app/profile/partouf.bsky.social">Partouf</a> (Patrick Quist)</strong> - Basically keeps CE running. They are fantastic and I don’t know what CE would do without them.</p>
<p><strong>Core team</strong>: <a href="https://github.com/jeremyrifkin">Jeremy Rifkin</a>, <a href="https://github.com/dkm">Marc Poulhiès</a>, <a href="https://github.com/ofersh">Ofek Shilon</a>, <a href="https://github.com/matsjla">Mats Jun Larsen</a>, <a href="https://github.com/abrillant">Abril Rincón Blanco</a>, and <a href="https://github.com/apmorton">Austin Morton</a> are all huge contributors and core developers.</p>
<p><strong>Community heroes</strong>: <a href="https://github.com/ojeda">Miguel Ojeda</a>, <a href="https://github.com/JohanEngelen">Johan Engelen</a>, <a href="https://github.com/narpfel">narpfel</a>, <a href="https://github.com/kevinjeon6">Kevin Jeon</a> and <a href="https://github.com/compiler-explorer/compiler-explorer/graphs/contributors">many, many more</a>.</p>
<ul>
<li>The compiler maintainers whose amazing work we’re privileged to showcase</li>
<li>Our corporate sponsors</li>
<li>Everyone who supports us on <a href="https://patreon.com/mattgodbolt">Patreon</a> and <a href="https://github.com/sponsors/mattgodbolt">GitHub Sponsors</a></li>
<li>You, for using the site and making all this infrastructure worthwhile</li>
</ul>
<p>Questions? Complaints? Compiler versions we’re missing? Drop by our <a href="https://discord.gg/B5WacA7">Discord</a>, or find me on <a href="https://bsky.app/profile/matt.godbolt.org">Bluesky</a> or <a href="https://hachyderm.io/@mattgodbolt">Mastodon</a>.</p>
<hr/>
<p><em>Want to support Compiler Explorer? Check out our <a href="https://patreon.com/mattgodbolt">Patreon</a> or <a href="https://github.com/sponsors/mattgodbolt">GitHub Sponsors</a>. Those AWS bills don’t pay themselves.</em></p>
<h3>Disclaimer</h3>
<p>This article was a collaboration between a human and an <a href="https://anthropic.com">LLM</a>. The LLM was set off to research the codebase and internet for things that have changed since 2016 (the last time I wrote a “how it works”). Then I used that to create a framework for an article. I did the first few edits, then got LLM assistance in looking for mistakes and typos, and some formatting assistance. The LLM wrote the <code>dot</code> file and the python code that generates the “wall” of compiler stats.</p>
<p>The LLM also reminded me that I usually put a disclaimer at the end of my articles saying that I used AI assistance, which I had forgotten to do. Thanks, Claude.</p>

    </div>
</div></div>
  </body>
</html>
