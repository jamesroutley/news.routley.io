<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://jolynch.github.io/posts/use_fast_data_algorithms/">Original</a>
    <h1>Use fast data algorithms (2021)</h1>
    
    <div id="readability-page-1" class="page"><div>
<p>As an engineer who primarily works with data and databases I spend a lot of
time moving data around, hashing it, compressing it, decompressing it and
generally trying to shovel it between VMs and blob stores over TLS. I am
constantly surprised by how many systems only support slow, inefficient, and
expensive ways of doing these operations.</p>
<p>In my experience, these poor algorithm choices are <em>orders of magnitude</em> slower
than modern alternatives. Indeed, using a fast algorithm can often be the
difference between actually doing compression/hashing/encryption and “Eh, I’ll
skip it”.  In the interest of using more of our CPU time to do useful work
instead of melting ice-caps and giving programmers excessively long coffee
breaks, we will explore some faster alternatives in this post.</p>
<p><strong>TLDR</strong></p>
<table>
<thead>
<tr>
<th>Application</th>
<th>Common Bad Performance Choices</th>
<th>Better Performance Choices</th>
<th>Expected Performance Gain</th>
</tr>
</thead>
<tbody>
<tr>
<td>Trusted data hashing</td>
<td><a href="https://en.wikipedia.org/wiki/MD5"><code>md5</code></a>, <a href="https://en.wikipedia.org/wiki/SHA-2"><code>sha2</code></a>, <a href="https://en.wikipedia.org/wiki/Cyclic_redundancy_check#CRC-32_algorithm"><code>crc32</code></a></td>
<td><a href="http://cyan4973.github.io/xxHash/"><code>xxhash</code></a></td>
<td>~10x</td>
</tr>
<tr>
<td>Untrusted data hashing</td>
<td><code>md5</code>, <code>sha2</code>, <a href="https://en.wikipedia.org/wiki/SHA-1"><code>sha1</code></a></td>
<td><a href="https://github.com/BLAKE3-team/BLAKE3"><code>blake3</code></a></td>
<td>~10x</td>
</tr>
<tr>
<td>Fast compression</td>
<td><a href="https://en.wikipedia.org/wiki/Snappy_(compression)"><code>snappy</code></a>, <a href="https://www.gnu.org/software/gzip/manual/gzip.html"><code>gzip</code></a> (zlib)</td>
<td><a href="https://github.com/lz4/lz4"><code>lz4</code></a></td>
<td>10x over <code>gzip</code>, ~2x over <code>snappy</code></td>
</tr>
<tr>
<td>Good compression</td>
<td><code>gzip</code> (zlib)</td>
<td><a href="https://facebook.github.io/zstd/"><code>zstd</code></a></td>
<td>~2-10x</td>
</tr>
<tr>
<td>Best compression</td>
<td><a href="https://en.wikipedia.org/wiki/XZ_Utils"><code>xz</code></a> (lzma)</td>
<td><a href="https://facebook.github.io/zstd/"><code>zstd -10+</code></a></td>
<td>~2-10x</td>
</tr>
<tr>
<td>Java crypto (<code>md5</code>, <code>aes-gcm</code>, etc …)</td>
<td>Built-in JVM crypto</td>
<td><a href="https://github.com/corretto/amazon-corretto-crypto-provider"><code>Amazon Corretto Crypto Provider (ACCP)</code></a></td>
<td>~4-10x</td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>Disclaimer</strong>: There are lies, damn lies, and benchmarks from some random person
on the internet.</p>
<p>If you are considering taking some of the advice in this
post please remember to test your specific workloads, which might have
different bottlenecks. Also the implementation quality in your particular
software stack for your particular hardware matters <em>a lot</em>.</p>
</blockquote>
<p>For this post I’ll be playing with a ~5 GiB real-world <a href="https://www.yelp.com/dataset/download">JSON
dataset</a> on my laptop’s
<code>Intel Core i7-8565U</code> pinned to
<a href="https://gist.github.com/jolynch/55185e455351d6b7febb266499207afa#file-benchmarkon-sh">4GHz</a>.
Since I want to benchmark the algorithms instead of disks I’ll be pre-touching
the file into RAM with <a href="https://hoytech.com/vmtouch/"><code>vmtouch</code></a>. Remember that
on most modern cloud servers with fast <code>NVMe</code> storage (multiple Gi<strong>B</strong>ps) and
good page-caching algorithms your disks are likely not your bottleneck.</p>
<div><pre><code data-lang="bash">$ du -shc yelp_academic_dataset_review.json
6.5G    yelp_academic_dataset_review.json
$ vmtouch -t yelp_academic_dataset_review.json
<span># ...</span>
$ vmtouch yelp_academic_dataset_review.json
           Files: <span>1</span>
     Directories: <span>0</span>
  Resident Pages: 1693525/1693525  6G/6G  100%
         Elapsed: 0.1256 seconds
</code></pre></div><h2 id="hashing">Hashing</h2>
<blockquote>
<p>I would like to check that this blob of data over here is the same as that
data over there.</p>
</blockquote>
<h3 id="trusted-data-hashes">Trusted Data Hashes</h3>
<p>These hash or checksum functions are used to ensure data integrity and usually
are defending against bugs/bitrot/cosmic rays instead of malicious attackers.
I typically see the following poor choices:</p>
<ul>
<li><code>md5</code>: This hash is both weak <em>and</em> slow. It does have the advantage of
being one of the fastest slow choices in standard libraries and therefore it
is somewhat common. Two of my favorite examples are slow Cassandra <a href="https://issues.apache.org/jira/browse/CASSANDRA-14611">Quorum
Reads</a> and slow S3
upload/download (seriously, just try disabling <code>md5</code> on parts and see how
much faster S3 is).</li>
<li><code>crc32</code> or <code>adler32</code>: I often hear “well crc32 is fast”, but to be honest I
haven’t in practice come across particularly fast implementations in
real-world deployments. Sure theoretically there are hardware
implementations, but at least in most Java or Python programs I profile
running on most servers I encounter these checksums are not particularly fast
and only generate 32 bits of output where I absolutely have to care about
collisions.</li>
<li><code>sha2</code>: An oddly common choice for trusted data hashes. Odd because it is
slow and you don’t need a cryptographic hash for syncing a file
between two hosts within an internal network with an authorized transfer
protocol (e.g. <code>rsync</code> or via backups to <code>S3</code> with proper AWS
<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html">IAM</a>
policies). If attackers have access to your backups or artifact store you
have a bigger problem than them forging a hash.</li>
</ul>
<p><strong>Faster Choice</strong></p>
<p>Try <a href="https://github.com/Cyan4973/xxHash"><code>xxHash</code></a>. It is blazing fast, high
quality, and the <code>XXH64</code> variant is usually sufficient for most data integrity
checks. It even performs well on small data inputs, where <code>XXH3</code> is
particularly impressive. If you find yourself needing 128 bits of entropy (e.g.
if you’re hashing data for a
<a href="https://en.wikipedia.org/wiki/Distributed_hash_table"><code>DHT</code></a>) you can either
use the 128 bit variant or <code>xxh3</code>. If you only have an old version available
that doesn’t support the new 128 bit variant <em>two</em> <code>XXH64</code> runs with
different seeds is usually still faster than any other choice.</p>
<h3 id="untrusted-data-hashes">Untrusted Data Hashes</h3>
<p>While most of the time the threat model for data transfer is “bugs / cosmic
rays”, some of the time people want to defend against bad actors. That’s
where cryptographic hashes come in, most commonly:</p>
<ul>
<li><code>md5</code>: It’s slow and not resistant to collisions … everything I love in a hash
function.</li>
<li><a href="https://en.wikipedia.org/wiki/SHA-1"><code>sha1</code></a>: This hash is at least somewhat
performant and isn’t as completely broken as <code>md5</code>. We should probably still
consider <a href="https://en.wikipedia.org/wiki/SHA-1#SHAttered_%E2%80%93_first_public_collision">it on shaky
ground</a>
though.</li>
<li><a href="https://en.wikipedia.org/wiki/SHA-2"><code>sha2</code></a> (specifically <code>sha256</code>): An
ubiquitous hash and unlike <code>md5</code> and <code>sha1</code> it is still considered secure, so
that is nice. Unfortunately this hash is noticeably slow when used in
performance critical applications.</li>
<li><a href="https://en.wikipedia.org/wiki/SHA-3"><code>sha3</code></a>: The latest hash that NIST
standardized (because they had concerns about <code>SHA-2</code> that appear at this
time somewhat unfounded). I was not able to find it packaged outside of
<code>openssl</code> and it doesn’t appear to have great <code>cli</code> or language support at
this time.  It’s also still pretty slow.</li>
</ul>
<p><strong>Faster Choice</strong></p>
<p>Try <a href="https://github.com/BLAKE3-team/BLAKE3"><code>BLAKE3</code></a>. Yes it is a new (2020)
algorithm and there are concerns about security margin but I’m just so tired of
waiting for <code>sha256</code>. In practice, this is probably a much better choice than
the known-to-be-completely-broken <code>md5</code> so if you’re reaching for <code>md5</code> over
<code>xxHash</code> because you need a cryptographic alternative, consider <code>blake3</code>
instead. Also <code>blake3</code> uses hash trees (<a href="https://en.wikipedia.org/wiki/Merkle_tree">merkle
trees</a>) which are wonderful when
implemented correctly and I wish more systems used them.</p>
<p>The one major downside of <code>blake3</code> in my opinion is that at this time (2021) I
only know of really good <code>cli</code>, <code>Rust</code>, <code>Go</code>, <code>C</code>, and <code>Python</code> implementations
and I can’t personally vouch for the <code>Java</code> JNI bindings. I’ve only needed to
use it so far from streaming pipeline verifications or artifact verification so
the <code>cli</code> and <code>python</code> implementations are good enough for me.</p>
<p>A quick reminder that there are lots of security-sensitive hashing situations
where you don’t want a fast hash. For example, one situation where you want an
intentionally slow algorithm is when dealing with passwords. In such cases you
want a very slow hash like
<a href="https://en.wikipedia.org/wiki/Argon2"><code>argon2</code></a>,
<a href="https://en.wikipedia.org/wiki/Bcrypt"><code>bcrypt</code></a>,
<a href="https://en.wikipedia.org/wiki/PBKDF2"><code>PBKFD2</code></a> or even just a high number of
rounds of <code>SHA-512</code>.</p>
<h3 id="show-me-the-data">Show me the Data</h3>
<p>Expect <code>xxHash</code> to net about a <code>~10x</code> improvement on <code>MD5</code> and ~5-10x improvement
on <code>CRC32</code> depending on your <code>CRC32</code> implementation (e.g. Java’s is truly terrible).
Expect <code>BLAKE3</code> to be slightly slower than <code>xxHash</code> with a single thread so
only use it if you actually care about cryptographic hashes. A simple
<a href="https://gist.github.com/jolynch/55185e455351d6b7febb266499207afa#file-hashing-sh">performance test</a>
on hashing a <code>6616 MiB</code> file confirms that indeed we have 10x performance on the table (
note I’m reporting <code>user</code> CPU time since the system time is not really up to the hash)</p>
<table>
<thead>
<tr>
<th>Algo</th>
<th>Hash Time (seconds)</th>
<th>Hash Speed (MiBps)</th>
<th>Cryptographic?</th>
<th>Bits of Entropy</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>MD5</code></td>
<td><code>9.1s</code></td>
<td><code>727</code></td>
<td>Not really</td>
<td><code>128</code></td>
</tr>
<tr>
<td><code>CRC32</code></td>
<td><code>4.8s</code></td>
<td><code>1378</code></td>
<td>No</td>
<td><code>32</code></td>
</tr>
<tr>
<td><strong><code>XXH64</code></strong></td>
<td><code>0.5s</code></td>
<td><code>13232</code></td>
<td>No</td>
<td><code>64</code></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>SHA256</code></td>
<td><code>27.5s</code></td>
<td><code>240</code></td>
<td>Yes</td>
<td><code>256</code></td>
</tr>
<tr>
<td><code>SHA3-256</code></td>
<td><code>16.8s</code></td>
<td><code>393</code></td>
<td>Yes</td>
<td><code>256</code></td>
</tr>
<tr>
<td><code>SHA1  </code></td>
<td><code>10.7s</code></td>
<td><code>618</code></td>
<td>Yes*</td>
<td><code>160</code></td>
</tr>
<tr>
<td><strong><code>BLAKE3</code></strong></td>
<td><code>1.8s</code></td>
<td><code>3675</code></td>
<td>Yes</td>
<td><code>256</code></td>
</tr>
</tbody>
</table>
<p><strong>Yes that’s right, ~0.5 seconds user CPU time for <code>xxh64</code> versus ~27 for <code>sha256</code></strong> and <code>~9s</code> for <code>md5</code>.
If all you need to do is verify a file transfer you could be doing that 10x faster with xxHash.</p>
<p>The language versions often do make a big deal, e.g. <code>JNI</code> versions that link
to fast native code in Java will often significantly out-perform pure Java
versions. “But Joey”, you say, “I have to use XYZ algorithm from the early ’00s
because of the specification!&#34;.  That is unfortunate, but at least make sure
you’re using fast implementations, for example
<a href="https://github.com/corretto/amazon-corretto-crypto-provider">ACCP</a> will speed
up <code>MD5</code> on most Java VMs by a factor of ~4 as well as <code>AES-GCM</code> by ~10x while
it is at it. ACCP achieves this by … linking in fast native implementations
of crypto.</p>
<h2 id="compression">Compression</h2>
<blockquote>
<p>I like my data transfers fast and don’t like paying for lots of disk or
network I don’t need. I heard <a href="https://en.wikipedia.org/wiki/Data_compression">data compression</a> is a thing.</p>
</blockquote>
<p>Most data compresses, especially text (e.g. <code>JSON</code>). The three cases where data
probably does not compress are if your data is random, the data was already
compressed, or the data was encrypted. Often in databases the metadata around
the data (e.g. write times, schemas, etc …) probably compresses even if the
data doesn’t. There are three primary measures of a compression algorithm:</p>
<ol>
<li><strong>Compression ratio</strong>: How much smaller is the result than the input?</li>
<li><strong>Compression speed</strong>: How quickly can I compress data?</li>
<li><strong>Decompression speed</strong>: How quickly can I decompress data?</li>
</ol>
<p>Depending on the use case, developers usually make some tradeoff between these
three metrics.  For example, databases doing page compression care most about
decompression speed, file transfers care most about compression speed, archival
storage cares most ratio, etc …</p>
<p>Fast compression that gives great ratio can significantly improve most
workloads but slow compression with bad ratio is painful and makes me sad.</p>
<h2 id="common-poor-choices">Common Poor Choices</h2>
<ul>
<li><a href="https://www.gnu.org/software/gzip/manual/gzip.html"><code>gzip</code></a> (<code>zlib</code>): A
trusty workhorse but also a very slow algorithm. In many cases where your
network is fast (<code>1 Gbps+</code>), compressing with gzip can actually hurt your system’s
performance (I have seen this numerous times).</li>
<li><a href="https://en.wikipedia.org/wiki/XZ_Utils"><code>xz</code></a> (<code>lzma</code>): An
algorithm that has pretty good ratios, but is so slow to compress that in
practice the only potential use cases are single-write
many-read use cases.</li>
<li><a href="https://en.wikipedia.org/wiki/Snappy_(compression)"><code>snappy</code></a>: One of the
first “I’ll trade off ratio for speed” algorithms, snappy was good for its
time. These days it is almost always outperformed by other choices.</li>
</ul>
<h3 id="better-choice---i-care-about-ratio">Better Choice - I care about ratio</h3>
<p>Try <a href="https://facebook.github.io/zstd/"><code>zstd</code></a>. To spend more compression
CPU time for better compression ratio increase the compression level or increase the block
size. I find that in most database workloads the default level (<code>3</code>) or even
level <code>1</code> is a good choice for write heavy datasets (getting closer to <code>lz4</code>)
and level <code>10</code> is good for read heavy datasets (surpassing <code>gzip</code> in every
dimension). Note that <code>zstd</code> strictly dominates <code>gzip</code> as it is faster and gets
better ratio.</p>
<p>Even better: <code>zstd</code> supports <a href="https://facebook.github.io/zstd/#small-data">training dictionaries</a>
which can really come in handy if you have lots of individually small but
collectively large <code>JSON</code> data (looking at you tracing systems).</p>
<h3 id="better-choice---i-only-care-about-speed">Better Choice - I only care about speed</h3>
<p>Try <a href="https://github.com/lz4/lz4"><code>lz4</code></a>. With near memory speeds and decent
ratio this algorithm is almost always a safe choice over not compressing at
all. It has excellent language support and is exceptionally good for real-time
compression/decompression as it is so cheap.</p>
<h3 id="better-choice---i-want-to-shovel-data-from-here-to-there">Better Choice - I want to shovel data from here to there</h3>
<p>Try <a href="https://facebook.github.io/zstd/"><code>zstd --adapt</code></a>. This feature
automatically adapts the compression ratio as the IO conditions change to make
the current optimal tradeoff between CPU and “keeping the pipe fed”.</p>
<p>For example, if you are have very little free CPU on your system but a fast
network (looking at you <code>i3en</code> instances) <code>zstd --adapt</code> will automatically
compress with a lower level to minimize total transfer time. If you have a slow
network and extra CPU it will automatically compress at a higher level.</p>
<h3 id="show-me-the-data-1">Show me the Data</h3>
<p>Compression is a bit trickier to measure because the read to write ratio
matters a lot and if you can get better ratio that might be worth it to pay
a more expensive compression step for cheaper decompression.</p>
<p>Historically we had to make tradeoffs between ratio, compression speed and
decompression speed, but as we see with <a href="https://gist.github.com/jolynch/55185e455351d6b7febb266499207afa#file-compression-sh">this quick
benchmark</a>
we no longer need to make tradeoffs. These days (2021), I just reach for <code>zstd</code>
with an appropriate level or <code>lz4</code> if I really need to minimize CPU cost.</p>
<p>First let’s look at the results for the <code>6.5GiB</code> review dataset.</p>
<table>
<thead>
<tr>
<th>Algo</th>
<th>Ratio</th>
<th>Compression Speed (MiBps)</th>
<th>Decompression Speed (MiBps)</th>
<th>Transfer Time</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>gzip</code></td>
<td><code>0.41</code></td>
<td><code>21</code></td>
<td><code>168</code></td>
<td><code>295s</code></td>
</tr>
<tr>
<td><code>lz4</code></td>
<td><code>0.65</code></td>
<td><code>361</code></td>
<td><code>1760</code></td>
<td><code>36s</code></td>
</tr>
<tr>
<td><code>zstd</code></td>
<td><code>0.38</code></td>
<td><code>134</code></td>
<td><code>730</code></td>
<td><code>47s</code></td>
</tr>
<tr>
<td><code>xz</code></td>
<td><code>??</code></td>
<td><code>??</code></td>
<td><code>???</code></td>
<td><code>??</code></td>
</tr>
</tbody>
</table>
<p>As <code>xz</code> was estimating 1.5 hours to compress and I didn’t have that kind of time I ran
that on a smaller <code>380MiB</code> dataset:</p>
<table>
<thead>
<tr>
<th>Algo</th>
<th>Ratio</th>
<th>Compression Speed (MiBps)</th>
<th>Decompression Speed (MiBps)</th>
<th>Transfer Time</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>xz</code></td>
<td><code>0.18</code></td>
<td><code>1.34</code></td>
<td><code>92 </code></td>
<td><code>299s</code></td>
</tr>
<tr>
<td><code>zstd -10</code></td>
<td><code>0.26</code></td>
<td><code>15.4</code></td>
<td><code>713</code></td>
<td><code>24s</code></td>
</tr>
<tr>
<td><code>zstd -19</code></td>
<td><code>0.21</code></td>
<td><code>1.18</code></td>
<td><code>404</code></td>
<td><code>24s</code></td>
</tr>
</tbody>
</table>
<p>As expected <code>lz4</code> is the fastest choice by a lot while still cutting the
dataset in half, followed by <code>zstd</code>. One of the really useful things about
<code>zstd</code> is that I am no longer reaching for specialty compressors depending on
the job, I just change the level/block sizes and I can get the trade-off I
want.</p>
<h2 id="pipeline">Pipeline</h2>
<p>Now that we have fast algorithms, it matters how we wire them together. One of
the number one performance mistakes I see is doing a single step of a data
movement at a time, for example decrypting a file to disk and then
decompressing it and then checksumming it. As the intermediate products must hit disk and
are done sequentially this necessarily slows down your data transfer.</p>
<p>When data transfer is slow there are usually either slow disks or slow Java
heap allocations in the way. I’ve found that if I can structure my pipelines as
unix pipelines of fast <code>C</code> programs (or Python with native extensions) with the
output from one stage always flowing to the input of the other I can much more
efficiently upload and download data by doing everything (IO, decrypt, decompress, checksum)
in parallel.</p>
<p>For example, something like the following will outperform almost any Java S3
upload at putting 100 files in S3 along with whole object checksums</p>
<div><pre><code data-lang="bash"><span># Assumption: You have xargs, xxh64sum, zstd and awscli installed</span>

<span># Simple version</span>
$ DIR<span>=</span>.

$ find $DIR -maxdepth <span>1</span> -type f | xargs -IX -P <span>8</span> xxh64sum X | sort -k <span>2</span> &gt; aws s3 cp - s3://bucket/prefix/checksums
$ find $DIR -maxdepth <span>1</span> -type f | xargs -IX -P <span>8</span> | bash -c <span>&#39;zstd --adapt X --stdout | aws s3 cp - s3://bucket/prefix/data/X.zst&#39;</span>

<span># Or the full pipeline option (full object hash and compress at the same time)</span>
$ find $DIR -maxdepth <span>1</span> -type f <span>\
</span><span></span>| xargs -IX -P <span>8</span> <span>\
</span><span></span>bash -c <span>&#39;export URL=s3://bucket/prefix; cat X | tee &gt;(xxh64sum - | aws s3 cp - ${URL}/X.zst.xxh) | zstd --adapt - --stdout | aws s3 cp - ${URL}/X.zstd&#39;</span>
</code></pre></div><h2 id="why-do-fast-data-algorithms-matter-">Why do fast data algorithms matter ?</h2>
<p>It matters because most systems choose the slow option and make routine
development activities take longer in modern cloud infrastructure (fast
networks). For example:</p>
<ul>
<li>Backing up data. Backing up 1 TiB of data to a fast blob store (can sink
multiple <code>GiBps</code>) using <code>gzip</code> and <code>sha256</code> would take around <code>15</code> hours. Doing
it with <code>zstd</code> and <code>xxhash</code> takes <code>2.2</code> hours. Timely backups are one of the
most important properties of a data system.</li>
<li>Software packages. By default debian packages are either not compressed or
compressed with <code>xz</code>. Installing 500 MiB of uncompressed debian packages over
a one gig network takes <code>~5s</code>, with <code>xz</code> compression it actually
slows down and takes <code>~1s + ~6s = ~7s</code>, and with <code>zstd -19</code> compression it
takes <code>~1s + ~1s = ~2s</code>. If the <code>sha256</code> is checked that would add another
<code>~20s</code> for a total of <code>~30s</code> versus if we check with <code>blake3</code> that adds
<code>~0.1s</code> for a total of <code>~3s</code>.  I’d take the <code>3s</code> over <code>30s</code> any day. This
matters every time you build container images, bake cloud VM images or
bootstrap servers with configuration management (puppet/chef).</li>
<li>Container Images. By default <code>docker</code> uses <code>gzip</code> compression with <code>sha256</code>
checksums, which means to decompress and checksum 4GiB of containers I’d need
about <code>60s</code> of CPU time instead of ~<code>6s</code> with <code>zstd</code> and <code>xxhash</code> (or <code>6.5</code>s
with <code>blake3</code>). This matters when your docker pull is in the startup path of
your application.</li>
</ul>
<h3 id="a-note-about-implementation-availability">A note about implementation availability</h3>
<p>A major disadvantage of using good algorithms is that they may not always show
up in your language or OS by default. I’ve had good experiences with the
following implementations:</p>
<ul>
<li><code>xxHash</code>: <code>cli</code>, <a href="https://pypi.org/project/xxhash/"><code>python</code></a>, and <a href="https://github.com/lz4/lz4-java#xxhash-java"><code>java</code></a></li>
<li><code>lz4</code>: <code>cli</code>, <a href="https://pypi.org/project/lz4/"><code>python</code></a>, and <a href="https://github.com/lz4/lz4-java#lz4-java"><code>java</code></a>.</li>
<li><code>zstd</code>: <code>cli</code>, <a href="https://pypi.org/project/zstandard/"><code>python</code></a>, and <a href="https://github.com/luben/zstd-jni"><code>java</code></a>.</li>
<li><code>blake3</code>: <a href="https://github.com/BLAKE3-team/BLAKE3/releases/tag/0.3.7"><code>cli</code></a>, and <a href="https://pypi.org/project/blake3/"><code>python</code></a></li>
<li><a href="https://github.com/jolynch/pinch"><code>Pinch</code></a> is a <a href="https://hub.docker.com/r/jolynch/pinch">docker image</a>
I built so I can bring my swiss-army-knife of hashing/compression techniques
to any server that can run docker. I use this a lot on CI/CD systems.</li>
</ul>

</div></div>
  </body>
</html>
