<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.sbert.net/index.html">Original</a>
    <h1>SentenceTransformers: Python framework for sentence, text and image embeddings</h1>
    
    <div id="readability-page-1" class="page"><div>
    
    <nav data-toggle="wy-nav-shift">
      
    </nav>

    <section data-toggle="wy-nav-shift">

      
      <nav aria-label="top navigation">
        
          <i data-toggle="wy-nav-top"></i>
          <a href="#">Sentence-Transformers</a>
        
      </nav>


      <div>
        
        <div>
        
          


















          <div role="main" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<div id="installation">

<p>You can install it using pip:</p>
<div><div><pre><span></span><span>pip</span> <span>install</span> <span>-</span><span>U</span> <span>sentence</span><span>-</span><span>transformers</span>
</pre></div>
</div>
<p>We recommend <strong>Python 3.8</strong> or higher, and at least <strong>PyTorch 1.11.0</strong>. See <a href="https://www.sbert.net/docs/installation.html">installation</a> for further installation options, especially if you want to use a GPU.</p>
</div>
<div id="usage">

<p>The usage is as simple as:</p>
<div><div><pre><span></span><span>from</span> <span>sentence_transformers</span> <span>import</span> <span>SentenceTransformer</span>
<span>model</span> <span>=</span> <span>SentenceTransformer</span><span>(</span><span>&#34;all-MiniLM-L6-v2&#34;</span><span>)</span>

<span># Our sentences to encode</span>
<span>sentences</span> <span>=</span> <span>[</span>
    <span>&#34;This framework generates embeddings for each input sentence&#34;</span><span>,</span>
    <span>&#34;Sentences are passed as a list of string.&#34;</span><span>,</span>
    <span>&#34;The quick brown fox jumps over the lazy dog.&#34;</span>
<span>]</span>

<span># Sentences are encoded by calling model.encode()</span>
<span>embeddings</span> <span>=</span> <span>model</span><span>.</span><span>encode</span><span>(</span><span>sentences</span><span>)</span>

<span># Print the embeddings</span>
<span>for</span> <span>sentence</span><span>,</span> <span>embedding</span> <span>in</span> <span>zip</span><span>(</span><span>sentences</span><span>,</span> <span>embeddings</span><span>):</span>
    <span>print</span><span>(</span><span>&#34;Sentence:&#34;</span><span>,</span> <span>sentence</span><span>)</span>
    <span>print</span><span>(</span><span>&#34;Embedding:&#34;</span><span>,</span> <span>embedding</span><span>)</span>
    <span>print</span><span>(</span><span>&#34;&#34;</span><span>)</span>
</pre></div>
</div>
</div>
<div id="performance">

<p>Our models are evaluated extensively and achieve state-of-the-art performance on various tasks. Further, the code is tuned to provide the highest possible speed. Have a look at <a href="https://www.sbert.net/docs/pretrained_models.html">Pre-Trained Models</a> for an overview of available models and the respective performance on different tasks.</p>
</div>

<div id="citing-authors">

<p>If you find this repository helpful, feel free to cite our publication <a href="https://arxiv.org/abs/1908.10084">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</a>:</p>
<blockquote>
<div><div><div><pre><span></span><span>@inproceedings</span><span>{</span><span>reimers-2019-sentence-bert</span><span>,</span>
<span>  </span><span>title</span><span> </span><span>=</span><span> </span><span>&#34;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&#34;</span><span>,</span>
<span>  </span><span>author</span><span> </span><span>=</span><span> </span><span>&#34;Reimers, Nils and Gurevych, Iryna&#34;</span><span>,</span>
<span>  </span><span>booktitle</span><span> </span><span>=</span><span> </span><span>&#34;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&#34;</span><span>,</span>
<span>  </span><span>month</span><span> </span><span>=</span><span> </span><span>&#34;11&#34;</span><span>,</span>
<span>  </span><span>year</span><span> </span><span>=</span><span> </span><span>&#34;2019&#34;</span><span>,</span>
<span>  </span><span>publisher</span><span> </span><span>=</span><span> </span><span>&#34;Association for Computational Linguistics&#34;</span><span>,</span>
<span>  </span><span>url</span><span> </span><span>=</span><span> </span><span>&#34;https://arxiv.org/abs/1908.10084&#34;</span><span>,</span>
<span>}</span>
</pre></div>
</div>
</div></blockquote>
<p>If you use one of the multilingual models, feel free to cite our publication <a href="https://arxiv.org/abs/2004.09813">Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation</a>:</p>
<blockquote>
<div><div><div><pre><span></span><span>@inproceedings</span><span>{</span><span>reimers-2020-multilingual-sentence-bert</span><span>,</span>
<span>  </span><span>title</span><span> </span><span>=</span><span> </span><span>&#34;Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation&#34;</span><span>,</span>
<span>  </span><span>author</span><span> </span><span>=</span><span> </span><span>&#34;Reimers, Nils and Gurevych, Iryna&#34;</span><span>,</span>
<span>  </span><span>booktitle</span><span> </span><span>=</span><span> </span><span>&#34;Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing&#34;</span><span>,</span>
<span>  </span><span>month</span><span> </span><span>=</span><span> </span><span>&#34;11&#34;</span><span>,</span>
<span>  </span><span>year</span><span> </span><span>=</span><span> </span><span>&#34;2020&#34;</span><span>,</span>
<span>  </span><span>publisher</span><span> </span><span>=</span><span> </span><span>&#34;Association for Computational Linguistics&#34;</span><span>,</span>
<span>  </span><span>url</span><span> </span><span>=</span><span> </span><span>&#34;https://arxiv.org/abs/2004.09813&#34;</span><span>,</span>
<span>}</span>
</pre></div>
</div>
</div></blockquote>
<p>If you use the code for <a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/data_augmentation">data augmentation</a>, feel free to cite our publication <a href="https://arxiv.org/abs/2010.08240">Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks</a>:</p>
<blockquote>
<div><div><div><pre><span></span><span>@inproceedings</span><span>{</span><span>thakur-2020-AugSBERT</span><span>,</span>
<span>  </span><span>title</span><span> </span><span>=</span><span> </span><span>&#34;Augmented {SBERT}: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks&#34;</span><span>,</span>
<span>  </span><span>author</span><span> </span><span>=</span><span> </span><span>&#34;Thakur, Nandan and Reimers, Nils and Daxenberger, Johannes  and Gurevych, Iryna&#34;</span><span>,</span>
<span>  </span><span>booktitle</span><span> </span><span>=</span><span> </span><span>&#34;Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies&#34;</span><span>,</span>
<span>  </span><span>month</span><span> </span><span>=</span><span> </span><span>jun</span><span>,</span>
<span>  </span><span>year</span><span> </span><span>=</span><span> </span><span>&#34;2021&#34;</span><span>,</span>
<span>  </span><span>address</span><span> </span><span>=</span><span> </span><span>&#34;Online&#34;</span><span>,</span>
<span>  </span><span>publisher</span><span> </span><span>=</span><span> </span><span>&#34;Association for Computational Linguistics&#34;</span><span>,</span>
<span>  </span><span>url</span><span> </span><span>=</span><span> </span><span>&#34;https://www.aclweb.org/anthology/2021.naacl-main.28&#34;</span><span>,</span>
<span>  </span><span>pages</span><span> </span><span>=</span><span> </span><span>&#34;296--310&#34;</span><span>,</span>
<span>}</span>
</pre></div>
</div>
</div></blockquote>






</div>


           </div>
           
          </div>
          

        </div>
      </div>

    </section>

  </div></div>
  </body>
</html>
