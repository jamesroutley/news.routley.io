<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.linode.com/blog/open-source/flow-ipc-introduction-low-latency-cpp-toolkit/">Original</a>
    <h1>Flow-IPC: Open-source toolkit for low-latency inter-process communication in C&#43;&#43;</h1>
    
    <div id="readability-page-1" class="page"><div data-node="5faaa566323b2">
	<div>
		  



<p>We <a href="https://www.akamai.com/blog/developers/flow-ipc-introduction-low-latency-cpp-toolkit">recently released</a> Flow-IPC – an interprocess communication toolkit in C++ – as open source under the Apache 2.0 and MIT licenses. Flow-IPC will be useful for C++ projects that transmit data between application processes and need to achieve near-zero latency without a trade-off against simple and reusable code.</p>



<p>In the announcement, we showed that Flow-IPC can transmit data structure payloads as large as 1GB  just as quickly as a 100K payload– and in less than 100 microseconds. With classic IPC, the latency depends on the payload size and can reach into the 1 second range. Thus the improvement can be as much as three or four orders of magnitude.</p>



<p>In this post, we show the source code that produced those numbers. Our example, which centers on the <a href="https://capnproto.org/">Cap’n Proto</a> integration, shows that Flow-IPC is both fast and easy to use. (Note that the Flow-IPC API is comprehensive in that it supports transmitting various types of payloads, but the Cap’n Proto-based payload transmission is a specific feature.) Cap’n Proto is an open source project not affiliated with Akamai whose use is subject to the license, as of the date of this blog’s publication date, found <a href="https://github.com/capnproto/capnproto/blob/v2/LICENSE">here</a>.</p>



<p><strong>What’s Included?</strong></p>



<p>Flow-IPC is a library with an extensible C++17 API. It is <a href="https://github.com/flow-ipc">hosted in GitHub</a> together with full documentation, automated tests and demos, and a CI pipeline. The example we explore below is the <code>perf_demo</code> test application. Flow-IPC currently supports Linux running on x86-64. We have plans to expand this to macOS and ARM64, followed by Windows and other OS variants depending on demand. You’re <a href="https://github.com/Flow-IPC/ipc/blob/main/CONTRIBUTING.md">welcome to contribute</a> and port.</p>



<p>The Flow-IPC API is in the spirit of the <a href="https://en.cppreference.com/w/cpp/standard_library">C++ standard library</a> and <a href="https://www.boost.org/">Boost</a>, focusing on integrating various concepts and their implementations in a modular way. It is designed to be extensible. Our CI pipeline tests across a range of GCC and Clang compiler versions and build configurations, including hardening via runtime sanitizers: <a href="https://github.com/google/sanitizers/wiki/AddressSanitizer">ASAN</a> (hardens against memory misuse), <a href="https://github.com/google/sanitizers/wiki/ThreadSanitizerCppManual">TSAN</a> (against race conditions), and <a href="https://clang.llvm.org/docs/UndefinedBehaviorSanitizer.html">UBSAN</a> (against miscellaneous undefined behavior).</p>



<p>At this time, Flow-IPC is for <em>local</em> communication: crossing process boundaries but not machine boundaries. However it has an extensible design, so expanding it to networked IPC is a natural next step. We think the use of <a href="https://en.wikipedia.org/wiki/Remote_direct_memory_access">Remote Direct Memory Access (RDMA)</a> provides an intriguing possibility for ultra-fast LAN performance.</p>



<p><strong>Who Should Use It?</strong></p>



<p>Flow-IPC is a pragmatic inter-process communication toolkit. In designing it, we came from the perspective of the modern C++ systems developer, specifically tailoring it to the IPC tasks one faces repeatedly, particularly in server application development. Many of us have had to throw together a Unix-domain-socket, named-pipe, or local HTTP-based protocol to transmit something from one process to another. Sometimes, to avoid the copying involved in such solutions, one might turn to shared-memory (SHM), a notoriously touchy and hard-to-reuse technique. Flow-IPC can help any C++ developer faced with such tasks, from common to advanced.</p>



<p><strong>Highlights include:</strong></p>



<ul>
<li><em>Cap’n Proto integration:</em> Tools for in-place schema-based serialization like Cap’n Proto, which is best-in-class, are very helpful for inter-process work. However, without Flow-IPC, you’d still have to <em>copy</em> the bits into a socket or pipe, etc., and then <em>copy them again</em> on receipt. Flow-IPC provides <em>end-to-end zero-copy</em> transmission of Cap’n Proto-encoded structures using shared memory.</li>



<li><em>Socket/FD support:</em> Any message transmitted via Flow-IPC can include a native I/O handle (also known as a file descriptor or FD). For example, in a web server architecture, you could split the server into two processes: a process for managing <em>endpoints</em> and a process for <em>processing requests</em>. After the endpoint process completes the TLS negotiation, it can pass the connected TCP socket handle directly to the request processing process.</li>



<li><em>Native C++ structure support:</em> Many algorithms require work directly on C++ <code>struct</code>s, most often ones involving multiple levels of STL containers and/or pointers. Two threads collaborating on such a structure is common and easy to code whereas two <em>processes</em> doing so via shared-memory is quite hard – even with thoughtful tools like <a href="https://www.boost.org/doc/libs/1_78_0/doc/html/interprocess.html">Boost.interprocess</a>. Flow-IPC simplifies this by enabling sharing of STL-compliant structures like containers, pointers, and plain-old-data.</li>



<li><em>jemalloc plus SHM:</em> One line of code allows you to allocate any necessary data in shared memory, whether it’s for behind the scenes operations like Cap’n Proto transmission or directly for native C++ data. These tasks can be delegated to <a href="https://jemalloc.net/">jemalloc</a>, the heap engine behind FreeBSD and Meta. This feature can be particularly valuable for projects that require intensive shared memory allocation, similar to regular heap allocation.</li>



<li><em>No naming or cleanup headaches:</em> With Flow-IPC, you need not name server-sockets, SHM segments, or pipes, or worry about leaked persistent RAM. Instead,  establish a Flow-IPC <em>session </em>between processes: this is your IPC context. From this single session object, communication channels can be opened at will, without additional naming. For tasks that require direct shared memory (SHM) access, a dedicated SHM arena is available. Flow-IPC performs automatic cleanup, even in the case of an abnormal exit, and it avoids conflicts among resource names.</li>



<li><em>Use for RPC:</em> Flow-IPC is designed to complement, not compete with higher-level communication frameworks like <a href="https://grpc.io/">gRPC</a> and <a href="https://capnproto.org/rpc.html">Cap’n Proto RPC</a>. There’s no need to choose between them and Flow-IPC. In fact, using Flow-IPC’s zero-copy features can generally enhance the performance of these protocols.</li>
</ul>



<p><strong>Example: Sending a Multi-Part File</strong></p>



<p>While Flow-IPC can transmit data of various kinds, we decided to focus on a data structure described by a <a href="https://capnproto.org/language.html">Cap’n Proto (capnp) schema</a>. This example will be particularly clear to those familiar with capnp and Protocol Buffers, but we’ll give plenty of context for those who are less familiar with these tools.</p>



<p>In this example, there are two apps that engage in a request-response scenario.</p>



<ul>
<li><strong>App 1 (server)</strong>: This is a memory-caching server that has pre-loaded files ranging from 100kb to 1GB into RAM. It stands by to handle get-cached file requests and issue responses.</li>



<li><strong>App 2 (client):</strong> This client requests a file of a certain size. App 1 (server) sends the file data in a single message broken down into a series of chunks. Each chunk includes the data along with its hash.</li>
</ul>



<pre><code># Cap&#39;n Proto schema (.capnp file, generates .h and .c++ source code
# using capnp compiler tool):

$Cxx.namespace(&#34;cache_demo::schema&#34;);
struct Body
{ 
  union
  {
    getCacheReq @0 :GetCacheReq;
    getCacheRsp @1 :GetCacheRsp;
  }
}

struct GetCacheReq
{ 
  fileName @0 :Text;
}
struct GetCacheRsp
{
  # We simulate the server returning file multiple parts, 
  # each (~equally) sized at its discretion.
  struct FilePart
  {
    data @0 :Data;
    dataSizeToVerify @1 :UInt64; 
    # Recipient can verify that `data` blob&#39;s size is indeed this.
    dataHashToVerify @2 :Hash; 
    # Recipient can hash `data` and verify it is indeed this.
  }
  fileParts @0 :List(FilePart);
}</code></pre>



<p>Our goal in this experiment is to issue a request for an N-sized file, receive the response, measure the time it took to process, and check the integrity of a part of the file. This interaction takes place through a communication <em>channel</em>. </p>



<p>To do this, we need to first establish this channel. While Flow-IPC allows you to manually establish a channel from its low-level constituent parts (local socket, POSIX message queue, and more), it is much easier to use Flow-IPC <em>sessions </em>instead. A <em>session</em> is simply the communication context between two live processes. Once established, channels are readily available.</p>



<p>From a bird’s eye view, here’s how the process works.</p>



<figure><img loading="lazy" decoding="async" width="1064" height="822" src="https://www.linode.com/wp-content/uploads/2024/04/Diagram1-1-1064x822.png" alt="" srcset="https://www.linode.com/wp-content/uploads/2024/04/Diagram1-1-1064x822.png 1064w, https://www.linode.com/wp-content/uploads/2024/04/Diagram1-1-632x488.png 632w, https://www.linode.com/wp-content/uploads/2024/04/Diagram1-1-416x321.png 416w, https://www.linode.com/wp-content/uploads/2024/04/Diagram1-1.png 1584w" sizes="(max-width: 1064px) 100vw, 1064px"/></figure>



<p><em>Process A </em>on the left is called the <em>session-server</em>. The process boxes on the right – <em>session-clients</em> – connect to Process A in order to establish <em>sessions</em>. Generally, any given session is completely symmetrical, so it doesn’t matter who initiated the connection. Both sides are equally capable and can be assigned any algorithmic role. Before the session is ready, though, we’ll need to assign roles. One side will be the session-client and will perform an instant <em>connect</em> of a single session, and the other side, the session-server, will <em>accept </em>as many sessions as it wants.</p>



<p>In this example, the setup is straightforward. There are two apps with one session between them and one channel in that session. The cache-client (App 2) plays the role of session-client, and the session-server role is taken by App 1. However, the reverse would work fine as well.</p>



<p>To set this up, each application (the cache-client and cache-server) must understand the same IPC universe, which just means that they need to know basic facts about the applications involved.</p>



<p>Here’s why:</p>



<ul>
<li>The client needs to know how to locate the server to initiate a session. If you’re the connecting-app (the client), you need to know the name of the accepting-app. Flow-IPC uses the server’s name to figure out details like the socket addresses and shared-memory segment names based on this name.</li>



<li>The  server app must know who’s allowed to connect to it, for security reasons. Flow-IPC checks the client’s details, like user/group and executable path, against the operating system to ensure everything matches up.</li>



<li>Standard OS safety mechanisms (owners, permissions) apply to various IPC transports (sockets, MQs, SHM). A single selector <code>enum</code> will set the high-level policy to use and Flow-IPC will then set permissions as restrictively as possible while respecting that choice.</li>
</ul>



<p>For us, we will just need to execute the following in each of the two applications. This can be in a single <code>.cpp</code> file linked into both the cache-server and cache-client apps.</p>



<pre><code>// IPC app universe: simple structs naming the 2 apps.
// The applications should share this code.
const ipc::session::Client_app
  CLI_APP{ &#34;cacheCli&#34;, // Name the app uniquely.
           // From where it will run (for safety).
           &#34;/usr/bin/cache_client.exec&#34;,
           CLI_UID, GID }; // The user and group ID (for safety).
const ipc::session::Server_app
  SRV_APP{ { &#34;cacheSrv&#34;, &#34;/usr/bin/cache_server.exec&#34;, SRV_UID, GID },
           // For the server, provide similar details --^.
           // Plus a few server-specific settings:

           // Safety: List client-app names that can connect to server-app.
           // So in our case this will just be { &#34;cacheCli&#34; }.
           { CLI_APP.m_name },
           &#34;&#34;, // An optional path override; don&#39;t worry about it here.
           // Safety/permissions selector:
           // We&#39;ve decided to run the two apps as different users
           // in the same group - so we indicate that here.
           ipc::util::Permissions_level::S_GROUP_ACCESS }; </code></pre>



<p>Note that more complex setups can have more of these definitions.</p>



<p>Having executed that code in each of our applications, we’ll simply pass these objects into the session object constructor so that the server will know what to expect when accepting sessions and the client will know which server to connect to.</p>



<p>So let’s open the session. In App 2 (the cache-client), we just want to open a session and a channel within it. While Flow-IPC allows instantly opening channels anytime (given a session object), it is typical to need a certain number of ready-to-go channels at the start of the session. Since we want to open one channel, we can let Flow-IPC create the channel when creating the session. This avoids any unnecessary asynchronicity. So, at the beginning of our cache-client’s <code>main()</code> function, we can connect and open a channel with a single <code>.sync_connect()</code> call:</p>



<pre><code>// Specify that we *do* want zero-copy behavior, by merely choosing our
// backing-session type.
// In other words, setting this alias says, “be fast about Cap’n Proto things.”
//
// Different (subsequent) capnp-serialization-backing and SHM-related behaviors
// are available; just change this alias. E.g., omit `::shm::classic` to disable
// SHM entirely; or specify `::shm::arena_lend::jemalloc` to employ
// jemalloc-based SHM. Subsequent code remains the same!
// This demonstrates a key design tenet of Flow-IPC. 
using Session = ipc::session::shm::classic::Client_session&lt;...&gt;;

// Tell Session object about the applications involved.
Session session{ CLI_APP, SRV_APP, /* detail omitted */ };

// Ask for 1 *channel* to be available on both sides
// from the very start of the session.
Session::Channels ipc_raw_channels(1);

// Instantly open session - and the 1 channel.
// (Fail if server is not running at this time.)
session.sync_connect(session.mdt_builder(), &amp;ipc_raw_channels);
auto&amp; ipc_raw_channel = ipc_raw_channels[0];

// (Can also instantly open more channel(s) anytime:
// `session.open_channel(&amp;channel)`.) </code></pre>







<p>For now, we just want to speak the Cap’n Proto <code>cache_demo::schema::Body</code> protocol (from our .capnp file). So we <em>upgrade</em> the raw channel object to a <em>structured channel</em> object like so:</p>



<pre><code>// Template arg indicates capnp schema. (Take a look at the .capnp file above.)
Session::Structured_channel&lt;cache_demo::schema::Body&gt;
  ipc_channel
    { nullptr, std::move(ipc_raw_channel), “Eat” the raw channel: take over it.
      ipc::transport::struc::Channel_base::S_SERIALIZE_VIA_SESSION_SHM,
      &amp;session }; </code></pre>



<p>That’s it for our setup. We are now ready to exchange capnp messages over the channel. Notice that we’ve bypassed dealing with OS specific details like object names, Unix permissions values, and so on. Our approach was merely to name our two applications. We also opted for end-to-end zero-copy transmission to maximize performance by leveraging shared-memory without a single <code>::shm_open()</code> or <code>::mmap()</code> in sight.</p>



<p>We are now ready for the fun part: issuing the <code>GetCacheReq</code> request, receiving the <code>GetCacheRsp </code>response, and accessing various parts of that response, namely the file parts and their hashes.</p>



<p>Here’s the code:</p>



<pre><code>// Issue request and process response.  TIMING FOR LATENCY GRAPH STARTS HERE --&gt;
auto req_msg = ipc_channel.create_msg();
req_msg.body_root() // Vanilla capnp code: call Cap&#39;n Proto-generated mutator API.
  -&gt;initGetCacheReq().setFileName(&#34;huge-file.bin&#34;);

// Send message; get ~instant reply.
const auto rsp_msg = ipc_channel.sync_request(req_msg); 
// More vanilla capnp work: accessors.
const auto rsp_root = rsp_msg-&gt;body_root().getGetCacheRsp();
// &lt;-- TIMING FOR LATENCY GRAPH STOPS HERE.

// ...
verify_hash(rsp_root, some_file_chunk_idx);

// ...

// More vanilla Cap&#39;n Proto accessor code.
void verify_hash(const cache_demo::schema::GetCacheRsp::Reader&amp; rsp_root,
                 size_t idx)
{
  const auto file_part = rsp_root.getFileParts()[idx];
  if (file_part.getHashToVerify() != compute_hash(file_part.getData()))
  {
    throw Bad_hash_exception(...);
  }
} </code></pre>



<p>In the code above we used the simple <code>.sync_request()</code> which both sends a message and awaits a specific response. The <code>ipc::transport::struc::Channel</code> API provides a number of niceties to make protocols natural to code, including async receiving, demultiplexing to handler functions by message type, notification versus request, and unsolicited-message versus response. There are no restrictions placed on your schema (<code>cache_demo::schema::Body</code> in our case). If it is expressible in capnp, you can use it with Flow-IPC structured channels.</p>



<p>That’s it! The server side is similar in spirit and level of difficulty. The <code>perf_demo</code> source code is available.</p>



<p>Without Flow-IPC, replicating this setup for end-to-end zero-copy performance would involve a significant amount of difficult code, including management of SHM segments whose names and cleanup would have to be coordinated between the two applications. Even without zero-copy – i.e., simply :<code>:write()</code>ing a copy of the capnp serialization of <code>req_msg</code> to and <code>::read()</code>ing <code>rsp_msg</code> from a Unix domain socket FD – sufficiently robust code would be non-trivial to write in comparison.</p>



<p>The graph below shows the latencies, where each point on the x-axis represents the sum of all <code>filePart.data</code> sizes for each given test run. The blue line shows the latencies from the basic method where App 1 writes capnp serializations to a Unix domain socket using <code>::write()</code>, and App 2 reads them with <code>::read()</code>. The orange line represents the latencies for the code using Flow-IPC, as discussed above.</p>



<figure><img loading="lazy" decoding="async" width="1064" height="505" src="https://www.linode.com/wp-content/uploads/2024/04/Graph1-crop-1064x505.png" alt="" srcset="https://www.linode.com/wp-content/uploads/2024/04/Graph1-crop-1064x505.png 1064w, https://www.linode.com/wp-content/uploads/2024/04/Graph1-crop-632x300.png 632w, https://www.linode.com/wp-content/uploads/2024/04/Graph1-crop-416x197.png 416w, https://www.linode.com/wp-content/uploads/2024/04/Graph1-crop.png 1452w" sizes="(max-width: 1064px) 100vw, 1064px"/></figure>



<p><strong>How to Contribute</strong></p>



<p>For feature requests and defect reports, please look at the Issue database on the <a href="https://github.com/flow-ipc">Flow-IPC GitHub site</a>. File Issues as needed.</p>



<p>To contribute changes and new features, please consult the <a href="https://github.com/Flow-IPC/ipc/blob/main/CONTRIBUTING.md">contribution</a> guide. We can be reached at the <a href="https://github.com/orgs/Flow-IPC/discussions">Flow-IPC discussions board</a> on GitHub.</p>



<p><strong>What’s Next?</strong></p>



<p>We tried to provide a realistic experiment in the above example, skipping nothing significant in the code shown. We did deliberately choose a scenario that lacks asynchronicity and callbacks. Accordingly, questions will arise, such as, “how do I integrate Flow-IPC with my event loop?” or, “how do I handle session and channel closing? Can I just open a simple stream socket without all this session stuff?”, etc. </p>



<p>Such questions are addressed in the full documentation. The docs include a reference generated from API comments in the code, a guided Manual with a gentler learning curve, and installation instructions. The <a href="https://github.com/Flow-IPC/ipc/blob/main/README.md">main repository’s README</a> will point you to all of these resources. The <a href="https://flow-ipc.github.io/doc/flow-ipc/versions/main/generated/html_public/about.html">Manual intro</a> and <a href="https://flow-ipc.github.io/doc/flow-ipc/versions/main/generated/html_public/api_overview.html">API synopsis</a> cover the breadth of available features.</p>



<p><strong>Resources</strong></p>



<ul>
<li><a href="https://www.akamai.com/blog/developers/flow-ipc-introduction-low-latency-cpp-toolkit">Announcement Blog Post</a></li>



<li><a href="https://github.com/flow-ipc">Flow-IPC Project at GitHub</a></li>



<li><a href="https://github.com/orgs/Flow-IPC/discussions">Flow-IPC Discussions</a></li>
</ul>
	</div>
</div></div>
  </body>
</html>
