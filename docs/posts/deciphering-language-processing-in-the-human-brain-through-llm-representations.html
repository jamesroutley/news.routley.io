<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://research.google/blog/deciphering-language-processing-in-the-human-brain-through-llm-representations/">Original</a>
    <h1>Deciphering language processing in the human brain through LLM representations</h1>
    
    <div id="readability-page-1" class="page"><div data-gt-publish-date="20250321">
                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <p data-block-key="6iix5">How does the human brain process natural language during everyday conversations? Theoretically, large language models (LLMs) and symbolic psycholinguistic models of human language provide a fundamentally different computational framework for coding natural language. Large language models do not depend on symbolic parts of speech or syntactic rules. Instead, they utilize simple self-supervised objectives, such as next-word prediction and generation enhanced by <a href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback" target="_blank" rel="noopener noreferrer">reinforcement learning</a>. This allows them to produce context-specific linguistic outputs drawn from real-world text corpora, effectively encoding the statistical structure of natural speech (sounds) and language (words) into a multidimensional embedding space.</p><p data-block-key="dc141">Inspired by the success of LLMs, our team at Google Research, in collaboration with <a href="https://hassonlab.princeton.edu/" target="_blank" rel="noopener noreferrer">Princeton University</a>, <a href="https://nyulangone.org/locations/comprehensive-epilepsy-center" target="_blank" rel="noopener noreferrer">NYU</a>, and <a href="https://www.deepcognitionlab.com/" target="_blank" rel="noopener noreferrer">HUJI</a>, sought to explore the similarities and differences in how the human brain and deep language models process natural language to achieve their remarkable capabilities. Through a series of studies over the past five years, we explored the similarity between the internal representations (embeddings) of specific deep learning models and human brain neural activity during natural free-flowing conversations, demonstrating the power of deep language model’s embeddings to act as a framework for understanding how the human brain processes language. We demonstrate that the word-level internal embeddings generated by deep language models align with the neural activity patterns in established brain regions associated with speech comprehension and production in the human brain.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <h2 data-block-key="6iix5">Similar embedding-based representations of language.</h2><p data-block-key="5u3tk">Our most <a href="https://www.nature.com/articles/s41562-025-02105-9" target="_blank" rel="noopener noreferrer">recent study</a>, published in <a href="https://www.nature.com/nathumbehav/" target="_blank" rel="noopener noreferrer"><i>Nature Human Behaviour</i></a>, investigated the alignment between the internal representations in a Transformer-based speech-to-text model and the neural processing sequence in the human brain during real-life conversations. In the study, we analyzed neural activity recorded using <a href="https://ecog.med.nyu.edu/intracranial-electrodes/" target="_blank" rel="noopener noreferrer">intracranial electrodes</a> during spontaneous conversations. We compared patterns of neural activity with the internal representations — embeddings — generated by the <a href="https://arxiv.org/abs/2212.04356" target="_blank" rel="noopener noreferrer">Whisper</a> speech-to-text model, focusing on how the model&#39;s linguistic features aligned with the brain&#39;s natural speech processing.</p><p data-block-key="ae2d8">For every word heard (during speech comprehension) or spoken (during speech production), two types of embeddings were extracted from the speech-to-text model — <i>speech</i> embeddings from the model’s speech encoder and word-based <i>language</i> embeddings from the model&#39;s decoder. A linear transformation was estimated to predict the brain’s neural signals from the speech-to-text embeddings for each word in each conversation. The study revealed a remarkable alignment between the neural activity in the human brain&#39;s speech areas and the model&#39;s speech embeddings and between the neural activity in the brain’s language area and the model&#39;s language embeddings. The alignment is illustrated in the following animation, modeling the sequence of the brain’s neural responses to subjects’ language comprehension:</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <p data-block-key="6iix5">As the listener processes the incoming spoken words, we observe a sequence of neural responses: Initially, as each word is articulated, speech embeddings enable us to predict cortical activity in speech areas along the <a href="https://en.wikipedia.org/wiki/Superior_temporal_gyrus" target="_blank" rel="noopener noreferrer">superior temporal gyrus</a> (STG). A few hundred milliseconds later, when the listener starts to decode the meaning of the words, language embeddings predict cortical activity in <a href="https://en.wikipedia.org/wiki/Broca%27s_area" target="_blank" rel="noopener noreferrer">Broca’s area</a> (located in the <a href="https://en.wikipedia.org/wiki/Inferior_frontal_gyrus" target="_blank" rel="noopener noreferrer">inferior frontal gyrus</a>; IFG).</p><p data-block-key="8nte6">Turning to participants&#39; production, we observe a different (reversed!) sequence of neural responses:</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <p data-block-key="6iix5">Looking at this alignment more closely, about 500 milliseconds before articulating the word (as the subject prepares to articulate the next word), language embeddings (depicted in blue) predict cortical activity in <a href="https://en.wikipedia.org/wiki/Broca%27s_area" target="_blank" rel="noopener noreferrer">Broca’s area</a>. A few hundred milliseconds later (still before word onset), speech embeddings (depicted in red) predict neural activity in the <a href="https://en.wikipedia.org/wiki/Motor_cortex" target="_blank" rel="noopener noreferrer">motor cortex</a> (MC) as the speaker plans the articulatory speech sequence. Finally, after the speaker articulates the word, speech embeddings predict the neural activity in the STG auditory areas as the listener listens to their own voice. This dynamic reflects the sequence of neural processing, starting with planning what to say in language areas, then how to articulate it in motor areas, and finally monitoring what was spoken in perceptual speech areas.</p><p data-block-key="ct6ei">The quantitative results of the whole-brain analysis are illustrated in figure below: for each word, given its speech embeddings (red) and language embedding (blue), we predicted the neural response in each electrode at time lags ranging from -2 seconds before to +2 seconds after the word onset (<i>x</i>-axis value of 0 in the figure). This was done during speech production (left panel) and speech comprehension (right panel). The related graphs illustrate the accuracy of our predictions of neural activity (correlation) for all words as a function of the lag in the electrodes across various brain regions.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <p data-block-key="6iix5">During speech production, it is evident that language embeddings (blue) in the IFG peaked before speech embeddings (red) peaked in the sensorimotor area, followed by the peak of speech encoding in the STG. In contrast, during speech comprehension, the peak encoding shifted to after the word onset, with speech embeddings (red) in the STG peaking significantly before language encoding (blue) in the IFG.</p><p data-block-key="638ar">All in all, our findings suggest that the speech-to-text model embeddings provide a cohesive framework for understanding the neural basis of processing language during natural conversations. Surprisingly, while Whisper was developed solely for speech recognition, without considering how the brain processes language, we found that its internal representations align with neural activity during natural conversations. This alignment was not guaranteed — a negative result would have shown little to no correspondence between the embeddings and neural signals, indicating that the model&#39;s representations did not capture the brain&#39;s language processing mechanisms.</p><p data-block-key="do4mr">A particularly intriguing concept revealed by the alignment between LLMs and the human brain is the notion of a &#34;soft hierarchy&#34; in neural processing. Although regions of the brain involved in language, such as the IFG, tend to prioritize word-level semantic and syntactic information — as indicated by stronger alignment with language embeddings (blue) — they also capture lower-level auditory features, which is evident from the lower yet significant alignment with speech embeddings (red). Conversely, lower-order speech areas such as the STG tend to prioritize acoustic and phonemic processing — as indicated by stronger alignment with speech embeddings (red) — they also capture word-level information, evident from the lower yet significant alignment with language embeddings (blue).</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <h2 data-block-key="6iix5">Shared objectives and geometry between LLMs and the human brain</h2><p data-block-key="6j9ts">LLMs are trained to process natural language by using a simple objective: predicting the next word in a sequence. In a <a href="https://www.nature.com/articles/s41593-022-01026-4" target="_blank" rel="noopener noreferrer">paper</a> published in <a href="https://www.nature.com/neuro/" target="_blank" rel="noopener noreferrer"><i>Nature Neuroscience</i></a>, we discovered that, similar to LLMs, the language areas of a listener’s brain attempt to predict the next word before it is spoken. Furthermore, like LLMs, listeners&#39; confidence in their predictions before the word’s onset modifies their surprise level (prediction error) after the word is articulated. These findings provide compelling new evidence for fundamental computational principles of pre-onset prediction, post-onset surprise, and embedding-based contextual representation shared by autoregressive LLMs and the human brain. In another <a href="https://www.nature.com/articles/s41467-024-46631-y" target="_blank" rel="noopener noreferrer">paper</a> published in <a href="https://www.nature.com/ncomms/" target="_blank" rel="noopener noreferrer"><i>Nature Communications</i></a>, the team also discovered that the relation among words in natural language, as captured by the geometry of the embedding space of an LLM, is aligned with the geometry of the representation induced by the brain (i.e., brain embeddings) in language areas.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <h2 data-block-key="6iix5">Differences between how LLMs and the human brain process natural language</h2><p data-block-key="abajj">While the human brain and Transformer-based LLMs share fundamental computational principles in processing natural language, their underlying neural circuit architectures are markedly different. For example, in a <a href="https://arxiv.org/abs/2310.07106" target="_blank" rel="noopener noreferrer">follow-up study</a>, we investigated how information is processed across layers in Transformer-based LLMs compared to the human brain. The team found that while the non-linear transformations across layers are similar in LLMs and language areas in the human brain, the implementations differ significantly. Unlike the Transformer architecture, which processes hundreds to thousands of words simultaneously, the language areas appear to analyze language serially, word by word, recurrently, and temporally.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <h2 data-block-key="6iix5">Summary and future directions</h2><p data-block-key="bajqj">The accumulated evidence from the team’s work uncovered several shared computational principles between how the human brain and deep learning models process natural language. These findings indicate that deep learning models could offer a new computational framework for understanding the brain&#39;s neural code for processing natural language based on principles of statistical learning, blind optimization, and a <a href="https://www.sciencedirect.com/science/article/pii/S089662731931044X" target="_blank" rel="noopener noreferrer">direct fit to nature</a>. At the same time, there are significant differences between the neural architecture, types and scale of linguistic data, the training protocols of Transformer-based language models, and the biological structure and developmental stages through which the human brain naturally acquires language in social settings environments. Moving forward, our goal is to create innovative, biologically inspired artificial neural networks that have improved capabilities for processing information and functioning in the real world. We plan to achieve this by adapting neural architecture, learning protocols, and training data that better match human experiences.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <h2 data-block-key="6iix5">Acknowledgments</h2><p data-block-key="3cmiq"><i>The work described is the result of Google Research&#39;s long-term collaboration with</i> <i>the</i> <a href="https://hassonlab.princeton.edu/" target="_blank" rel="noopener noreferrer"><i>Hasson Lab</i></a><i> at the Neuroscience Institute and the Psychology Department at Princeton University, the</i> <a href="https://www.deepcognitionlab.com/" target="_blank" rel="noopener noreferrer"><i>DeepCognitionLab</i></a> <i>at the Hebrew University Business School and Cognitive Department, and researchers from the</i> <a href="https://nyulangone.org/locations/comprehensive-epilepsy-center" target="_blank" rel="noopener noreferrer"><i>NYU Langone Comprehensive Epilepsy Center.</i></a></p>
</div>

    </div>
</section>

                    
                </div></div>
  </body>
</html>
