<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ai.meta.com/blog/audiocraft-musicgen-audiogen-encodec-generative-ai-audio/">Original</a>
    <h1>Open-sourcing AudioCraft: Generative AI for audio</h1>
    
    <div id="readability-page-1" class="page"><div><div><ul><p>RECOMMENDED READS</p><li><a href="https://ai.meta.com/blog/generative-ai-text-images-cm3leon/" data-ms-clickable="true" data-ms="{&#34;creative&#34;:&#34;click_external&#34;}" target="_blank" rel="noreferrer noopener" data-lnfb-mode="ie" id="u_0_a_IA"><svg viewBox="0 0 38 38" fill="none" xmlns="http://www.w3.org/2000/svg"><path opacity="0.4" fill-rule="evenodd" clip-rule="evenodd" d="M19 37C9.05887 37 1 28.9411 1 19C1 9.05887 9.05887 1 19 1C28.9411 1 37 9.05887 37 19C37 28.9411 28.9411 37 19 37Z" stroke="CurrentColor"></path><path d="M21.9657 12L28.9287 18.963L21.9657 25.926L20.7348 24.7193L25.6203 19.8334L10.0001 19.8334V18.0926L25.5966 18.0926L20.7348 13.2309L21.9657 12Z" fill="CurrentColor"></path><path d="M21.9657 12L28.9287 18.963L21.9657 25.926L20.7348 24.7193L25.6203 19.8334L10.0001 19.8334V18.0926L25.5966 18.0926L20.7348 13.2309L21.9657 12Z" fill="CurrentColor"></path></svg><p>Introducing CM3leon, a more efficient, state-of-the-art generative model for text and images</p></a></li><li><a href="https://ai.meta.com/blog/voicebox-generative-ai-model-speech/" data-ms-clickable="true" data-ms="{&#34;creative&#34;:&#34;click_external&#34;}" target="_blank" rel="noreferrer noopener" data-lnfb-mode="ie" id="u_0_b_4t"><svg viewBox="0 0 38 38" fill="none" xmlns="http://www.w3.org/2000/svg"><path opacity="0.4" fill-rule="evenodd" clip-rule="evenodd" d="M19 37C9.05887 37 1 28.9411 1 19C1 9.05887 9.05887 1 19 1C28.9411 1 37 9.05887 37 19C37 28.9411 28.9411 37 19 37Z" stroke="CurrentColor"></path><path d="M21.9657 12L28.9287 18.963L21.9657 25.926L20.7348 24.7193L25.6203 19.8334L10.0001 19.8334V18.0926L25.5966 18.0926L20.7348 13.2309L21.9657 12Z" fill="CurrentColor"></path><path d="M21.9657 12L28.9287 18.963L21.9657 25.926L20.7348 24.7193L25.6203 19.8334L10.0001 19.8334V18.0926L25.5966 18.0926L20.7348 13.2309L21.9657 12Z" fill="CurrentColor"></path></svg><p>Introducing Voicebox: The first generative AI model for speech to generalize across tasks with state-of-the-art performance</p></a></li><li><a href="https://ai.meta.com/blog/multilingual-model-speech-recognition/" data-ms-clickable="true" data-ms="{&#34;creative&#34;:&#34;click_external&#34;}" target="_self" data-lnfb-mode="ie"><svg viewBox="0 0 38 38" fill="none" xmlns="http://www.w3.org/2000/svg"><path opacity="0.4" fill-rule="evenodd" clip-rule="evenodd" d="M19 37C9.05887 37 1 28.9411 1 19C1 9.05887 9.05887 1 19 1C28.9411 1 37 9.05887 37 19C37 28.9411 28.9411 37 19 37Z" stroke="CurrentColor"></path><path d="M21.9657 12L28.9287 18.963L21.9657 25.926L20.7348 24.7193L25.6203 19.8334L10.0001 19.8334V18.0926L25.5966 18.0926L20.7348 13.2309L21.9657 12Z" fill="CurrentColor"></path><path d="M21.9657 12L28.9287 18.963L21.9657 25.926L20.7348 24.7193L25.6203 19.8334L10.0001 19.8334V18.0926L25.5966 18.0926L20.7348 13.2309L21.9657 12Z" fill="CurrentColor"></path></svg><p>Introducing speech-to-text, text-to-speech, and more for 1,100+ languages	</p></a></li></ul><div><p>AudioCraft consists of three models: <a href="https://huggingface.co/spaces/facebook/MusicGen" target="_blank" data-lnfb-mode="ie"><u>MusicGen</u></a>, <a href="https://felixkreuk.github.io/audiogen/" target="_blank" data-lnfb-mode="ie"><u>AudioGen</u></a>, and <a href="https://ai.meta.com/blog/ai-powered-audio-compression-technique/" target="_blank" data-lnfb-mode="ie"><u>EnCodec</u></a>. MusicGen, which was trained with Meta-owned and specifically licensed music, generates music from text-based user inputs, while AudioGen, which was trained on public sound effects, generates audio from text-based user inputs. Today, we’re excited to release an improved version of our EnCodec decoder, which allows for higher quality music generation with fewer artifacts; our pre-trained AudioGen model, which lets you generate environmental sounds and sound effects like a dog barking, cars honking, or footsteps on a wooden floor; and all of the AudioCraft model weights and code. The models are available for research purposes and to further people’s understanding of the technology. We’re excited to give researchers and practitioners access so they can train their own models with their own datasets for the first time and help advance the state of the art.</p><br/></div></div><p>From text to audio with ease</p></div><div><div><p>In recent years, generative AI models including language models have made huge strides and shown exceptional abilities: from the <a href="https://ai.facebook.com/blog/generative-ai-text-to-video/" target="_blank"><u>generation of a wide-variety of images and video from text descriptions</u></a> exhibiting spatial understanding to text and speech models that perform <a href="https://ai.facebook.com/blog/nllb-200-high-quality-machine-translation/" target="_blank"><u>machine translation</u></a> or even text or <a href="https://ai.facebook.com/blog/generating-chit-chat-including-laughs-yawns-ums-and-other-nonverbal-cues-from-raw-audio/" target="_blank"><u>speech dialogue agents</u></a>. Yet while we’ve seen a lot of excitement around generative AI for images, video, and text, audio has always seemed to lag a bit behind. There’s some work out there, but it’s highly complicated and not very open, so people aren’t able to readily play with it.</p></div><p>A simple approach to audio generation</p></div><div><div><p>Generating audio from raw audio signals is challenging as it requires modeling extremely long sequences. A typical music track of a few minutes sampled at 44.1 kHz (which is the standard quality of music recordings) consists of millions of timesteps. In comparison, text-based generative models like Llama and Llama 2 are fed with text processed as sub-words that represent just a few thousands of timesteps per sample.</p></div><figure><img src="https://scontent-iad3-1.xx.fbcdn.net/v/t39.8562-6/362278500_245853288291883_2304974600919081225_n.png?_nc_cat=109&amp;ccb=1-7&amp;_nc_sid=6825c5&amp;_nc_ohc=rYPmAEl2rtIAX-XCQaC&amp;_nc_ht=scontent-iad3-1.xx&amp;oh=00_AfAvBfGISzff7lD95FQEZD_shc1w3kSlW0YqibneCZOmRQ&amp;oe=64CF5A8C" alt=""/></figure><p>Training audio language models</p><div><p>We then use a single autoregressive language model to recursively model the audio tokens from EnCodec. We introduce a simple approach to leverage the internal structure of the parallel streams of tokens and show that with a single model and elegant token interleaving pattern, our approach efficiently models audio sequences, simultaneously capturing the long-term dependencies in the audio and allowing us to generate high-quality sound.</p><br/></div><div><p>Generating audio from text descriptions</p><br/></div><div><p>Text Prompt: Whistling with wind blowing</p></div><div><p>Text Prompt: Sirens and a humming engine approach and pass</p></div><div><p>Text Prompt: Pop dance track with catchy melodies, tropical percussions, and upbeat rhythms, perfect for the beach</p></div><div><p>Text Prompt: Earthy tones, environmentally conscious, ukulele-infused, harmonic, breezy, easygoing, organic instrumentation, gentle grooves</p></div><p>Building on this research</p><div><p>Our team continues working on the research behind advanced generative AI audio models. As part of this AudioCraft release, we further provide new approaches to push the quality of synthesized audio through a diffusion-based approach for discrete representation decoding. We plan to keep investigating better controllability of generative models for audio, exploring additional conditioning methods, and pushing the ability of models to capture even longer range dependencies. Finally, we will continue investigating the limitations and biases of such models trained on audio.</p></div><p>Responsibility and transparency as the cornerstones of our research</p><div><p>It’s important to be open about our work so the research community can build on it and continue the important conversations we’re having about how to build AI responsibly. We recognize that the datasets used to train our models lack diversity. In particular, the music dataset used contains a larger portion of western-style music and only contains audio-text pairs with text and metadata written in English. By sharing the code for AudioCraft, we hope other researchers can more easily test new approaches to limit or eliminate potential bias in and misuse of generative models.</p><br/></div><p>The importance of open source</p><div><p>Responsible innovation can’t happen in isolation. Open sourcing our research and resulting models helps ensure that everyone has equal access.</p></div><div><p><i>This blog post was made possible by the work of: Yossi Adi, Jade Copet, Alexandre Défossez, Itai Gat, David Kant, Felix Kreuk, Rashel Moritz, Tal Remez, Robin San Roman, Gabriel Synnaeve, and Mary Williamson.</i></p><br/></div></div></div>
  </body>
</html>
