<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.mmlab-ntu.com/project/vtoonify/">Original</a>
    <h1>VToonify: Controllable High-Resolution Portrait Video Style Transfer</h1>
    
    <div id="readability-page-1" class="page"><div>
            <!-- Section -->
            <!--31, 39, 73, .5-->
            <section>
            	
                
                <div>
                    <div>                  	
                        <div>
                            
                            <p>ACM TOG (SIGGRAPH Asia), 2022<br/></p>                       
                        </div>
                    </div>
                </div>
            </section>
            <!-- End of Section -->

            <!-- Section Abstract -->
			<section>
                <div>
                    <div>
                        <div>
                            <h3>Paper</h3>
                            <h2>Abstract</h2>
                            <p>
                            	Generating high-quality artistic portrait videos is an important and desirable task in computer graphics and vision. Although a series of successful portrait image toonification models built upon the powerful StyleGAN have been proposed, these image-oriented methods have obvious limitations when applied to videos, such as the fixed frame size, the requirement of face alignment, missing non-facial details and temporal inconsistency. In this work, we investigate the challenging controllable high-resolution portrait video style transfer by introducing a novel <b>VToonify</b> framework. Specifically, VToonify leverages the mid- and high-resolution layers of StyleGAN to render high-quality artistic portraits based on the multi-scale content features extracted by an encoder to better preserve the frame details. The resulting fully convolutional architecture accepts non-aligned faces in videos of variable size as input, contributing to complete face regions with natural motions in the output. Our framework is compatible with existing StyleGAN-based image toonification models to extend them to video toonification, and inherits appealing features of these models for flexible style control on color and intensity. This work presents two instantiations of VToonify built upon Toonify and DualStyleGAN for collection-based and exemplar-based portrait video style transfer, respectively. Extensive experimental results demonstrate the effectiveness of our proposed VToonify framework over existing methods in generating high-quality and temporally-coherent artistic portrait videos with flexible style controls.
                            </p>
                        </div>
 						<p><img src="https://www.mmlab-ntu.com/project/vtoonify/img/TOG22.jpg" alt=""/><br/>
                        </p>                       
                    </div>
                </div>
            </section>
            <!-- End of Section --> 

            <!-- Section Downloads -->
            <section>
                
            </section>
            <!-- End of Section -->

            <!-- Section Methodology -->
            <section>
                <div>
                    <div>
                        <div>
                            <div>
                                <h3>The</h3>
                                <h2>VToonify Framework</h2>
                                <p>
                                     VToonify combines the merits of the image translation framework and the StyleGAN-based framework to achieve controllable high-resolution portrait video style transfer. (a) Image translation framework uses fully convolutional networks to support variable input size. However, training from scratch renders it difficult for high-resolution and controllable style transfer. (b) StyleGAN-based framework leverages the pre-trained StyleGAN model for high-resolution and controllable style transfer, but is limited to fixed image size and detail losses. (c) Our hybrid framework adapts StyleGAN by removing its fixed-sized input feature and low-resolution layers to construct a fully convolutional encoder-generator architecture akin to that in the image translation framework. We train an encoder to extract multi-scale content features of the input frame as the additional content condition to the generator to preserve frame details. By incorporating the StyleGAN model into the generator to distill both its data and model, VToonify inherits the style control flexibility of StyleGAN.
                                </p>
                            </div>
                        </div>
                        <div> 
                            <p><img src="https://www.mmlab-ntu.com/project/vtoonify/img/framework.jpg" alt=""/>
                            </p>                                                         
                        </div>          
                    </div>
                </div>
            </section>
            <!-- End of Section -->  

            <!-- Section Results -->
            <section>
                <div>
                    <div>
                        <div>
                            <div>
                                <h3>Toonification with</h3>
                                <h2>Flexible Style Control</h2>
                                <p>
                                     Our framework is compatible with the existing StyleGAN-based image toonification models to extend them to video toonification, and inherits their appealing features for flexible style control. Taking the <a href="https://github.com/williamyang1991/DualStyleGAN">DualStyleGAN</a> model as the StyleGAN backbone, our VToonify provides: 1) exemplar-based structure style transfer, 2) adjustment of style degree 3) exemplar-based color style transfer.
                                </p>
                            </div>
                        </div>              
                    </div>
                                   
                </div>
            </section>
            <!-- End of Section -->

            <!-- Section Results -->
            <section>
                <div>
                    <div>
                        <p>
                            <h3>Experimental</h3>
                            <h2>Results</h2>
                            <h4>
                                Comparison with state-of-the-art face style transfer methods
                            </h4><br/>
                        </p>                        
                    </div>
	                <div>                                
	                    <div>
	                    	<div data-aos="zoom-in">
	                        <p><img src="https://www.mmlab-ntu.com/project/vtoonify/img/compare.jpg"/></p><p>
                                	We perform visual comparison to our two backbones <a href="https://github.com/justinpinkney/toonify">Toonify</a>, <a href="https://github.com/williamyang1991/DualStyleGAN">DualStyleGAN</a> and the high-resolution image-to-image translation baseline <a href="https://github.com/NVIDIA/pix2pixHD">Pix2pixHD</a> for StyleGAN distillation. VToonify-T and VToonify-D surpass their corresponding backbones <a href="https://github.com/justinpinkney/toonify">Toonify</a> and <a href="https://github.com/williamyang1991/DualStyleGAN">DualStyleGAN</a> in stylizing the complete video, while maintaining the same high quality and visual features as the backbones for each single frame. For example, VToonify-T follows <a href="https://github.com/justinpinkney/toonify">Toonify</a> to impose a strong style effect like violet hair in the Arcane style. VToonify-D, on the other hand, is better at preserving the facial features. Compared with VToonify-D, <a href="https://github.com/NVIDIA/pix2pixHD">Pix2pixHD</a> suffers from flickers and artifacts. 
                                </p>
	                     </div>
	                    </div>
	                </div>      
                    <div>
                        <p>
                            <h4>
                                Face toonification on more styles
                            </h4><br/>
                        </p>                        
                    </div>
	                  	                             
                </div>
            </section>
            <!-- End of Section --> 


            <!-- Section Citation -->
            <section>
                <div>                       
                    <div> 
                        <div>
                                <h3>Paper</h3>
                                <h2>Citation</h2>
                                <p>
									@article{yang2022vtoonify,</p>
                        </div>
                    </div>
                </div>
            </section>
            <!-- End of Section -->

            <!-- Section Related Projects -->
            <section>
                <div>
                    
                    <div>
                        <div>
                            <div>
                                <ul>
                                    <li>
                                       <span>Pastiche Master: Exemplar-Based High-Resolution Portrait Style Transfer</span> 
                                       </li>                                    
                                    <li>
                                       <span>Deep Plastic Surgery: Robust and Controllable Image Editing with Human-Drawn Sketches</span> 
                                       </li>                                    
                                    <li>
                                       <span>Talk-to-Edit: Fine-Grained Facial Editing via Dialog </span> 
                                       </li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
            </section>
            <!-- End of Section -->


            <!-- Section Contact -->
            <section>
                <div>
                    <div>
                        <div>
                            <p><img src="https://www.mmlab-ntu.com/project/vtoonify/img/shuai_yang.jpg" alt=""/>
                            </p>
                            
                        </div>
                    </div>
                </div>
            </section>
            <!-- End of Section -->  

        </div></div>
  </body>
</html>
