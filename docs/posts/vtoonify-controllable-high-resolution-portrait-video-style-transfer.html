<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/williamyang1991/VToonify">Original</a>
    <h1>VToonify: Controllable high-resolution portrait video style transfer</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path fill-rule="evenodd" d="M16 3.75a.75.75 0 00-1.136-.643L11 5.425V4.75A1.75 1.75 0 009.25 3h-7.5A1.75 1.75 0 000 4.75v6.5C0 12.216.784 13 1.75 13h7.5A1.75 1.75 0 0011 11.25v-.675l3.864 2.318A.75.75 0 0016 12.25v-8.5zm-5 5.075l3.5 2.1v-5.85l-3.5 2.1v1.65zM9.5 6.75v-2a.25.25 0 00-.25-.25h-7.5a.25.25 0 00-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-4.5z"></path>
</svg>
    <span aria-label="Video description overview.mp4">overview.mp4</span>
    <span></span>
  </summary>

  <video src="https://user-images.githubusercontent.com/18130694/189483939-0fc4a358-fb34-43cc-811a-b22adb820d57.mp4" data-canonical-src="https://user-images.githubusercontent.com/18130694/189483939-0fc4a358-fb34-43cc-811a-b22adb820d57.mp4" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">This repository provides the official PyTorch implementation for the following paper:</p>
<p dir="auto"><strong>VToonify: Controllable High-Resolution Portrait Video Style Transfer</strong></p>
<p dir="auto"><a href="http://colab.research.google.com/github/williamyang1991/VToonify/blob/master/notebooks/inference_playground.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="google colab logo" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"/></a>
<a href="https://huggingface.co/spaces/PKUWilliamYang/VToonify" rel="nofollow"><img src="https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565" alt="Hugging Face Spaces" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue"/></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/9b7dd39eb0d802f96feabdfe568020ed3e6c08e57492a09baae37118aa3163e7/68747470733a2f2f76697369746f722d62616467652e6c616f62692e6963752f62616467653f706167655f69643d77696c6c69616d79616e67313939312f56546f6f6e696679"><img src="https://camo.githubusercontent.com/9b7dd39eb0d802f96feabdfe568020ed3e6c08e57492a09baae37118aa3163e7/68747470733a2f2f76697369746f722d62616467652e6c616f62692e6963752f62616467653f706167655f69643d77696c6c69616d79616e67313939312f56546f6f6e696679" alt="visitors" data-canonical-src="https://visitor-badge.laobi.icu/badge?page_id=williamyang1991/VToonify"/></a></p>

<blockquote>
<p dir="auto"><strong>Abstract:</strong> <em>Generating high-quality artistic portrait videos is an important and desirable task in computer graphics and vision.
Although a series of successful portrait image toonification models built upon the powerful StyleGAN have been proposed,
these image-oriented methods have obvious limitations when applied to videos, such as the fixed frame size, the requirement of face alignment, missing non-facial details and temporal inconsistency.
In this work, we investigate the challenging controllable high-resolution portrait video style transfer by introducing a novel <strong>VToonify</strong> framework.
Specifically, VToonify leverages the mid- and high-resolution layers of StyleGAN to render high-quality artistic portraits based on the multi-scale content features extracted by an encoder to better preserve the frame details. The resulting fully convolutional architecture accepts non-aligned faces in videos of variable size as input, contributing to complete face regions with natural motions in the output.
Our framework is compatible with existing StyleGAN-based image toonification models to extend them to video toonification, and inherits appealing features of these models for flexible style control on color and intensity.
This work presents two instantiations of VToonify built upon Toonify and DualStyleGAN for collection-based and exemplar-based portrait video style transfer, respectively.
Extensive experimental results demonstrate the effectiveness of our proposed VToonify framework over existing methods in generating high-quality and temporally-coherent artistic portrait videos with flexible style controls.</em></p>
</blockquote>
<p dir="auto"><strong>Features</strong>:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/18130694/189509940-91c5e1e2-83a8-491e-962e-64775e56d7f6.jpg"><img src="https://user-images.githubusercontent.com/18130694/189509940-91c5e1e2-83a8-491e-962e-64775e56d7f6.jpg" alt="overview"/></a></p>
<h2 dir="auto"><a id="user-content-updates" aria-hidden="true" href="#updates"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Updates</h2>
<ul dir="auto">
<li>[10/2022] Integrate <a href="https://gradio.app/" rel="nofollow">Gradio</a> interface into <a href="http://colab.research.google.com/github/williamyang1991/VToonify/blob/master/notebooks/inference_playground.ipynb" rel="nofollow">Colab notebook</a>. Enjoy the web demo!</li>
<li>[10/2022] Integrated to <g-emoji alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png">ðŸ¤—</g-emoji> <a href="https://huggingface.co/spaces/PKUWilliamYang/VToonify" rel="nofollow">Hugging Face</a>. Enjoy the web demo!</li>
<li>[09/2022] Input videos and video results are released.</li>
<li>[09/2022] Paper is released.</li>
<li>[09/2022] Code is released.</li>
<li>[09/2022] This website is created.</li>
</ul>
<h2 dir="auto"><a id="user-content-web-demo" aria-hidden="true" href="#web-demo"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Web Demo</h2>
<p dir="auto">Integrated into <a href="https://huggingface.co/spaces" rel="nofollow">Huggingface Spaces <g-emoji alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png">ðŸ¤—</g-emoji></a> using <a href="https://github.com/gradio-app/gradio">Gradio</a>. Try out the Web Demo <a href="https://huggingface.co/spaces/PKUWilliamYang/VToonify" rel="nofollow"><img src="https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565" alt="Hugging Face Spaces" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue"/></a></p>
<h2 dir="auto"><a id="user-content-installation" aria-hidden="true" href="#installation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h2>
<p dir="auto"><strong>Clone this repo:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/williamyang1991/VToonify.git
cd VToonify"><pre>git clone https://github.com/williamyang1991/VToonify.git
<span>cd</span> VToonify</pre></div>
<p dir="auto"><strong>Dependencies:</strong></p>
<p dir="auto">We have tested on:</p>
<ul dir="auto">
<li>CUDA 10.1</li>
<li>PyTorch 1.7.0</li>
<li>Pillow 8.3.1; Matplotlib 3.3.4; opencv-python 4.5.3; Faiss 1.7.1; tqdm 4.61.2; Ninja 1.10.2</li>
</ul>
<p dir="auto">All dependencies for defining the environment are provided in <code>environment/vtoonify_env.yaml</code>.
We recommend running this repository using <a href="https://docs.anaconda.com/anaconda/install/" rel="nofollow">Anaconda</a> (you may need to modify <code>vtoonify_env.yaml</code> to install PyTorch that matches your own CUDA version following <a href="https://pytorch.org/" rel="nofollow">https://pytorch.org/</a>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="conda env create -f ./environment/vtoonify_env.yaml"><pre>conda env create -f ./environment/vtoonify_env.yaml</pre></div>
<p dir="auto">If you have a problem regarding the cpp extention (fused and upfirdn2d), or no GPU is available, you may refer to <a href="https://brian.abelson.live/williamyang1991/VToonify/blob/main/model/stylegan/op_cpu#readme">CPU compatible version</a>.</p>
</article>
          </div></div>
  </body>
</html>
