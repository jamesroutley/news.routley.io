<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2">Original</a>
    <h1>DeepScaleR: Surpassing O1-Preview with a 1.5B Model by Scaling RL</h1>
    
    <div id="readability-page-1" class="page"><div><p id="19681902-c146-81ab-93b1-f8da25e09557"><strong>Michael Luo*, Sijun Tan*, Justin Wong‚Ä†, Xiaoxiang Shi, William Tang, Manan Roongta, Colin Cai, Jeffrey Luo</strong></p><p id="19681902-c146-813c-a43c-df83500d69cb"><strong>Advisors: Tianjun Zhang*, Li Erran Li, Raluca Ada Popa, Ion Stoica</strong></p><p id="19681902-c146-8145-960c-f6ea26cda736"><strong>*</strong>: Project Leads; <strong>‚Ä†</strong>: Significant Contributor</p><figure id="19681902-c146-8176-a552-c2e973dc27ef"><p><span>‚ú®</span></p><div><h3 id="19681902-c146-810a-8f1e-e4b0ed67216d">TL;DR</h3><p id="19681902-c146-8197-b3eb-f9d12640692d">RL magic is in the air! We introduce <code><strong>DeepScaleR-1.5B-Preview</strong></code>, a language model finetuned from <code><strong>Deepseek-R1-Distilled-Qwen-1.5B</strong></code> using simple reinforcement learning (RL). It achieves an impressive <strong>43.1% Pass@1</strong> accuracy on AIME2024 (+<strong>14.3% improvement</strong> over the base model), surpassing the performance of OpenAI‚Äôs <code>o1-preview</code> with just <strong>1.5B</strong> parameters. We <strong>open sourced </strong>our dataset, code and training logs for everyone to progress on scaling intelligence with RL.</p><p id="19681902-c146-8120-b700-f279662bcb8a">üåê <a href="https://agentica-project.com/">Website</a>, üë®‚Äçüíª¬†<a href="https://github.com/agentica-project/deepscaler">Github</a>, ü§ó¬†<a href="https://huggingface.co/agentica-org/DeepScaleR-1.5B-Preview">HF Model</a>, ü§ó¬†<a href="https://huggingface.co/datasets/agentica-org/DeepScaleR-Preview-Dataset">HF Dataset</a>, üìà¬†<a href="https://wandb.ai/mluo/deepscaler-1.5b">Wandb Logs</a>, üîé¬†<a href="https://drive.google.com/file/d/1V_rYKoL35WmubbmWN6PeFg4zo5QOug8X/view?usp=sharing">Eval Logs</a></p></div></figure><table id="19681902-c146-810d-b98a-c1766a1746d9"><thead><tr id="19681902-c146-81af-b43d-f5e3559e7590"><th id=":ds?">Model</th><th id="^yFv">AIME 2024</th><th id="`O?I">MATH 500</th><th id="ldX&gt;">AMC 2023</th><th id="J|FZ">Minerva Math</th><th id="\=Ir">Olympiad Bench</th><th id="_UaU">Avg.</th></tr></thead><tbody><tr id="19681902-c146-8153-a78e-eae12504dc2d"><th id=":ds?"><strong>DeepScaleR-1.5B-Preview</strong></th><td id="^yFv"><strong>43.1</strong></td><td id="`O?I"><strong>87.8</strong></td><td id="ldX&gt;"><strong>73.6</strong></td><td id="J|FZ"><strong>30.2</strong></td><td id="\=Ir"><strong>50.0</strong></td><td id="_UaU"><strong>57.0</strong></td></tr><tr id="19681902-c146-815f-af57-e8feb365c1a2"><th id=":ds?">DeepSeek-R1-Distill-Qwen-1.5B</th><td id="^yFv">28.8</td><td id="`O?I">82.8</td><td id="ldX&gt;">62.9</td><td id="J|FZ">26.5</td><td id="\=Ir">43.3</td><td id="_UaU">48.9</td></tr><tr id="19681902-c146-815e-9199-fa467c0760ed"><th id=":ds?">O1-Preview</th><td id="^yFv">40.0</td><td id="`O?I">81.4</td><td id="ldX&gt;">-</td><td id="J|FZ">-</td><td id="\=Ir">-</td><td id="_UaU">-</td></tr></tbody></table><figure id="19681902-c146-8136-9e3c-e0dc650c8a68"><a href="https://pretty-radio-b75.notion.site/image/attachment%3Ad130adab-3b12-49f9-a058-6f5e701e41dd%3Aimage.png?table=block&amp;id=19681902-c146-8136-9e3c-e0dc650c8a68&amp;userId=&amp;cache=v2"><img src="https://pretty-radio-b75.notion.site/image/attachment%3Ad130adab-3b12-49f9-a058-6f5e701e41dd%3Aimage.png?table=block&amp;id=19681902-c146-8136-9e3c-e0dc650c8a68&amp;userId=&amp;cache=v2"/></a><figcaption>Figure1: DeepScaleR‚Äôs Pass@1 accuracy on AIME2024 as training progresses. At step 1040 and 1520, the context length is extended to 16K and 24K. </figcaption></figure><p id="19681902-c146-8157-a39c-f6e0214566fa">In this blog, we take a step towards unveiling the recipe of using RL to turn a small model into a strong reasoning model. We introduce <strong>DeepScaleR-1.5B-Preview</strong>, trained on 40K high-quality math problems with 3,800 A100 hours ($4500), outperforming OpenAI‚Äôs o1-preview on multiple competition-level math benchmarks.</p><p id="19681902-c146-81d3-9d2c-d9bde7b22e83">The recent open-source release of <a href="https://api-docs.deepseek.com/news/news250120">Deepseek-R1</a> (a model comparable to <a href="https://openai.com/index/openai-o1-system-card/">OpenAI‚Äôs o1</a>) marks a significant leap forward in democratizing reasoning models. Yet, its‚Äô exact training recipe, hyperparameters, and underlying systems are still unavailable. In this work, we take a major step towards a fully open-recipe that scales up RL for reasoning models. </p><p id="19681902-c146-81bd-a2a6-fe6c61c39ae2">One of the biggest challenges in scaling RL is the high computational cost. For instance, we found that directly replicating DeepSeek-R1‚Äôs experiments (‚©æ32K context, ~8000 steps) takes at least <strong>70,000 </strong>A100 GPU hours‚Äîeven for a 1.5B model. To address this, we leverage a distilled model and introduce a novel iterative lengthening scheme for RL, reducing the compute requirement to just <strong>3,800 </strong>A100 GPU hours‚Äîan <strong>18.42√ó reduction</strong>‚Äîwhile achieving performance surpassing OpenAI‚Äôs <code>o1-preview</code> with just a 1.5B model.</p><p id="19681902-c146-81db-98da-f7bbe26981e8">Our work demonstrates that developing customized reasoning models through RL can be both scalable and cost-efficient. In the remaining blog post, we‚Äôll walk through our dataset curation and training approach, present evaluation results, and share key insights from our findings.</p><h2 id="19681902-c146-8109-84dd-c33ee8deb0b6">Dataset Curation</h2><p id="19681902-c146-81c0-8850-ca99bf7698af">For our training dataset, we compiled <strong>AIME problems from 1984-2023</strong> and <strong>AMC problems prior to 2023</strong>, along with questions from the <a href="https://omni-math.github.io/">Omni-MATH</a> and <a href="https://github.com/RUCAIBox/Slow_Thinking_with_LLMs">Still</a> datasets, which feature problems from various national and international math competitions.</p><p id="19681902-c146-81c3-b140-e990ad42cb24">Our data processing pipeline consists of three key steps:</p><ol type="1" id="19681902-c146-8158-a5d0-e9263711d5cb" start="1"><li><strong>Extracting Answers</strong>: For datasets such as AMC and AIME, we use <code>gemini-1.5-pro-002</code> to extract answers from official AoPS solutions.</li></ol><ol type="1" id="19681902-c146-81c5-9093-e0dcf9c2032e" start="2"><li><strong>Removing Redundant Questions</strong>: We employ RAG with embeddings from <code>sentence-transformers/all-MiniLM-L6-v2</code> to eliminate duplicate problems. To prevent data contamination, we also check for overlaps between the training and test sets.</li></ol><ol type="1" id="19681902-c146-81d3-8438-e7ee7a83b1c4" start="3"><li><strong>Filtering Ungradable Questions</strong>: Some datasets, such as Omni-MATH, include problems that cannot be evaluated using <code>sympy</code> and require an LLM judge. Since using LLM judges may slow down training and introduce noisy reward signals, we apply an additional filtering step to remove these ungradable questions.</li></ol><p id="19681902-c146-819d-815c-c78cf390e9c7">After deduplication and filtering, our final training dataset consists of approximately <strong>40,000</strong> unique problem-answer pairs. We will expand our dataset for future runs.</p></div></div>
  </body>
</html>
