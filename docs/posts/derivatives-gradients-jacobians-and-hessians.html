<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.demofox.org/2025/08/16/derivatives-gradients-jacobians-and-hessians-oh-my/">Original</a>
    <h1>Derivatives, Gradients, Jacobians and Hessians</h1>
    
    <div id="readability-page-1" class="page"><div>
				
<p>This article explains how these four things fit together and shows some examples of what they are used for.</p>



<h2>Derivatives</h2>



<p>Derivatives are the most fundamental concept in calculus.  If you have a function, a derivative tells you how much that function changes at each point.</p>



<p>If we start with the function <img src="https://s0.wp.com/latex.php?latex=y%3Dx%5E2-6x%2B13&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=y%3Dx%5E2-6x%2B13&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=y%3Dx%5E2-6x%2B13&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="y=x^2-6x+13"/>, we can calculate the derivative as <img src="https://s0.wp.com/latex.php?latex=y%27%3D2x-6&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=y%27%3D2x-6&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=y%27%3D2x-6&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="y&#39;=2x-6"/>.  Here are those two functions graphed.</p>



<figure><img data-attachment-id="17897" data-permalink="https://blog.demofox.org/2025/08/16/derivatives-gradients-jacobians-and-hessians-oh-my/image-118/#main" data-orig-file="https://blog.demofox.org/wp-content/uploads/2025/08/image-4.png" data-orig-size="664,347" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://blog.demofox.org/wp-content/uploads/2025/08/image-4.png?w=300" data-large-file="https://blog.demofox.org/wp-content/uploads/2025/08/image-4.png?w=664" width="664" height="347" src="https://blog.demofox.org/wp-content/uploads/2025/08/image-4.png?w=664" alt=""/></figure>







<figure><img data-attachment-id="17892" data-permalink="https://blog.demofox.org/2025/08/16/derivatives-gradients-jacobians-and-hessians-oh-my/image-116/#main" data-orig-file="https://blog.demofox.org/wp-content/uploads/2025/08/image-2.png" data-orig-size="664,340" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://blog.demofox.org/wp-content/uploads/2025/08/image-2.png?w=300" data-large-file="https://blog.demofox.org/wp-content/uploads/2025/08/image-2.png?w=664" width="664" height="340" src="https://blog.demofox.org/wp-content/uploads/2025/08/image-2.png?w=664" alt=""/></figure>







<p>One use of derivatives is for optimization – also known as finding the lowest part on a graph.</p>



<p>If you were at <img src="https://s0.wp.com/latex.php?latex=x+%3D+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=x+%3D+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=x+%3D+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="x = 1"/> and wanted to know whether you should go left or right to get lower, the derivative can tell you.  Plugging 1 into <img src="https://s0.wp.com/latex.php?latex=2x-6&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=2x-6&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=2x-6&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="2x-6"/> gives the value -4. A negative derivative means taking a step to the right will make the y value go down, so going right is down hill. We could take a step to the right and check the derivative again to see if we’ve walked far enough.  As we are taking steps, if the derivative becomes positive, that means we went too far and need to turn around, and start going left.  If we shrink our step size whenever we go too far in either direction, we can get arbitrarily close to the actual minimum point on the graph.</p>



<p>What I just described is an iterative optimization method that is similar to gradient descent. Gradient descent simulates a ball rolling down hill to find the lowest point that we can, adjusting step size, and even adding momentum to try and not get stuck in places that are not the true minimum.</p>



<p>We can make an observation though: The minimum of a function is flat, and has a derivative of 0. If not, that would mean it was on a hill, which means that going either left or right is lower, so it wouldn’t be the minimum.</p>



<p>Armed with this knowledge, another way to use derivatives to find the minimum is to find where the derivative is 0.  We can do that by solving the equation <img src="https://s0.wp.com/latex.php?latex=2x-6+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=2x-6+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=2x-6+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="2x-6 = 0"/> and getting the value <img src="https://s0.wp.com/latex.php?latex=x%3D3&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=x%3D3&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=x%3D3&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="x=3"/>.  Without iteration, we found that the minimum of the function is at <img src="https://s0.wp.com/latex.php?latex=x%3D3&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=x%3D3&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=x%3D3&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="x=3"/> and we can plug 3 into the original equation <img src="https://s0.wp.com/latex.php?latex=y%3Dx%5E2-6x%2B13&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=y%3Dx%5E2-6x%2B13&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=y%3Dx%5E2-6x%2B13&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="y=x^2-6x+13"/> to find out that the minimum y value is 4.</p>



<p>Things get more complicated when the functions are higher order than quadratic.  Higher order functions have both minimums and maximums, and both of those have 0 derivatives. Also, if the <img src="https://s0.wp.com/latex.php?latex=x%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=x%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=x%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="x^2"/> term of a quadratic is negative, then it only has a maximum, instead of a minimum.</p>



<p>Higher dimensional functions also get more complex, where for instance you could have a point on a two dimensional function <img src="https://s0.wp.com/latex.php?latex=z%3Df%28x%2Cy%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=z%3Df%28x%2Cy%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=z%3Df%28x%2Cy%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="z=f(x,y)"/> that is a local minimum for x but a local maximum for y. The gradient will be zero in each direction, despite it not being a minimum, and the simulated ball will get stuck.</p>



<figure><img data-attachment-id="17939" data-permalink="https://blog.demofox.org/2025/08/16/derivatives-gradients-jacobians-and-hessians-oh-my/saddle_point-svg/#main" data-orig-file="https://blog.demofox.org/wp-content/uploads/2025/08/saddle_point.svg_.png" data-orig-size="615,480" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="Saddle_point.svg" data-image-description="" data-image-caption="" data-medium-file="https://blog.demofox.org/wp-content/uploads/2025/08/saddle_point.svg_.png?w=300" data-large-file="https://blog.demofox.org/wp-content/uploads/2025/08/saddle_point.svg_.png?w=615" width="615" height="480" src="https://blog.demofox.org/wp-content/uploads/2025/08/saddle_point.svg_.png?w=615" alt=""/></figure>



<h2>Gradients</h2>



<p>Speaking of higher dimensional functions, that is where gradients come in.</p>



<p>If you have a function <img src="https://s0.wp.com/latex.php?latex=w%3Df%28x%2Cy%2Cz%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=w%3Df%28x%2Cy%2Cz%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=w%3Df%28x%2Cy%2Cz%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="w=f(x,y,z)"/>, a gradient is a vector of derivatives, where you consider changing only one variable at a time, leaving the other variables constant.  The notation for a gradient looks like this:</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cnabla+f%28x%2Cy%2Cz%29+%3D+%5Cbegin%7Bbmatrix%7D+%5Cfrac%7B%5Cpartial+w%7D%7B%5Cpartial+x%7D+%26+%5Cfrac%7B%5Cpartial+w%7D%7B%5Cpartial+y%7D+%26+%5Cfrac%7B%5Cpartial+w%7D%7B%5Cpartial+z%7D+%5Cend%7Bbmatrix%7D+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cnabla+f%28x%2Cy%2Cz%29+%3D+%5Cbegin%7Bbmatrix%7D+%5Cfrac%7B%5Cpartial+w%7D%7B%5Cpartial+x%7D+%26+%5Cfrac%7B%5Cpartial+w%7D%7B%5Cpartial+y%7D+%26+%5Cfrac%7B%5Cpartial+w%7D%7B%5Cpartial+z%7D+%5Cend%7Bbmatrix%7D+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cnabla+f%28x%2Cy%2Cz%29+%3D+%5Cbegin%7Bbmatrix%7D+%5Cfrac%7B%5Cpartial+w%7D%7B%5Cpartial+x%7D+%26+%5Cfrac%7B%5Cpartial+w%7D%7B%5Cpartial+y%7D+%26+%5Cfrac%7B%5Cpartial+w%7D%7B%5Cpartial+z%7D+%5Cend%7Bbmatrix%7D+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\nabla f(x,y,z) = \begin{bmatrix} \frac{\partial w}{\partial x} &amp; \frac{\partial w}{\partial y} &amp; \frac{\partial w}{\partial z} \end{bmatrix} "/></p>



<p>Looking at a single entry in the vector, <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial+w%7D%7B%5Cpartial+x%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial+w%7D%7B%5Cpartial+x%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial+w%7D%7B%5Cpartial+x%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\frac{\partial w}{\partial x}"/>, that means “The derivative of w with respect to x”. Another way of saying that is “If you added 1 to x before plugging it into the function, this is how much w would change, if the function was a straight line”. These are called partial derivatives, because they are derivatives of one variable, in a function that takes multiple variables.</p>



<p>Let’s work through calculating the gradient of the function <img src="https://s0.wp.com/latex.php?latex=w%3D3x%5E2%2B6yz%5E3%2B4&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=w%3D3x%5E2%2B6yz%5E3%2B4&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=w%3D3x%5E2%2B6yz%5E3%2B4&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="w=3x^2+6yz^3+4"/>.</p>



<p>To calculate the derivative of w with regard to x (<img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial+w%7D%7B%5Cpartial+x%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial+w%7D%7B%5Cpartial+x%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial+w%7D%7B%5Cpartial+x%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\frac{\partial w}{\partial x}"/>), we take the derivative of the function as usual, but we only treat x as a variable, and all other variables as constants. That gives us with <img src="https://s0.wp.com/latex.php?latex=6x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=6x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=6x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="6x"/>.</p>



<p>Calculating the derivative of w with regard to y, we treat y as a variable and all others as constants to get: <img src="https://s0.wp.com/latex.php?latex=6z%5E3&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=6z%5E3&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=6z%5E3&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="6z^3"/>.</p>



<p>Lastly, to calculate the derivative of w with regard to z, we treat z as a variable and all others as constants. That gives us <img src="https://s0.wp.com/latex.php?latex=18yz%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=18yz%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=18yz%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="18yz^2"/>.</p>



<p>The full gradient of the function is: <img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Bbmatrix%7D+6x+%26+6z%5E3+%26+18yz%5E2+%5Cend%7Bbmatrix%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cbegin%7Bbmatrix%7D+6x+%26+6z%5E3+%26+18yz%5E2+%5Cend%7Bbmatrix%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cbegin%7Bbmatrix%7D+6x+%26+6z%5E3+%26+18yz%5E2+%5Cend%7Bbmatrix%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\begin{bmatrix} 6x &amp; 6z^3 &amp; 18yz^2 \end{bmatrix}"/>.</p>



<p>An interesting thing about gradients is that when you calculate them for a specific point, they give a vector that points in the direction of the biggest increase in the function, or equivalently, in the steepest uphill direction.  The opposite direction of the gradient is the biggest decrease of the function, or the steepest downhill direction.  This is why gradients are used in the optimization method “Gradient Descent”. The gradient (multiplied by a step size) is subtracted from a point to move it down hill.</p>



<p>Besides optimization, gradients can also be used in rendering.  For instance, here it’s used for rendering anti aliased signed distance fields: <a href="https://iquilezles.org/articles/distance/">https://iquilezles.org/articles/distance/</a></p>



<h2>Jacobian Matrix</h2>



<p>Let’s say you had a function that took in multiple values and gave out multiple values: <img src="https://s0.wp.com/latex.php?latex=v%2Cw+%3Df%28x%2Cy%2Cz%29+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=v%2Cw+%3Df%28x%2Cy%2Cz%29+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=v%2Cw+%3Df%28x%2Cy%2Cz%29+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="v,w =f(x,y,z) "/>.</p>



<p>We could calculate the gradient of this function for v, and we could calculate it for w.  If we put those two gradient vectors together to make a matrix, we would get the Jacobian matrix! You can also think of a gradient vector as being the Jacobian matrix of a function that outputs a single scalar value, instead of a vector.</p>



<p>Here is the Jacobian for <img src="https://s0.wp.com/latex.php?latex=v%2Cw+%3Df%28x%2Cy%2Cz%29+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=v%2Cw+%3Df%28x%2Cy%2Cz%29+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=v%2Cw+%3Df%28x%2Cy%2Cz%29+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="v,w =f(x,y,z) "/>:</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BJ%7D+%3D+%5Cbegin%7Bbmatrix%7D+%5Cfrac%7B%5Cpartial+v%7D%7B%5Cpartial+x%7D+%26+%5Cfrac%7B%5Cpartial+v%7D%7B%5Cpartial+y%7D+%26+%5Cfrac%7B%5Cpartial+v%7D%7B%5Cpartial+z%7D+%5C%5C+%5Cfrac%7B%5Cpartial+w%7D%7B%5Cpartial+x%7D+%26+%5Cfrac%7B%5Cpartial+w%7D%7B%5Cpartial+y%7D+%26+%5Cfrac%7B%5Cpartial+w%7D%7B%5Cpartial+z%7D+%5Cend%7Bbmatrix%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BJ%7D+%3D+%5Cbegin%7Bbmatrix%7D+%5Cfrac%7B%5Cpartial+v%7D%7B%5Cpartial+x%7D+%26+%5Cfrac%7B%5Cpartial+v%7D%7B%5Cpartial+y%7D+%26+%5Cfrac%7B%5Cpartial+v%7D%7B%5Cpartial+z%7D+%5C%5C+%5Cfrac%7B%5Cpartial+w%7D%7B%5Cpartial+x%7D+%26+%5Cfrac%7B%5Cpartial+w%7D%7B%5Cpartial+y%7D+%26+%5Cfrac%7B%5Cpartial+w%7D%7B%5Cpartial+z%7D+%5Cend%7Bbmatrix%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbb%7BJ%7D+%3D+%5Cbegin%7Bbmatrix%7D+%5Cfrac%7B%5Cpartial+v%7D%7B%5Cpartial+x%7D+%26+%5Cfrac%7B%5Cpartial+v%7D%7B%5Cpartial+y%7D+%26+%5Cfrac%7B%5Cpartial+v%7D%7B%5Cpartial+z%7D+%5C%5C+%5Cfrac%7B%5Cpartial+w%7D%7B%5Cpartial+x%7D+%26+%5Cfrac%7B%5Cpartial+w%7D%7B%5Cpartial+y%7D+%26+%5Cfrac%7B%5Cpartial+w%7D%7B%5Cpartial+z%7D+%5Cend%7Bbmatrix%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\mathbb{J} = \begin{bmatrix} \frac{\partial v}{\partial x} &amp; \frac{\partial v}{\partial y} &amp; \frac{\partial v}{\partial z} \\ \frac{\partial w}{\partial x} &amp; \frac{\partial w}{\partial y} &amp; \frac{\partial w}{\partial z} \end{bmatrix}"/></p>



<p>If that’s hard to read, the top row is the gradient for v, and the bottom row is the gradient for w.</p>



<p>When you evaluate the Jacobian matrix at a specific point in space (of whatever space the input parameters are in), it tells you how the space is warped in that location – like how much it is rotated and squished. You can also take the determinant of the Jacobian to see if things in that area get bigger (determinant greater than 1), smaller (determinant less than 1 but greater than 0), or if they get flipped inside out (determinant is negative). If the determinant is zero, it means it squishes everything into a single point (or line, etc. at least one dimension is scaled to 0), and also means that the operation can’t be reversed (the matrix can’t be inverted).</p>



<p>Here’s a great 10 minute video that goes into Jacobian Matrices a little more deeply and shows how they can be useful in machine learning: <a href="https://www.youtube.com/watch?v=AdV5w8CY3pw">https://www.youtube.com/watch?v=AdV5w8CY3pw</a></p>



<p>Since Jacobians describe warping of space, they are also useful in computer graphics, where for instance, you might want to use alpha transparency to fade an object out over a specific number of pixels to perform anti aliasing, but the object may be described in polar coordinates, or be warped in way that makes it hard to know how many units to fade out over in that modified space. This has come up for me when doing 2D SDF rendering in shadertoy.</p>



<h2>Hessian Matrix</h2>



<p>If you take all partial derivatives (aka make a gradient) of a function <img src="https://s0.wp.com/latex.php?latex=w%3Df%28x%2Cy%2Cz%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=w%3Df%28x%2Cy%2Cz%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=w%3Df%28x%2Cy%2Cz%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="w=f(x,y,z)"/>, that will give you a vector with three partial derivatives out – one for x, one for y, one for z.</p>



<p>What if we wanted to get the 2nd derivatives? In other words, what if we wanted to take the derivative of the derivatives?</p>



<p>You could just take the derivative with respect to the same variables again, but to really understand the second derivatives of the function, we should take all three partial derivatives (one for x, one for y, one for z) of EACH of those three derivatives in the gradient.</p>



<p>That would give us 9 derivatives total, and that is exactly what the Hessian Matrix is.</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BH%7D+%3D+%5Cbegin%7Bbmatrix%7D+%5Cfrac%7B%5Cpartial%5E2+w%7D%7B%5Cpartial+x%5E2%7D+%26+%5Cfrac%7B%5Cpartial%5E2+w%7D%7B%5Cpartial+xy%7D+%26+%5Cfrac%7B%5Cpartial%5E2+w%7D%7B%5Cpartial+xz%7D+%5C%5C+%5Cfrac%7B%5Cpartial%5E2+w%7D%7B%5Cpartial+yx%7D+%26+%5Cfrac%7B%5Cpartial%5E2+w%7D%7B%5Cpartial+y%5E2%7D+%26+%5Cfrac%7B%5Cpartial%5E2+w%7D%7B%5Cpartial+yz%7D+%5C%5C+%5Cfrac%7B%5Cpartial%5E2+w%7D%7B%5Cpartial+zx%7D+%26+%5Cfrac%7B%5Cpartial%5E2+w%7D%7B%5Cpartial+zy%7D+%26+%5Cfrac%7B%5Cpartial%5E2+w%7D%7B%5Cpartial+z%5E2%7D+%5Cend%7Bbmatrix%7D+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BH%7D+%3D+%5Cbegin%7Bbmatrix%7D+%5Cfrac%7B%5Cpartial%5E2+w%7D%7B%5Cpartial+x%5E2%7D+%26+%5Cfrac%7B%5Cpartial%5E2+w%7D%7B%5Cpartial+xy%7D+%26+%5Cfrac%7B%5Cpartial%5E2+w%7D%7B%5Cpartial+xz%7D+%5C%5C+%5Cfrac%7B%5Cpartial%5E2+w%7D%7B%5Cpartial+yx%7D+%26+%5Cfrac%7B%5Cpartial%5E2+w%7D%7B%5Cpartial+y%5E2%7D+%26+%5Cfrac%7B%5Cpartial%5E2+w%7D%7B%5Cpartial+yz%7D+%5C%5C+%5Cfrac%7B%5Cpartial%5E2+w%7D%7B%5Cpartial+zx%7D+%26+%5Cfrac%7B%5Cpartial%5E2+w%7D%7B%5Cpartial+zy%7D+%26+%5Cfrac%7B%5Cpartial%5E2+w%7D%7B%5Cpartial+z%5E2%7D+%5Cend%7Bbmatrix%7D+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbb%7BH%7D+%3D+%5Cbegin%7Bbmatrix%7D+%5Cfrac%7B%5Cpartial%5E2+w%7D%7B%5Cpartial+x%5E2%7D+%26+%5Cfrac%7B%5Cpartial%5E2+w%7D%7B%5Cpartial+xy%7D+%26+%5Cfrac%7B%5Cpartial%5E2+w%7D%7B%5Cpartial+xz%7D+%5C%5C+%5Cfrac%7B%5Cpartial%5E2+w%7D%7B%5Cpartial+yx%7D+%26+%5Cfrac%7B%5Cpartial%5E2+w%7D%7B%5Cpartial+y%5E2%7D+%26+%5Cfrac%7B%5Cpartial%5E2+w%7D%7B%5Cpartial+yz%7D+%5C%5C+%5Cfrac%7B%5Cpartial%5E2+w%7D%7B%5Cpartial+zx%7D+%26+%5Cfrac%7B%5Cpartial%5E2+w%7D%7B%5Cpartial+zy%7D+%26+%5Cfrac%7B%5Cpartial%5E2+w%7D%7B%5Cpartial+z%5E2%7D+%5Cend%7Bbmatrix%7D+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\mathbb{H} = \begin{bmatrix} \frac{\partial^2 w}{\partial x^2} &amp; \frac{\partial^2 w}{\partial xy} &amp; \frac{\partial^2 w}{\partial xz} \\ \frac{\partial^2 w}{\partial yx} &amp; \frac{\partial^2 w}{\partial y^2} &amp; \frac{\partial^2 w}{\partial yz} \\ \frac{\partial^2 w}{\partial zx} &amp; \frac{\partial^2 w}{\partial zy} &amp; \frac{\partial^2 w}{\partial z^2} \end{bmatrix} "/></p>



<p>If that is hard to read, each row is the gradient, but then the top row is differentiated with respect to x, the middle row is differentiated with respect to y, and the bottom row is differentiated with respect to z.</p>



<p>Another way to think about the Hessian is that it’s the transpose of the Jacobian matrix of the gradient. That’s a mouthful, but it hopefully helps you better see how these things fit together.</p>



<p>Taking the 2nd derivative of a function tells you how the function curves, which can be useful (again!) for optimization.</p>



<p>This 11 minute video talks about how the Hessian is used in optimization to get the answer faster, by knowing the curvature of the functions: <a href="https://www.youtube.com/watch?v=W7S94pq5Xuo">https://www.youtube.com/watch?v=W7S94pq5Xuo</a></p>



<p>Where a derivative approximates a function locally with a line, a second order derivative approximates a function locally with a quadratic. So, a Hessian can let you model a function at a point as a quadratic type of function, and then do the neat trick from the derivative section of going straight to the minimum instead of having to iterate.  That takes you to the minimum of the quadratic, not the minimum of the function you are trying to optimize, but that can be a great speed up for certain types of functions.  You can also use the eigenvalues of the Hessian to know if it’s positive definite – aka if it’s a parabola pointing upwards and so actually has a minimum – vs if it’s pointing downwards, or is a saddle point.  The eigenvectors can tell you the orientation of the paraboloid as well.  Here is more information on analyzing a Hessian matrix: <a href="https://web.stanford.edu/group/sisl/k12/optimization/MO-unit4-pdfs/4.10applicationsofhessians.pdf">https://web.stanford.edu/group/sisl/k12/optimization/MO-unit4-pdfs/4.10applicationsofhessians.pdf</a></p>



<p>Calculating the Hessian can be quite costly both computationally and in regards to how much memory it uses, for machine learning problems that have millions of parameters or more.  In those cases, there are quasi newton methods, which you can watch an 11 minute video about here: <a href="https://www.youtube.com/watch?v=UvGQRAA8Yms">https://www.youtube.com/watch?v=UvGQRAA8Yms</a></p>



<p>Thanks for reading and hopefully this helps clear up some scary sounding words!</p>
			</div></div>
  </body>
</html>
