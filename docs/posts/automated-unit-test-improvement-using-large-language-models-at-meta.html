<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2402.09171">Original</a>
    <h1>Automated Unit Test Improvement Using Large Language Models at Meta</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
                
    <p><a href="https://arxiv.org/pdf/2402.09171.pdf">Download PDF</a></p><blockquote>
            <span>Abstract:</span>This paper describes Meta&#39;s TestGen-LLM tool, which uses LLMs to automatically improve existing human-written tests. TestGen-LLM verifies that its generated test classes successfully clear a set of filters that assure measurable improvement over the original test suite, thereby eliminating problems due to LLM hallucination. We describe the deployment of TestGen-LLM at Meta test-a-thons for the Instagram and Facebook platforms. In an evaluation on Reels and Stories products for Instagram, 75% of TestGen-LLM&#39;s test cases built correctly, 57% passed reliably, and 25% increased coverage. During Meta&#39;s Instagram and Facebook test-a-thons, it improved 11.5% of all classes to which it was applied, with 73% of its recommendations being accepted for production deployment by Meta software engineers. We believe this is the first report on industrial scale deployment of LLM-generated code backed by such assurances of code improvement.
    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Alexandru Marginean [<a href="https://arxiv.org/show-email/af902b12/2402.09171">view email</a>]      </p></div></div>
  </body>
</html>
