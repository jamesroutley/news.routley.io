<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.eficode.com/blog/the-future-of-kubernetes-and-why-developers-should-look-beyond-kubernetes-in-2022">Original</a>
    <h1>The Future of Kubernetes</h1>
    
    <div id="readability-page-1" class="page"><div id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text"><p>Kubernetes is ubiquitous in container orchestration, and its popularity has yet to weaken. This does, however, not mean that evolution in the container orchestration space is at a stand-still. This blog will put forward some arguments for why Kubernetes users, and developers, in particular, should look beyond the traditional Kubernetes we have learned over the past few years to paradigms that may be better suited for cloud-native applications.<!--more--></p>
<p><em>If you want to discuss this topic further with Kelsey Hightower, join our free online event on March 8: The DEVOPS Conference.  <!--HubSpot Call-to-Action Code --><span id="hs-cta-wrapper-0231fa05-a9e8-44eb-a2ea-53ee0970983c"><span id="hs-cta-0231fa05-a9e8-44eb-a2ea-53ee0970983c"><!--[if lte IE 8]><div id="hs-cta-ie-element"></div><![endif]--><a href="https://cta-redirect.hubspot.com/cta/redirect/2714969/0231fa05-a9e8-44eb-a2ea-53ee0970983c" target="_blank" rel="noopener"><img id="hs-cta-img-0231fa05-a9e8-44eb-a2ea-53ee0970983c" src="https://no-cache.hubspot.com/cta/default/2714969/0231fa05-a9e8-44eb-a2ea-53ee0970983c.png" alt="See agenda and sign up"/></a></span></span><!-- end HubSpot Call-to-Action Code --></em></p>
<h2>The rise of Kubernetes</h2>
<p>Part of the reason why Kubernetes has become so popular is that it was built on top of Docker. Containers have a long history in Linux and BSD variants; however, Docker made containers extremely popular by focusing on the user experience and made building and running containers very easy.  Kubernetes built on the popularity of containers and made running (aka. orchestrating) containers on a cluster of compute nodes easy.</p>
<p>Another reason for Kubernetes&#39; popularity and extensive adoption is that <strong>it didn&#39;t change the model for running software too much</strong>. It was reasonably easy to envision a path from how we ran software before Kubernetes to how we could run software on Kubernetes.</p>
<h2>You can’t teach old paradigms new tricks</h2>
<p>Building container images to freeze dependencies and a &#39;run everywhere&#39; experience combined with Kubernetes <a href="https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/deployment-v1/" rel="noopener" target="_blank"><span>Deployment</span></a> resource specifications to manage the orchestration of container replicas is very powerful. However,<strong> it&#39;s not radically different from how we operated VMs before Docker and Kubernetes</strong>. The small mental leap made it easy to adopt Kubernetes, but it is also why we should look beyond the &#39;traditional&#39; Kubernetes we know today.</p>
<p>This blog will look at the future of Kubernetes as seen from the developer&#39;s perspective. Generally, <strong>the Kubernetes we know today will disappear, and developers will not care</strong>. This is not to say that we will not have Kubernetes in our stack, but we will improve the way we build and operate applications using new abstractions, which are themselves built on top of Kubernetes. <strong>Applications will be built using platforms built on the Kubernetes platform</strong>:</p>
<p><a href="https://twitter.com/kelseyhightower/status/935252923721793536?lang=en" rel="noopener" target="_blank"><img src="https://www.eficode.com/hs-fs/hubfs/Screen%20Shot%202022-02-22%20at%201-40-58%20PM-png.png?width=1188&amp;name=Screen%20Shot%202022-02-22%20at%201-40-58%20PM-png.png" alt="Kelsey hightower&#39;s tweet" width="1188" loading="lazy" srcset="https://www.eficode.com/hs-fs/hubfs/Screen%20Shot%202022-02-22%20at%201-40-58%20PM-png.png?width=594&amp;name=Screen%20Shot%202022-02-22%20at%201-40-58%20PM-png.png 594w, https://www.eficode.com/hs-fs/hubfs/Screen%20Shot%202022-02-22%20at%201-40-58%20PM-png.png?width=1188&amp;name=Screen%20Shot%202022-02-22%20at%201-40-58%20PM-png.png 1188w, https://www.eficode.com/hs-fs/hubfs/Screen%20Shot%202022-02-22%20at%201-40-58%20PM-png.png?width=1782&amp;name=Screen%20Shot%202022-02-22%20at%201-40-58%20PM-png.png 1782w, https://www.eficode.com/hs-fs/hubfs/Screen%20Shot%202022-02-22%20at%201-40-58%20PM-png.png?width=2376&amp;name=Screen%20Shot%202022-02-22%20at%201-40-58%20PM-png.png 2376w, https://www.eficode.com/hs-fs/hubfs/Screen%20Shot%202022-02-22%20at%201-40-58%20PM-png.png?width=2970&amp;name=Screen%20Shot%202022-02-22%20at%201-40-58%20PM-png.png 2970w, https://www.eficode.com/hs-fs/hubfs/Screen%20Shot%202022-02-22%20at%201-40-58%20PM-png.png?width=3564&amp;name=Screen%20Shot%202022-02-22%20at%201-40-58%20PM-png.png 3564w" sizes="(max-width: 1188px) 100vw, 1188px"/></a></p>
<p>Interestingly, Linux was the platform upon which we built everything a decade or more ago. Linux is still ubiquitous, and part of our stack, but few developers care much about it because we have since added a few abstractions on top. It&#39;s the same that will happen to the traditional Kubernetes we know today.</p>

<h2>New paradigms sweep clean(er)</h2>
<h3>Security: OIDC is better than secrets</h3>
<p>Kubernetes provides a <a href="https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/secret-v1/" rel="noopener" target="_blank"><span>Secret</span></a> resource to specify static secrets such as API keys, passwords, etc. <strong>Developers should not use the Kubernetes Secret resources</strong>.</p>
<p>Explicit secrets encoded in Secret resources can be leaked and are troublesome to rotate and revoke. With GitOps workflows, secrets also need special attention to avoid being stored in clear-text. Applications should instead apply a role-based approach to authentication and authorization. This means that instead of &#39;things you know&#39; (passwords, API keys), application authentication and authorization should be based on &#39;who we are&#39;.</p>
<p>Strong identities are the foundation for all security. It does not make sense to encrypt network traffic if you are not sure about the identity of the server you communicate with. This is what certificates and Certificate Authorities do for HTTPS traffic, which generally secures the internet.</p>
<p>Kubernetes has a system for strong workload identity. All workloads are associated with service accounts, and they have short-lived OpenID-Connect (OIDC) <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection" rel="noopener" target="_blank"><span>identity-tokens issued by Kubernetes</span></a>. The Kubernetes API server signs these OIDC tokens, and other workloads can validate tokens through the Kubernetes API server. This provides strong identities for workloads running on Kubernetes and can be used as a foundation for role-based authentication and authorization.</p>
<p>Instead of using Kubernetes Secrets, developers should base authentication and authorization on OIDC tokens. This means that instead of, e.g., storing a database password in a Secret resource, we should ensure that our database only accepts requests when presented with a valid, unexpired token.</p>
<p>Examples of OIDC token usage to integrate with external systems are <a href="https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html" rel="noopener" target="_blank"><span>AWS IAM roles for service accounts</span></a> and <a href="https://www.vaultproject.io/docs/auth/kubernetes" rel="noopener" target="_blank"><span>Hashicorp Vault Kubernetes auth</span></a>.</p>
<p><span><!--HubSpot Call-to-Action Code --><span id="hs-cta-wrapper-11c9c508-89d8-42e8-a736-5865ebb3b271"><span id="hs-cta-11c9c508-89d8-42e8-a736-5865ebb3b271"><!--[if lte IE 8]><div id="hs-cta-ie-element"></div><![endif]--><a href="https://cta-redirect.hubspot.com/cta/redirect/2714969/11c9c508-89d8-42e8-a736-5865ebb3b271" target="_blank" rel="noopener"><img id="hs-cta-img-11c9c508-89d8-42e8-a736-5865ebb3b271" src="https://no-cache.hubspot.com/cta/default/2714969/11c9c508-89d8-42e8-a736-5865ebb3b271.png" alt="Learn more in this training course on OIDC"/></a></span></span><!-- end HubSpot Call-to-Action Code --></span></p>
<h3>Networking: Ingress does not cut the mustard</h3>
<p>Kubernetes provides an <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/" rel="noopener" target="_blank"><strong><span>Ingress</span></strong></a><strong> resource</strong> to specify how to route HTTP traffic into workloads. As Tim Hockin (Kubernetes co-founder) acknowledges, <a href="https://kubernetespodcast.com/episode/041-ingress/" rel="noopener" target="_blank"><span>there is a lot wrong with the Ingress resource</span></a>. The primary problem is that it only lets us manage the very basics of HTTP traffic routing. Allowing developers to use Ingress resources will be a headache for infrastructure and Site Reliability Engineering (SRE) teams that need to interconnect an extensive infrastructure and make it run reliably. The Ingress resource is too simple, and <strong>developers should not use it</strong> to configure networking.</p>
<p>The need for more control and programmability of the Kubernetes network can be seen in the rise of service meshes (see our training course on <a href="https://www.eficode.com/academy/introduction-to-the-istio-service-mesh-kiali-and-jaeger?hsLang=en" rel="noopener" target="_blank"><span>Istio service mesh, Kiali and Jaeger</span></a>). They divide the Ingress resource into multiple resources for a better separation of duties and provide additional functionality in routing, observability, security, and fault tolerance.</p>
<p>More and more abstractions built on top of Kubernetes assume a programmable network beyond what is possible with Ingress (Knative, Kubeflow, continuous-deployment tools like Argo Rollouts, etc.). This emphasizes that <strong>a more robust network model in Kubernetes is already a de-facto standard</strong>.</p>
<p>The Kubernetes community has evolved an &#39;Ingress v2&#39; – the <a href="https://kubernetes.io/blog/2021/04/22/evolving-kubernetes-networking-with-the-gateway-api/" rel="noopener" target="_blank"><span>gateway-API</span></a>. While this addresses some of the concerns of Ingress, it only covers a small subset of the functionalities that most service meshes support.</p>
<p>Kubernetes supports ACLs for limiting which workloads can communicate through the <a href="https://kubernetes.io/docs/reference/kubernetes-api/policy-resources/network-policy-v1/" rel="noopener" target="_blank"><span>NetworkPolicy</span></a> resource. This resource is implemented in the Kubernetes network plugins and often translates into Linux iptables filtering rules, i.e., an <strong>IP address-based solution much like firewalls – again, an old paradigm</strong>. Some service meshes extend the strong Kubernetes OIDC-based workload identities to implement mutual TLS between workloads. This brings confidentiality and authenticity to Kubernetes network communication based on stronger principles than IP addresses.</p>
<p>In Kubernetes application packaging, there is some divergence in how to include network configuration. Many Helm charts come with Ingress resource templates. However, as we move to more advanced network models, those definitions cannot be used. Looking forward, application deployments like Helm charts should consider <strong>network configuration an orthogonal concern</strong> that should be left out of the application deployment artifact. There may not be a one-size-fits-all solution regarding application network configuration, and organizations most likely want to develop their own &#39;routing-for-applications&#39; deployment artifacts.</p>
<p>Kubernetes made networking easy by creating a homogeneous network across all nodes in the cluster. If your application is multi-cluster or multi-cloud, it may similarly benefit from a homogeneous network across clusters or clouds. The Kubernetes network model does not do this, and you need something more capable like a service mesh.</p>
<p>Thus, from an organizational and architectural perspective, there are several reasons why developers should not program the network with Ingress resources. It is essential to consider the options with an overall organizational view to ensure a manageable and long-term viable approach to network configuration and management.</p>
<h3>Workload definition: To the point</h3>
<p>At the core of practically all Kubernetes applications is a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/" rel="noopener" target="_blank"><span>Deployment</span></a> resource. A Deployment is a resource that defines how our workload, in the form of containers inside Pods, should be executed. Deployment scaling can be controlled with a <a href="https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/horizontal-pod-autoscaler-v2/" rel="noopener" target="_blank"><span>HorizontalPodAutoscaler</span></a> (HPA) resource to account for varying capacity demand. HPAs often use container CPU load as a measure for adding or removing Pods, and due to the HPA algorithm often with a target utilization in the area of 70%. This means we are designing for a waste of 30%. Another reason for using conservative target utilizations is that the HPA often works with a response time of a minute or more. To handle varying capacity demand, we need some spare capacity while the HPA adds more Pods.</p>
<p>Managing workloads with Deployments and HPAs works well if our application sees slowly varying capacity demand. However, with the shift towards microservices, event-driven driven architectures, and functions (which handle one or possibly a few events/requests and then terminate), this form of workload management is far from ideal.</p>
<p>The <a href="https://keda.sh/" rel="noopener" target="_blank"><span>Kubernetes Event-Driven Autocaler</span></a> (KEDA) can improve the scaling behavior of microservices and fast-changing workloads such as functions. KEDA defines its own set of Kubernetes resources to define scaling behavior and can be considered an &#39;HPA v3&#39; (as the HPA resource is already at &#39;v2&#39;).</p>
<p>A framework that combines the Kubernetes Deployment model, scaling, and event and network routing is <a href="https://knative.dev/docs/" rel="noopener" target="_blank"><span>Knative</span></a>.  Knative is a platform that builds on top of Kubernetes and takes an opinionated view on workload management through a <a href="https://knative.dev/docs/serving/"><span>Knative-Service</span></a> resource. At the core of Knative is <a href="https://cloudevents.io/" rel="noopener" target="_blank"><span>CloudEvents</span></a>, and Knative services are basically functions triggered and scaled by events, either CloudEvents or plain HTTP requests. Knative uses a Pod sidecar to monitor event rates and thus scales very quickly on changes in event rates. Knative also supports scaling to zero and thus allows for a finer-grained workload scaling better suited for microservices and functions.</p>
<p>Knative services are implemented using traditional Kubernetes Deployments/Services, and updates to Knative services (e.g., a new container image) create parallel Kubernetes Deployment/Service resources. Knative uses this to implement blue/green and canary deployment patterns, with the routing of HTTP traffic being part of the Knative service resource definition.</p>
<p>Thus, the <strong>Knative service resource and its associated resources for defining routing of events become the primary resource for developers to use</strong> when defining their application deployment on Kubernetes.  Much like we today often interact with Kubernetes through Deployment resources and let Kubernetes handle Pods, using Knative means developers will mainly concern themself with the Knative service, and Deployments are handled by the Knative platform.</p>
<p>While I expect the Knative model to suit a large majority of use cases, your mileage may vary. If you instead are doing machine learning, then maybe <a href="https://knative.dev/docs/serving/" rel="noopener" target="_blank"><span>Kubeflow</span></a> is a better abstraction. If you are more focused on DevOps and delivery pipelines, then <a href="https://github.com/pivotal/kpack" rel="noopener" target="_blank"><span>kpack</span></a>, <a href="https://tekton.dev/" rel="noopener" target="_blank"><span>Tekton</span></a> or <a href="https://cartographer.sh/" rel="noopener" target="_blank"><span>Cartographer</span></a> may be the abstraction for you. <strong>Whatever you do on Kubernetes, there’s an abstraction for that!</strong></p>
<h3>Storage: Moving away from persistent volumes</h3>
<p>Kubernetes provides <a href="https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-v1/" rel="noopener" target="_blank"><span>PersistentVolume</span></a> and <a href="https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-claim-v1/" rel="noopener" target="_blank"><span>PersistentVolumeClaim</span></a> resources for managing storage for workloads. It&#39;s probably <strong>my least favorite resource to allow developers to use</strong> for anything but ephemeral cache data.</p>
<p>From a high-level perspective, a problem with PersistentVolumes (PVs) is that they <strong>combine the primary concern of our application with a storage concern</strong>, which is not an ideal cloud-native design pattern. The <a href="https://12factor.net/" rel="noopener" target="_blank"><span>twelve-factor app methodology</span></a> guides us to consider any <a href="https://12factor.net/backing-services" rel="noopener" target="_blank">backing services</a> as network-attached. This is due to how we horizontally scale workloads in Kubernetes and the management of data (think <a href="https://en.wikipedia.org/wiki/CAP_theorem" rel="noopener" target="_blank"><span>CAP theorem</span></a>).</p>
<p>PVs represent file systems of files and directories, and we operate on data with a POSIX file-system interface. Access rights are also based on a POSIX model, with users and groups being allowed read or write access. Not only is this model poorly matched to cloud-native application design, but it’s also tricky to use in practice, which means that most often, PVs are mounted in a &#39;container can access all data&#39; mode.</p>
<p><strong>Developers should build stateful applications that are stateless</strong>. This means data should be handled externally to the application using other abstractions than filesystems, e.g., in databases or <a href="https://en.wikipedia.org/wiki/Object_storage" rel="noopener" target="_blank"><span>object stores</span></a>. Database and object store applications may use PVs for their storage needs, but these systems should be administered by infrastructure/SRE teams and consumed as-a-service by developers.</p>
<p>A <strong>dramatic improvement in data security</strong> is possible when we consider storage as network-attached, e.g., consider object storage through REST APIs. With REST APIs, we can implement authentication and authorization through short-lived access tokens based on Kubernetes workload identities as described above.</p>
<p>With the adoption of a serverless workload pattern, we should expect more dynamic and shorter-lived workloads (e.g., serverless functions handling one event per Pod). The mismatch between workloads and &#39;old-fashioned disks&#39; becomes even more apparent in such situations.</p>
<p>In Kubernetes, the container storage interface (CSI) has been the interface for adding file-system and block storage to workloads through PVs. The Kubernetes special interest group on object storage is working on a <a href="https://container-object-storage-interface.github.io/" rel="noopener" target="_blank"><span>container object storage interface</span></a> (COSI) which may turn object storage into a first-class citizen in Kubernetes.</p>
<h2>O brave new world</h2>
<p>In this blog, I have argued that there are good reasons to look beyond the &#39;traditional&#39; Kubernetes resources when defining Kubernetes applications. This is not to say that we will never use the traditional resource types. There will still be legacy applications that we cannot easily convert, and SRE teams may still need to run stateful services that can be consumed by applications built by developers. This will particularly be the case for private cloud infrastructures.</p>
<p><strong>The future of Kubernetes is in the </strong><a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/" rel="noopener" target="_blank"><strong><span>custom resource definitions (CRDs) </span></strong></a><strong>and abstractions which we build on top of Kubernetes and make available to users through CRDs.</strong> Kubernetes becomes a control plane for abstractions, and it’s the CRDs of these abstractions that developers should focus on. Kubernetes control planes may manage resources inside Kubernetes or even outside Kubernetes as, e.g., <a href="https://www.eficode.com/blog/outgrowing-terraform-and-adopting-control-planes?hsLang=en" rel="noopener" target="_blank"><span>Crossplane manages cloud infrastructure.</span></a></p>
<p><img src="https://www.eficode.com/hs-fs/hubfs/graph%20future%20of%20kubernetes-1.png?width=1035&amp;name=graph%20future%20of%20kubernetes-1.png" alt="graph future of kubernetes-1" width="1035" loading="lazy" srcset="https://www.eficode.com/hs-fs/hubfs/graph%20future%20of%20kubernetes-1.png?width=518&amp;name=graph%20future%20of%20kubernetes-1.png 518w, https://www.eficode.com/hs-fs/hubfs/graph%20future%20of%20kubernetes-1.png?width=1035&amp;name=graph%20future%20of%20kubernetes-1.png 1035w, https://www.eficode.com/hs-fs/hubfs/graph%20future%20of%20kubernetes-1.png?width=1553&amp;name=graph%20future%20of%20kubernetes-1.png 1553w, https://www.eficode.com/hs-fs/hubfs/graph%20future%20of%20kubernetes-1.png?width=2070&amp;name=graph%20future%20of%20kubernetes-1.png 2070w, https://www.eficode.com/hs-fs/hubfs/graph%20future%20of%20kubernetes-1.png?width=2588&amp;name=graph%20future%20of%20kubernetes-1.png 2588w, https://www.eficode.com/hs-fs/hubfs/graph%20future%20of%20kubernetes-1.png?width=3105&amp;name=graph%20future%20of%20kubernetes-1.png 3105w" sizes="(max-width: 1035px) 100vw, 1035px"/></p>
<p>As summarized above, the majority of the traditional Kubernetes resources may have better alternatives for developers. Using alternatives will improve how we develop and operate cloud-native applications in the years to come. After all, Kubernetes is a platform for building platforms. It&#39;s not the end-game!</p>
</div></div>
  </body>
</html>
