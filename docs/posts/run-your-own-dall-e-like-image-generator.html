<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/">Original</a>
    <h1>Run your own DALL-E-like image generator</h1>
    
    <div id="readability-page-1" class="page"><div><p>In this post, we’ll look at getting setup with running your own A.I. image generator. You need a Linux system with a CUDA enabled card to get this working through to the end. I did get the basic txt-2img script working in Windows. Unfortunately, for the July released <em>retrieval-augmented diffusion models</em>, you need <code>scann</code> in order to index the openimages dataset, which is only available on Linux. <a href="https://docs.microsoft.com/en-us/windows/wsl/install">WSL</a> worked fine for me.</p><table><thead><tr><th><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807154420530.png" alt="dog lion - rdm model"/></th><th><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807154436894.png" alt="dog lion - rdm model"/></th><th><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807154515353.png" alt="dog lion - rdm model"/></th><th><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807154507821.png" alt="dog lion - rdm model"/></th></tr></thead></table><p>Here’s roughly what I gather you’ll need:</p><ul><li>Linux (WSL fine)</li><li>11.3 CUDA graphics</li><li>&gt;= 32GB RAM</li><li>50GB disk space (~30GB without openimages)</li><li>3xxx series card (I am just guessing here. This is all being done with a 3090 FE)</li></ul><p>I randomly stumbled upon the <a href="https://github.com/CompVis/latent-diffusion">latent-diffusion repo</a> while I was browsing AI subreddits. A few posts mentioned it and I didn’t know what it was so I went googling.</p><p>I am not a computer scientist, mathmatician, or any of the things that would be required to really understand what’s going on here. The paper about the new RAD models are understandable and interesting, but I haven’t done much research yet beyond that.</p><p><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807154208076.png#center" alt="image-20220807154208076"/></p><h2 id="text-to-image-vs-rad">Text-to-image vs. RAD</h2><p>There are 2 image generation techniques possible with Latent Diffusion. We will install and take a look at both.</p><p>I believe the txt2-img model that we’ll setup first is what we are used to with other image generation tools online – it makes a super low res image clip thinks is a good prompt match and denoises and upscales it.</p><p>The RAD model uses a configurable database of images as a reference AND does the diffusion like we are used to. This seems to mean it’s less good as a general purpose generator, but could be used with a specific training set you could feasibly create. I will try this in the future.</p><p>RAD also generates 768X768 images which is pretty impressive.</p><table><thead><tr><th><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807154240381.png#centered" alt="giant cantaloupe at the beach"/></th><th><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807155507820.png" alt="Giant cantaloupe at the beach"/></th></tr></thead></table><h2 id="text-to-image">Text-to-Image</h2><ul><li><p>Make sure you have <a href="https://developer.nvidia.com/cuda-toolkit">CUDA 11.3</a> installed and <code>conda</code> works in your shell. You can get <a href="https://docs.conda.io/en/latest/">Conda here</a></p></li><li><p><a href="https://pytorch.org/get-started/locally/">(Re)install Pytorch now</a>, lest ye end up in dependency hell later. I did this via <code>pip</code> but conda may work too.</p></li><li><p><a href="https://github.com/CompVis/latent-diffusion">Clone the repo to a drive with a lot of free space</a></p></li><li><p><code>cd</code> into the repo and run:</p><div><pre><code data-lang="bash">conda env create -f environment.yaml
conda activate ldm
</code></pre></div><p>This will setup the initial environment. If you get errors with Pytorch later and need to reinstall it, come back to environment.yaml and reinstall using the correct package versions.</p></li><li><p>Download the model and you’re all ready to go so long as your torch dependencies are all setup correct.</p></li></ul><div><pre><code data-lang="bash">mkdir -p models/ldm/text2img-large/
wget -O models/ldm/text2img-large/model.ckpt https://ommer-lab.com/files/latent-diffusion/nitro/txt2img-f8-large/model.ckpt
</code></pre></div><p>You should be able to sample images now using:</p><pre><code>python scripts/txt2img.py --prompt &#34;my cool image&#34;
</code></pre><p>Make sure to see the options you’re able to set in the README.</p><p>You can use this bash script to make running everything from a shell easier. You can include this in your <code>~/.bashrc</code> file and reopen your shell. Make sure to change the 3 paths to suit your needs. We’ll look at PLMS later.</p><div><pre><code data-lang="bash"><span>function</span> txt2img<span>()</span> <span>{</span>
  <span>if</span> <span>[</span> <span>$#</span> -eq <span>0</span> <span>]</span><span>;</span> <span>then</span>
    <span>echo</span> <span>&#34;Usage: diffuse &#39;[prompt]&#39; [--plms]&#34;</span>
    <span>exit</span> <span>1</span>
  <span>fi</span>
  <span>cd</span> /mnt/c/diffusion/latent-diffusion<span>;</span>
  conda activate ldm<span>;</span>
  <span>if</span> <span>[</span> <span>$2</span> <span>=</span> <span>&#39;--plms&#39;</span> <span>]</span><span>;</span> <span>then</span>
    python scripts/txt2img.py --prompt <span>$1</span> --outdir <span>&#39;/mnt/g/My Drive/AI Image Tests/text2img&#39;</span> --plms --n_iter <span>4</span> --ddim_eta 0.0
  <span>else</span>
    python scripts/txt2img.py --prompt <span>$1</span> --outdir <span>&#39;/mnt/g/My Drive/AI Image Tests/text2img&#39;</span> --n_iter <span>4</span> --ddim_eta 0.0
  <span>fi</span>
<span>}</span>
</code></pre></div><p>The images are 512X512 by default. You can use <a href="https://colab.research.google.com/drive/1xqzUi2iXQXDqXBHQGP9Mqt2YrYW6cx-J?usp=sharing#scrollTo=aCoHC2jbq_Ka">This Jupiter Notebook</a> to upscale the images with pretty good results.</p><h3 id="troubleshooting">Troubleshooting</h3><ul><li>If you get CUDA memory errors running commands, try decreasing the batch size. Use <code>htop</code> to troubleshoot/view memory usage.</li><li>If using WSL, make sure all your RAM is available to linux.</li><li>If you get torch related errors, you probably need to reinstall Pytorch. This will likely cause other issues with packages. Check environment.yaml and manually install the correct package versions with <code>pip</code> and/or <code>conda</code> (I did both and am not sure which made everything work)</li></ul><table><thead><tr><th><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807155754576.png" alt="image-20220807155754576"/></th><th><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807155807229.png" alt="image-20220807155807229"/></th></tr></thead></table><h2 id="rdm">RDM</h2><p>You can read more about RDM <a href="https://arxiv.org/abs/2204.11824">here</a>. As said before, it essentially combines a database of images with Clip descriptions with the existing diffusion process, from what I understand. This part takes a lot longer to get running.</p><p>Note, this model, especially when using the openimages training is best at recreating real things and doesn’t seem very good (yet) and creating the weird images we’re used to from the diffusion models.</p><ul><li><p>Get the Text-to-Image model working first</p></li><li><p>Install new packages and download training model.</p><div><pre><code data-lang="bash">pip install <span>transformers</span><span>==</span>4.19.2 scann <span>kornia</span><span>==</span>0.6.4 <span>torchmetrics</span><span>==</span>0.6.0
pip install git+https://github.com/arogozhnikov/einops.git

mkdir -p models/rdm/rdm768x768/
wget -O models/rdm/rdm768x768/model.ckpt https://ommer-lab.com/files/rdm/model.ckpt
</code></pre></div></li><li><p>Test everything is working so far with</p><div><pre><code data-lang="bash">python scripts/knn2img.py  --prompt <span>&#34;I&#39;m a computer&#34;</span>

</code></pre></div></li><li><p>If everything went well, you should see a success text in your shell. Now we need to download all the image indexes/models. The openimages zip is 11GB. The ArtBench data is pretty small.</p><div><pre><code data-lang="bash">mkdir -p data/rdm/retrieval_databases
wget -O data/rdm/retrieval_databases/artbench.zip https://ommer-lab.com/files/rdm/artbench_databases.zip
wget -O data/rdm/retrieval_databases/openimages.zip https://ommer-lab.com/files/rdm/openimages_database.zip
unzip data/rdm/retrieval_databases/artbench.zip -d data/rdm/retrieval_databases/
unzip data/rdm/retrieval_databases/openimages.zip -d data/rdm/retrieval_databases/

mkdir -p data/rdm/searchers
wget -O data/rdm/searchers/artbench.zip https://ommer-lab.com/files/rdm/artbench_searchers.zip
unzip data/rdm/searchers/artbench.zip -d data/rdm/searchers

</code></pre></div></li></ul><p>We’re ready to use the Artbench models now (which work pretty well in my limited testing), but what we really want is to use the massive openimages model as our reference. We downloaded the data, but we need to create the index.</p><p>If you want to test the Artbench database, run</p><div><pre><code data-lang="bash">python scripts/knn2img.py --prompt <span>&#34;A blue pig&#34;</span> --use_neighbors --knn <span>20</span> 
</code></pre></div><table><thead><tr><th><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807155912575.png" alt="image-20220807155912575"/></th><th><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807155927854.png" alt="image-20220807155927854"/></th></tr></thead></table><h3 id="openimages-index">Openimages Index</h3><p>Unless you have a super computer, the terrible memory management in python multiprocessing (from what I can tell) will stop us from using the 4 files we unzipped with the script from the repo to index them. Everything else I tried, the processes would run out of memory.</p><p>Luckily, we just need to concatenate the files. You can delete the old part files after if you would like. We also need to move the part files out of the openimages folder.</p><div><pre><code data-lang="bash"><span>cd</span> data/rdm/retrieval_databases/openimages/
cat 2000000x768_part1.npz 2000000x768_part2.npz 2000000x768_part3.npz 2000000x768_part4.npz &gt; 2000000x768.npz
mv 2000000x768_* ~/temp
<span>cd</span> ../../../../
</code></pre></div><p>Now when we run the training script, we shouldn’t run out of memory. I think this is some issue in their multi file function in the training script.</p><div><pre><code data-lang="bash">python scripts/train_searcher.py
</code></pre></div><ul><li><p>We need to change the batch size in <code>scripts/knn2img.py</code> so we don’t run out of GPU memory. You may need to lower to 1, but I was able to set to 2 on a 3090.</p></li><li><p>Open <code>scripts/knn2img.py</code> in an editor</p></li><li><p>Go to line 243 or find “n_samples”</p></li><li><p>Change the default value to 1 or 2 and save</p></li><li><p>You can also do this using the command line parameter, but since it will never work for me, I wanted to change the default value. Note if you update the repo in the future this will likely get overwritten.</p></li></ul><p>Now we can generate RDM images using the openimages database:</p><div><pre><code data-lang="bash">python scripts/knn2img.py --prompt <span>&#34;A blue pig&#34;</span> --database openimages --use_neighbors --knn <span>20</span> 
</code></pre></div><p>Here is a <code>~/.bashrc</code> script to run it conveniently. Again, make sure to change the 3 paths to suit your needs:</p><div><pre><code data-lang="bash"><span>function</span> diffuse<span>()</span> <span>{</span>
    <span>if</span> <span>[</span> <span>$#</span> -eq <span>0</span> <span>]</span><span>;</span> <span>then</span>
       <span>echo</span> <span>&#34;Usage: diffuse &#39;[prompt]&#39; [--plms]&#34;</span>
       <span>exit</span> <span>1</span>
    <span>fi</span>
    <span>cd</span> /mnt/c/diffusion/latent-diffusion<span>;</span>
    conda activate ldm<span>;</span>
    <span>if</span> <span>[</span> <span>$2</span> <span>=</span> <span>&#39;--plms&#39;</span> <span>]</span><span>;</span> <span>then</span>
        python scripts/knn2img.py  --database openimages --prompt <span>$1</span> --use_neighbors --outdir <span>&#39;/mnt/g/My Drive/AI Image Tests/RDM&#39;</span> --knn <span>20</span> --plms
    <span>else</span>
        python scripts/knn2img.py  --database openimages --prompt <span>$1</span> --use_neighbors --outdir <span>&#39;/mnt/g/My Drive/AI Image Tests/RDM&#39;</span> --knn <span>20</span>
    <span>fi</span>
<span>}</span>
</code></pre></div><h2 id="speeding-things-up">Speeding Things Up</h2><p>You can also run prompts in batches by creating a file (in this case <code>prompts.txt</code>) and pointing the script to it with the <code>–from-file</code> parameter like this</p><div><pre><code data-lang="bash">python scripts/knn2img.py  --database openimages --from-file ./prompts.txt --use_neighbors --outdir <span>&#39;/mnt/g/My Drive/AI Image Tests/RDM&#39;</span> --knn <span>20</span>
</code></pre></div><p>sample <code>prompts.txt</code></p><pre><code>prompt 1
prompt 2
prompt 3
</code></pre><p>This should save a lot of time if you’re running a lot of queries since the model only gets loaded once. The text-to-image model does not have this option.</p><table><thead><tr><th><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807154938662.png" alt="sports car in the desert"/></th><th><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807155008070.png" alt="sports car in the desert"/></th><th><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807155036735.png" alt="sports car in the desert"/></th></tr></thead></table><p>Below are some inital results from both the models with PLMS on and off. PLMS should make things run faster. For now, we won’t measure the speed increase but will look subjectively at image quality/results.</p><p>Since the RDM model will only make 2 photos at a time for my system, I included both photos instead of a grid for that model.</p><p>I will almost certainly do more specific testing in the future and these models will be included in future comparison posts.</p><hr/><p><em><em>A wood house on a hill, landscape photography</em></em></p><p><strong>Text-to-image</strong></p><table><thead><tr><th><em>without plms</em></th><th><em>with plms</em></th></tr></thead><tbody><tr><td><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807150052836.png" alt="image-20220807150052836"/></td><td><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807153545252.png" alt="image-20220807153545252"/></td></tr></tbody></table><p><strong>RDM</strong></p><table><thead><tr><th><em>without plms</em></th><th><em>with plms</em></th></tr></thead><tbody><tr><td><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807145854952.png" alt="image-20220807145854952"/></td><td><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807145920778.png" alt="image-20220807145920778"/></td></tr><tr><td><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807145911367.png" alt="image-20220807145911367"/></td><td><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807145931866.png" alt="image-20220807145931866"/></td></tr></tbody></table><p><em><em>A purple stop sign</em></em></p><hr/><p><strong>Text-to-image</strong></p><table><thead><tr><th><em>without plms</em></th><th><em>with plms</em></th></tr></thead><tbody><tr><td><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807154729984.png" alt="image-20220807154729984"/></td><td><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807153711466.png" alt="image-20220807153711466"/></td></tr></tbody></table><p><strong>RDM</strong></p><table><thead><tr><th><em>without plms</em></th><th><em>with plms</em></th></tr></thead><tbody><tr><td><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807150500598.png" alt="image-20220807150500598"/></td><td><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807153000850.png" alt="image-20220807153000850"/></td></tr><tr><td><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807151421393.png" alt="image-20220807151421393"/></td><td><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807153013778.png" alt="image-20220807153013778"/></td></tr></tbody></table><hr/><p><em><em>fox den, digital art</em></em></p><p><strong>Text-to-image</strong></p><table><thead><tr><th><em>without plms</em></th><th><em>with plms</em></th></tr></thead><tbody><tr><td><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807155228088.png" alt="image-20220807155228088"/></td><td><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807155634227.png" alt="image-20220807155634227"/></td></tr></tbody></table><p><strong>RDM</strong></p><table><thead><tr><th><em>without plms</em></th><th><em>with plms</em></th></tr></thead><tbody><tr><td><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807151441359.png" alt="image-20220807151441359"/></td><td><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807152859265.png" alt="image-20220807152859265"/></td></tr><tr><td><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807152235267.png" alt="image-20220807152235267"/></td><td><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807152911615.png" alt="image-20220807152911615"/></td></tr></tbody></table><hr/><p><em><em>winning the big game, award winning photography</em></em></p><p><strong>Text-to-image</strong></p><table><thead><tr><th><em>without plms</em></th><th><em>with plms</em></th></tr></thead><tbody><tr><td><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807160007336.png" alt="image-20220807160007336"/></td><td><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807160228471.png" alt="image-20220807160228471"/></td></tr></tbody></table><p><strong>RDM</strong></p><table><thead><tr><th><em>without plms</em></th><th><em>with plms</em></th></tr></thead><tbody><tr><td><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807151456027.png" alt="image-20220807151456027"/></td><td><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807152928304.png" alt="image-20220807152928304"/></td></tr><tr><td><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807152216729.png" alt="image-20220807152216729"/></td><td><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807152940229.png" alt="image-20220807152940229"/></td></tr></tbody></table><hr/><p><em><em>a sports car driving in sand dunes</em></em></p><p><strong>Text-to-image</strong></p><table><thead><tr><th><em>without plms</em></th><th><em>with plms</em></th></tr></thead><tbody><tr><td><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807160557241.png" alt="image-20220807160557241"/></td><td><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807160838768.png" alt="image-20220807160838768"/></td></tr></tbody></table><p><strong>RDM</strong></p><table><thead><tr><th><em>without plms</em></th><th><em>with plms</em></th></tr></thead><tbody><tr><td><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807151401781.png" alt="image-20220807151401781"/></td><td><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807152825508.png" alt="image-20220807152825508"/></td></tr><tr><td><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807152200386.png" alt="image-20220807152200386"/></td><td><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807152839511.png" alt="image-20220807152839511"/></td></tr></tbody></table><p>And there you have it – no more credits or monthly limits, just pure AI image generation all your own. Once I figure out some tuning settings and create my own image training database, we’ll likely talk about this again.</p><p>Hopefully with some tuning and new releases/models this will get closer to the WOW factor DALL-E has.</p><p><img loading="lazy" src="https://reticulated.net/dailyai/running-your-own-ai-image-generator-with-latent-diffusion/img/image-20220807160755376.png" alt="image-20220807160755376"/></p><p>Please share any tips, questions and creations in the comments!</p></div></div>
  </body>
</html>
