<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.latent.space/p/rwkv#%C2%A7the-eleuther-mafia">Original</a>
    <h1>The Eleuther AI Mafia</h1>
    
    <div id="readability-page-1" class="page"><div><div><div dir="auto"><p><em><span>The AI Engineer Summit Expo has been </span><a href="https://twitter.com/aiDotEngineer/status/1696532657197256815/retweets/with_comments" rel="">announced</a><span>, presented by AutoGPT (and future guest </span><a href="https://twitter.com/SigGravitas" rel="">Toran Bruce-Richards</a><span>!) Stay tuned for more updates on </span><a href="https://www.ai.engineer/summit" rel="">the Summit livestream</a><span> and </span><a href="https://www.latent.space/p/lsu-beta" rel="">Latent Space University</a><span>.</span></em></p><p><strong>What comes after the Transformer?</strong><span> This is one of the </span><a href="https://huyenchip.com/2023/08/16/llm-research-open-challenges.html" rel="">Top 10 Open Challenges in LLM Research</a><span> that has been the talk of the AI community this month. Jon Frankle (</span><a href="https://www.latent.space/p/mosaic-mpt-7b" rel="">friend of the show</a><span>!) has an ongoing </span><a href="https://www.isattentionallyouneed.com/" rel="">bet</a><span> with Sasha Rush on whether </span><strong>Attention is All You Need</strong><span>, and the most significant challenger to emerge this year has been </span><a href="https://huggingface.co/blog/rwkv" rel="">RWKV - Receptance Weighted Key Value models</a><span>, which revive the RNN</span></p><p><span> for GPT-class LLMs, inspired by a 2021 paper on </span><a href="https://arxiv.org/abs/2105.14103" rel="">Attention Free Transformers</a><span> from Apple (surprise!).</span></p><p>What this means practically is that RWKV models tend to scale in all directions (both in training and inference) much better than Transformers-based open source models:</p><p>While remaining competitive on standard reasoning benchmarks:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F665f5880-65b1-46a6-b38e-4550e8a47cec_1366x779.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F665f5880-65b1-46a6-b38e-4550e8a47cec_1366x779.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F665f5880-65b1-46a6-b38e-4550e8a47cec_1366x779.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F665f5880-65b1-46a6-b38e-4550e8a47cec_1366x779.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F665f5880-65b1-46a6-b38e-4550e8a47cec_1366x779.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F665f5880-65b1-46a6-b38e-4550e8a47cec_1366x779.png" width="1366" height="779" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/665f5880-65b1-46a6-b38e-4550e8a47cec_1366x779.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:779,&#34;width&#34;:1366,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:370484,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:false,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F665f5880-65b1-46a6-b38e-4550e8a47cec_1366x779.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F665f5880-65b1-46a6-b38e-4550e8a47cec_1366x779.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F665f5880-65b1-46a6-b38e-4550e8a47cec_1366x779.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F665f5880-65b1-46a6-b38e-4550e8a47cec_1366x779.png 1456w" sizes="100vw"/></picture></div></a></figure></div><p><span>swyx was recently in Singapore for meetings with </span><a href="https://aisingapore.org/" rel="">AI government and industry folks</a></p><p><span>, and grabbed 2 hours with RWKV committee member Eugene Cheah for a deep dive, the full recording of which is now up on </span><a href="https://youtu.be/dvk6X5zeIfY" rel="">Latent Space TV</a><span>:</span></p><div id="youtube2-dvk6X5zeIfY" data-attrs="{&#34;videoId&#34;:&#34;dvk6X5zeIfY&#34;,&#34;startTime&#34;:null,&#34;endTime&#34;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/dvk6X5zeIfY?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><p>Today we release both the 2hr video and an edited 1hr audio version, to cater to the different audiences and provide “ablation opportunities” on RWKV interest level.</p><p>The RWKV project is notable not merely because of the credible challenge to the Transformers dominance. It is also a distributed, international, mostly uncredentialed community reminiscent of early 2020s Eleuther AI:</p><ul><li><p><span>Primarily Discord, pseudonymous, </span><a href="https://www.semianalysis.com/p/google-gemini-eats-the-world-gemini" rel="">GPU-poor</a><span> volunteer community somehow coordinating enough to train &gt;10B, OPT/BLOOM-competitive models</span></p></li><li><p>Being driven by the needs of its community, it is extremely polyglot (e.g. English, Chinese, Japanese, Arabic) not because it needs to beat some benchmarks, but because its users want it to be for their own needs.</p></li><li><p><span>“Open Source” in both the good and the bad way - properly Apache 2.0 licensed (</span><a href="https://www.alessiofanelli.com/blog/llama2-isnt-open-source" rel="">not “open but restricted</a><span>”), yet trained on data taken from commercially compromised sources like the Pile (where </span><a href="https://www.theatlantic.com/technology/archive/2023/08/books3-ai-meta-llama-pirated-books/675063/" rel="">Shawn Presser’s Books3 dataset has been recently taken down</a><span>) and Alpaca (taking from </span><a href="https://lmsys.org/blog/2023-03-30-vicuna/" rel="">Steven Tey’s ShareGPT</a><span> which is technically against OpenAI TOS)</span></p></li></ul><p>The threadboi class has loved tracking the diffusion of Transformers paper authors out into the industry:</p><p>But perhaps the underdog version of this is tracking the emerging Eleuther AI mafia:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F387d54b3-2b2d-4b57-8c06-87de8b9da89f_826x591.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F387d54b3-2b2d-4b57-8c06-87de8b9da89f_826x591.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F387d54b3-2b2d-4b57-8c06-87de8b9da89f_826x591.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F387d54b3-2b2d-4b57-8c06-87de8b9da89f_826x591.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F387d54b3-2b2d-4b57-8c06-87de8b9da89f_826x591.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F387d54b3-2b2d-4b57-8c06-87de8b9da89f_826x591.png" width="826" height="591" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/387d54b3-2b2d-4b57-8c06-87de8b9da89f_826x591.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:591,&#34;width&#34;:826,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:129917,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F387d54b3-2b2d-4b57-8c06-87de8b9da89f_826x591.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F387d54b3-2b2d-4b57-8c06-87de8b9da89f_826x591.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F387d54b3-2b2d-4b57-8c06-87de8b9da89f_826x591.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F387d54b3-2b2d-4b57-8c06-87de8b9da89f_826x591.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a><figcaption>source: swyx rough mapping. Not authoritative!</figcaption></figure></div><p>It will be fascinating to see how both Eleuther and Eleuther alums fare as they build out the future of both LLMs and open source AI.</p><p><em><span>assisted by </span><a href="https://github.com/FanaHOVA/smol-podcaster" rel="">smol-podcaster</a><span>. Different timestamps vs the 2hr YouTube</span></em></p><ul><li><p>[00:05:35] Eugene&#39;s path into AI at UIlicious</p></li><li><p>[00:07:33] Tokenizer penalty and data efficiency of Transformers</p></li><li><p>[00:08:02] Using Salesforce CodeGen</p></li><li><p>[00:10:17] The limitations of Transformers for handling large context sizes</p></li><li><p>[00:13:17] RWKV compute costs compared to Transformers</p></li><li><p>[00:16:06] How Eugene found RWKV early</p></li><li><p>[00:18:52] RWKV&#39;s focus on supporting many languages, not just English</p></li><li><p>[00:21:24] Using the RWKV model for fine-tuning for specific languages</p></li><li><p>[00:24:45] What is RWKV?</p></li><li><p>[00:33:46] Overview of the different RWKV models like World, Raven, Novel</p></li><li><p>[00:41:34] Background of Blink, the creator of RWKV</p></li><li><p>[00:49:55] The linear vs quadratic scaling of RWKV vs Transformers</p></li><li><p>[00:53:29] RWKV matching Transformer performance on reasoning tasks</p></li><li><p>[00:54:31] The community&#39;s lack of marketing for RWKV</p></li><li><p>[00:57:00] The English-language bias in AI models</p></li><li><p>[01:00:33] Plans to improve RWKV&#39;s memory and context handling</p></li><li><p>[01:03:10] Advice for AI engineers wanting to get more technical knowledge</p></li></ul><p>Companies/Organizations:</p><ul><li><p><span>RWKV - </span><a href="https://huggingface.co/blog/rwkv" rel="">HF blog</a><span>, </span><a href="https://arxiv.org/abs/2305.13048" rel="">paper</a><span>, </span><a href="https://wiki.rwkv.com/" rel="">docs</a><span>, </span><a href="https://github.com/BlinkDL/RWKV-LM" rel="">GitHub</a><span>, </span><a href="https://huggingface.co/docs/transformers/model_doc/rwkv" rel="">Huggingface</a></p><ul><li><p><span>Raven 14B (finetuned on Alpaca+ShareGPT+...) </span><a href="https://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio" rel="">Demo</a></p></li><li><p><span>World 7B (supports 100+ world languages) </span><a href="https://huggingface.co/spaces/BlinkDL/RWKV-World-7B" rel="">Demo</a></p></li><li><p><a href="https://johanwind.github.io/2023/03/23/rwkv_details.html" rel="">How RWKV works in 100 LOC</a><span>, </span><a href="https://johanwind.github.io/2023/03/23/rwkv_overview.html" rel="">RWKV overview</a></p></li></ul></li><li><p><a href="https://www.eleuther.ai/" rel="">EleutherAI</a><span> - Decentralized open source AI research group</span></p></li><li><p><a href="https://stability.ai/" rel="">Stability AI</a><span> - Creators of Stable Diffusion </span></p></li><li><p><a href="https://www.conjecture.dev/" rel="">Conjecture</a><span> - Spun off from EleutherAI</span></p></li></ul><p>People:</p><ul><li><p><span>Eugene Chia - CTO of </span><a href="https://uilicious.com/" rel="">UIlicious</a><span>, member of RWKV committee (</span><a href="https://github.com/PicoCreator" rel="">GitHub</a><span>, </span><a href="https://twitter.com/picocreator" rel="">Twitter</a><span>)</span></p></li><li><p><a href="https://github.com/BlinkDL" rel="">Blink/Bo Peng</a><span> - Creator of RWKV architecture</span></p></li><li><p><a href="https://www.latent.space/p/transformers-math#details" rel="">Quentin Anthony</a><span> - our Latent Space pod on Eleuther, coauthor on RWKV </span></p></li><li><p><a href="https://www.latent.space/p/sharif-shameem#details" rel="">Sharif Shameem</a><span> - our Latent Space pod on being early to Stable Diffusion</span></p></li><li><p><a href="https://www.latent.space/p/flashattention#details" rel="">Tri Dao</a><span> - our Latent Space pod on FlashAttention making Attention subquadratic</span></p></li><li><p><a href="https://www.latent.space/p/ai-interfaces-and-notion" rel="">Linus Lee</a><span> - our Latent Space pod in NYC</span></p></li><li><p><a href="https://www.latent.space/p/mosaic-mpt-7b" rel="">Jonathan Frankle</a><span> - our Latent Space pod about Transformers longevity</span></p></li><li><p><a href="https://cs.stanford.edu/~chrismre/" rel="">Chris Re</a><span> - Genius at Stanford working on state-space models</span></p></li><li><p><span>Andrej Karpathy - </span><a href="https://karpathy.ai/zero-to-hero.html" rel="">Zero to Hero</a><span> series</span></p></li><li><p><span>Justine Tunney (&#34;</span><a href="https://justine.lol/" rel="">Justine.lol</a><span>&#34;) - </span><a href="https://justine.lol/mmap/" rel="">mmap trick</a></p></li></ul><p>Models/Papers:</p><ul><li><p><a href="https://huyenchip.com/2023/08/16/llm-research-open-challenges.html" rel="">Top 10 Open Challenges in LLM Research</a></p></li><li><p><a href="https://arxiv.org/pdf/2307.08621.pdf" rel="">Retentive Network: A Successor to Transformer for Large Language Models</a><span> </span></p></li><li><p><a href="https://github.com/EleutherAI/gpt-neox" rel="">GPT-NeoX</a><span> - Open source replica of GPT-3 by EleutherAI </span></p></li><li><p><a href="https://github.com/salesforce/CodeGen" rel="">Salesforce CodeGen</a><span> and </span><a href="https://paperswithcode.com/paper/codegen2-lessons-for-training-llms-on" rel="">CodeGen 2</a></p></li><li><p><a href="https://arxiv.org/abs/2105.14103" rel="">Attention Free Transformers paper</a></p></li><li><p><a href="https://pile.eleuther.ai/" rel="">The Pile</a></p></li><li><p><a href="https://github.com/togethercomputer/RedPajama-Data" rel="">RedPajama dataset</a></p></li><li><p><a href="https://hazyresearch.stanford.edu/blog/2023-07-25-m2-bert" rel="">Monarch Mixer - </a><span>Revisiting BERT, Without Attention or MLPs</span></p></li></ul><p><span>RWKV is not without known weaknesses - Transformers do well in reasoning because they are </span><a href="https://twitter.com/karpathy/status/1593417989830848512?s=20" rel="">expressive in the forward pass</a><span>, yet the RWKV docs already note that it is </span><a href="https://wiki.rwkv.com/#tldr-vs-existing-transformer-models" rel="">sensitive to prompt formatting and poor at lookback tasks</a><span>. We also asked pointed questions about RWKV’s challenges in the full podcast.</span></p></div></div></div></div>
  </body>
</html>
