<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.sigarch.org/fast-memcpy-a-system-design/">Original</a>
    <h1>Fast memcpy, a system design</h1>
    
    <div id="readability-page-1" class="page"><div>
					<p><img loading="lazy" decoding="async" src="https://www.sigarch.org/wp-content/uploads/2022/12/monk_copying_rounded.png" alt="" width="1491" height="943" srcset="https://www.sigarch.org/wp-content/uploads/2022/12/monk_copying_rounded.png 1491w, https://www.sigarch.org/wp-content/uploads/2022/12/monk_copying_rounded-1280x810.png 1280w, https://www.sigarch.org/wp-content/uploads/2022/12/monk_copying_rounded-980x620.png 980w, https://www.sigarch.org/wp-content/uploads/2022/12/monk_copying_rounded-480x304.png 480w" sizes="(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) and (max-width: 980px) 980px, (min-width: 981px) and (max-width: 1280px) 1280px, (min-width: 1281px) 1491px, 100vw"/></p>
<p>When I worked at Google, fleet-wide profiling revealed that 25-35% of all CPU time was spent just moving bytes around: memcpy, strcmp, copying between user and kernel buffers in network and disk I/O, hidden copy-on-write in soft page faults, checksumming, compressing, decrypting, assembling/disassembling packets and HTML pages, etc. If data movement were faster, more work could be done on the same processors.</p>
<h4><strong>Thought Experiment</strong></h4>
<p>We look here at a Gedankenexperiment: <strong>move 16 bytes per cycle</strong>, addressing not just the CPU movement, but also the surrounding system design. We will propose five new general-purpose instructions that are key to meeting this goal.</p>
<p>We assume a base multi-core processor four-way-issue load/store machine with 64-bit integer/address registers Rx, 128-bit (16-byte) data registers Vx, and an L1 D-cache that can do two operations per cycle, each reading or writing an aligned 16-byte memory word. A lesser design cannot possibly move 16 bytes per cycle. This base design can map easily onto many current chips.</p>
<h4><strong>Fails</strong></h4>
<p>Proposals to do in-DRAM copying fail here because source and data fields are not likely to be aligned, and are unlikely to be in the same memory chip. They also fail if the source or desired destination is somewhere in the cache hierarchy, not DRAM. Proposals to use a specialized DMA engine for movement fail for short moves if that engine has non-trivial setup and has interrupt processing at completion, and fails for long moves if the multiple processors must take out a software lock to use the single engine, causing waits when there is contention (as there will be at 25-35% use per core).</p>
<p>The standard memcpy(dest, src, len) function does not know the subsequent use of the dest field, so can do no better than leave it wherever it lands in the cache hierarchy. We thus eschew non-temporal loads and stores, which assume non-use of dest. (Cache pollution is addressed in a section below.)</p>
<h4><strong>Cache Underpinning</strong></h4>
<p>To move 16 bytes per cycle, the L1 D-cache must do an aligned 16-byte load and 16 byte store per cycle, while simultaneously doing cache-line refills and writebacks at the same rate, about 50 GB/sec for a 3.2 GHz processor core. Lower levels of cache must support multiple such streams, all the way out to main memory.</p>
<h4><strong>Instruction Underpinning</strong></h4>
<p>To move 16 bytes per cycle requires at least four instructions per cycle: load, store, test-done, and branch. In reality there is more to do, but modest loop unrolling can fit the rest into four instructions per cycle, as we see below.</p>
<h4><strong>Short Moves</strong></h4>
<p>We break the discussion onto three parts: short ~10 byte, medium ~100 byte, and long ~1000+ byte moves. Short moves are common in assembling/disassembling packet headers and HTML tags, moving single 1..4-byte UTF-8 characters, and moving small byte strings during compression/decompression. Medium moves are common in juxtaposing paragraphs of text or copying modest-size control blocks. Long moves are common in filling the payload part of packets and in network/disk buffer transfers.</p>
<p>The standard short move code tests that len is small, say &lt; 16, and then does</p>
<pre>while (len-- &gt; 0) {*dest++ = *src++;}</pre>
<p>compiling into something like</p>
<pre>compare, sub, branch, loadbyte, add, storebyte, add</pre>
<p>taking at least two cycles per byte, or 32 cycles for 16 bytes, far from our goal of 1 cycle.</p>
<p>We introduce two new instructions to handle the short move case quickly. Chips that can already do unaligned 1/2/4/8/16-byte loads and stores <em>already</em> have all the datapath for these; they just need small modifications to the control logic.</p>
<h5><strong>Load Partial</strong></h5>
<pre>ldp  V1, R2, R3</pre>
<p>Load and zero extend 0..15 bytes from 0(R2) into V1, length specified by R3&lt;3:0&gt;. Length 0 does no load and takes no traps.</p>
<h5><strong>Store Partial</strong></h5>
<pre>stp  V1, R2, R3</pre>
<p>Store 0..15 bytes from V1 to 0(R2), length specified by R3&lt;3:0&gt;. Length 0 does no store and takes no traps.</p>
<p>Chips that do unaligned load/store already have logic to decide whether to access one or two cache words, have logic to shift two memory words to align bytes with V1, and logic to zero-extend 1/2/4/8 byte loads. The new instructions just need control logic to detect zero data movement, and control logic to zero-extend 0..15 byte loads.</p>
<p>Using these instructions, memcpy with dest, src, and len already in registers becomes</p>
<pre>memcpy:
    ldp V1, 0(src), len
    stp V1, 0(dest), len
    andnot len, 15
    bnz medium
    return</pre>
<p>The first four instructions can issue in a single cycle, moving 0..15 bytes <em>branch-free</em>, close to our goal. If that is the total length, the return takes a second cycle. (This is best-case timing; cache misses, branch mispredicts, and unaligned memory accesses can slow things down modestly.)</p>
<h4><strong>Medium Moves</strong></h4>
<p>After ldp/stp for 0..15 bytes, the remaining length is a non-zero multiple of 16 bytes. To cover medium moves, we increment src and dst to account for the variable-length ldp/stp and then conditionally move 16, 32, 64, and 128 bytes.</p>
<pre>medium:
    and  Rx, len, 15    ; length (mod 16)
    add  src, Rx        ; increment past the 0..15 bytes
    add  dest, Rx       ; increment past the 0..15 bytes</pre>
<p>Conditionally move 16 bytes with ordinary 16-byte load/store and update src/dest; 2 cycles for 16 bytes</p>
<pre>    and  Rx, len, 0x10  ; does length have a 16 bit?
    bz   1f
    ld                  ; yes, move 16 bytes
    st                  ; yes, move 16 bytes
    add  src, src, 16   ; increment past the 16 bytes
    add  dest, dest, 16 ; increment past the 16 bytes
1:
    and  Rx, len, 0x20  ; does length have a 32 bit?
    ...</pre>
<p>The same pattern repeats for len bits 0x20/0x40/0x80 moving 32/64/128 bytes via more ld/st pairs between bz and add. These pieces all achieve 16 bytes per cycle best case.</p>
<p>The short plus medium code covers all lengths up to 255 bytes. We finish by testing for len &gt; 255</p>
<pre>    andnot len, 255
    bnz long
    return

</pre>
<h4><strong>Long Moves</strong></h4>
<p>If len is more than 255 bytes, we end up here with a nonzero multiple of 256 in len&lt;63:8&gt;. For speed, we next want a loop that does only aligned 16-byte loads and aligned 16-byte stores and we unroll that loop modestly to fit the four overhead instructions around the load/store pairs. The loop has a previously-loaded src word, loads a new aligned src word every cycle, shifts two words to align with dest and stores the dest word.</p>
<p><img decoding="async" loading="lazy" src="https://www.sigarch.org/wp-content/uploads/2022/12/memcpy-blog-figure.png" alt="" width="484" height="242"/></p>
<p>We introduce a new double-width 32-byte shift instruction to handle the alignment quickly. This is the same shift logic already in a chip’s unaligned-load path.</p>
<h5><strong>Byte Shift Double</strong></h5>
<pre>shrbd  V1, V2, R3</pre>
<p>Shift the register pair V2V3 right by 0..15 bytes, as specified by R3&lt;3:0&gt;. Place the result in V1.</p>
<p>The overall long move uses ldp/stp to advance dest to the next higher multiple of 16, moves chunks of 64 bytes using load/shift/store plus room for the two pointer increments, test, and branch per iteration, fitting into 4 cycles per 64 bytes. 16 bytes per cycle exactly!</p>
<p>After moving multiples of 64 bytes to aligned dest addresses, there will be a tail of 0..63 bytes remaining to move. Use the medium then short patterns to finish up. This overall design comes very close to moving 16 bytes per cycle over the <em>entire range</em> of short, medium, and long memcpy. But we aren’t done yet …</p>
<h5><strong>Prefetching</strong></h5>
<p>With L3 cache about 40 cycles away from the CPU and main memory 200-250 cycles away, prefetching becomes important for long moves. We can’t do much about getting the initial bytes quickly, but we can prefetch subsequent bytes.</p>
<p>Moving 1KB at 16 bytes/cycle takes 64 cycles. This is enough time while moving 1KB to prefetch the next 1KB from L3 cache. Similarly, moving 4KB takes about 256 cycles, enough time to prefetch the next 4KB from main memory. But today’s computers only prefetch single cache lines, often 64 bytes each.</p>
<p>We introduce two prefetch instructions.</p>
<p><strong>Prefetch_read, Prefetch_write</strong></p>
<pre>PRE_R  R2, R3</pre>
<p>Prefetch data for reading from 0(R2), length min(R3, 4096).</p>
<pre>PRE_W  R2, R3</pre>
<p>Prefetch data for writing from 0(R2), length min(R3, 4096).</p>

<p>The 4KB upper bound on the length is important. It prevents describing a prefetch of megabytes, and it guarantees that the prefetch will need no more than two TLB lookups for 4KB or larger pages. It is big enough to give code time to start subsequent prefetches as needed, pacing prefetching to data use.<strong> </strong></p>
<p>The desired implementation of PRE_W does allocation of exclusive cache lines but <em>defers</em> filling them with any data, except possibly partial first and last cache lines. If such a cache line is completely overwritten, the read is never done. Without this optimization, memory bandwidth goes up by 50%</p>
<p>The long move loop can be built as a pair of nested loops, the inner one issuing read and write prefetches and then moving 4KB at a time.</p>
<h5><strong>DRAM row access</strong></h5>
<p>DRAMs internally copy bits out of very weak capacitors to a row buffer and then serve bytes to a CPU chip from there. Row buffers are typically 1KB. Accessing bytes from an already-filled row buffer is three times faster than starting from scratch, approximately 15ns vs. 45ns. This hasn’t changed much in 40 years.</p>
<p>If an implementation gets a prefetch address <em>and length</em> to the memory controller logic at the beginning of a long prefetch, the memory controller has enough information to optimize doing row-buffer fetches, even in the presence of competing memory requests.</p>
<h5><strong>Cache Pollution</strong></h5>
<p>Quickly moving many kilobytes of data through the caches normally has the downside of evicting other data belonging to other programs. Cache isolation remains a problem in the datacenter part of our industry.</p>
<p>Preventing cache pollution could be done by assigning a few bits of “ownership” to each cache line fetched and using that to keep track of how many lines each CPU/etc. owns in the cache. For an L1 cache in a hyperthreaded chip, each logical CPU is an owner. In a cache shared across many cores, each physical core might be an owner. To prevent kernel code from polluting user data while doing disk and network bulk moves, “kernel” could by an owner unto itself.</p>
<p>Giving owners a <em>limit</em> on how many cache lines they can use allows an implementation to switch allocation strategies for owners that are over their limit, preferentially replacing their <em>own</em> lines or placing their new fills near the replacement end of a pseudo-LRU replacement list. A loose limit is good enough to prevent starving other users.</p>
<p>This approach is superior to fixed <em>way partitioning</em> of an N-way associative cache because it does not leave as many resources stranded.</p>
<p>An alternate approach requiring no extra ownership bits is to track each owner’s fill rate and essentially rate-limit each owner. Those over their limit get the alternate allocation strategy.</p>
<h4><strong>Summary </strong></h4>
<p>Our simple “Move 16 bytes per cycle” quest for memcpy and its ilk reveals a nuanced set of instruction-set and microarchitecture issues:</p>
<ul>
<li>Fail: Copy-in-RAM and DMA engines</li>
<li>Both the CPU side and the memory side of caches matter</li>
<li>Load/Store Partial and Double-width-shift help significantly</li>
<li>Longer Prefetching matters</li>
<li>Letting the memory controller know about prefetch <em>length</em> can be 3x faster</li>
<li>Controlling cache pollution matters</li>
</ul>
<p>We suggest a few new instructions and some cache management that can help achieve the goal.</p>
<p><strong>Disclaimer:</strong> <em>These posts are written by individual contributors to share their thoughts on the Computer Architecture Today blog for the benefit of the community. Any views or opinions represented in this blog are personal, belong solely to the blog author and do not represent those of ACM SIGARCH or its parent organization, ACM.</em></p>
					</div></div>
  </body>
</html>
