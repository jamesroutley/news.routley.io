<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://modal.com/blog/serverless-http">Original</a>
    <h1>Lambda on hard mode: serverless HTTP in Rust</h1>
    
    <div id="readability-page-1" class="page"><div>




<div><p>At Modal, we built an HTTP and WebSocket stack on our platform. In other words,
your serverless functions can take web requests.</p>
<p>This was tricky! HTTP has quite a few edge cases, so we used Rust for its speed
and to help manage the complexity. But even so, it took a while to get right. We
recently wrapped up this feature by introducing
<a href="https://www.jakef.science/blog/websocket-launch">full WebSocket support</a> (real-time bidirectional
messaging).</p>
<p>We call this service <code>modal-http</code>, and it sits between the Web and our core
runtime.</p>
<p><img src="https://www.jakef.science/_app/immutable/assets/modal-http-20.c41d7b53.png" alt="Simple schematic with modal-http at the center"/>



</p>
<p>You can deploy a simple <a href="https://www.jakef.science/docs/guide/webhooks">web endpoint</a> to a <code>*.modal.run</code>
URL by running some Python code:</p>


<p>(<em>This takes <strong>0.747 seconds</strong> to deploy today.</em>)</p>
<p>But you can also run a much larger compute workload. For example, to set up a
data-intensive video processing endpoint:</p>


<p>This post is about the behind-the-scenes of serving a web endpoint on Modal. How
does your web request get translated into an autoscaling serverless invocation?</p>
<p>What makes our HTTP/WebSocket implementation particularly interesting is its
lack of limits. Serverless computing is
<a rel="nofollow" href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2019/EECS-2019-3.pdf">traditionally understood</a>
to prioritize small, lightweight tasks, but Modal can’t compromise on speed or
compute capacity.</p>
<p>When resource limits are removed, handling web requests gets proportionally more
difficult. Users may ask to upload a gigabyte of video to their machine learning
model or data pipeline, and we want to help them do that! We can’t just say,
“sorry, either make your video 200x smaller or split it up yourself.” So we had
a bit of a challenge on our hands.</p>
<h2 id="lambda-on-hard-mode">“Lambda on hard mode”</h2>

<p>Serverless function platforms have constraints. A lot of them, too!</p>
<ul><li>Functions on
<a rel="nofollow" href="https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html">AWS Lambda</a>
are limited to 15-minute runs and 50 MB images. As of 2024, they can only use
3 CPUs (6 threads) and 10 GB of memory. Response bandwidth is 2 Mbps.</li>
<li><a rel="nofollow" href="https://cloud.google.com/run/quotas">Google Cloud Run</a> is a bit better, with
4 CPUs and 32 GB of memory, plus 75 Mbps bandwidth.</li>
<li><a rel="nofollow" href="https://developers.cloudflare.com/workers/platform/limits/">Cloudflare Workers</a>
are the most restricted. Their images can only be 10 MB in size and have 6
HTTP connections. Execution is limited to 30 seconds of CPU time, 128 MB of
memory.</li></ul>
<p>But modern compute workloads can be <a href="https://www.jakef.science/examples">much more demanding</a>: training
neural networks, rendering graphics, simulating physics, running data pipelines,
and so on.</p>
<p>Modal containers can each use up to <strong>64 CPUs</strong>, <strong>336 GB of memory</strong>, and <strong>8
Nvidia H100 GPUs</strong>. And they may need to download up to <strong>hundreds of
gigabytes</strong> of model weights and image data on container startup. As a result,
we care about having them spin up and shut down quickly, since having any idle
time is expensive. We scale to zero and bill <a href="https://www.jakef.science/pricing">by the second</a>.</p>
<p>As a user, this is freeing. I often get questions like, “does Modal have enough
compute to run my <a href="https://www.jakef.science/docs/examples/blender_video">fancy bread-baking simulation</a>”
— and I tell them, are you kidding? You can spin up dozens of 64-CPU containers
at a snap of your fingers. Simulate your whole bakery!</p>
<p>In summary: Modal containers are potentially long-running and compute-heavy,
with big inputs and outputs. This is the opposite of what “serverless” is
usually good at. How can we ensure quick and reliable delivery of HTTP requests
under these conditions?</p>
<h3 id="a-distributed-operating-system">A distributed operating system</h3>
<p>Let’s take a step back and review the concept of serverless computing. Run code
in containers. Increase the number of containers when there’s work to be done,
and then decrease it when there’s less work. You can imagine a factory that
makes cars: when there are many orders, the factory operates more machines, and
when there are fewer orders, the factory shifts its focus. (Except in computers,
everything happens faster than in a car factory, since they’re processing
thousands of requests per second.)</p>
<p>This isn’t unique to serverless computing; it’s how most applications scale
today. If you deploy a web server, chances are you’d use a PaaS to manage
replicas and scaling, or an orchestrator like Kubernetes. Each of these
offerings can be conceptualized by a two-part schematic:</p>
<ol><li><strong>Autoscaling:</strong> Write code in a stateless way, replicate it, then track how
much work needs to be done via latency, CPU, and memory metrics.</li>
<li><strong>Load balancing:</strong> Distribute work across many machines and route traffic to
them.</li></ol>
<p>Together autoscaling and load balancing constitute a kind of analogue to an
<em>operating system</em> in the distributed services world: something that manages
compute resources and provides a common execution environment, allowing software
to be run.</p>
<p>Although a unified goal, there are many approaches. (A lot of
<a rel="nofollow" href="https://research.google/pubs/maglev-a-fast-and-reliable-software-network-load-balancer/">ink</a>
<a rel="nofollow" href="https://aosabook.org/en/v2/nginx.html">has</a>
<a rel="nofollow" href="https://www.eecs.harvard.edu/~michaelm/postscripts/mythesis.pdf">been</a>
<a rel="nofollow" href="https://github.com/tangchq74/papers/blob/fad260ab66567e843e5ad6e238f7051ffe384e8a/XFaaS-SOSP23-Final.pdf">spilled</a>
on load balancing in particular.) Here’s a brief summary to illustrate how this
schematic maps onto a few popular deployment systems. We’re in good company!</p>
<div><table><thead><tr><th>System (release date)</th>
<th>Autoscaling</th>
<th>Load balancing</th></tr></thead>
<tbody><tr><td><em>Heroku (2009)</em></td>
<td>By p95 latency</td>
<td>HTTP reverse proxy</td></tr>
<tr><td><em>Kubernetes (2014)</em></td>
<td>Resource metrics (custom)</td>
<td>Custom reverse proxy and/or load balancer</td></tr>
<tr><td><em>AWS Lambda (2014)</em></td>
<td>Traffic</td>
<td>HTTP reverse proxy</td></tr>
<tr><td><em>Azure Functions (2016)</em></td>
<td>Traffic</td>
<td>HTTP reverse proxy</td></tr>
<tr><td><em>AWS Fargate (2017)</em></td>
<td>Resource metrics (custom)</td>
<td>HTTP/TCP/UDP load balancer</td></tr>
<tr><td><em>Render (2019)</em></td>
<td>CPU/memory target</td>
<td>HTTP reverse proxy</td></tr>
<tr><td><em>Google Cloud Run (2019)</em></td>
<td>Traffic and CPU target</td>
<td>HTTP reverse proxy</td></tr>
<tr><td><em>Fly.io (2020)</em></td>
<td>Traffic (custom)</td>
<td>HTTP/TCP/TLS proxy, by distance and load</td></tr>
<tr><td><em>Modal (2023)</em></td>
<td>Traffic (custom)</td>
<td><strong>Translate HTTP to function calls</strong></td></tr></tbody>
</table></div>


<p>So… I spot a difference there. Hang on a second. I want to talk about Modal’s
HTTP ingress.</p>
<h3 id="translating-http-to-function-calls">Translating HTTP to function calls</h3>
<p>You might notice that setting up an HTTP reverse proxy in front of serverless
functions is a popular option. This means that you scale up your container, and
some service in front handles TLS termination and directly forwards traffic to a
backend server. For most of these platforms, HTTP is the main way you can talk
to these serverless functions, as a network service.</p>
<p>But for Modal, we’re focused on building a platform based on the idea that
serverless functions are just <em>ordinary functions</em> that you can call. If you
want to define a function on Modal, that should be easy! You don’t need to set
up a REST API. Just call it directly with <code>.remote()</code>.</p>


<p>Since <code>run_batch_job()</code> can be invoked in any region, and <code>compute_embeddings()</code>
can be called remotely from it, we needed to build generic high-performance
infrastructure for serverless <em>function calls</em>. Like, actually “calling a
function.” Not wrapping it in some REST API.</p>
<p>Calling a function is a bit different from handling an HTTP request. There’s a
mismatch if you try to conflate them! By supporting both of these workloads, we
can:</p>
<ul><li>Use a faster, optimized path (for calls between functions) that can be
location and data cache-aware, rather than relying on the same HTTP protocol.</li>
<li>Fully support real-time streaming in network requests, rather than limiting it
to fit the use case of a typical function call.</li>
<li>Offer first-class support for complex heterogeneous workloads on CPU and GPU.</li></ul>
<p>Modal’s bread and butter is systems engineering for heavy-duty function calls.
We’re already focused on making that fast and reliable. As a result, we decided
to handle web requests by translating them into function calls, which gives us a
foundation of shared infrastructure to build upon.</p>
<h2 id="understanding-the-http-protocol">Understanding the HTTP protocol</h2>
<p>To understand how HTTP gets turned into a function calls, first we need to
understand HTTP. HTTP follows a request-response model. Here’s what a typical
flow looks like. On the top, you can see a standard <code>GET</code> request with no body,
and on the bottom is a <code>POST</code> request with body.</p>
<p><em><strong>Note:</strong> HTTP GET requests can technically have bodies too, though they should
be ignored. Also, a less-known fact is that request and response bodies can be
interleaved,
<a rel="nofollow" href="https://datatracker.ietf.org/doc/html/rfc6202">sometimes even in HTTP/1.1</a>!</em></p>
<p><img src="https://www.jakef.science/_app/immutable/assets/modal-http-10.80f2e446.png" alt="Diagram of two requests, HTTP GET on top and HTTP POST on the bottom"/>



</p>
<p>The client sends some headers to the server, followed by an optional body. Once
the server receives the request, it does some processing, then responds in turn
with a set of a headers and its own response body.</p>
<p>Both the client and server directions are sent over a specific wire protocol,
which varies between HTTP versions. For example, HTTP/1.0 uses a TCP stream for
each request, HTTP/1.1 added keepalive support, HTTP/2 has concurrent stream
multiplexing over a single TCP stream, and HTTP/3 uses QUIC (UDP) instead of
TCP. They’re all unified by this request-response model.</p>
<p>Here’s what an HTTP/1.1 GET looks like, as displayed by <code>curl</code> in verbose mode.
The <code>&gt;</code> lines are request headers, the <code>&lt;</code> lines are response headers, and the
response body is at the end:</p>


<p>To iron out the differences between HTTP protocol versions, we needed a backend
data representation for the request. In a reverse proxy, the backend protocol
would just be HTTP/1.1, but in our case that would add additional complexity for
reliably reconnecting TCP streams and parsing the wire format. We instead
decided to base our protocol on a stream of <em>events</em>.</p>
<p>Luckily, there was already a well-specified protocol for representing HTTP as
event data: <a rel="nofollow" href="https://github.com/django/asgiref">ASGI</a>, typically used as a
standard interface for web frameworks in Python.</p>
<p><em><strong>Note:</strong> ASGI was made for a different purpose! Usually the web server and
ASGI application run on the same machine. Here we’re using it as the internal
communication language for a distributed runtime. So we adjusted the protocol to
our use case by serializing events as binary Protocol Buffers.</em></p>
<p>ASGI doesn’t support every internal detail of HTTP (e.g., gRPC servers need
access to HTTP/2 stream IDs), but it’s a common denominator that’s enough for
web apps built with all the popular Python web frameworks: Flask, Django,
FastAPI, and more. That’s a lot of web applications, and the benefit of this
maturity is that it lets us greatly simplify our model of HTTP serving.</p>
<p>Here’s what a POST request looks like in ASGI. The blue arrows represent client
events, while the green arrows are events sent from the server.</p>
<p><img src="https://www.jakef.science/_app/immutable/assets/modal-http-11.9ebf9ddb.png" alt="Diagram of an HTTP POST request with events marked"/>



</p>
<ol><li>At the start of a request, when headers are received, we begin by parsing the
headers to generate a <em>function input</em> via the <code>http</code> request scope. This
triggers a new function call, which is scheduled on a running task according
to availability and locality.</li>
<li>Then, the request body is streamed in, and we begin reading it in chunks to
produce real-time <code>http.request</code> events that are sent to the serverless
function call. If the server falls behind, backpressure is propagated to the
client via TCP (for HTTP/1.1) or HTTP/2 flow control.</li>
<li>The function starts executing immediately after getting the request headers,
then begins reading the request body. It sends back its own headers and
status code, followed by the response body in chunks.</li>
<li>The request-response cycle finishes, optionally with HTTP trailers.</li></ol>
<p>In this way, we’re able to send an entire HTTP request and response over a
generic serverless function call. And it’s efficient too, with proper batching
and backpressure. We don’t need to establish a single TCP stream or anything; we
can use reliable, low-latency message queues to send the events.</p>
<p>Unlike AWS Lambda’s 6 MB limit for request and response bodies, this
architecture lets us support request bodies of up to 4 GiB (682x bigger), and
streaming response bodies of unlimited size.</p>
<p>Of course, although conceptually simple, it’s still a pretty tricky thing to
implement correctly since there are a lot of concurrent moving parts. Our
implementation is in Rust, based on the <a rel="nofollow" href="https://hyper.rs/">hyper</a> HTTP server
library and <a rel="nofollow" href="https://tokio.rs/">Tokio</a> async runtime. Here’s a snippet of the
code that buffers the request body in chunks of up to 1 MiB in size, or waits
for 2 milliseconds of duration.</p>


<p>You might have noticed the <code>disconnect_rx</code> channel used in the snippet above.
This hints at one of the realities of making reliable distributed systems that
we glossed over: needing to thoroughly handle failure cases everywhere, all the
time.</p>
<h3 id="edge-cases-and-errors">Edge cases and errors</h3>
<p>First, if a client sends an HTTP request but exits in the middle of sending the
body, then we propagate that disconnection to the serverless function.</p>
<p><img src="https://www.jakef.science/_app/immutable/assets/modal-http-12.350fa27e.png" alt="Diagram of a disconnected HTTP request"/>



</p>
<p>We reify this using an ASGI <code>http.disconnect</code> event, which allows the user’s
code to stop executing gracefully. Otherwise, we might have a function call
that’s still running even after the user has canceled their request.</p>
<p>Another issue is if the server has a failure. It might throw an exception, crash
due to running out of memory, hit a user-defined timeout, be preempted if on a
spot instance, and so on. If a malicious user is on the system, they also might
send malformed response events, or events in the wrong order!</p>
<p>We keep track of any violations and display an error message to the user. Rust’s
pattern matching and ownership help with managing the casework.</p>
<h3 id="dealing-with-http-idle-timeouts">Dealing with HTTP idle timeouts</h3>
<p>Okay, so if we had been a standard runtime, we would be done with HTTP now. But
we’re still not done! There’s one more thing to consider: long-running requests.</p>
<p>If you make an HTTP request and the server doesn’t respond for 300 seconds, then
Chrome cancels the request and gives you an error. This is not configurable.
Other browsers and pieces of web infrastructure have varying timeouts. Our users
often end up running expensive models that take longer than 5 minutes, so we
need a way to support long-running requests.</p>
<p>Luckily, there’s a solution. After 150 seconds (2.5 minutes), we send a
temporary “303 See Other” redirect to the browser, pointing them to an
alternative URL with an ID for this specific request. The browser or HTTP client
will follow this redirect, ending their current stream and starting a new one.</p>
<p>Browsers will follow up to 20 redirects for a link, so this effectively
<a rel="nofollow" href="https://modal.com/docs/guide/webhook-timeouts">increases the idle timeout to 50 minutes</a>.
An example of this in action is shown below, with a single redirect.</p>
<p><img src="https://www.jakef.science/_app/immutable/assets/modal-http-13.e1bc7734.png" alt="Diagram of a long-running request with one 303 See Other response"/>



</p>
<p>Is this behavior a little strange? Yes. But it just works “out-of-the-box” for a
lot of people who have web endpoints that might execute for a long time. And if
your function finishes processing and begins its response in less than 2.5
minutes, you’ll never notice a difference anyway.</p>
<p>For people who need to have very long-running web requests, Modal <em>just works</em>.</p>
<h3 id="websocket-connections">WebSocket connections</h3>
<p>That’s it for HTTP. What if a user makes a WebSocket connection? Well, the
WebSocket protocol works by starting an HTTP/1.1 connection, then establishing a
<em>handshake</em> via HTTP’s connection upgrade mechanism. The
<a rel="nofollow" href="https://datatracker.ietf.org/doc/html/rfc6455#section-1.2">handshake</a> looks
something like this:</p>


<p><em><strong>Note:</strong> There is also another version of the WebSocket protocol that
bootstraps from HTTP/2, but it’s not supported by many web servers yet. For now,
you need a dedicated TCP connection.</em></p>
<p>The <code>Sec-WebSocket-Key</code> header is random, while <code>Sec-WebSocket-Accept</code> is
derived from an arbitrary hash function on the key. (This is just some protocol
junk that we had to implement, see
<a rel="nofollow" href="https://datatracker.ietf.org/doc/html/rfc6455">RFC 6455</a>.) ASGI has a separate
<a rel="nofollow" href="https://asgi.readthedocs.io/en/latest/specs/www.html#websocket">WebSocket interface</a>
that encodes this handshake into a pair of <code>websocket.connect</code> and
<code>websocket.accept</code> events, so we translated our incoming request into those
events.</p>
<p>After the handshake, all of the infrastructure is already in place, and we
transmit messages between <code>modal-http</code> and the serverless function via data
channels in the same way as we did for HTTP.</p>
<p><img src="https://www.jakef.science/_app/immutable/assets/modal-http-14.d413aaba.png" alt="Diagram of a WebSocket connection"/>



</p>
<p>Our server-side Rust implementation is based on hyper as before, but it upgrades
the connection to an asynchronous
<a rel="nofollow" href="https://github.com/snapview/tokio-tungstenite">tokio-tungstenite</a> stream once
the handshake is accepted.</p>
<h2 id="building-on-open-source-infrastructure">Building on open-source infrastructure</h2>
<p>We’ve built a lot of infrastructure to support HTTP and WebSocket connections,
but we didn’t start from scratch. The Rust ecosystem was invaluable to making
this custom network service, which needed to be high-performance and correct.</p>
<p>But while we’ve talked a lot about the serverless backend and design choices
made to support heavy workloads, we haven’t talked yet about how requests
actually <em>get</em> to <code>modal-http</code>. For this part, we relied on boring, mature
open-source cloud infrastructure pieces.</p>
<p>Let’s still take a look though. Modal web endpoints run on the wildcard domain
<code>*.modal.run</code>, as well as on
<a href="https://www.jakef.science/docs/guide/webhook-urls#custom-domains">custom domains</a> as assigned by users
via a CNAME record to <code>cname.modal.domains</code>. The most basic way you’d deploy a
Rust service like <code>modal-http</code> is by pointing a
<a rel="nofollow" href="https://en.wikipedia.org/wiki/Domain_Name_System">DNS record</a> at a running
server, which has the compiled binary listen on a port.</p>
<p><img src="https://www.jakef.science/_app/immutable/assets/modal-http-0.8a9d6b39.png" alt="A browser sends a request to modal-http"/>



</p>
<p>Rust is pretty fast, so this is a reasonable design for most real-world
services. A single node nevertheless doesn’t scale well to the traffic of a
cloud platform. We wanted:</p>
<ul><li><strong>Multiple replicas.</strong> Replication of the service provides fault tolerance and
eases the process of rolling deployments. When we rollout a new version, old
replicas need a gradual timeout.</li>
<li><strong>Encryption.</strong> Support for TLS is missing here. We <em>could</em> handle it in the
server directly, but rather than reinventing the wheel, it’s easier and safer
to rely on well-vetted software for TLS termination. (We also need to allocate
<a rel="nofollow" href="https://caddyserver.com/docs/automatic-https#on-demand-tls">on-demand certificates</a>
for custom domains.)</li></ul>
<p>So, rather than the simplified flow above, our actual ingress architecture to
<code>modal-http</code> looks like this. We placed a TCP network load balancer in front of
a <a rel="nofollow" href="https://kubernetes.io/">Kubernetes</a> cluster, which runs a
<a rel="nofollow" href="https://caddyserver.com/docs/">Caddy</a> deployment, as well as a separate
deployment for <code>modal-http</code> itself.</p>
<p><img src="https://www.jakef.science/_app/immutable/assets/modal-http-1.e632ef42.png" alt="Full path of a request through L4 NLB and Caddy"/>



</p>
<p>Note that none of our <em>serverless functions</em> run in this Kubernetes cluster.
Kubernetes isn’t well-suited for the workloads we described, so we wrote our own
high-performance serverless runtime based on <a rel="nofollow" href="https://gvisor.dev/">gVisor</a>, our
own file system, and our own job scheduler — which we’ll talk about another
time!</p>
<p>But Kubernetes is still a rock-solid tool for the more traditional parts of our
cloud infrastructure, and we’re happy to use it here.</p>
<h3 id="caveat-multi-region-request-handling">Caveat: Multi-region request handling</h3>
<p>It’s a fact of life that light takes time to travel through fiber-optic cables
and routers. Ideally, <code>modal-http</code> should run on the edge in geographically
distributed data center regions, and requests should be routed to the nearest
replica. This is important to minimize baseline latency for web serving.</p>
<p>We’re not there yet though. It’s early days! While our serverless functions are
already running in many different clouds and data centers based on <em>compute
availability</em>, since GPUs are scarce, our actual servers only run in Ashburn,
Virginia for now.</p>
<p>This is a bit of a tradeoff for us, but it’s not a fundamental one. It gives us
more flexibility at the moment, although <code>modal-http</code> will be deployed to more
regions in the future for latency reasons. Right now heavyweight workloads on
Modal probably aren’t affected, but for very latency-sensitive workloads (under
100 ms), you’ll likely want to specify your container to run in Ashburn.</p>
<h2 id="lessons-learned">Lessons learned</h2>
<p>So, there you have it. Serverless functions are traditionally limited to a
request-response model, but Modal just released full support for WebSockets,
with GPUs and fully managed autoscaling. And we did this by translating web
requests into function calls.</p>
<p>Our service, <code>modal-http</code>, is written in Rust and based on several components
that let us handle HTTP and WebSocket requests at scale. We’ve placed it behind
infrastructure to handle the ingress of requests, and we’re planning to expand
to more regions in the future.</p>
<p>Some may wonder: If Modal translates HTTP to this message format, wouldn’t that
stop people from being able to use the traditional container model of
<a rel="nofollow" href="https://docs.docker.com/reference/dockerfile/#expose"><code>EXPOSE</code></a>-ing TCP ports?
This is a good question, but it’s not a fundamental limitation. The events can
be losslessly translated back to HTTP on the other end! We
<a href="https://www.jakef.science/docs/examples/comfy_ui">wrote examples</a> of this for systems like ComfyUI, and
we’re
<a rel="nofollow" href="https://github.com/modal-labs/modal-client/pull/1513">building it into the runtime</a>
with just a bit of added code.</p>
<p>We’ve already been running Rust to power our serverless runtime for the past two
years, but <code>modal-http</code> gives us more confidence to run standard Rust services
in production. Just for comparison, when we first introduced this system to
replace our previous Python-based ingress, the number of <code>502 Bad Gateway</code>
errors in production decreased by 99.7%, due to clearer error handling and
tracking of request lifetimes. And it laid the groundwork for WebSocket support
without fundamental changes.</p>
<p>Today, web endpoints and remote function calls on Modal use a common system.
Having uniformity allows us to focus on impactful work that makes our cloud
runtime faster and lower-priced, while improving security and reliability over
time.</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>Thanks to the Modal team for their feedback on this post. Special thanks to
Jonathon Belotti, Erik Bernhardsson, Akshat Bubna, Richard Gong, and Daniel
Norberg for their work and design discussions related to <code>modal-http</code>.</p>
<p>If you’re interested in fast, reliable, and heavy-duty systems for the cloud,
<a href="https://www.jakef.science/company">Modal is hiring</a>.</p>
</div></div></div>
  </body>
</html>
