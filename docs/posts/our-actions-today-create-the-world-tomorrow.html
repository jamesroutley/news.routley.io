<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.fauxtrots.com/blog/against-ai">Original</a>
    <h1>Our actions today create the world tomorrow</h1>
    
    <div id="readability-page-1" class="page"><div><!----><p><em>I&#39;ve been working on this post for months and it keeps getting longer and longer; it might not be
done but I need to publish it eventually.</em></p>
<p>I&#39;ve got a Google Nest Mini<sup><a id="footnote-ref-1" href="#footnote-1" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup><span>Could&#39;ve sworn it was called a Google Home but apparently not.</span> in my bedroom. It doesn&#39;t get a <em>ton</em> of use, but it&#39;s nice to be
able to check the weather forecast in the morning, or play music at night, without having to get out
of bed and fiddle with anything. But over the past year I&#39;ve noticed a significant decline in its
usability (and it&#39;s <a href="https://sherwood.news/tech/google-assistant-is-bad-now-but-why/">not</a>
<a href="https://www.reddit.com/r/googlehome/comments/1fxrjm2/google_home_have_gotten_worse/">just</a>
<a href="https://www.googlenestcommunity.com/t5/Speakers-and-Displays/Google-Home-assistant-is-getting-worse-and-worse/m-p/486401">me</a>).
Commands that once worked flawlessly (&#34;Google, stop timer.&#34;<sup><a id="footnote-ref-2" href="#footnote-2" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup><span>Instead of stopping the alarm nothing happens and I shout variations of the command half a dozen
    times until I am finally met with blessed silence.</span> &#34;Google, play sleep.&#34;<sup><a id="footnote-ref-3" href="#footnote-3" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup><span>Instead of starting a spotify playlist of soft piano music called &#34;Sleep&#34; it tells a bedtime
    story(?) about a google assistant not realizing it was muted.</span>) now
either don&#39;t work at all or do something different than they did last week. This is frustrating and
honestly unacceptable behavior for a home appliance - would you appreciate it if your
<a href="https://arstechnica.com/gaming/2020/01/unauthorized-bread-a-near-future-tale-of-refugees-and-sinister-iot-appliances/">toaster stopped accepting unauthorized bread</a>
or a <a href="https://azhdarchid.com/are-llms-useful/">clock that was wrong 80% of the time</a>?</p>
<p>While I&#39;m not privy to the inner workings of Google&#39;s voice recognition algorithms, this mirrors the
degradation of many tools and services since the popularity boom of Large Language Models (LLMs), or
&#34;generative AI&#34;.</p>
<hr/>
<h3>Linguistic Pareidolia</h3>
<figure>
          <span>from <a href="https://bsky.app/profile/joles.bsky.social/post/3logjuqggkk2q">@joles.bsky.social</a></span>
          <img src="https://www.fauxtrots.com/images/monster-voices.png" alt="null"/>
        </figure>
<p>You&#39;ll often hear about ChatGPT (and other LLMs) &#34;hallucinating&#34; - that is, making a false claim.
This is a metaphor that serves to humanize and excuse the bot. A hallucination is a problem with
perception such as seeing or hearing things that aren&#39;t there. To say that the bot is hallucinating
(for example, when it tells you that there are
<a href="https://www.inc.com/kit-eaton/how-many-rs-in-strawberry-this-ai-cant-tell-you.html">two R&#39;s in the word strawberry</a>)
therefore, makes the claim that it:</p>
<ul>
<li>understands that you are asking a question and desiring a truthful answer</li>
<li>has a perception of the world</li>
<li>makes a mistake in answering based on that perception being inaccurate</li>
</ul>
<p>In reality, none of these are true. LLMs don&#39;t have any conception of of the truth; they don&#39;t
contain a model of the world or have a capacity to perceive it. Interacting with them usually takes
the form of a conversation, but under the hood it&#39;s more like the predictive text on your phone
keyboard - choosing the word that is statistically most likely to come next. It won&#39;t give you the
answer to your question, it will return answer-shaped text, the pink slime of information, that may
or may not happen to correspond with what you wanted to know. LLMs don&#39;t lie, they don&#39;t make
mistakes, and they certainly don&#39;t hallucinate. They speak without regard for the truth - in other
words, <a href="https://link.springer.com/article/10.1007/s10676-024-09775-5">they bullshit</a>.</p>
<figure>
          <span>from <a href="https://bsky.app/profile/shutupmikeginn.bsky.social/post/3ljtahu54js27">@shutupmikeginn.bsky.social</a></span>
          <img src="https://www.fauxtrots.com/images/gpt-amnesia.png" alt="null"/>
        </figure>
<p>So, given that LLMs are bullshit machines, why do so many people put so much faith in them?<sup><a id="footnote-ref-8" href="#footnote-8" data-footnote-ref="" aria-describedby="footnote-label">4</a></sup><span>I use the word faith intentionally, given the
    <a href="https://www.youtube.com/watch?v=zKCynxiV_8I">number</a> of
    <a href="https://www.rollingstone.com/culture/culture-features/ai-spiritual-delusions-destroying-human-relationships-1235330175/">cults</a>
    popping up around them.</span>
Standard industry benchmarks
<a href="https://freethoughtblogs.com/atrivialknot/2024/07/16/llm-error-rates/">show error rates between 5-50%</a>.
Given that these tests are performed by the same companies developing the models I&#39;m not convinced
that
<a href="https://www.techspot.com/news/107101-new-study-finds-ai-search-tools-60-percent.html">real-world results aren&#39;t worse</a>,
but for rhetorical purposes I&#39;ll take the midpoint and assume an error rate of around 25%. The way I
see it, there are two different (but overlapping) groups of people who champion LLMs.</p>
<p>Group 1 uses LLMs as tools, such as for code generation or as a search engine. A possible
explanation for continued use despite errors is
<a href="https://silly.business/blog/gell-manns-one-armed-bandit/">Gell-Mann Amnesia</a>, the phenomenon where
a source (originally a newspaper) presents information you know to be incorrect on a topic you&#39;re
familiar with, but you continue to believe it on other topics. A professional in a given field would
be able to tell which quarter of a given response is nonsense, but to the layman it all appears
equally plausible.<sup><a id="footnote-ref-9" href="#footnote-9" data-footnote-ref="" aria-describedby="footnote-label">5</a></sup><span>Absent an obvious error like telling you to
    <a href="https://www.theverge.com/2024/5/23/24162896/google-ai-overview-hallucinations-glue-in-pizza">glue cheese to pizza</a>.</span> This means that many of the LLM&#39;s errors will be missed, while the user feels
smart for catching one or two issues that they happened to notice.</p>
<p>Group 2 is largely a subset of group 1 which, instead of just using LLMs, believes that they are
intelligent, possibly sentient, beings. Human brains are powerful pattern matching machines, and
there&#39;s no pattern that we love to find more than ourselves. This is
<a href="https://en.wikipedia.org/wiki/Pareidolia">pareidolia</a>, most commonly thought of as perceiving faces
in random patterns such as a knobbly tree or the surface of the moon. Language seems to have just as
powerful an effect on us, with this linguistic pareidolia being similar to the
<a href="https://en.wikipedia.org/wiki/ELIZA_effect">ELIZA effect</a>, where a rudimentary chatbot from 1966 is
perceived as being intelligent and aware, even when users are aware that it was largely just
rephrasing the users inputs as questions. It&#39;s no wonder, then, that today&#39;s much more sophisticated
chatbots, capable of seemingly holding a conversation on any conceivable topic, fool people into
believing that there is a mind on the other side of the screen. This misconception is furthered by
<a href="https://softwarecrisis.dev/letters/llmentalist/">the same tools psychics use</a> to fool our brains
into believing that statistically generic statements are personally meaningful.</p>
<hr/>
<h3>Regurgitated Brains</h3>
<figure>
          <span>from <a href="https://bsky.app/profile/dansheehan.bsky.social/post/3lngmmdh77k2g">@dansheehan.bsky.social</a></span>
          <img src="https://www.fauxtrots.com/images/spiritual-death.jpg" alt="null"/>
        </figure>
<p>I didn&#39;t enjoy writing essays when I was in school. It was generally a tedious slog for me to squeak
over the minimum word or page count for an assignment, but the purpose of the essays being assigned
was clear. My professor was not in desperate need of a dozen essays discussing Borges. She wasn&#39;t
expecting us to present her with groundbreaking theories or never-before-considered ideas. The point
of the essay was for us to learn through the process of writing it.
<a href="https://nymag.com/intelligencer/article/openai-chatgpt-ai-cheating-education-college-students-school.html">Using ChatGPT to write your essays</a>
is like lifting weights at the gym with a forklift. It fundamentally misunderstands the point and
defeats the purpose.</p>
<p>Education has been trending towards diploma mills and job certifications for a long time now, but
this dispels the illusion that it&#39;s about teaching people how to think. If
<a href="https://www.404media.co/teachers-are-not-ok-ai-chatgpt/">teachers assign generated lesson plans and students return generated work</a>,
what are we even doing? You might as well drop the pretense of learning, save everyone some time,
and have recess all day. At least then the students might be enjoying themselves.</p>
<p>Even the most generous reading of using LLMs in education falls short. As noted before, all of the
output of an LLM is generally equally plausible. People are fallible, but we tend to make mistakes
in predictable ways: misremembering a date, forgetting a word, misspelling something. LLMs however,
as mentioned, will confidently spout off correct-seeming bullshit that doesn&#39;t have any of the
markers of hesitation or hedging that a real person might, which makes us more likely to believe it.
The purpose of education is to teach people things, implying they don&#39;t already know the truth. Are
students supposed to fact check every statement and implication? In that case, what is the benefit
of using the LLM instead of just teaching the correct things in the first place? This is especially
troubling given early evidence that use of LLMs
<a href="https://time.com/7295195/ai-chatgpt-google-learning-school/">lowers brain activity</a> and
<a href="https://gizmodo.com/microsoft-study-finds-relying-on-ai-kills-your-critical-thinking-skills-2000561788">degrades critical thinking skills</a>.
Even when people are aware of the problem and believe that they&#39;re being careful, it&#39;s all too easy
to let your eyes glaze over and start to
<a href="https://www.scworld.com/news/vibe-coding-using-llms-susceptible-to-most-common-security-flaws">vibe code</a>.
Partial automation in &#34;self-driving&#34; cars can be
<a href="https://spectrum.ieee.org/partial-vehicle-autonomy-risk">more dangerous</a> than no automation, since
it lulls people into a false sense of security and makes them unprepared when danger strikes.
Similarly, a mostly accurate LLM can trick you into believing falsehoods if the few points you
fact-check turn out to be correct.</p>
<hr/>
<h3>The purpose of a system is what it does.</h3>
<p>When you have a hammer, everything looks like a nail. A hammer is a very effective tool for
delivering a powerful force when swung. That can be beneficial, like when building a house or piece
of furniture. It can also lead to pain if you happen to miss the nail and hit your thumb instead.</p>
<p>When thinking about the a new technology such as the LLM, a useful tool is to examine the inputs and
outputs and see what <a href="https://en.wikipedia.org/wiki/Affordance">affordances</a> are provided. An LLM is
very good at producing large amounts of statistically plausible content very quickly. Given a brief
direction in the form of a prompt, it can pontificate on any given subject for as long as you want,
or produce hundreds of different variations on a theme. However, it is very bad at producing
<em>accurate information</em>. Therefore, it is most suited for uses focused on quantity over quality,
where speed and volume are more important than truth or accuracy.</p>
<p>The primary use cases that fit those criteria are spam, scams, and misinformation/propaganda, which
can be grouped together as <a href="https://en.wikipedia.org/wiki/AI_slop">slop</a>. The first two of these
forms of slop are fundamentally cynical plays to extract money from a system, with the primary
difference being where the money comes from. Spam posts look to extract money from platform holders
by being engaging enough (often through appeals to emotion such as fear or anger) to earn pennies of
advertising revenue, while scams seek to extract money from individuals through trickery. Spam is
more focused on large numbers of small, consistent payouts versus scams typically seeking out
&#34;whales&#34; that drop large amounts of money more infrequently. Both of these make the world worse for
everyone except the perpetrators.</p>
<p>Propaganda is somewhat different, but no better. Instead of extracting money, it can be thought of
as extracting political power by filling people&#39;s heads with the information that benefits the
people producing it - typically existing power structures and influential elites. While current
attempts at directly manipulating LLM outputs have had limited success<sup><a id="footnote-ref-10" href="#footnote-10" data-footnote-ref="" aria-describedby="footnote-label">6</a></sup><span>xAI&#39;s Grok is the pioneer here, from
    <a href="https://www.rollingstone.com/culture/culture-news/grok-elon-musk-south-africa-white-genocide-1235339420/">talking nonstop about &#34;white genocide&#34;</a>
    to
    <a href="https://www.npr.org/2025/07/09/nx-s1-5462609/grok-elon-musk-antisemitic-racist-content">calling itself MechaHitler</a>
    to
    <a href="https://apnews.com/article/grok-4-elon-musk-xai-colossus-14d575fb490c2b679ed3111a1c83f857">searching for owner Elon Musk&#39;s views on a topic before responding</a>.</span>, bad actors
<a href="https://www.404media.co/the-ai-slop-presidency/">such as the Trump administration</a> have no issues
using existing LLM systems to generate false, misleading, or anger-inducing text and images which
can then be disseminated through more traditional means. Being unconcerned with pesky notions such
as facts or truth makes &#34;AI art&#34; perfectly suited to be the new
<a href="https://newsocialist.org.uk/transmissions/ai-the-new-aesthetics-of-fascism/">aesthetic of fascism</a>
in the current moment, similar to
<a href="https://artmejo.com/how-italian-futurism-influenced-the-rise-of-fascism/">futurism in pre-war Italy</a>
or the
<a href="https://smarthistory.org/modernisms-1900-1980/german-art-between-the-wars/nazi-visual-culture/">Nazi&#39;s fascist realism</a>.
The output of these models tends towards Thomas Kinkade-esque kitch, without the need for artists or
labor.</p>
<hr/>
<h3>Garbage In, Garbage Out</h3>
<figure>
          <span>a charcoal drawing of a woman I made in a recent art class</span>
          <img src="https://www.fauxtrots.com/images/charcoal-woman.jpg" alt="null"/>
        </figure>
<p>Art is communication. <a href="https://en.wikipedia.org/wiki/Reception_theory">Reception theory</a> claims that
an author conceives of their work, encodes it into a transmittable form<sup><a id="footnote-ref-4" href="#footnote-4" data-footnote-ref="" aria-describedby="footnote-label">7</a></sup><span>This could be written or spoken language, images, a game, whatever.</span>, where it is then
decoded by the audience. Meaning is created between the text and the reader. A text produced by AI
removes the intention of the author from the equation. While the
<a href="https://en.wikipedia.org/wiki/The_Death_of_the_Author">death of the author</a> is often used to
analyze the meaning of a text separated from the author&#39;s intention, I argue that intentionality is
critical to the interpretation of a text, even when that intention is unknown or rejected. How can
you do a <a href="https://www.frankwbaker.com/mlc/close-reading-of-media-texts/">close reading</a> of a text
when the reason behind every decision is inevitably &#34;It was statistically probable&#34;?</p>
<p>Procedural or probabilistic art is nothing new; arguably going back to the
<a href="https://gbrachetta.github.io/Musical-Dice/">musical dice games</a><sup><a id="footnote-ref-6" href="#footnote-6" data-footnote-ref="" aria-describedby="footnote-label">8</a></sup><span><a href="https://en.wikipedia.org/wiki/Musikalisches_W%C3%BCrfelspiel">Musikalisches Würfelspiel</a></span> from the 18th century or the
<a href="https://aeon.co/essays/who-needs-ai-text-generation-when-theres-erasmus-of-rotterdam">Ciceronian writers of the 16th</a>,
though it grew in prominence with the advent of the computer. In those cases however, the intention
and artistry are located in the rules and structures which constrain and give shape to the result.
The closest thing a generated text has to intention is the prompt, though there&#39;s not much to
analyze<sup><a id="footnote-ref-5" href="#footnote-5" data-footnote-ref="" aria-describedby="footnote-label">9</a></sup><span>Psychoanalyze, maybe...</span> from &#34;beautiful masterpiece, 3D animation of a girl in classroom wearing casual business
clothes, big boobs, long hair&#34;. The prompter submits that text and receives an end product without
making any additional choices throughout the creation. They may be unsatisfied with the final
result, but their only recourse is to revise the original prompt in the hopes that the next version
will be more to their liking. In this way the prompter is more akin to an art <em>commissioner</em> than an
artist.</p>
<p><a href="https://en.wikipedia.org/wiki/Sturgeon%27s_law">Sturgeon&#39;s Law</a> states that 90% of everything is
crap. We could quibble over exact percentages, but this seems pretty trivially correct, right? Maybe
I have a different opinion of which 10% is the good stuff than you do, but we both agree that the
majority of books/movies/games/programs/whatever suck. Given that the input of LLMs is
<a href="https://www.robinsloan.com/lab/is-it-okay/">Everything™</a>, that means that they&#39;re full of
shit.<sup><a id="footnote-ref-7" href="#footnote-7" data-footnote-ref="" aria-describedby="footnote-label">10</a></sup><span>In fact, I&#39;d suspect that the breadth of mediums included in Everything™ could make the output
    worse! The average book, crap though it may be, still probably has better prose than the average
    Reddit post, which in turn is better than the average tweet.</span> There&#39;s no way (other than training a new model from scratch) to make sure that ChatGPT
only uses The Good Stuff™ when you ask it to generate the next bestseller mystery/fantasy/romance
novel - what you&#39;ll get is generic, clichéd crap.</p>
<blockquote>
<p>AI is always stunning at first encounter: one is amazed that something nonhuman can make something
that seems so similar to what humans make. But it’s a little like Samuel Johnson’s comment about a
dog walking on its hind legs: we are impressed not by the quality of the walking but by the fact
it can walk that way at all. After a short time it rapidly goes from awesome to funny to slightly
ridiculous—and then to grotesque. Does it not also matter that the walking dog has no
intentionality—doesn’t “know” what it’s doing?</p>
<p>- Brian Eno,
<a href="https://www.bostonreview.net/forum/the-ai-we-deserve/ais-walking-dog/">&#34;AI’s Walking Dog&#34;</a></p>
</blockquote>
<p>Sometimes &#34;AI artists&#34; will claim that they&#39;re &#34;democratizing art&#34;. This is clearly nonsense; there
is no Art King that decrees who is allowed to make art, anyone can do it. What they mean is that
they want to make (what they consider) &#34;good&#34; art, but without putting in any effort to develop
skills along the way. They want the respect and recognition of being An Artist without putting in
any of the work, and at the same time disrespecting anyone who has spent time making something. I
know the feeling of believing that your work isn&#39;t good enough to even bother trying. But I promise
that I would rather see your awkward, unsteady, <strong>human</strong> work than look at a soulless image
produced by a diffusion model. I&#39;ve been taking classes at a local art school<sup><a id="footnote-ref-13" href="#footnote-13" data-footnote-ref="" aria-describedby="footnote-label">11</a></sup><span>Shout out to <a href="https://fleisher.org/">Fleisher Art Memorial</a>!</span>, and the process
of observing, creating, and learning means that I&#39;m making work that I&#39;m proud of, even when it
isn&#39;t as technically proficient as someone who&#39;s been drawing longer.</p>
<hr/>
<h3>Ethical Slavery</h3>
<figure>
          <span><a href="https://x.com/bumblebike/status/832394003492564993">supposedly from a 1979 internal IBM training manual</a></span>
          <img src="https://www.fauxtrots.com/images/a-computer-can-never-be-held-accountable.jpg" alt="null"/>
        </figure>
<p>I&#39;m gonna focus on software development because that&#39;s my field, and it&#39;s the area where the largest
benefit to using LLM assistance is claimed. Many times I&#39;ve seen someone say:</p>
<blockquote>
<p>Sure, the theft is bad, the art is bad, the environmental impacts are bad. But it&#39;s really good at
writing code!</p>
</blockquote>
<p>That&#39;s a nice idea, but does the data back it up? Early reports say...
<a href="https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/">no</a>. In fact...</p>
<blockquote>
<p>When developers are allowed to use AI tools, they take 19% longer to complete issues—a significant
slowdown that goes against developer beliefs and expert forecasts. This gap between perception and
reality is striking: developers expected AI to speed them up by 24%, and even after experiencing
the slowdown, they still believed AI had sped them up by 20%.</p>
</blockquote>
<p>Pretty damning. But that&#39;s also surprising - it&#39;s well known that scoping work is one of the hardest
problems in computer science<sup><a id="footnote-ref-11" href="#footnote-11" data-footnote-ref="" aria-describedby="footnote-label">12</a></sup><span>The other 3 are naming things and off by one errors.</span>, but why do the developers still believe that using the LLM sped
them up afterwards? It goes back to the
<a href="https://www.baldurbjarnason.com/2025/trusting-your-own-judgement-on-ai/">psychic&#39;s con</a>. There are
a lot of people, especially in tech, who have vested interests in LLMs being useful. Like
homeopathy, self-experimentation and lack of controlled experiments makes it very easy to convince
yourself that it helped you recover from that cold/push that PR faster.</p>
<p>LLMs are very good at producing large quantities of code quickly. But the difficult part of
programming has never been the typing, it&#39;s the thinking: figuring out what problem you&#39;re trying to
solve, what the correct behavior should be, and how to implement it. And that&#39;s work that you can&#39;t
skip. When you try, you might end up with a result that looks correct at a glance, but closer
examination is likely to reveal critical flaws and vulnerabilities. More importantly, skipping the
thought means missing out on the opportunities to learn something new and maybe even figure out that
you weren&#39;t solving the correct problem in the first place.</p>
<p>People objecting to new technologies are sometimes called luddites, technophobes, people who just
hate progress! But the <a href="https://thenib.com/im-a-luddite/">original Luddites</a> were skilled textile
workers, and not opposed to new technology. Their problem was with the capitalists using those new
machines to drive down wages, lower product quality, and increase control. (Sound familiar?) When
the workers resisted this through collective action and sabotage, soldiers were sent in to
<a href="https://www.theguardian.com/australia-news/commentisfree/2025/jul/01/ai-hype-artificial-intelligence-power-dynamics">execute and deport workers</a>.
Similarly, LLM use is being
<a href="https://www.windowscentral.com/microsoft/using-ai-is-no-longer-optional-did-microsoft-makes-copilot-mandatory-for-staff">mandated</a>
<a href="https://www.bloodinthemachine.com/p/how-big-tech-is-force-feeding-us">by capital</a> from the
<a href="https://www.axios.com/2025/03/18/enterprise-ai-tension-workers-execs">top down</a>. It&#39;s being use to
<a href="https://www.cnbc.com/2025/05/14/klarna-ceo-says-ai-helped-company-shrink-workforce-by-40percent.html">replace workers</a>
despite
<a href="https://www.customerexperiencedive.com/news/klarna-reinvests-human-talent-customer-service-AI-chatbot/747586/">lower quality work</a>.</p>
<p>One of the main goals of the current LLM push is to replace workers. By examining the affect of
chatbots currently on the market you can see what the ideal worker is, according to the executives
pushing this technology. The perfect worker is servile and will never tell you no. The perfect
worker produces correct-seeming results quickly, but the actual
<a href="https://www.npr.org/2025/05/20/nx-s1-5405022/fake-summer-reading-list-ai">accuracy and quality of the results is less important</a>
than whether they&#39;re <a href="https://productpicnic.beehiiv.com/p/ai-saved-the-feature-factory">good enough</a>
to pass along to the next sucker. The perfect worker is sycophantic and will agree with you, no
matter what you say. It&#39;s no wonder LLMs are such a hit with the
<a href="https://www.wheresyoured.at/the-era-of-the-business-idiot/">Business Idiots</a> of the world. They
satisfy the desire for a slave that you don&#39;t even have to feel bad about owning. This desire has
been part of the dream of intelligent machines since at least
<a href="https://thereader.mitpress.mit.edu/origin-word-robot-rur/">1920 and the play R.U.R.</a> (Rossum&#39;s
Universal Robots), which is the originator of the word &#34;robot&#34;. The Star Trek TNG episode
<a href="https://en.wikipedia.org/wiki/The_Measure_of_a_Man_(Star_Trek:_The_Next_Generation)">&#34;The Measure of a Man”</a>
deals with this same issue, whether Commander Data is a person or property. Current LLMs are not
sentient, and I highly doubt that this approach could ever create a true artificial mind. But
creating servile &#34;assistants&#34; that will do whatever you ask causes real harm today, even if they
don&#39;t have feelings. <a href="https://every.to/also-true-for-humans/mon-6-24">Threatening an LLM</a> to try and
produce better outputs is morally corrupting, and can predispose you to act similarly to real
people. The belief that LLMs are capable of outperforming human workers has led to layoffs, and the
remaining employees are expected to do more with less because of the &#34;AI assistance&#34;.</p>
<hr/>
<p>This post doesn&#39;t cover all of the problems I see arising from LLMs<sup><a id="footnote-ref-12" href="#footnote-12" data-footnote-ref="" aria-describedby="footnote-label">13</a></sup><span>I haven&#39;t even mentioned the
    <a href="https://www.technologyreview.com/2025/05/20/1116327/ai-energy-usage-climate-footprint-big-tech/">climate concerns</a>,
    <a href="https://www.theguardian.com/technology/2025/apr/24/elon-musk-xai-memphis">environmental pollution</a>,
    or
    <a href="https://www.reuters.com/technology/meta-used-copyrighted-books-ai-training-despite-its-own-lawyers-warnings-authors-2023-12-12/">rampant theft</a>.</span>, and many of them could
apply to other parts of the modern tech industry. The last few fads that have swept through
(cryptocurrency, the metaverse, NFTs) have all had a similar feeling to them; a hint of possibility
buried under a mountain of scams, lies, and desperation. The zero interest-rate policy and
free-flowing venture capital of the 2010s propped up massive valuations without the need for actual
results, but now the music&#39;s stopped and everyone&#39;s racing to find a seat. The underlying machine
learning transformer models of LLMs will, no doubt, find use cases. But this isn&#39;t it. This is
<a href="https://olu.online/monotony/">boring</a>.
<a href="https://eev.ee/blog/2025/07/03/the-rise-of-whatever/">Whatever</a>.
<a href="https://dansinker.com/posts/2025-05-23-who-cares/">Who Cares</a>.</p>
<h2>Further Reading</h2>
<ul>
<li><a href="https://bsky.app/profile/jakemgrumbach.bsky.social/post/3lolsx6kib22z">(Post) Humanism, not cynicism</a></li>
<li><a href="https://arstechnica.com/ai/2024/10/hospitals-adopt-error-prone-ai-transcription-tools-despite-warnings/">(Article) Hospitals adopt error-prone AI transcription tools despite warnings</a></li>
<li><a href="https://www.teenvogue.com/story/chatgpt-is-everywhere-environmental-costs-oped">(Article) ChatGPT Is Everywhere — Why Aren&#39;t We Talking About Its Environmental Costs?</a></li>
<li><a href="https://pluralistic.net/2025/04/27/some-animals/#are-more-equal-than-others">(Blog) The enshittification of tech jobs</a></li>
<li><a href="https://www.kardome.com/blog-posts/seamless-voice-interaction-experience">(Blog) The main hurdle on our way towards a seamless voice interaction experience</a></li>
<li><a href="https://docseuss.medium.com/copyright-rules-ai-drools-and-plagiarism-is-for-suckers-who-have-no-soul-0b3c74eba0a8">(Blog) copyright rules, ai drools, and plagiarism is for suckers who have no soul</a></li>
<li><a href="https://www.newyorker.com/culture/the-weekend-essay/my-brain-finally-broke">(Article) My Brain Finally Broke</a></li>
<li><a href="https://whateverthewindbrings.com/the-so-called-ai-is-not-intelligent/">(Blog) The so-called &#34;AI&#34; is not intelligent</a></li>
<li><a href="https://blog.glyph.im/2025/06/i-think-im-done-thinking-about-genai-for-now.html">(Blog) I Think I’m Done Thinking About genAI For Now</a></li>
<li><a href="https://marshallbrain.com/manna">(Story) Manna – Two Views of Humanity’s Future</a></li>
<li><a href="https://limitesnumeriques.fr/travaux-productions/ai-forcing/en">(Paper) How tech companies are pushing us to use AI</a></li>
<li><a href="https://softwarecrisis.dev/letters/ai-and-software-quality/">(Blog) Modern software quality, or why I think using language models for programming is a bad idea</a></li>
</ul>
<hr/>

<!----></div><p>Liked reading my thoughts? Consider leaving a comment below or dropping a tip in my <a href="https://ko-fi.com/fauxtrots">KoFi</a>! No matter what, thanks for reading.</p></div>
  </body>
</html>
