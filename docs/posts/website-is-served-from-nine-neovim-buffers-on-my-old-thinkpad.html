<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://vim.gabornyeki.com/">Original</a>
    <h1>Website is served from nine Neovim buffers on my old ThinkPad</h1>
    
    <div id="readability-page-1" class="page"><div id="main-content">
<section id="This-Website-is-Served-from-Nine-Neovim-Buffers-on-My-Old-ThinkPad">

<p>August 18, 2025</p>
<p><strong>TL;DR:</strong>
I wrote a Neovim plugin in Lua that serves HTTP requests from open buffers.
It has no external dependencies, it has first-class support for serving content in <a href="https://djot.net">Djot</a>, and it is faster than Nginx so it won’t be a performance bottleneck behind a reverse proxy.
What’s not to like?</p>
<p>There is that <a href="https://www.reddit.com/r/emacs/comments/lly7po/comment/gnvzisy/">famous story</a> from the 1990s about the man who was a Lisper but could not afford any of the commercial Lisps, so he deployed message routing for a German air traffic control system in a headless instance of Emacs.
This, of course, is horrific.
But it does remind us: our editors are capable of more, if we just let them out of the little nook that they occupy in our imagination.</p>
<p>Like Emacs, Vim is also fairly well-regarded for its versatility, although not in the typical systems programming sense.
Yet part of the origin story of Neovim specifically is a desire for an editor that can handle asynchronous IO.<a id="fnref1" href="#fn1" role="doc-noteref"><sup>1</sup></a>
The result of the efforts that that desire spurred is an API that can be put to good use in networking.</p>
<div id="fig1">
<p><a href="https://vim.gabornyeki.com/screenshot.png"><img alt="A screenshot of nvim-web-server" src="https://vim.gabornyeki.com/screenshot.png"/></a></p>
<p><strong>Fig. 1.</strong>
A running instance of nvim-web-server.</p>
</div>
<p>I’ve written a plugin called <a href="https://github.com/gn0/nvim-web-server">nvim-web-server</a> that serves HTTP requests in pure Lua.
It doesn’t require Node.js, a Python interpreter, or any other external tools.
Only Neovim’s Lua API.</p>
<p>Benefits (tongue-in-cheek):</p>
<ul>
<li>
Instant deployment of new content.<a id="fnref2" href="#fn2" role="doc-noteref"><sup>2</sup></a>
</li>
<li>
The lowest-overhead content management system in existence.<a id="fnref3" href="#fn3" role="doc-noteref"><sup>3</sup></a>
</li>
<li>
Seamless Git integration.<a id="fnref4" href="#fn4" role="doc-noteref"><sup>4</sup></a>
</li>
<li>
Native support for Vim keybindings.
</li>
</ul>
<p>Downsides:</p>
<ul>
<li>
Are there any?
</li>
</ul>
<p>Of course there are but we will ignore them.</p>
<section id="Contents">
<h2>Contents</h2>
<ol>
<li>
<a href="#This-must-be-slow">This must be slow</a>
</li>
<li>
<a href="#Deploying-on-an-old-ThinkPad">Deploying on an old ThinkPad</a>
</li>
<li>
<a href="#Is-this-even-safe">Is this even safe?</a>
</li>
</ol>
</section>
<section id="This-must-be-slow">
<h2>This must be slow</h2>
<p>I had expected nvim-web-server to be slow, given that Lua is a dynamically typed, interpreted language.
But it’s not.
It is faster than Nginx.</p>
<p>How can that be?
Well, for one thing, it is purposefully built for serving a static website and nothing more.
Nginx can do a lot more than that (even though in this benchmark it doesn’t).
Then, nvim-web-server also leverages Neovim’s bindings to libuv, a library that provides an efficient event loop.</p>
<p>But asynchronous IO does not seem to be the only reason for nvim-web-server’s speed.
The asyncio-based Python library <a href="https://docs.aiohttp.org/en/stable/index.html">aiohttp</a> is <em>slower</em> than Nginx, at least on Python 3.10.
Historically, libuv (via <a href="https://github.com/MagicStack/uvloop">uvloop’s</a> bindings) was faster than asyncio, and this still seems to be the case as of Python 3.12.
But asyncio’s speed disadvantage appears to be <a href="https://github.com/MagicStack/uvloop/issues/566#issuecomment-2424812498">no more than</a> <a href="https://github.com/MagicStack/uvloop/issues/566#issuecomment-2498811498">10-to-20 percent</a>, which would not account for aiohttp’s underperformance compared with Nginx.</p>
<table>
<caption><strong>Table 1.</strong>  Concurrency and Web Server Performance</caption>
<tbody><tr>
<th>Server</th>
<th>Concurrent Requests<a id="fnref5" href="#fn5" role="doc-noteref"><sup>5</sup></a></th>
<th>Response Rate</th>
<th>Average</th>
<th>95%</th>
<th>99%</th>
</tr>
<tr>
<td>nvim-web-server</td>
<td>1</td>
<td>3,980.63/s</td>
<td>0.3 ms</td>
<td>0.3 ms</td>
<td>0.5 ms</td>
</tr>
<tr>
<td>nvim-web-server</td>
<td>50</td>
<td><mark>15,284.43/s</mark></td>
<td>3.2 ms</td>
<td>5.6 ms</td>
<td>7.3 ms</td>
</tr>
<tr>
<td>nvim-web-server</td>
<td>100</td>
<td><mark>15,124.05/s</mark></td>
<td>6.4 ms</td>
<td>11.4 ms</td>
<td>16.9 ms</td>
</tr>
<tr>
<td>nvim-web-server</td>
<td>200</td>
<td><mark>14,475.55/s</mark></td>
<td>13.5 ms</td>
<td>20.2 ms</td>
<td>36.3 ms</td>
</tr>
<tr>
<td>nvim-web-server</td>
<td>400</td>
<td><mark>14,445.56/s</mark></td>
<td>26.7 ms</td>
<td>43.6 ms</td>
<td>77.0 ms</td>
</tr>
<tr>
<td>Nginx</td>
<td>1</td>
<td>4,450.65/s</td>
<td>0.2 ms</td>
<td>0.4 ms</td>
<td>0.5 ms</td>
</tr>
<tr>
<td>Nginx</td>
<td>50</td>
<td>11,305.71/s</td>
<td>4.8 ms</td>
<td>10.1 ms</td>
<td>15.9 ms</td>
</tr>
<tr>
<td>Nginx</td>
<td>100</td>
<td>11,575.76/s</td>
<td>8.2 ms</td>
<td>21.8 ms</td>
<td>34.5 ms</td>
</tr>
<tr>
<td>Nginx</td>
<td>200</td>
<td>10,010.94/s</td>
<td>18.5 ms</td>
<td>53.6 ms</td>
<td>95.7 ms</td>
</tr>
<tr>
<td>Nginx</td>
<td>400</td>
<td>10,461.02/s</td>
<td>33.9 ms</td>
<td>96.4 ms</td>
<td>139.4 ms</td>
</tr>
<tr>
<td>aiohttp<a id="fnref6" href="#fn6" role="doc-noteref"><sup>6</sup></a></td>
<td>1</td>
<td><mark>6,391.33/s</mark></td>
<td>0.2 ms</td>
<td>0.2 ms</td>
<td>0.3 ms</td>
</tr>
<tr>
<td>aiohttp</td>
<td>50</td>
<td>8,477.42/s</td>
<td>5.9 ms</td>
<td>7.0 ms</td>
<td>9.3 ms</td>
</tr>
<tr>
<td>aiohttp</td>
<td>100</td>
<td>8,447.58/s</td>
<td>11.7 ms</td>
<td>15.2 ms</td>
<td>18.4 ms</td>
</tr>
<tr>
<td>aiohttp</td>
<td>200</td>
<td>7,696.38/s</td>
<td>25.7 ms</td>
<td>35.0 ms</td>
<td>56.9 ms</td>
</tr>
<tr>
<td>aiohttp</td>
<td>400</td>
<td>7,132.18/s</td>
<td>55.0 ms</td>
<td>62.7 ms</td>
<td>114.9 ms</td>
</tr>
</tbody></table>
<p>So the other reason may just be that LuaJIT is extremely fast.
If the conclusion of <a href="https://magic.io/blog/uvloop-blazing-fast-python-networking/">this 2016 benchmark</a> still holds, then even though uvloop is a little faster than asyncio, aiohttp is not bottlenecked by asyncio but by its <a href="https://github.com/aio-libs/aiohttp/blob/1633547a1b0541aac38c0b68bb0bd8f8c639460c/aiohttp/http_parser.py#L129">HTTP parser</a>.<a id="fnref7" href="#fn7" role="doc-noteref"><sup>7</sup></a>
That HTTP parser is written in pure Python, using re to process strings with regular expressions.</p>
<p>There is more than one reason that Python is slow.
But one is that CPython has to box every integer, float, boolean, etc., which means more time spent allocating and deallocating memory rather than serving HTTP requests.
This penalty also affects code that performs FFI to offload computation to a compiled library, since data that crosses the FFI boundary has to be boxed, too.</p>
<p>LuaJIT spends less time managing allocations.
First, it does not box numbers, booleans, nil values, and raw pointers.
Instead, it embeds all such values into 64-bit double-precision floats <a href="https://github.com/LuaJIT/LuaJIT/blob/v2.1/src/lj_obj.h#L224">using NaN tagging</a>.
Not only does this make pure Lua code faster but it also reduces FFI overhead.</p>
<p>Second, LuaJIT implements <a href="https://web.archive.org/web/20220519142742/http://wiki.luajit.org/Allocation-Sinking-Optimization">allocation sinking</a> through which it can avoid allocating temporary values.
Traditional escape analysis can turn a heap allocation into a stack allocation if the compiler can prove that the allocated value doesn’t escape the local scope.
Allocation sinking is more aggressive, and in certain cases it can even eliminate stack allocations.
Importantly, this makes many uses of tables, Lua’s do-it-all data structure, more memory efficient and thus faster.</p>
<p>Third, LuaJIT has a very low memory footprint overall, between <a href="https://programming-language-benchmarks.vercel.app/lua">1-to-2x</a> of standard Lua in popular benchmarks.
This is very good for a JIT compiler.
Ruby’s YJIT also does well, with only a <a href="https://programming-language-benchmarks.vercel.app/ruby">0-to-5-percent</a> overhead.
But PyPy typically uses <a href="https://programming-language-benchmarks.vercel.app/python">2-to-6x</a> as much memory as CPython, and TruffleRuby often uses 15-to-25x as much memory as vanilla Ruby.</p>
<p>The result is that nvim-web-server doesn’t only use a very fast event loop.
It also has a fast (albeit thoroughly rudimentary) HTTP parser, and a fast mechanism for resolving requested paths and serving content.</p>
<p>In practical terms, what all this means is that if nvim-web-server is hosted behind an Nginx reverse proxy, then it won’t be the throughput bottleneck.
And even less so if Nginx accepts HTTPS connections because then <a href="https://www.f5.com/pdf/report/NGINX-SSL-Performance_2020-revision.pdf">SSL termination</a> will constrain Nginx’s throughput further.
(Although it has to be said that nvim-web-server will necessarily increase <em>latency</em> since we are replacing a web server with a reverse proxy <em>and</em> a web server.)</p>
</section>
<section id="Deploying-on-an-old-ThinkPad">
<h2>Deploying on an old ThinkPad</h2>
<div>
<p><a href="https://vim.gabornyeki.com/laptop.jpg"><img alt="Photo of the ThinkPads keyboard from above, with the screen visible at a sharp angle" src="https://vim.gabornyeki.com/laptop.jpg"/></a></p>
<p><strong>Fig. 2.</strong>
ThinkPad E430 as a stand-in for a private cloud.</p>
</div>
<p>It has become normalized to change our phones, computers, and cars every two to five years.
But were it not for planned obsolescence (and, to be fair, the enormous improvements in car safety in the last couple decades), old hardware with minimal maintenance could still perform many tasks effectively.</p>
<p>The ThinkPad that serves this website, an Edge E430 from 2012, was my only computer throughout grad school.
Now it is old by some measures, barely middle-aged by others.
It runs a Core i3-2350M with two physical cores.
Although this CPU is 14 years old, it has the same L1/L2 cache per physical core (64 KB/256 KB) as my i7-8565U which is eight years its junior.
But, for example, it doesn’t support <a href="https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#Advanced_Vector_Extensions_2">the AVX2 instruction set</a>, unlike 95 percent of computers in <a href="https://store.steampowered.com/hwsurvey">the June 2025 Steam Survey</a>.</p>
<p>This poor laptop also shows signs of wear.
The speaker cover is gone.
The battery is all but dead.
The original CPU fan died and the aftermarket fan I replaced it with now constantly spins.
Also, from some point on, the OS failed to boot if the room was below 18°C (65°F).
It was probably an issue with the old SSD which I have replaced.
Aside from all of this, it still works and doesn’t complain.</p>
<p>Its abundance of ports is a showcase of an earlier era.
VGA, HDMI, two USB2 ports, two USB3 ports, a headphone jack, ethernet, an SD card slot, a DVD drive, and a fingerprint reader.

And a WiFi adapter that supports 802.11n, for a maximum speed of a whopping 300 Mb/s.</p>
<p>And it only has 8 GB of RAM.
That is not a problem though, Neovim barely consumes 80 MB.</p>
<p>Speaking of Neovim, the web server is started with a straightforward Vim script.
The script initializes the server, opens the files to serve, and adds them to the routing table.</p>
<details>
<summary><code>setup.vim</code></summary>
<pre><code>&#34; Run this with `nvim -c &#39;source %&#39; setup.vim`.
&#34;

lua require(&#34;web-server&#34;).init()

split template.html
WSSetBufferAsTemplate

edit index.dj
WSAddBuffer /

edit screenshot.png
WSAddBuffer /screenshot.png image/png

edit laptop.jpg
WSAddBuffer /laptop.jpg image/jpg

edit arch_mix.png
WSAddBuffer /arch_mix.png image/png

edit arch_mix_dark.png
WSAddBuffer /arch_mix_dark.png image/png

edit favicon.ico
WSAddBuffer /favicon.ico image/x-icon

edit github-mark.svg
WSAddBuffer /github-mark.svg image/svg+xml

edit github-mark-white.svg
WSAddBuffer /github-mark-white.svg image/svg+xml

close
</code></pre>
</details>
<p>And that’s all there is to it.<a id="fnref8" href="#fn8" role="doc-noteref"><sup>8</sup></a>
Almost.</p>
</section>
<section id="Is-this-even-safe">
<h2>Is this even safe?</h2>
<div>
<a href="https://vim.gabornyeki.com/arch_mix.png">
    <picture>
        <source srcset="/arch_mix_dark.png" media="(prefers-color-scheme: dark)"/>
        <img src="https://vim.gabornyeki.com/arch_mix.png" alt="Architectural diagram"/>
    </picture>
</a>
<p><strong>Fig. 3.</strong>
Neovim is deployed in a confined Docker container behind an Nginx reverse proxy.</p>
</div>
<p>nvim-web-server itself is implemented in a memory-safe language, Lua.
It never evaluates code and it never accesses the file system in response to requests, only content that has previously been loaded into its routing table.</p>
<p>However, the underlying LuaJIT runtime, as well as Neovim and libuv (which nvim-web-server relies on for core functionality), are largely written in C, and LuaJIT also includes a significant amount of hand-written assembly.
While LuaJIT is deployed as part of <a href="https://openresty.org/en/">OpenResty</a>, Neovim is not typically used as a web server in production, so it has not been subject to the kind of security-minded scrutiny that a web server gets.</p>
<p>It is only reasonable then to take some precautions.
To mitigate these risks, I have deployed Neovim</p>
<ol>
<li>
in a Docker container,
</li>
<li>
running as an unprivileged user.
</li>
</ol>
<p>Furthermore, the container is confined by</p>
<ol start="3">
<li>
an AppArmor profile that restricts file system access,
</li>
<li>
a seccomp profile that restricts access to system calls, and
</li>
<li>
a netfilter ruleset that blocks outgoing network connections.
</li>
</ol>
<p>I’ve also considered replacing 3, 4, and 5 with <a href="https://gvisor.dev/">gVisor</a> which is a container runtime that reimplements a commonly used subset of Linux’s system calls in Go.
It has a feature that <a href="https://gvisor.dev/docs/user_guide/runtimemonitor/">allows an external monitoring process</a> to trace the system calls made by the containerized process.
This looks like a very neat alternative.
But it is left for a future hobby project.</p>
</section>
</section>
<section role="doc-endnotes">
<hr/>
<ol>
<li id="fn1">
<p>This concern was so central that <a href="https://github.com/neovim/neovim/issues/3">Issue #3</a> back in 2014 was about using libuv for OS calls.<a href="#fnref1" role="doc-backlink">↩︎︎</a></p>
</li>
<li id="fn2">
<p>No need for build systems, scp, rsync, or anything else.
Content is updated when the buffer is saved.
Djot buffers are converted to HTML automatically.<a href="#fnref2" role="doc-backlink">↩︎︎</a></p>
</li>
<li id="fn3">
<p>No need to set up and maintain WordPress, MariaDB, etc.<a href="#fnref3" role="doc-backlink">↩︎︎</a></p>
</li>
<li id="fn4">
<p>See <a href="https://github.com/tpope/vim-fugitive">Fugitive</a>.<a href="#fnref4" role="doc-backlink">↩︎︎</a></p>
</li>
<li id="fn5">
<p>For 50 concurrent requests, I ran <code>hey -c 50 -n 10000 ...</code> (simulating 50 concurrent users making 200 requests each).
For the other scenarios, I only changed <code>-c</code> and kept the total number of requests as <code>-n 10000</code>.<a href="#fnref5" role="doc-backlink">↩︎︎</a></p>
</li>
<li id="fn6">
<p>I ran http.server and aiohttp with Python 3.10.12.
The speed improvements of Python <a href="https://docs.python.org/3/whatsnew/3.11.html#faster-cpython">3.11</a> and <a href="https://docs.python.org/3/whatsnew/3.12.html#asyncio">3.12</a> may improve the numbers in the table.</p>
<p>The aiohttp app, like nvim-web-server, preloaded the content into memory:</p>
<pre><code>from aiohttp import web

with open(&#34;index.html&#34;, &#34;r&#34;) as handle:
    INDEX = handle.read()

app = web.Application()
routes = web.RouteTableDef()

@routes.get(&#34;/&#34;)
async def index(request):
    return web.Response(text=INDEX, content_type=&#34;text/html&#34;)

app.add_routes(routes)
web.run_app(app)
</code></pre>
<p><a href="#fnref6" role="doc-backlink">↩︎︎</a></p>
</li>
<li id="fn7">
<p>The aiohttp library benefits a lot from using asyncio, which is illustrated by how the Python standard library’s http.server fares by comparison.
Instead of asyncio, http.server uses threading, and this strategy does not scale well.
Each HTTP request starts a new thread, so http.server is slow even when serving non-concurrent requests, and its performance deteriorates as the number of concurrent requests increases.
Both throughput and latency suffer:</p>
<table>
<tbody><tr>
<th>Server</th>
<th>Concurrent Requests</th>
<th>Response Rate</th>
<th>Average</th>
<th>95%</th>
<th>99%</th>
</tr>
<tr>
<td>http.server</td>
<td>1</td>
<td>2,096.12/s</td>
<td>0.5 ms</td>
<td>0.5 ms</td>
<td>2.3 ms</td>
</tr>
<tr>
<td>http.server</td>
<td>50</td>
<td>1,275.15/s</td>
<td>16.8 ms</td>
<td>7.5 ms</td>
<td>17.3 ms</td>
</tr>
<tr>
<td>http.server</td>
<td>100</td>
<td>491.20/s</td>
<td>41.9 ms</td>
<td>12.0 ms</td>
<td>1,031.8 ms</td>
</tr>
<tr>
<td>http.server</td>
<td>200</td>
<td>360.43/s</td>
<td>72.5 ms</td>
<td>15.6 ms</td>
<td>2,292.9 ms</td>
</tr>
<tr>
<td>http.server</td>
<td>400</td>
<td>243.11/s</td>
<td>225.6 ms</td>
<td>1,022.2 ms</td>
<td>7,227.2 ms</td>
</tr>
</tbody></table>
<p>http.server logs every request to stderr which slows down execution, so for the benchmark I ran it with <code>python3 -m http.server 2&gt;/dev/null</code>.<a href="#fnref7" role="doc-backlink">↩︎︎</a></p>
</li>
<li id="fn8">
<p>The script that produces <a href="#fig1">Fig. 1</a> is just a little more complex.
To keep every buffer visible in a separate window, the script uses <code>split</code> and <code>vsplit</code> instead of <code>edit</code>, and it doesn’t call <code>close</code> at the end.
This strategy would fail with too many splits because Neovim refuses to split windows that are too small.
To prevent this, before each split, the script also maximizes the available space in the active window.
Then once every buffer is open and everything is set up, it equalizes the space given to each window.</p>
<p>So opening every buffer and setting up the routing table looks like this:</p>
<pre><code>&#34; ...

command MaximizeWindow normal &lt;C-w&gt;&lt;C-_&gt;&lt;C-w&gt;&lt;C-|&gt;
command EqualizeWindows normal &lt;C-w&gt;=

MaximizeWindow
split index.dj
WSAddBuffer /

MaximizeWindow
vsplit screenshot.png
WSAddBuffer /screenshot.png image/png

&#34; ...

EqualizeWindows
</code></pre>
<p><a href="#fnref8" role="doc-backlink">↩︎︎</a></p>
</li>
</ol>
</section>

</div></div>
  </body>
</html>
