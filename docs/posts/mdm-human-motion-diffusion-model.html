<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://guytevet.github.io/mdm-page/">Original</a>
    <h1>MDM: Human Motion Diffusion Model</h1>
    
    <div id="readability-page-1" class="page">




<section>
  <div>
    <div>
      <div>
        <div>
          

          <p><span>Tel Aviv University, Israel</span></p>
          


          

        </div>
      </div>
    </div>
  </div>
</section>

<!-- 
<section class="hero is-small">
  <!~~ <div class="hero-body"> ~~>
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!~~ <div id="results-carousel" class="carousel results-carousel"> ~~>
      <div class="container">
      <div class="item">
      <div class="column is-centered has-text-centered">
        <img src="static/figures/teaser.png" alt="MotionCLIP"/>
      </div>

    </div>
  </div>
 <!~~  </div> ~~>
  </div>
  </div>
 <!~~  </div> ~~>
</section>
 -->

<section>
  <div>
    <!-- Abstract. -->
    <div>
      <div>
        
        <h2>Abstract</h2>
        <p>
            Natural and expressive human motion generation is the holy grail of computer animation.
It is a challenging task, due to the diversity of possible motion, human perceptual sensitivity to it, and the difficulty of accurately describing it. Therefore, current generative solutions are either low-quality or limited in expressiveness. 
Diffusion models, which have already shown remarkable generative capabilities in other domains, are promising candidates for human motion due to their many-to-many nature, but they tend to be resource hungry and hard to control. 
In this paper, we introduce Motion Diffusion Model (MDM), a carefully adapted classifier-free diffusion-based generative model for the human motion domain. 
MDM is transformer-based, combining insights from motion generation literature. 
A notable design-choice is the prediction of the sample, rather than the noise, in each diffusion step. This facilitates the use of established geometric losses on the locations and velocities of the motion, such as the foot contact loss. As we demonstrate, MDM is a generic approach, enabling different modes of conditioning, and different generation tasks. We show that our model is trained with lightweight resources and yet achieves state-of-the-art results on leading benchmarks for text-to-motion and action-to-motion.</p>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>



<section>
  <div>
    <div>

      <p><img src="https://guytevet.github.io/mdm-page/static/figures/mdm_arch.png" alt="cars peace"/>
      </p>
     
    
  </div>
</div>

</section>


<section>
  <div>
    <!-- Abstract. -->
    <div>
      <div>
<!--         <h2 class="title is-3">How does it work?</h2> -->
        <p>
The MDM framework has a generic design enabling different forms of conditioning. 
We showcase three tasks: text-to-motion, action-to-motion, and unconditioned generation. 
We train the model in a classifier-free manner, which enables trading-off diversity to fidelity, and sampling both conditionally and unconditionally from the same model.
In the text-to-motion task, our model generates coherent motions that achieve state-of-the-art results on the HumanML3D and KIT benchmarks. 
Moreover, our user study shows that human evaluators 
prefer our generated motions over real motions 42% of the time.
In action-to-motion, MDM outperforms the state-of-the-art, even though they were specifically designed for this task, 
on the common HumanAct12 and UESTC benchmarks.
</p>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section> 


<!-- 
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">

      <div class="column is-centered has-text-centered">
        <img src="static/figures/sampling.png" alt="cars peace"/>
      </div>
     
    
  </div>
</div>
</div>
</section>
 -->



<section>
  <div>
    <div>
      <div id="results-carousel">
      <div>
      	<p><b> “A person walks forward, bends down to pick something up off the ground.”</b></p>
                <video poster="" id="tree" autoplay="" controls="" muted="" loop="" height="100%">
          <source src="static/figures/bend.mp4" type="video/mp4"/>
        </video>
      </div>
      <div>
      	<p><b>“a person turns to his right and paces back and forth.”</b></p>
                <video poster="" id="tree" autoplay="" controls="" muted="" loop="" height="100%">
          <source src="static/figures/turn.mp4" type="video/mp4"/>
      </video></div>
      <div>
      	<p><b>“A person punches in a manner consistent with martial arts.”</b></p>
                <video poster="" id="tree" autoplay="" controls="" muted="" loop="" height="100%">
          <source src="static/figures/punch.mp4" type="video/mp4"/>
      </video></div>
  </div>
</div>
</div>
</section>


<section>
  <div>
    <!-- Abstract. -->
    <div>
      <div>
        <h2>Text-to-Motion</h2>
          <p>
Text-to-motion is the task of generating motion given an input text prompt. 
The output motion is expected to be both implementing the textual description, and a valid sample from the data distribution (i.e. adhering to general human abilities and the rules of physics). 
In addition, for each text prompt, we also expect a distribution of motions matching it, rather than just a single result.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  
</section> 



<section>
  
</section>



<section>
  <div>
    <div>
      <div id="results-carousel">
      <div>
      	<p><b> “A person is skipping rope.”</b></p>
                <video poster="" id="tree" autoplay="" controls="" muted="" loop="" height="100%">
          <source src="static/figures/rope1.mp4" type="video/mp4"/>
        </video>
      </div>
      <div>
      	<p><b> “A person is skipping rope.”</b></p>
                <video poster="" id="tree" autoplay="" controls="" muted="" loop="" height="100%">
          <source src="static/figures/rope2.mp4" type="video/mp4"/>
        </video>
      </div>
      <div>
      	<p><b> “A person is skipping rope.”</b></p>
                <video poster="" id="tree" autoplay="" controls="" muted="" loop="" height="100%">
          <source src="static/figures/rope3.mp4" type="video/mp4"/>
        </video>
      </div>
      <div>
      	<p><b> “A person is skipping rope.”</b></p>
                <video poster="" id="tree" autoplay="" controls="" muted="" loop="" height="100%">
          <source src="static/figures/rope4.mp4" type="video/mp4"/>
        </video>
      </div>
            <div>
      	<p><b> “A person is skipping rope.”</b></p>
                <video poster="" id="tree" autoplay="" controls="" muted="" loop="" height="100%">
          <source src="static/figures/rope5.mp4" type="video/mp4"/>
        </video>
      </div>
      <div>
      	<p><b> “A person is skipping rope.”</b></p>
                <video poster="" id="tree" autoplay="" controls="" muted="" loop="" height="100%">
          <source src="static/figures/rope6.mp4" type="video/mp4"/>
        </video>
      </div>
            <div>
      	<p><b> “A person is skipping rope.”</b></p>
                <video poster="" id="tree" autoplay="" controls="" muted="" loop="" height="100%">
          <source src="static/figures/rope7.mp4" type="video/mp4"/>
        </video>
      </div>
      <div>
      	<p><b> “A person is skipping rope.”</b></p>
                <video poster="" id="tree" autoplay="" controls="" muted="" loop="" height="100%">
          <source src="static/figures/rope8.mp4" type="video/mp4"/>
        </video>
      </div>



  </div>
</div>
</div>
</section>


<section>
  <div>
    <!-- Abstract. -->
    <div>
      <div>
        <h2>Action-to-Motion</h2>
          <p>
Action-to-motion is the task of generating motion given an input action class, represented by a scalar. 
The output motion should faithfully animate the input action, and at the same time be natural and reflect the distribution of the dataset on which the model is trained.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  
</section> 





<section>
  
</section>



<section>
  <div>
    <!-- Abstract. -->
    <div>
      <div>
        <h2>Motion Editing</h2>
          <p>
          We also demonstrate completion and editing. By adapting diffusion image-inpainting, we set a motion prefix and suffix, and use our model to fill in the gap. 
Doing so under a textual condition guides MDM to fill the gap with a specific motion that still maintains the semantics of the original input. 
By performing inpainting in the joints space rather than temporally, we also demonstrate the semantic editing of specific body parts, without changing the others. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  
</section> 


<section>

      <div>
      	<h3>Upper body editing (lower body is fixed)</h3>
      	<p>(Blue=Input, Gold=Synthesis)</p>
         <video poster="" id="tree" autoplay="" controls="" muted="" loop="" height="100%">
          <source src="static/figures/edit_upper.mp4" type="video/mp4"/>
        </video>
      </div>

</section>


<section>
<div>
      <div>
      	<h3>Motion In-Betweening</h3>
      	<p>(Blue=Input, Gold=Synthesis)</p>
         <video poster="" id="tree" autoplay="" controls="" muted="" loop="" height="100%">
          <source src="static/figures/in_between.mp4" type="video/mp4"/>
        </video>
      </div>
</div>
</section>



<!-- 
<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
    @article{tevet2022motionclip,
    title={MotionCLIP: Exposing Human Motion Generation to CLIP Space},
    author={Tevet, Guy and Gordon, Brian and Hertz, Amir and Bermano, Amit H and Cohen-Or, Daniel},
    journal={arXiv preprint arXiv:2203.08063},
    year={2022}
    }</code></pre>
    </div>
</section>
 -->






  
  
  
    <!-- End of Statcounter Code -->

  
  
</div>
  </body>
</html>
