<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://factory.strongdm.ai/">Original</a>
    <h1>Software factories and the agentic moment</h1>
    
    <div id="readability-page-1" class="page"><div><p>We built a <strong>Software Factory</strong>: non-interactive development where specs + scenarios drive agents that write code, run harnesses, and converge without human review.</p><p>The narrative form is included below. If you&#39;d prefer to work from first principles, I offer a few constraints &amp; guidelines that, applied iteratively, will accelerate any team toward the same intuitions, convictions<sup><a href="#fn1">1</a></sup>, and ultimately a factory<sup><a href="#fn2">2</a></sup> of your own. In k≈çan or mantra form:</p><ul><li>Why am I doing this? (implied: the model should be doing this instead)</li></ul><p>In rule form:</p><ul><li>Code <strong>must not be</strong> written by humans</li><li>Code <strong>must not be</strong> reviewed by humans</li></ul><p>Finally, in practical form:</p><ul><li>If you haven&#39;t spent at least <strong>$1,000 on tokens today</strong> per human engineer, your software factory has room for improvement</li></ul><h2>The StrongDM AI Story</h2><p>On July 14th, 2025, Jay Taylor and Navan Chauhan joined me (Justin McCarthy, co-founder, CTO) in founding the StrongDM AI team.</p><p>The catalyst was a transition observed in late 2024: with the second revision of Claude 3.5 (October 2024), long-horizon agentic coding workflows began to compound correctness rather than error.</p><figure><img alt="Compounding correctness vs compounding error" loading="lazy" width="720" height="480" decoding="async" data-nimg="1" src="https://factory.strongdm.ai/images/growth.jpeg"/><figcaption>Compounding correctness vs compounding error</figcaption></figure><p>By December of 2024, the model&#39;s long-horizon coding performance was unmistakable via Cursor&#39;s<!-- --> <a href="https://forum.cursor.com/t/yolo-mode-is-amazing/36262" target="_blank" rel="noopener noreferrer">YOLO mode</a>.</p><p>Prior to this model improvement, iterative application of LLMs to coding tasks would accumulate errors of all imaginable varieties (misunderstandings, hallucinations, syntax, version DRY violations, library incompatibility, etc). The app or product would decay and ultimately &#34;collapse&#34;: death by a thousand cuts, etc.</p><p>Together with YOLO mode, the updated model from Anthropic provided the first glimmer of what we now refer to internally as <strong>non-interactive</strong> development or<!-- --> <strong>grown</strong> software.</p><h2>Find Knobs, Turn To Eleven</h2><figure><img alt="These go to 11" loading="lazy" width="720" height="480" decoding="async" data-nimg="1" src="https://factory.strongdm.ai/images/eleven.jpg"/><figcaption>&#34;These go to 11&#34;</figcaption></figure><p>In the first hour of the first day of our AI team, we established a charter which set us on a path toward a series of findings (which we refer to as our &#34;unlocks&#34;). In retrospect, the most important line in the charter document was the following:</p><figure><img alt="Hands off" loading="lazy" width="720" height="480" decoding="async" data-nimg="1" src="https://factory.strongdm.ai/images/no-hand-coded.jpg"/><figcaption>Hands off!</figcaption></figure><p>Initially it was just a hunch. An experiment. How far could we get, without writing any code by hand?</p><p>Not very far! At least: not very far, until we added tests. However, the agent, obsessed with the immediate task, soon began to take shortcuts: <strong>return true</strong> is a great way to pass narrowly written tests, but probably won&#39;t generalize to the software you want.</p><p>Tests were not enough. How about integration tests? Regression tests? End-to-end tests? Behavior tests?</p><h2>From Tests to Scenarios and Satisfaction</h2><p>One recurring theme of the agentic moment: we need new language. For example, the word &#34;test&#34; has proven insufficient and ambiguous. A test, stored in the codebase, can be lazily rewritten to match the code. The code could be rewritten to trivially pass the test.</p><p>We repurposed the word <strong>scenario</strong> to represent an end-to-end &#34;user story&#34;, often stored outside the codebase (similar to a &#34;holdout&#34; set in model training), which could be intuitively understood and flexibly validated by an LLM.</p><figure><img alt="Synthetic scenario curation and shaping interface" loading="lazy" width="720" height="480" decoding="async" data-nimg="1" src="https://factory.strongdm.ai/images/synthchat.png"/><figcaption>Synthetic scenario curation and shaping interface</figcaption></figure><p>Because much of the software we grow itself has an agentic component, we transitioned from boolean definitions of success (&#34;the test suite is green&#34;) to a probabilistic and empirical one. We use the term <strong>satisfaction</strong> to quantify this validation: of all the observed trajectories through all the scenarios, what fraction of them likely satisfy the user?</p><h2>Validating Scenarios in the Digital Twin Universe</h2><p>In previous regimes, a team might rely on integration tests, regression tests, UI automation to answer &#34;is it working?&#34;</p><p>We noticed two limitations of previously reliable techniques:</p><ol><li><strong>Tests are too rigid</strong> - we were coding with agents, but we&#39;re also building with LLMs and agent loops as design primitives; evaluating success often required LLM-as-judge</li><li><strong>Tests can be reward hacked</strong> - we needed validation that was less vulnerable to the model cheating</li></ol><p>The Digital Twin Universe is our answer: behavioral clones of the third-party services our software depends on. We built twins of Okta, Jira, Slack, Google Docs, Google Drive, and Google Sheets, replicating their APIs, edge cases, and observable behaviors.</p><p>With the DTU, we can validate at volumes and rates far exceeding production limits. We can test failure modes that would be dangerous or impossible against live services. We can run thousands of scenarios per hour without hitting rate limits, triggering abuse detection, or accumulating API costs.</p><figure><figcaption>Digital Twin Universe: behavioral clones of Okta, Jira, Google Docs, Slack, Drive, and Sheets</figcaption></figure><h2>Unconventional Economics</h2><p>Our success with DTU illustrates one of the many ways in which the Agentic Moment has profoundly changed the economics of software. Creating a high fidelity clone of a significant SaaS application was always possible, but never economically feasible. Generations of engineers may have <em>wanted</em> a full in-memory replica of their CRM to test against, but self-censored the proposal to build it. They didn&#39;t even bring it to their manager, because they knew the answer would be no.</p><p>Those of us building software factories must practice a <strong>deliberate naivete</strong>: finding and removing the habits, conventions, and constraints of<!-- --> <a href="https://www.youtube.com/watch?v=LCEmiRjPEtQ&amp;t=95s" target="_blank" rel="noopener noreferrer">Software 1.0</a>. The DTU is our proof that what was unthinkable six months ago is now routine.</p><h2>Read Next</h2><ul><li><a href="https://factory.strongdm.ai/principles">Principles</a>: what we believe is true about building software with agents</li><li><a href="https://factory.strongdm.ai/techniques">Techniques</a>: repeated patterns for applying those principles</li><li><a href="https://factory.strongdm.ai/products">Products</a>: tools we use daily and believe others will benefit from</li></ul><p>Thank you for reading. We wish you the best of luck constructing your own Software Factory.</p><hr/><hr/></div></div>
  </body>
</html>
