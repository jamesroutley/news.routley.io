<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.pinecone.io/learn/bertopic/">Original</a>
    <h1>BERTopic: The Future of Topic Modeling</h1>
    
    <div id="readability-page-1" class="page"><div><p>90% of the world’s data is unstructured. It is built by humans, for humans. That’s great for human consumption, but it is <em>very</em> hard to organize when we begin dealing with the massive amounts of data abundant in today’s information age.</p><p>Organization is complicated because unstructured text data is not intended to be understood by machines, and having humans process this abundance of data is wildly expensive and <em>very slow</em>.</p><p>Fortunately, there is light at the end of the tunnel. More and more of this unstructured text is becoming accessible and understood by machines. We can now <a href="https://www.wildlondon.org.uk/learn/dense-vector-embeddings-nlp/">search text based on <em>meaning</em></a>, identify the sentiment of text, extract entities, and much more.</p><p><a href="https://www.wildlondon.org.uk/learn/transformers/">Transformers</a> are behind much of this. These transformers are (unfortunately) not Michael Bay’s Autobots and Decepticons and (fortunately) not buzzing electrical boxes. Our NLP transformers lie somewhere in the middle, they’re not sentient Autobots (yet), but they can understand language in a way that existed only in sci-fi until a short few years ago.</p><p>Machines with a human-like comprehension of language are pretty helpful for organizing masses of unstructured text data. In machine learning, we refer to this task as <em>topic modeling</em>, the automatic clustering of data into particular topics.</p><p>BERTopic takes advantage of the superior language capabilities of these (not yet sentient) transformer models and uses some other ML magic like UMAP and HDBSCAN (more on these later) to produce what is one of the most advanced techniques in language topic modeling today.</p><hr/><h2 id="bertopic-at-a-glance">BERTopic at a Glance</h2><p><iframe src="https://www.youtube-nocookie.com/embed/fb7LENb9eag" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></p><p>We will dive into the details behind BERTopic [1], but before we do, let us see how we can use it and take a first glance at its components.</p><p>To begin, we need a dataset. We can download the dataset from HuggingFace datasets with:</p><p>The dataset contains data extracted using the Reddit API from the <em>/r/python</em> subreddit. The code used for this (and all other examples) can <a href="https://github.com/pinecone-io/examples/tree/master/learn/algos-and-libraries/bertopic">be found here</a>.</p><p>Reddit thread contents are found in the <code>selftext</code> feature. Some are empty or short, so we remove them with:</p><p>We perform topic modeling using the <code>BERTopic</code> library. The <em>“basic”</em> approach requires just a few lines of code.</p><p>From <code>model.fit_transform</code> we return two lists:</p><ul><li><code>topics</code> contains a one-to-one mapping of inputs to their modeled <em>topic</em> (or cluster).</li><li><code>probs</code> contains a list of probabilities that an input belongs to their assigned topic.</li></ul><p>We can then view the topics using <code>get_topic_info</code>.</p><p>The top <code>-1</code> topic is typically assumed to be irrelevant, and it usually contains stop words like <em>“the”</em>, <em>“a”</em>, and <em>“and”</em>. However, we removed stop words via the <code>vectorizer_model</code> argument, and so it shows us the <em>“most generic”</em> of topics like <em>“Python”</em>, <em>“code”</em>, and <em>“data”</em>.</p><p>The library has several built-in visualization methods like <code>visualize_topics</code>, <code>visualize_hierarchy</code>, and <code>visualize_barchart</code>.</p><p><img loading="lazy" src="https://d33wubrfki0l68.cloudfront.net/2e3f85bcc4f2ceaf6352ea652a55333940d87f9c/6261e/images/bertopic-1.png" alt="bertopic-viz-hierarchy" width="100%"/>
<small>BERTopic’s <code>visualize_hierarchy</code> visualization allows us to view the “hierarchy” of topics.</small></p><p>These represent the surface level of the BERTopic library, which has excellent documentation, so we will not rehash that here. Instead, let’s try and understand <em>how</em> BERTopic works.</p><h2 id="overview">Overview</h2><p>There are <em>four</em> key components used in BERTopic [2], those are:</p><ul><li>A transformer embedding model</li><li>UMAP dimensionality reduction</li><li>HDBSCAN clustering</li><li>Cluster tagging using c-TF-IDF</li></ul><p>We already did <em>all</em> of this in those few lines of BERTopic code; everything is just abstracted away. However, we can optimize the process by understanding the essentials of each component. This section will work through each component <em>without</em> BERTopic, and learn how they work before returning to BERTopic at the end.</p><h3 id="transformer-embedding">Transformer Embedding</h3><p>BERTopic supports several libraries for encoding our text to dense vector embeddings. If we build poor quality embeddings, nothing we do in the other steps will be able to help us, so it is <em>very important</em> that we choose a suitable embedding model from one of the supported libraries, which include:</p><ul><li>Sentence Transformers</li><li>Flair</li><li>SpaCy</li><li>Gensim</li><li>USE (from TF Hub)</li></ul><p>Of the above, the <em>Sentence Transformers</em> library provides the most extensive library of high-performing sentence embedding models. They can be found on HuggingFace Hub by searching for <em>“sentence-transformers”</em>.</p><p><img loading="lazy" src="https://d33wubrfki0l68.cloudfront.net/f55223cfdcd4d7b08c10f4ffb5f784a7f2b1a6e2/eedb3/images/bertopic-2.jpg" alt="hf-hub-screenshot" width="100%"/></p><p><small>We can find <em>official</em> sentence transformer models by searching for <em>“sentence-transformers”</em> on HuggingFace Hub.</small></p><p>The first result of this search is <code>sentence-transformers/all-MiniLM-L6-v2</code>, this is a popular high-performing model that creates <em>384</em>-dimensional sentence embeddings.</p><p>To initialize the model and encode our Reddit topics data, we first <code>pip install sentence-transformers</code> and then write:</p><p>Here we have encoded our text in batches of <code>16</code>. Each batch is added to the <code>embeds</code> array. Once we have all of the <a href="https://www.wildlondon.org.uk/learn/sentence-embeddings/">sentence embeddings</a> in <code>embeds</code> we’re ready to move on to the next step.</p><h3 id="dimensionality-reduction">Dimensionality Reduction</h3><p>After building our embeddings, BERTopic compresses them into a lower-dimensional space. This means that our 384-dimensional vectors are transformed into two/three-dimensional vectors.</p><p>We can do this because 384 dimensions are <em>a lot</em>, and it is unlikely that we really need that many dimensions to represent our text [4]. Instead, we attempt to <em>compress</em> that information into two or three dimensions.</p><p>We do this so that the following HDBSCAN clustering step can be done more efficiently. Performing the clustering step with 384-dimensions would be desperately slow [5].</p><p>Another benefit is that we can visualize our data; this is incredibly helpful when assessing whether our data can be clustered. Visualization also helps when tuning the dimensionality reduction parameters.</p><p>To help us understand dimensionality reduction, we will start with a 3D representation of the world. You can find <a href="https://github.com/pinecone-io/examples/tree/master/learn/algos-and-libraries/bertopic">the code for this part here</a>.</p><p><small>3D scatter plot of points from the <a href="https://huggingface.co/datasets/jamescalam/world-cities-geo"><code>jamescalam/world-cities-geo</code></a> dataset.</small></p><p>We can apply many dimensionality reduction techniques to this data; two of the most popular choices are PCA and t-SNE.</p><p><small>Our 2D world reduced using PCA.</small></p><p>PCA works by preserving <em>larger distances</em> (using mean squared error). The result is that the <em>global structure</em> of data is usually preserved [6]. We can see that behavior above as each continent is grouped with its neighboring continent(s). When we have easily distinguishable clusters in datasets, this can be good, but it performs poorly for more nuanced data where <em>local structures</em> are important.</p><p><small>2D Earth reduced using t-SNE.</small></p><p>t-SNE is the opposite; it preserves <em>local structures</em> rather than <em>global</em>. This localized focus results from t-SNE building a graph, connecting all of the nearest points. These local structures can indirectly suggest the global structure, but they are not strongly captured.</p><hr/><p>PCA focuses on preserving <em>dissimilarity</em> whereas t-SNE focuses on preserving <em>similarity</em>.</p><hr/><p>Fortunately, we can capture the best of both using a lesser-known technique called <strong>U</strong>niform <strong>M</strong>anifold <strong>A</strong>pproximation and <strong>P</strong>roduction (UMAP).</p><p>We can apply UMAP in Python using the UMAP library, installed using <code>pip install umap</code>. To map to a 3D or 2D space using the default UMAP parameters, all we write is:</p><p>The UMAP algorithm can be fine-tuned using several parameters. Still, the simplest and most effective tuning can be achieved with just the <code>n_neighbors</code> parameter.</p><p>For each datapoint, UMAP searches through other points and identifies the <strong>k</strong>th nearest neighbors [3]. It is <strong>k</strong>, controlled by the <code>n_neighbors</code> parameter.</p><video autoplay="" loop="" muted="" playsinline="">
<source src="https://d33wubrfki0l68.cloudfront.net/86a053139b81f9a4840265eedb8a01ef1e8c696e/58e51/images/bertopic-9.mp4" type="video/mp4"/></video><p><small><strong>k</strong> and <code>n_neighbors</code> are synonymous here. As we increase <code>n_neighbors</code> the graph built by UMAP can consider more distant points and better represent the global structure.</small></p><p>Where we have many points (high-density regions), the distance between our point and its <strong>k</strong>th nearest neighbor is usually smaller. In low-density regions with fewer points, the distance will be much greater.</p><p><img loading="lazy" src="https://d33wubrfki0l68.cloudfront.net/c7f8e2c115e837dce6a8cdb3c5f99bc2cc304747/aa9f9/images/bertopic-3.png" alt="bertopic-umap-density" width="100%"/></p><p><small>Density is measured indirectly using the distances between <strong>k</strong>th nearest neighbors in different regions.</small></p><p>UMAP will attempt to preserve distances to the <strong>k</strong>th nearest point is what UMAP attempts to preserve when shifting to a lower dimension.</p><p>By increasing <code>n_neighbors</code> we can preserve more global structures, whereas a lower <code>n_neighbors</code> better preserves local structures.</p><p><img loading="lazy" src="https://d33wubrfki0l68.cloudfront.net/615554b1cdb3bec913e688eb7b22b124c437b435/902e4/images/bertopic-4.png" alt="bertopic-n_neighbors-global" width="100%"/>
<small>Higher <code>n_neighbors</code> (<strong>k</strong>) means we preserve larger distances and thus maintain more of the global structure.</small></p><p>Compared to other dimensionality reduction techniques like PCA or t-SNE, finding a good <code>n_neighbours</code> value allows us to preserve <em>both</em> local and global structures relatively well.</p><p>Applying it to our 3D globe, we can see neighboring countries remain neighbors. At the same time, continents are placed correctly (with North-South inverted), and islands are separated from continents. We even have what seems to be the Spanish Peninsula in “western Europe”.</p><p><small>The UMAP-reduced Earth.</small></p><p>UMAP maintains distinguishable features that are not preserved by PCA and a better global structure than t-SNE. This is a great overall example of where the benefit of UMAP lies.</p><p>UMAP can also be used as a supervised dimensionality reduction method by passing labels to the <code>target</code> argument if we have labeled data. It is possible to produce even more meaningful structures using this supervised approach.</p><p>With all that in mind, let us apply UMAP to our Reddit topics data. Using <code>n_neighbors</code> of <code>3</code>-<code>5</code> seems to work best. We can add <code>min_dist=0.05</code> to allow UMAP to place points closer together (the default value is <code>1.0</code>); this helps us separate the three <em>similar</em> topics from <em>r/Python</em>, <em>r/LanguageTechnology</em>, and <em>r/pytorch</em>.</p><p><small>Reddit topics data reduced to 3D space using UMAP.</small></p><p>With our data reduced to a lower-dimensional space and topics easily visually identifiable, we’re in an excellent spot to move on to clustering.</p><h3 id="hdbscan-clustering">HDBSCAN Clustering</h3><p>We have visualized the UMAP reduced data using the existing <em>sub</em> feature to color our clusters. It looks pretty, but we don’t usually perform topic modeling to label already labeled data. If we assume that we have no existing labels, our UMAP visual will look like this:</p><p><small>UMAP reduced cities data, we can distinguish many clusters/continents, but it is much more difficult without label coloring.</small></p><p>Now let us look at how HDBSCAN is used to cluster the (now) low-dimensional vectors.</p><p>Clustering methods can be broken into flat or hierarchical <em>and</em> centroid or density-based techniques [5]. Each of which has its own benefits and drawbacks.</p><p>Flat or hierarchical focuses simply on whether there is (or is not) a hierarchy in the clustering method. For example, we may (ideally) view our graph hierarchy as moving from continents to countries to cities. These methods allow us to view a given hierarchy and try to identify a logical “cut” along the tree.</p><p><img loading="lazy" src="https://d33wubrfki0l68.cloudfront.net/f2476bcee1ed1648120832edd68d190d329690eb/dca25/images/bertopic-5.png" alt="bertopic-hierarchy" width="100%"/>
<small>Hierarchical techniques begin from one large cluster and split this cluster into smaller and smaller parts and try to find the <em>ideal</em> number of clusters in the hierarchy.</small></p><p>The other split is between centroid-based or density-based clustering. That is clustering based on proximity to a centroid or clustering based on the density of points. Centroid-based clustering is ideal for <em>“spherical”</em> clusters, whereas density-based clustering can handle more irregular shapes <em>and</em> identify outliers.</p><p><img loading="lazy" src="https://d33wubrfki0l68.cloudfront.net/69b4730f3edf7c28baa4bd83fb77d370e9911d33/13869/images/bertopic-6.png" alt="bertopic-density-centroid" width="100%"/>
<small>Centroid-based clustering (left) vs density-based clustering (right).</small></p><p>HDBSCAN is a hierarchical, density-based method. Meaning we can benefit from the easier tuning and visualization of hierarchical data, handle irregular cluster shapes, <em>and</em> identify outliers.</p><p>When we first apply HDBSCAN clustering to our data, we will return <em>many</em> tiny clusters, identified by the red <em>circles</em> in the <em>condensed tree plot</em> below.</p><hr/><p><em>The condensed tree plot shows the drop-off of points into outliers and the splitting of clusters as the algorithm scans by increasing lambda values.</em></p><p><em>HDBSCAN chooses the final clusters based on their size and persistence over varying lambda values. The tree’s thickest, most persistent “branches” are viewed as the most stable and, therefore, best candidates for clusters.</em></p><hr/><p>These clusters are not very useful because the <em>default</em> minimum number of points needed to “create” a cluster is just <code>5</code>. Given our ~3K points dataset where we aim to produce ~4 subreddit clusters, this is small. Fortunately, we can increase this threshold using the <code>min_cluster_size</code> parameter.</p><p>Better, but not quite there, we can try to reduce the <code>min_cluster_size</code> to <code>60</code> to pull in the three clusters below the green block.</p><p>Unfortunately, this still pulls in the green block and even allows <em>too small</em> clusters (as on the left). Another option is to keep <code>min_cluster_size=80</code> but add <code>min_samples=40</code>, to allow for more sparse core points.</p><p>Now we have four clusters, and we can visualize them using the data in <code>clusterer.labels_</code>.</p><p><small>HDBSCAN clustered Reddit topics, accurately identifying the different subreddit clusters. The sparse blue points are outliers and are not identified as belonging to any cluster.</small></p><p>A few outliers are marked in blue, some of which make sense (pinned daily discussion threads) and others that do not. However, overall, these clusters are very accurate. With that, we can try to identify the meaning of these clusters.</p><h3 id="topic-extraction-with-c-tf-idf">Topic Extraction with c-TF-IDF</h3><p>The final step in BERTopic is extracting topics for each of our clusters. To do this, BERTopic uses a modified version of TF-IDF called c-TF-IDF.</p><p>TF-IDF is a popular technique for identifying the most relevant <em>“documents”</em> given a term or set of terms. c-TF-IDF turns this on its head by finding the most relevant <em>terms</em> given all of the “documents” within a cluster.</p><p><img loading="lazy" src="https://d33wubrfki0l68.cloudfront.net/2d6bc5d48ac667f318c8d27b8ad0e7cfe966f95b/10c6a/images/bertopic-7.png" alt="bertopic-ctfidf-intuition" width="100%"/>
<small>c-TF-IDF looks at the most relevant terms from each class (cluster) to create topics.</small></p><p>In our Reddit topics dataset, we have been able to identify very distinct clusters. However, we still need to determine what these clusters talk about. We start by preprocessing the <code>selftext</code> to create tokens.</p><p>Part of c-TF-IDF requires calculating the frequency of term <em>t</em> in class <em>c</em>. For that, we need to see which tokens belong in each class. We first add the cluster/class labels to <code>data</code>.</p><p>Now create class-specific lists of tokens.</p><p>We can calculate <strong>T</strong>erm <strong>F</strong>requency (TF) per class.</p><p>Note that this can take some time; our TF process prioritizes readability over any notion of efficiency. Once complete, we’re ready to calculate <strong>I</strong>nverse <strong>D</strong>ocument <strong>F</strong>requency (IDF), which tells us how common a term is. Rare terms signify greater relevance than common terms (and will output a greater IDF score).</p><p>We now have TF and IDF scores for every term, and we can calculate the c-TF-IDF score by simply multiplying both.</p><p>In <code>tf_idf</code>, we have a <code>vocab</code> sized list of TF-IDF scores for each class. We can use Numpy’s <code>argpartition</code> function to retrieve the index positions containing the greatest TF-IDF scores per class.</p><p>Now we map those index positions back to the original words in the <code>vocab</code>.</p><p>Here we have the top five most relevant words for each cluster, each identifying the most relevant topics in each subreddit.</p><h2 id="back-to-bertopic">Back to BERTopic</h2><p>We’ve covered a considerable amount, but can we apply what we have learned to the BERTopic library?</p><p>Fortunately, all we need are a few lines of code. As before, we initialize our custom embedding, UMAP, and HDBSCAN components.</p><p>You might notice that we have added <code>prediction_data=True</code> as a new parameter to <code>HDBSCAN</code>. We need this to avoid an <strong>AttributeError</strong> when integrating our custom HDBSCAN step with BERTopic. Adding <code>gen_min_span_tree</code> adds another step to HDBSCAN that can improve the resultant clusters.</p><p>We must also initialize a <code>vectorizer_model</code> to handle stopword removal during the c-TF-IDF step. We will use the list of stopwords from NLTK but add a few more tokens that seem to pollute the results.</p><p>We’re now ready to pass all of these to a <code>BERTopic</code> instance and process our Reddit topics data.</p><p>We can visualize the new topics with <code>model.visualize_barchart()</code></p><p><img loading="lazy" src="https://d33wubrfki0l68.cloudfront.net/fb3a6bddebba1df59edd9d7ec32fb8b886ab4c45/50fd1/images/bertopic-8.png" alt="bertopic-final-barchart" width="100%"/></p><p><small>Our final topics produced using the BERTopic library with the tuned UMAP and HDBSCAN parameters.</small></p><p>We can see the topics align perfectly with <em>r/investing</em>, <em>r/pytorch</em>, <em>r/LanguageTechnology</em>, and <em>r/Python</em>.</p><p>Transformers, UMAP, HDBSCAN, and c-TF-IDF are clearly powerful components that have huge applications when working with unstructured text data. BERTopic has abstracted away much of the complexity of this stack, allowing us to apply these technologies with nothing more than a few lines of code.</p><p>Although BERTopic can be simple, you have seen that it is possible to dive quite deeply into the individual components. With a high-level understanding of those components, we can greatly improve our topic modeling performance.</p><p>We have covered the essentials here, but we genuinely are just scratching the surface of topic modeling in this article. There is much more to BERTopic and each component than we could ever hope to cover in a single article.</p><p>So go and apply what you have learned here, and remember that despite showing the incredible performance of BERTopic shown here, there is even more that it can do.</p><h2 id="resources">Resources</h2><p>🔗 <a href="https://github.com/pinecone-io/examples/tree/master/learn/algos-and-libraries/bertopic">All Notebook Scripts</a></p><p>[1] M. Grootendorst, <a href="https://github.com/MaartenGr/BERTopic">BERTopic Repo</a>, GitHub</p><p>[2] M. Grootendorst, <a href="https://arxiv.org/abs/2203.05794">BERTopic: Neural Topic Modeling with a Class-based TF-IDF Procedure</a> (2022)</p><p>[3] L. McInnes, J. Healy, J. Melville, <a href="https://arxiv.org/abs/1802.03426">UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction</a> (2018)</p><p>[4] L. McInnes, <a href="https://www.youtube.com/watch?v=nq6iPZVUxZU">Talk on UMAP for Dimension Reduction</a> (2018), SciPy 2018</p><p>[5] J. Healy, <a href="https://www.youtube.com/watch?v=dGsxd67IFiU">HDBSCAN, Fast Density Based Clustering, the How and the Why</a> (2018), PyData NYC 2018</p><p>[6] L. McInnes, <a href="https://www.youtube.com/watch?v=9iol3Lk6kyU">A Bluffer’s Guide to Dimension Reduction</a> (2018), PyData NYC 2018</p><p><a href="https://www.youtube.com/watch?v=6BPl81wGGP8">UMAP Explained</a>, AI Coffee Break with Letitia, YouTube</p><section><hr/><h3>Comments</h3></section></div></div>
  </body>
</html>
