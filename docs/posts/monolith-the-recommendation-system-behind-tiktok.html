<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://gantry.io/blog/papers-to-know-20230110/">Original</a>
    <h1>Monolith: The Recommendation System Behind TikTok</h1>
    
    <div id="readability-page-1" class="page"><div><blockquote>
<p>Among all of the social networks, TikTok may be the one that is best known for its recommendations. It‚Äôs downright addictive.</p>
<p>However, until recently, the ML world was left guessing how TikTok achieve such high quality recs.</p>
<p><a href="https://arxiv.org/pdf/2209.07663.pdf">This paper</a> finally lifts the lid on how TikTok‚Äôs recommender system works. Let‚Äôs dive in.</p>
</blockquote>
<h2>Production ML Papers to Know</h2>
<p>Welcome to <strong>Production ML Papers to Know</strong>, a series from Gantry highlighting papers we think have been important to the evolving practice of production ML.</p>
<p>We have covered a few papers already in our newsletter, <a href="https://gantry.io/continual-learnings-sub/">Continual Learnings</a>, and on <a href="https://twitter.com/gantry_ml">Twitter</a>. Due to the positive reception we decided to turn these into blog posts.</p>
<h2><a href="https://arxiv.org/pdf/2209.07663.pdf">Monolith: The Recommendation System Behind TikTok</a></h2>
<h3>The challenge of changing user preferences</h3>
<p>One of the biggest challenges in recommender systems is non-stationarity. Users&#39; tastes and behaviors change (often as a result of the predictions your model makes!), and as they do, the data distribution changes, degrading the performance of your model.</p>
<p>The key innovation in <strong>Monolith</strong>, TikTok‚Äôs large-scale recommendation system, is that it can respond to changes in preferences ‚Äî fast ‚Äî using an online training system.</p>
<h3>Online Training</h3>
<p>Online training is the key to Monolith‚Äôs ability to quickly respond to changes in user preferences. Here‚Äôs how they do it.</p>
<p><span>
      <span></span>
  <img alt="1" title="1" src="https://gantry.io/static/b589b4412318fd37912a977cb25db540/5a190/1.png" srcset="/static/b589b4412318fd37912a977cb25db540/772e8/1.png 200w,
/static/b589b4412318fd37912a977cb25db540/e17e5/1.png 400w,
/static/b589b4412318fd37912a977cb25db540/5a190/1.png 800w,
/static/b589b4412318fd37912a977cb25db540/c1b63/1.png 1200w,
/static/b589b4412318fd37912a977cb25db540/29007/1.png 1600w,
/static/b589b4412318fd37912a977cb25db540/e8950/1.png 2000w" sizes="(max-width: 800px) 100vw, 800px" loading="lazy" decoding="async"/>
    </span></p>
<p>First, both features and user actions are needed to train the model, but they arrive at different times from different parts of the system. Monolith handles this by logging each to separate Kafka queues, and joining them using an <em>online joiner</em> module written in Flink.</p>
<p>Next, a <em>training worker</em> picks up training examples and performs training. One of the clever parts of the architecture is that it always uses the same training worker, whether you‚Äôre building a new model with a batch of historical data or you‚Äôre updating an existing model online.</p>
<p>As model parameters continue to change on-the-fly, they need to be periodically synchronized with the model server. This presents two technical challenges. First, there can‚Äôt be a gap in model serving, and second, they need to avoid transferring the multi-terabyte set of model parameters over the network for each update.</p>
<p>It does this by frequently updating the sparse parameters of the embedding tables, which make up a large part of the DNN. This results in a relatively small update to be pushed across the network. The dense parameters of the DNN weights are updated less frequently. This inconsistency in updating the model has not led to a loss of model performance.</p>
<p>Monolith was tested in a series of experiments, which found that real-time online training consistently improved model quality, and that models with smaller parameter synchronization interval periods performed better than those with larger intervals.</p>
<h3>Collisionless Hashing</h3>
<p>The paper also covers an interesting approach to addressing the challenge of the sparse, categorical and dynamic nature of user data, which can result in the embeddings used to preprocess this data becoming &#34;enormous.&#34;</p>
<p>Hashing is typically used to solve this problem, but this can result in collisions that reduce model quality. Monolith addresses this through a <em>collisionless hash table</em> that has the elasticity to adjust as embeddings grow, and which was shown in the paper to consistently outperform models which use collision-based approaches.</p>
<p>The <strong>collisionless hash table</strong> is based on <a href="https://en.wikipedia.org/wiki/Cuckoo_hashing">cuckoo hashing</a>, The figure below illustrates how it works: it maintains two tables, ùëá0, ùëá1, each with different hash functions, h0(ùë•),h1(ùë•). An element can be stored in either table. If an element is already in place, this element is evicted and placed elsewhere, and this process continues until all elements are stabilized.</p>
<p><span>
      <span></span>
  <img alt="2 1" title="2 1" src="https://gantry.io/static/3ce311a90f0b0d6cd84c5f5159eb5266/774b6/2-1.png" srcset="/static/3ce311a90f0b0d6cd84c5f5159eb5266/772e8/2-1.png 200w,
/static/3ce311a90f0b0d6cd84c5f5159eb5266/e17e5/2-1.png 400w,
/static/3ce311a90f0b0d6cd84c5f5159eb5266/774b6/2-1.png 738w" sizes="(max-width: 738px) 100vw, 738px" loading="lazy" decoding="async"/>
    </span></p>
<p>There is also a focus on <strong>memory footprint reduction</strong> through ID filtering. IDs that appear only a handful of times, or have been inactive for a period of time, are filtered out, with the threshold for filtering treated as a tunable hyperparameter during model training.</p>
<h3>The upshot</h3>
<p>This paper provides a fascinating insight into how recommender systems operate at an industrial scale, and how companies like Bytedance are driving improvement in their operations.</p>
<p>You may not be operating at TikTok scale, but if you work with rapidly-changing user data, you might want to consider moving to something like their online training approach.</p>
<p>The paper is <a href="https://arxiv.org/pdf/2209.07663.pdf">here</a>.</p></div></div>
  </body>
</html>
