<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://wandb.ai/byyoung3/ml-news/reports/How-to-Run-Mistral-7B-on-an-M1-Mac-With-Ollama--Vmlldzo2MTg4MjA0">Original</a>
    <h1>Run Mistral 7B on M1 Mac</h1>
    
    <div id="readability-page-1" class="page"><div data-test="report-editable-content" data-slate-editor="true" data-slate-node="value" contenteditable="false" zindex="-1"><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">As smaller LLM&#39;s quickly become more capable, the potential use cases for running them on edge devices is also quickly growing. This tutorial will focus on </span></span></span><a data-slate-node="element" data-slate-inline="true" href="https://wandb.ai/byyoung3/ml-news/reports/Fine-Tuning-Mistral7B-on-Python-Code-With-A-Single-GPU---Vmlldzo1NTg0NzY5" target="_blank"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">deploying the Mistral 7B model</span></span></span></a><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> locally on Mac devices, including Macs with M series processors! In addition, I will also show you how to use custom Mistral 7B adapters locally! To do this easily and efficiently, we will leverage Ollama and the llama.cpp repository! </span></span></span></p><div><div><p><img draggable="false" src="https://api.wandb.ai/files/byyoung3/images/projects/37269171/933a3874.png"/></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-zero-width="z" data-slate-length="0">﻿</span></span></span></p></div></div><h4 id="what-we&#39;ll-be-covering-in-this-tutorial"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">What We&#39;ll Be Covering In This Tutorial</span></span></span></h4><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-zero-width="z" data-slate-length="0">﻿</span></span></span></p><h2 id="ollama-"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Ollama	</span></span></span></h2><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Ollama is a versatile and user-friendly platform that enables you to set up and run </span></span></span><a data-slate-node="element" data-slate-inline="true" href="https://wandb.ai/site" target="_blank"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">large language models</span></span></span></a><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> locally easily. It supports various operating systems, including macOS, Windows, and Linux, and can also be used in Docker environments.</span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Ollama offers a range of pre-built open-source models, such as Neural Chat, Starling, Mistral, and different versions of Llama, with varying parameters and sizes to cater to different needs and system capabilities.</span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Ollama also has a straightforward command-line interface for creating, pulling, removing, copying, and running models. It supports multiline input and the ability to pass prompts as arguments. For those who prefer a programmatic approach, Ollama provides a REST API, allowing integration into various applications and services. This makes it an ideal tool for developers and hobbyists interested in experimenting with and deploying large language models locally.</span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">We will be working with the Mistral 7B, which works with almost all M1 Macs; however, if your machine has 8GB of RAM, it will be a little slow, but the good news is it will run! </span></span></span></p><h2 id="step-1:-mac-install-"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Step 1: Mac Install </span></span></span></h2><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Installing Ollama on a macOS system is a straightforward process. You can begin by visiting the Ollama </span></span></span><a data-slate-node="element" data-slate-inline="true" href="https://ollama.ai/" target="_blank"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">website</span></span></span></a><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> and navigating to the download section. Here, you will find a dedicated link for the macOS version of Ollama. The installation package will be downloaded to their system by clicking on the&#39; Download&#39; button.</span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Once the download is complete, you can open the downloaded file and follow the on-screen instructions to complete the installation. This typically involves dragging the Ollama application into the Applications folder. After the installation, you can finally launch Ollama from their Applications folder or through Spotlight search.</span></span></span></p><h2 id="run-the-base-mistral-model-"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Run the Base Mistral Model </span></span></span></h2><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">To run the base Mistral model using Ollama, you first need to open the Ollama app on your machine, and then open your terminal. Then, enter the command </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">ollama run mistral</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> and press Enter. This command pulls and initiates the Mistral model, and Ollama will handle the setup and execution process.</span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Once the model is running, you can interact with it directly from your terminal, experimenting with its capabilities or testing specific queries and inputs as per your requirements.</span></span></span></p><div><div><p><img draggable="false" src="https://api.wandb.ai/files/byyoung3/images/projects/37269171/86fa32f7.png"/></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-zero-width="z" data-slate-length="0">﻿</span></span></span></p></div></div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">You can end the conversation using the </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">/bye</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> command. </span></span></span></p><h2 id="creating-a-custom-mistral-model"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Creating a Custom Mistral Model</span></span></span></h2><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">This tutorial assumes you have already fine-tuned your Mistral model using Lora or </span></span></span><a data-slate-node="element" data-slate-inline="true" href="https://wandb.ai/sauravmaheshkar/QLoRA/reports/What-is-QLoRA---Vmlldzo2MTI2OTc5" target="_blank"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">QLoRA</span></span></span></a><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">, allowing you to obtain the weights for your adapters. Feel free to check out my other </span></span></span><a data-slate-node="element" data-slate-inline="true" href="https://wandb.ai/byyoung3/ml-news/reports/Fine-Tuning-Mistral7B-on-Python-Code-With-A-Single-GPU---Vmlldzo1NTg0NzY5" target="_blank"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">tutorial</span></span></span></a><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> for more details on this if you haven&#39;t!</span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">To utilize these adapters with Ollama, convert your adapter to</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> GGML forma</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">t. Navigate to the llama.cpp directory, which is part of the tutorial repository. You&#39;ll run a script in this directory to convert the adapter to GGML format. Use the command </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">python convert-lora-to-ggml.py /path/to/your/lora/adapter</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> where the path leads to the directory containing your Lora adapter, usually named something like </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">adapter.bin</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> and </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">config.bin</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> files. </span></span></span></p><div><pre><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">python convert</span></span><span data-slate-leaf="true"><span data-slate-string="true">-</span></span><span data-slate-leaf="true"><span data-slate-string="true">lora</span></span><span data-slate-leaf="true"><span data-slate-string="true">-</span></span><span data-slate-leaf="true"><span data-slate-string="true">to</span></span><span data-slate-leaf="true"><span data-slate-string="true">-</span></span><span data-slate-leaf="true"><span data-slate-string="true">ggml</span></span><span data-slate-leaf="true"><span data-slate-string="true">.</span></span><span data-slate-leaf="true"><span data-slate-string="true">py </span></span><span data-slate-leaf="true"><span data-slate-string="true">/</span></span><span data-slate-leaf="true"><span data-slate-string="true">path_to_your_adapter</span></span></span></p></pre></div><h3 id="creating-the-model-file-"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Creating the Model File </span></span></span></h3><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Next, you&#39;ll create a Ollama model file (which is similar to a Dockerfile, but for LLM&#39;s). Begin with specifying the base model using the `FROM` keyword, like `FROM mistral:latest`, indicating the use of the latest version of the Mistral model. Then, specify the adapter using the `ADAPTER` keyword and provide the path to your GGML adapter model binary file. Heres my modelfile: </span></span></span></p><div><pre><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">FROM mistral</span></span><span data-slate-leaf="true"><span data-slate-string="true">:</span></span><span data-slate-leaf="true"><span data-slate-string="true">latest</span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">ADAPTER </span></span><span data-slate-leaf="true"><span data-slate-string="true">/</span></span><span data-slate-leaf="true"><span data-slate-string="true">Users</span></span><span data-slate-leaf="true"><span data-slate-string="true">/</span></span><span data-slate-leaf="true"><span data-slate-string="true">brettyoung</span></span><span data-slate-leaf="true"><span data-slate-string="true">/</span></span><span data-slate-leaf="true"><span data-slate-string="true">Desktop</span></span><span data-slate-leaf="true"><span data-slate-string="true">/</span></span><span data-slate-leaf="true"><span data-slate-string="true">IMPORTANT</span></span><span data-slate-leaf="true"><span data-slate-string="true">/</span></span><span data-slate-leaf="true"><span data-slate-string="true">mistral</span></span><span data-slate-leaf="true"><span data-slate-string="true">-</span></span><span data-slate-leaf="true"><span data-slate-string="true">7b</span></span><span data-slate-leaf="true"><span data-slate-string="true">-</span></span><span data-slate-leaf="true"><span data-slate-string="true">alpacapython10kCheckpoints</span></span><span data-slate-leaf="true"><span data-slate-string="true">/</span></span><span data-slate-leaf="true"><span data-slate-string="true">checkpoint</span></span><span data-slate-leaf="true"><span data-slate-string="true">-</span></span><span data-slate-leaf="true"><span data-slate-string="true">150</span></span><span data-slate-leaf="true"><span data-slate-string="true">/</span></span><span data-slate-leaf="true"><span data-slate-string="true">ggml</span></span><span data-slate-leaf="true"><span data-slate-string="true">-</span></span><span data-slate-leaf="true"><span data-slate-string="true">adapter</span></span><span data-slate-leaf="true"><span data-slate-string="true">-</span></span><span data-slate-leaf="true"><span data-slate-string="true">model</span></span><span data-slate-leaf="true"><span data-slate-string="true">.</span></span><span data-slate-leaf="true"><span data-slate-string="true">bin</span></span></span></p></pre></div><h3 id="model-creation-"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Model Creation </span></span></span></h3><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">After setting up your Modelfile, create your custom model using the `ollama create` command followed by your chosen model name and the Modelfile path, like `ollama create custom_mistral -f ./Modelfile`. </span></span></span></p><div><pre><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">ollama create custom_mistral </span></span><span data-slate-leaf="true"><span data-slate-string="true">-</span></span><span data-slate-leaf="true"><span data-slate-string="true">f </span></span><span data-slate-leaf="true"><span data-slate-string="true">.</span></span><span data-slate-leaf="true"><span data-slate-string="true">/</span></span><span data-slate-leaf="true"><span data-slate-string="true">Modelfile</span></span></span></p></pre></div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Finally, run your custom model with `ollama run custom_mistral`. This completes the process, allowing you to interact with your tailored Mistral model.</span></span></span></p><div><pre><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">ollama run custom_mistral</span></span></span></p></pre></div><h2 id="using-our-mistral-model-in-python"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Using Our Mistral Model in Python</span></span></span></h2><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">To use Ollama in Python, we will need to make a simple function called the Ollama local API, which runs automatically when we run the application on our system. This API is normally called with curl, but we will make a custom Python function to call the API programmatically. Below is the script: </span></span></span></p><div><pre><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">import</span></span><span data-slate-leaf="true"><span data-slate-string="true"> subprocess</span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">import</span></span><span data-slate-leaf="true"><span data-slate-string="true"> json</span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-zero-width="n" data-slate-length="0">﻿<br/></span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">def</span></span><span data-slate-leaf="true"><span data-slate-string="true"> </span></span><span data-slate-leaf="true"><span data-slate-string="true">generate_response</span></span><span data-slate-leaf="true"><span data-slate-string="true">(</span></span><span data-slate-leaf="true"><span data-slate-string="true">prompt</span></span><span data-slate-leaf="true"><span data-slate-string="true">)</span></span><span data-slate-leaf="true"><span data-slate-string="true">:</span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">    curl_command </span></span><span data-slate-leaf="true"><span data-slate-string="true">=</span></span><span data-slate-leaf="true"><span data-slate-string="true"> </span></span><span data-slate-leaf="true"><span data-slate-string="true">f&#34;&#34;&#34;curl -s http://localhost:11434/api/generate -d &#39;{{&#34;model&#34;: &#34;custom_mistral&#34;, &#34;prompt&#34;:&#34;{prompt}&#34;}}&#39;&#34;&#34;&#34;</span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">    process </span></span><span data-slate-leaf="true"><span data-slate-string="true">=</span></span><span data-slate-leaf="true"><span data-slate-string="true"> subprocess</span></span><span data-slate-leaf="true"><span data-slate-string="true">.</span></span><span data-slate-leaf="true"><span data-slate-string="true">Popen</span></span><span data-slate-leaf="true"><span data-slate-string="true">(</span></span><span data-slate-leaf="true"><span data-slate-string="true">curl_command</span></span><span data-slate-leaf="true"><span data-slate-string="true">,</span></span><span data-slate-leaf="true"><span data-slate-string="true"> shell</span></span><span data-slate-leaf="true"><span data-slate-string="true">=</span></span><span data-slate-leaf="true"><span data-slate-string="true">True</span></span><span data-slate-leaf="true"><span data-slate-string="true">,</span></span><span data-slate-leaf="true"><span data-slate-string="true"> stdout</span></span><span data-slate-leaf="true"><span data-slate-string="true">=</span></span><span data-slate-leaf="true"><span data-slate-string="true">subprocess</span></span><span data-slate-leaf="true"><span data-slate-string="true">.</span></span><span data-slate-leaf="true"><span data-slate-string="true">PIPE</span></span><span data-slate-leaf="true"><span data-slate-string="true">,</span></span><span data-slate-leaf="true"><span data-slate-string="true"> stderr</span></span><span data-slate-leaf="true"><span data-slate-string="true">=</span></span><span data-slate-leaf="true"><span data-slate-string="true">subprocess</span></span><span data-slate-leaf="true"><span data-slate-string="true">.</span></span><span data-slate-leaf="true"><span data-slate-string="true">PIPE</span></span><span data-slate-leaf="true"><span data-slate-string="true">,</span></span><span data-slate-leaf="true"><span data-slate-string="true"> text</span></span><span data-slate-leaf="true"><span data-slate-string="true">=</span></span><span data-slate-leaf="true"><span data-slate-string="true">True</span></span><span data-slate-leaf="true"><span data-slate-string="true">)</span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">    full_response </span></span><span data-slate-leaf="true"><span data-slate-string="true">=</span></span><span data-slate-leaf="true"><span data-slate-string="true"> </span></span><span data-slate-leaf="true"><span data-slate-string="true">&#34;&#34;</span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-zero-width="n" data-slate-length="0">﻿<br/></span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">    </span></span><span data-slate-leaf="true"><span data-slate-string="true">while</span></span><span data-slate-leaf="true"><span data-slate-string="true"> </span></span><span data-slate-leaf="true"><span data-slate-string="true">True</span></span><span data-slate-leaf="true"><span data-slate-string="true">:</span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">        output_line </span></span><span data-slate-leaf="true"><span data-slate-string="true">=</span></span><span data-slate-leaf="true"><span data-slate-string="true"> process</span></span><span data-slate-leaf="true"><span data-slate-string="true">.</span></span><span data-slate-leaf="true"><span data-slate-string="true">stdout</span></span><span data-slate-leaf="true"><span data-slate-string="true">.</span></span><span data-slate-leaf="true"><span data-slate-string="true">readline</span></span><span data-slate-leaf="true"><span data-slate-string="true">(</span></span><span data-slate-leaf="true"><span data-slate-string="true">)</span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">        </span></span><span data-slate-leaf="true"><span data-slate-string="true">if</span></span><span data-slate-leaf="true"><span data-slate-string="true"> </span></span><span data-slate-leaf="true"><span data-slate-string="true">not</span></span><span data-slate-leaf="true"><span data-slate-string="true"> output_line </span></span><span data-slate-leaf="true"><span data-slate-string="true">and</span></span><span data-slate-leaf="true"><span data-slate-string="true"> process</span></span><span data-slate-leaf="true"><span data-slate-string="true">.</span></span><span data-slate-leaf="true"><span data-slate-string="true">poll</span></span><span data-slate-leaf="true"><span data-slate-string="true">(</span></span><span data-slate-leaf="true"><span data-slate-string="true">)</span></span><span data-slate-leaf="true"><span data-slate-string="true"> </span></span><span data-slate-leaf="true"><span data-slate-string="true">is</span></span><span data-slate-leaf="true"><span data-slate-string="true"> </span></span><span data-slate-leaf="true"><span data-slate-string="true">not</span></span><span data-slate-leaf="true"><span data-slate-string="true"> </span></span><span data-slate-leaf="true"><span data-slate-string="true">None</span></span><span data-slate-leaf="true"><span data-slate-string="true">:</span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">            </span></span><span data-slate-leaf="true"><span data-slate-string="true">break</span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">        </span></span><span data-slate-leaf="true"><span data-slate-string="true">if</span></span><span data-slate-leaf="true"><span data-slate-string="true"> output_line</span></span><span data-slate-leaf="true"><span data-slate-string="true">:</span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">            </span></span><span data-slate-leaf="true"><span data-slate-string="true">try</span></span><span data-slate-leaf="true"><span data-slate-string="true">:</span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">                response_data </span></span><span data-slate-leaf="true"><span data-slate-string="true">=</span></span><span data-slate-leaf="true"><span data-slate-string="true"> json</span></span><span data-slate-leaf="true"><span data-slate-string="true">.</span></span><span data-slate-leaf="true"><span data-slate-string="true">loads</span></span><span data-slate-leaf="true"><span data-slate-string="true">(</span></span><span data-slate-leaf="true"><span data-slate-string="true">output_line</span></span><span data-slate-leaf="true"><span data-slate-string="true">.</span></span><span data-slate-leaf="true"><span data-slate-string="true">strip</span></span><span data-slate-leaf="true"><span data-slate-string="true">(</span></span><span data-slate-leaf="true"><span data-slate-string="true">)</span></span><span data-slate-leaf="true"><span data-slate-string="true">)</span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">                full_response </span></span><span data-slate-leaf="true"><span data-slate-string="true">+=</span></span><span data-slate-leaf="true"><span data-slate-string="true"> response_data</span></span><span data-slate-leaf="true"><span data-slate-string="true">.</span></span><span data-slate-leaf="true"><span data-slate-string="true">get</span></span><span data-slate-leaf="true"><span data-slate-string="true">(</span></span><span data-slate-leaf="true"><span data-slate-string="true">&#34;response&#34;</span></span><span data-slate-leaf="true"><span data-slate-string="true">,</span></span><span data-slate-leaf="true"><span data-slate-string="true"> </span></span><span data-slate-leaf="true"><span data-slate-string="true">&#34;&#34;</span></span><span data-slate-leaf="true"><span data-slate-string="true">)</span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">            </span></span><span data-slate-leaf="true"><span data-slate-string="true">except</span></span><span data-slate-leaf="true"><span data-slate-string="true"> json</span></span><span data-slate-leaf="true"><span data-slate-string="true">.</span></span><span data-slate-leaf="true"><span data-slate-string="true">JSONDecodeError</span></span><span data-slate-leaf="true"><span data-slate-string="true">:</span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">                </span></span><span data-slate-leaf="true"><span data-slate-string="true">return</span></span><span data-slate-leaf="true"><span data-slate-string="true"> </span></span><span data-slate-leaf="true"><span data-slate-string="true">&#34;Invalid response format&#34;</span></span><span data-slate-leaf="true"><span data-slate-string="true">,</span></span><span data-slate-leaf="true"><span data-slate-string="true"> </span></span><span data-slate-leaf="true"><span data-slate-string="true">500</span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-zero-width="n" data-slate-length="0">﻿<br/></span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">    </span></span><span data-slate-leaf="true"><span data-slate-string="true">return</span></span><span data-slate-leaf="true"><span data-slate-string="true"> full_response</span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-zero-width="n" data-slate-length="0">﻿<br/></span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">def</span></span><span data-slate-leaf="true"><span data-slate-string="true"> </span></span><span data-slate-leaf="true"><span data-slate-string="true">get_user_input_and_generate</span></span><span data-slate-leaf="true"><span data-slate-string="true">(</span></span><span data-slate-leaf="true"><span data-slate-string="true">)</span></span><span data-slate-leaf="true"><span data-slate-string="true">:</span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">    prompt </span></span><span data-slate-leaf="true"><span data-slate-string="true">=</span></span><span data-slate-leaf="true"><span data-slate-string="true"> </span></span><span data-slate-leaf="true"><span data-slate-string="true">input</span></span><span data-slate-leaf="true"><span data-slate-string="true">(</span></span><span data-slate-leaf="true"><span data-slate-string="true">&#34;Enter a prompt: &#34;</span></span><span data-slate-leaf="true"><span data-slate-string="true">)</span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">    response </span></span><span data-slate-leaf="true"><span data-slate-string="true">=</span></span><span data-slate-leaf="true"><span data-slate-string="true"> generate_response</span></span><span data-slate-leaf="true"><span data-slate-string="true">(</span></span><span data-slate-leaf="true"><span data-slate-string="true">prompt</span></span><span data-slate-leaf="true"><span data-slate-string="true">)</span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">    </span></span><span data-slate-leaf="true"><span data-slate-string="true">print</span></span><span data-slate-leaf="true"><span data-slate-string="true">(</span></span><span data-slate-leaf="true"><span data-slate-string="true">&#34;Response:&#34;</span></span><span data-slate-leaf="true"><span data-slate-string="true">,</span></span><span data-slate-leaf="true"><span data-slate-string="true"> response</span></span><span data-slate-leaf="true"><span data-slate-string="true">)</span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-zero-width="n" data-slate-length="0">﻿<br/></span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">if</span></span><span data-slate-leaf="true"><span data-slate-string="true"> __name__ </span></span><span data-slate-leaf="true"><span data-slate-string="true">==</span></span><span data-slate-leaf="true"><span data-slate-string="true"> </span></span><span data-slate-leaf="true"><span data-slate-string="true">&#39;__main__&#39;</span></span><span data-slate-leaf="true"><span data-slate-string="true">:</span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">    get_user_input_and_generate</span></span><span data-slate-leaf="true"><span data-slate-string="true">(</span></span><span data-slate-leaf="true"><span data-slate-string="true">)</span></span></span></p></pre></div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">The script executes this curl command using Python&#39;s subprocess, Popen. This method allows the script to run the curl command in a separate process and capture its output. The script reads the output line by line, parsing it as JSON, and concatenates the responses. </span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Note that the generate function is a bit complex since the Ollama local API streams its output in multiple chunks instead of generating a single response. Below is the command our Python script runs, which specifies our model as well as the prompt input! </span></span></span></p><div><pre><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> curl_command </span></span><span data-slate-leaf="true"><span data-slate-string="true">=</span></span><span data-slate-leaf="true"><span data-slate-string="true"> </span></span><span data-slate-leaf="true"><span data-slate-string="true">f&#34;&#34;&#34;curl -s http://localhost:11434/api/generate -d &#39;{{&#34;model&#34;: &#34;custom_mistral&#34;, &#34;prompt&#34;:&#34;{prompt}&#34;}}&#39;&#34;&#34;&#34;</span></span></span></p></pre></div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">The while loop reads from the process&#39;s output until there is no more data (i.e., the process has completed). The function returns an error message if the JSON decoding fails (indicating an unexpected response format).</span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">You can use this script to send prompts to the custom Mistral model and receive generated responses. This is achieved by simply calling generate_response with the desired prompt. The main block of the script (under if __name__ == &#39;__main__&#39;:) is set up for testing this functionality: it prompts the user to enter a prompt, calls the generate_response function with this prompt, and then prints out the model&#39;s response. Here is an example of the above script! </span></span></span></p><div><div><p><img draggable="false" src="https://api.wandb.ai/files/byyoung3/images/projects/37269171/3ed40e41.png"/></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-zero-width="z" data-slate-length="0">﻿</span></span></span></p></div></div><h2 id="conclusion"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Conclusion</span></span></span></h2><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Hopefully, this tutorial serves as a gateway to a world where the power of AI can sit comfortably on your desktop. Whether you&#39;re a developer seeking to infuse your applications with cutting-edge AI capabilities or a hobbyist eager to explore the frontiers of language modeling, there are tons of applications for local LLM&#39;s!</span></span></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">As always, feel free to drop a comment with any questions, and feel free to check out the Github repo </span></span></span><a data-slate-node="element" data-slate-inline="true" href="https://github.com/bdytx5/m1_mistral_local" target="_blank"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">here</span></span></span></a><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">. </span></span></span></p></div></div>
  </body>
</html>
