<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://jepsen.io/analyses/bufstream-0.1.0">Original</a>
    <h1>Jepsen: Bufstream 0.1.0</h1>
    
    <div id="readability-page-1" class="page"><p><a href="https://buf.build/product/bufstream">Bufstream</a> is a <a href="https://kafka.apache.org/">Kafka</a>-compatible streaming system which stores records directly in an object storage service like S3. We found three safety and two liveness issues in Bufstream, including stuck consumers and producers, spurious zero offsets, and the loss of acknowledged writes in healthy clusters. These problems were resolved by version 0.1.3. We also characterize four issues related to Kafka more generally, including the lack of authoritative documentation for transaction semantics, a deadlock in the official Java client, and write loss, aborted read, and torn transactions caused by the lack of message ordering constraints in the Kafka transaction protocol. These issues affect Kafka, Bufstream, and (presumably) other Kafka-compatible systems, and remain unresolved. A companion <a href="https://buf.build/blog/bufstream-jepsen-report">blog post</a> from Buf is available as well. This report was funded by Buf Technologies, Inc. and conducted in accordance with the <a href="https://jepsen.io/analyses/ethics">Jepsen ethics policy</a>.</p><article>
  <div>

<p><a href="https://kafka.apache.org/">Kafka</a> is a popular streaming system which provides replicated, sharded, append-only logs. <a href="https://buf.build/product/bufstream">Bufstream</a> is a drop-in replacement for Kafka designed to prioritize <a href="https://buf.build/blog/bufstream-kafka-lower-cost">data governance and cost efficiency</a> in cloud environments.</p>
<p>Like Kafka, Bufstream provides a collection of named, partially ordered logs called <em>topics</em>. Each topic is divided into <em>partitions</em>.<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a> Each partition is a totally ordered, append-only list of <em>records</em> (also called <em>messages</em> or <em>events</em>). Within a partition, each record is uniquely identified by a monotonically advancing integer <em>offset</em>. Offsets may be sparse: some offsets are used for storing internal metadata and are invisible to clients.</p>
<p>Bufstream works with standard Kafka clients. There are two main types of clients in Kafka-compatible systems. <em>Producers</em> append records to partitions by calling <code>producer.send()</code>. <em>Consumers</em> read those records. Consumers are first bound to partitions via <code>consumer.assign()</code> or <code>consumer.subscribe()</code> operations.<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a> Once bound, one repeatedly calls <code>consumer.poll()</code> to receive records from any of those partitions. Each consumer can belong to a <em>consumer group</em>, which shares responsibility for processing records from a set of topics.</p>
<p>Each partition has a <em>last stable offset (LSO)</em>, which is the highest offset below which every transaction has completed. It also has a <em>committed offset</em> for each consumer group, which is the highest offset below which that consumer group has processed all records in the partition.<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>As in Kafka, records are opaque blobs of bytes by default. However, Bufstream can integrate with the <a href="https://buf.build/product/bsr">Buf Schema Registry</a> to introspect <a href="https://protobuf.dev/">Protocol Buffer</a> records. This allows Bufstream to <a href="https://buf.build/docs/bufstream/data-governance/schema-enforcement">validate records</a> before committing them, enforce field-level access control policies, and reformat data to interoperate with other systems. Unlike Kafka, which stores data on local disks and has its own <a href="https://docs.confluent.io/kafka/design/replication.html">replication protocol</a>, Bufstream <a href="https://buf.build/docs/bufstream/kafka-compatibility/configure-clients#optimizing-performance-and-write-throughput">writes its data directly to an object storage service</a>. By relying on object storage, which often bundles the cost of replication traffic, Bufstream aims to <a href="https://buf.build/docs/bufstream/cost">reduce costs</a>. This also allows Bufstream nodes to run as stateless, auto-scaled VMs.<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p>Bufstream comprises three subsystems: an <em>agent</em>, an <em>object store</em>, and a <em>coordination service</em>. The agent is a stateless service which provides the Kafka API. Clients connect to agents to publish and consume records. The object store (e.g. S3) stores chunks of records as they are written, and makes them available to readers. The coordination service (presently <a href="https://etcd.io">etcd</a>) helps agents establish which chunks in storage are committed, and what the order of records should be.</p>
<p>As of October 2024, Bufstream was deployed only with select customers. Its documentation claimed to be “a drop-in replacement for Apache Kafka,” and listed compatibility with Kafka’s <a href="https://buf.build/docs/bufstream/kafka-compatibility/conformance">transactions and exactly-once semantics</a>. However, there were few specific safety claims beyond Kafka compatibility. Throughout this work we use Kafka’s documentation as our benchmark for evaluating Bufstream.</p>
<h2 data-number="1.1" id="client-safety"> Client Safety</h2>
<p>Like Kafka, Bufstream is intended for a variety of streaming applications with different throughput, latency, and safety tradeoffs. As in Jepsen’s <a href="https://jepsen.io/analyses/redpanda-21.10.1">previous work</a> on Kafka-compatible systems, we set a variety of client configuration options to obtain safer behavior.</p>
<p>We generally used the default <a href="https://docs.confluent.io/platform/current/installation/configuration/producer-configs.html#acks"><code>acks = all</code></a> for our producers. In Bufstream, <code>acks = 0</code> allows the server to acknowledge a write immediately without waiting for storage. As in Kafka, this may lose committed writes. Both <code>acks = 1</code> and <code>acks = all</code> block until Bufstream is certain the write is durably persisted.</p>
<p>Kafka producers can automatically retry writes. We used the default setting <a href="https://docs.confluent.io/platform/current/installation/configuration/producer-configs.html#enable-idempotence"><code>enable.idempotence = true</code></a> to prevent appending multiple copies of a record to the log.</p>
<p><a href="https://docs.confluent.io/kafka/design/delivery-semantics.html#exactly-once-support">Confluent’s documentation</a> claims that “… by default Kafka guarantees at-least-once delivery.” This is untrue. By default, Kafka consumers may automatically mark offsets as committed, regardless of whether they have actually been processed by the application. This means that a consumer can poll a series of records, mark them as committed, then crash—effectively causing those records to be lost. We set <a href="https://docs.confluent.io/platform/7.7/installation/configuration/consumer-configs.html#enable-auto-commit"><code>enable.auto.commit = false</code></a> to prevent this.<a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<p>When a consumer subscribes to a topic, it starts at the last committed offset. If no offset has been committed, it defaults to the most recent offset. This is another way in which Kafka’s defaults do not ensure at-least-once delivery. We used <a href="https://docs.confluent.io/platform/current/clients/consumer.html#offset-management-configuration"><code>auto.offset.reset = earliest</code></a> to make sure consumers had a chance to observe the entire log.</p>
<h2 data-number="1.2" id="transactions"> Transactions</h2>
<p>Bufstream supports Kafka’s <a href="https://developer.confluent.io/courses/architecture/transactions/">transaction system</a>. Kafka transactions have <a href="https://jepsen.io/analyses/bufstream-0.1.0/kafka-transactions.jpg">complex semantics</a> determined by a wide array of configuration settings and the specific calls executed by producer and consumer. As we discussed <a href="https://jepsen.io/analyses/redpanda-21.10.1#discussion">two years ago</a>, Kafka’s <a href="https://kafka.apache.org/38/documentation.html">official documentation</a> largely omits any description of transaction invariants. Instead, documentation remains scattered across various <a href="https://www.confluent.io/blog/transactions-apache-kafka/">blog</a> <a href="https://www.confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it/">posts</a>, <a href="https://cwiki.apache.org/confluence/display/KAFKA/Transactional+Messaging+in+Kafka">Wiki pages</a>, <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging">Kafka Improvement Proposals</a>, <a href="https://docs.google.com/document/d/11Jqy_GjUGtdXJK94XGsEIK7CP1SnQGdp2eF0wSw9ra8/edit#heading=h.xq0ee1vnpz4o">Google Docs</a>, <a href="https://developer.confluent.io/learn/kafka-transactions-and-guarantees/">introductory guides</a>, and the Java client’s <a href="https://kafka.apache.org/0110/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html">API documentation</a>. These resources are often confusing, underspecified, contradictory, or outright wrong. They generally focus on implementation mechanics rather than invariants. We have inferred Kafka’s intended transaction semantics as best we can from these documents and observed behavior.</p>
<p>In broad terms, a Kafka transaction comprises a set of records sent by a producer and a map of partitions to the maximum offsets polled by a consumer. Transactions provide a weak form of atomicity. If and only if the transaction commits, every sent record is durable and eventually visible to <code>read_committed</code> consumers in their respective partitions, and the committed offset for each partition is at least as high as the corresponding offset specified in the transaction. If the transaction does not commit, committed offsets do not advance, and some (or all) writes may (or may not) be visible depending on consumer configuration.</p>
<p>With the right implicit assumptions—for instance, that consumers process every record seen, that every record is processed in the scope of a transaction, that every transaction commits its highest consumed offsets, and so on—the committed offsets of a transaction can be understood as the set of records it consumed. With care, one can theoretically use transactions to obtain what Kafka terms “<a href="https://www.confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it/">exactly-once semantics</a>.”</p>
<p>Consumers can run in one of two <a href="https://docs.confluent.io/platform/current/installation/configuration/consumer-configs.html?#isolation-level">isolation levels</a>: <code>read_uncommitted</code> or <code>read_committed</code>. At the default <code>read_uncommitted</code>, consumers may read values written by transactions which actually aborted. This is known as <em>aborted read (G1a)</em>. Transactions may observe none, part, or all of an aborted transaction’s writes. Two or more read-write transactions may also observe each other’s writes: a form of <em>circular information flow (G1c)</em>. Kafka’s documentation says that <code>read_committed</code> prevents G1a, and sort of<a href="#fn6" id="fnref6" role="doc-noteref"><sup>6</sup></a> guarantees<a href="#fn7" id="fnref7" role="doc-noteref"><sup>7</sup></a> that either all or none of a transaction’s writes are eventually visible. In our tests of Kafka, Redpanda, and Bufstream, <code>read_commmitted</code> also prevented G1c involving only write-read dependencies.</p>
<p>On the other hand, our tests of Kafka, Redpanda, and Bufstream all found write cycles <a href="https://pmg.csail.mit.edu/papers/icde00.pdf">analogous to phenomenon G0</a> in normal operation. One transaction’s writes can appear in the middle of a second transaction’s writes. The major formalisms for Read Uncommitted generally prohibit G0, and the Kafka wiki <a href="https://cwiki.apache.org/confluence/display/KAFKA/Transactional+Messaging+in+Kafka">claims it should not occur</a> at <code>read_committed</code>:</p>
<blockquote>
<p>Since X2 is committed first, each partition will expose messages from X2 before X1.</p>
</blockquote>
<p>Nevertheless, all three systems exhibited G0 both at <code>read_uncommitted</code> and <code>read_committed</code>. Either the documentation is wrong or Kafka’s transaction isolation is broken. We identified this problem in our 2022 Redpanda analysis, but it remains unaddressed.<a href="#fn8" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
<p>Moreover, our tests show that Kafka, Redpanda, and Bufstream, even at <code>read_committed</code>, allow a different form of G1c proscribed by Read Committed. Specifically, they allow cycles of transactions linked by at least one write-write dependency. For instance, transaction <span><em>T</em><sub>1</sub></span> can write <span><em>a</em></span> to topic-partition <span><em>P</em></span> before <span><em>T</em><sub>2</sub></span> writes <span><em>b</em></span> to <span><em>P</em></span>. Then <span><em>T</em><sub>1</sub></span> can read <span><em>b</em></span>, forming a cycle. Kafka’s documentation still declines to state whether this should be legal.</p>
<p>Finally, a word about producer identifiers. Users can provide a <em>client ID</em>, which is helpful for logging but has no semantic effects. Producers also have an internal <a href="https://strimzi.io/blog/2023/05/03/kafka-transactions/"><em>producer ID</em></a>, which is used to de-duplicate sent records. Third, producers have a user-specified <a href="https://www.confluent.io/blog/transactions-apache-kafka/"><em>transactional ID</em></a>,<a href="#fn9" id="fnref9" role="doc-noteref"><sup>9</sup></a> which identifies a logical producer across multiple client instances. When a logical producer crashes and restarts, it provides its transactional ID to the server, which increments an <em>epoch</em> associated with that ID. Transactional writes from older epochs are rejected.</p>

<p>We tested Bufstream 0.1.0 through 0.1.3, including several release candidate builds. We based <a href="https://github.com/jepsen-io/bufstream/tree/f1706313f171a69e9497743956aabcd131a5b248">our Bufstream test harness</a> on Jepsen’s previous work on <a href="https://jepsen.io/analyses/redpanda-21.10.1">Redpanda and Kafka</a>. We used the <a href="https://github.com/jepsen-io/jepsen">Jepsen testing library</a> and the <a href="https://docs.confluent.io/kafka-clients/java/current/overview.html">Java Kafka Client</a> at version 3.8.0. Since Bufstream implements the Kafka API, we were able to reuse much of the <a href="https://github.com/jepsen-io/redpanda">Redpanda/Kafka</a> test as a library.</p>
<p>We ran our tests on three to five Debian Bookworm nodes, both as LXC containers and EC2 VMs. We reserved one node for etcd, one for <a href="https://min.io/">Minio</a> (an S3-compatible object store), and the remainder for Bufstream agents. Producers, consumers, and admin clients were always initialized with a single node for <code>bootstrap_servers</code>, but we did not interfere with smart client discovery: clients could talk to any node freely. As with all smart clients, this may have reduced our chances to observe safety violations.</p>
<p>All consumers shared a <a href="https://github.com/jepsen-io/redpanda/blob/71becb5d5811396d81ef6dedd22f3e64ea2cdc80/src/jepsen/redpanda/client.clj#L114-L115">single consumer group</a>, and <a href="https://github.com/jepsen-io/redpanda/blob/cf66299d62bb3595c69d558c930419475e7c672d/src/jepsen/redpanda/workload/queue.clj#L765-L768">committed their offsets manually</a> after each non-transactional poll operation. For transactional workloads we gave each producer a <a href="https://github.com/jepsen-io/redpanda/blob/cf66299d62bb3595c69d558c930419475e7c672d/src/jepsen/redpanda/workload/queue.clj#L669-L675">unique transactional ID</a> and used <a href="https://github.com/jepsen-io/redpanda/blob/cf66299d62bb3595c69d558c930419475e7c672d/src/jepsen/redpanda/workload/queue.clj#L339-L348"><code>sendOffsetsToTransaction</code></a> for any transaction which performed a poll—including read-only transactions.</p>
<p>We applied several configuration changes to clients in order to achieve faster recovery during failures, and to ensure safety. Our consumers ran with <a href="https://github.com/jepsen-io/redpanda/blob/cf66299d62bb3595c69d558c930419475e7c672d/src/jepsen/redpanda/client.clj#L83C45-L105C1">significantly shorter timeouts</a> (generally under 10 seconds), and with tunable <code>isolation_level</code>, <code>auto_offset_reset</code>, and <code>enable_auto_commit</code>. Producers also ran with <a href="https://github.com/jepsen-io/redpanda/blob/cf66299d62bb3595c69d558c930419475e7c672d/src/jepsen/redpanda/client.clj#L146-L159">shorter timeouts</a>, and configurable <code>acks</code>, <code>enable_idempotence</code>, and <code>retries</code>. In general we tested with the <a href="https://github.com/jepsen-io/bufstream/blob/f1706313f171a69e9497743956aabcd131a5b248/src/jepsen/bufstream/cli.clj#L327-L337">safest possible settings</a>: auto-commit false, acks <code>all</code>, retries 1,000, idempotence enabled, isolation level <code>read_committed</code>, <code>auto_offset_reset</code> of <code>earliest</code>, and automatic creation of topics on the server disabled. We tested both with and without transactions.</p>
<p>Like most Jepsen tests, we injected the usual suite of faults into Bufstream: process pauses (via <code>SIGSTOP</code>), crashes (via <code>SIGKILL</code>), clock skew (via <code>clock_settime</code>) and partitions (via <code>iptables</code>). Because Bufstream comprises three distinct subsystems, we designed new <a href="https://github.com/jepsen-io/jepsen/blob/ce07779ca3b81feb8553276581faa73d963a95b7/jepsen/src/jepsen/role.clj">subsystem-aware</a> DB automation, client, and fault injection tools for Jepsen. This allowed us to target faults to a particular subsystem, such as just crashing Bufstream nodes or pausing only the etcd coordinator. We mixed these faults together in shifting combinations <a href="https://github.com/jepsen-io/bufstream/blob/f1706313f171a69e9497743956aabcd131a5b248/src/jepsen/bufstream/nemesis.clj#L78-L103">over time</a>, producing, say, 30 seconds of partitions, 30 seconds of no faults, then Bufstream crashes with storage pauses, and so on.</p>
<h2 data-number="2.1" id="queue"> Queue</h2>
<p>In <a href="https://jepsen.io/analyses/redpanda-21.10.1">prior work on Redpanda and Kafka</a> we designed a <a href="https://github.com/jepsen-io/redpanda/blob/cf66299d62bb3595c69d558c930419475e7c672d/src/jepsen/redpanda/workload/queue.clj">queue workload</a> which performed sophisticated safety analysis geared towards Kafka’s data model. That workload is now <a href="https://github.com/jepsen-io/jepsen/blob/ce07779ca3b81feb8553276581faa73d963a95b7/jepsen/src/jepsen/tests/kafka.clj">part of the core Jepsen library</a>, and we updated it for use in Bufstream.</p>
<p>In this workload each logical process launched a producer, a consumer, and an admin client. For concision, we defined a logical numeric <em>key</em> which uniquely identified a specific topic-partition. Topics were created dynamically on first use. Keys were selected with exponential frequency: some keys accessed quite often, while others only infrequently. After sending a configurable number of records to a key, we abandoned it and moved on to a new one.</p>
<p>The queue workload performed three basic kinds of operations. <a href="https://github.com/jepsen-io/redpanda/blob/cf66299d62bb3595c69d558c930419475e7c672d/src/jepsen/redpanda/workload/queue.clj#L697">The first</a>, <code>crash</code>, simulated a client failure: it terminated the logical process, closing all three clients. Jepsen would then create a fresh process with new clients to take its place. The <a href="https://github.com/jepsen-io/redpanda/blob/cf66299d62bb3595c69d558c930419475e7c672d/src/jepsen/redpanda/workload/queue.clj#L689-L694">second class of operations</a>, <code>subscribe</code> or <code>assign</code>, updated the set of topics or partitions the consumer received records from when calling <code>poll</code>: either assigning a specific set of keys (topic-partitions), or subscribing to the set of topics which covered the requested keys.</p>
<p>The <a href="https://github.com/jepsen-io/redpanda/blob/cf66299d62bb3595c69d558c930419475e7c672d/src/jepsen/redpanda/workload/queue.clj#L735-L779">third class</a> included <code>txn</code>, <code>poll</code>, and <code>send</code> operations. Each contained a sequence of <code>poll</code> or <code>send</code> micro-operations. Those which performed only polls or sends were labeled <code>poll</code> or <code>send</code>, rather than <code>txn</code>, but their structure was otherwise identical. Each <code>send</code> micro-operation sent a single <em>value</em> (a unique integer) to a specific key, and returned an <code>[offset, value]</code> pair, based on the offset Bufstream returned. Each <code>poll</code> micro-operation called <code>consumer.poll</code> once, and returned a map of keys to sequences of <code>[offset, value]</code> pairs observed for that key. For example:</p>
<div id="cb1"><pre><code><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a>[[<span>:poll</span> {<span>1</span> [[<span>2</span> <span>3</span>] [<span>4</span> <span>5</span>]]}]]</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a> [<span>:send</span> <span>6</span> [<span>7</span> <span>8</span>]]</span></code></pre></div>
<p>This transaction polled key <code>1</code> and received two records back: at offset <code>2</code>, value <code>3</code>; and at offset <code>4</code>, value <code>5</code>. Then it sent a single record <code>8</code> to key <code>6</code>, which was assigned offset <code>7</code>.</p>
<p>For non-transactional workloads we constrained every <code>send</code> or <code>poll</code> operation to contain exactly one micro-operation. For transactional workloads, we allowed multiple micro-operations and wrapped them all in a <a href="https://github.com/jepsen-io/redpanda/blob/cf66299d62bb3595c69d558c930419475e7c672d/src/jepsen/redpanda/workload/queue.clj#L434-L470">Kafka transaction</a>.</p>
<p>To analyze histories of these operations we first constructed, for each key, a mapping of offsets to sets of values observed at that offset, either via <code>send</code> or <code>poll</code>. If we observed multiple values at a single offset, we called it an <a href="https://github.com/jepsen-io/jepsen/blob/ce07779ca3b81feb8553276581faa73d963a95b7/jepsen/src/jepsen/tests/kafka.clj#L907-L998">inconsistent offset</a>. Since every value was unique within a key, we also expected to observe each value at most once. If we observed the same value at multiple offsets, we called that a <a href="https://github.com/jepsen-io/jepsen/blob/ce07779ca3b81feb8553276581faa73d963a95b7/jepsen/src/jepsen/tests/kafka.clj#L1454-L1479">duplicate error</a>.</p>
<p>If our mapping between a key’s values and offsets was bijective, we could construct a total order over values. This order might not cover all values: calls to <code>send</code> and <code>poll</code> might not have returned offsets. Nor could we necessarily tell which offset an indeterminate, unobserved <code>send</code> might have produced. Moreover, not every offset contained a value: like Kafka, Bufstream uses some log offsets to store transaction metadata. We therefore <a href="https://github.com/jepsen-io/jepsen/blob/ce07779ca3b81feb8553276581faa73d963a95b7/jepsen/src/jepsen/tests/kafka.clj#L993-L998">collapsed our sparse offset logs</a> into a dense version order which mapped each value to a unique <em>index</em> 0, 1, 2, ….</p>
<p>From this version order we looked for <a href="https://github.com/jepsen-io/jepsen/blob/ce07779ca3b81feb8553276581faa73d963a95b7/jepsen/src/jepsen/tests/kafka.clj#L2226-L2338">several additional errors</a>, which came in symmetric flavors. We checked subsequent pairs of send micro-operations, and subsequent pairs of polls as well, to see if the offsets for their values were strictly monotonic and did not skip over intermediate indices. We looked for non-monotonic or skipped offsets both within a single transaction and between successive transactions by the same process.</p>
<p>For aborted reads, we <a href="https://github.com/jepsen-io/jepsen/blob/ce07779ca3b81feb8553276581faa73d963a95b7/jepsen/src/jepsen/tests/kafka.clj#L1005-L1017">searched for any poll which returned a value sent by a failed operation</a>. We verified that transactions never observed their own writes, even if they later committed: we called this phenomenon <em>pre-committed read</em>.</p>
<p>When Bufstream confirmed receipt of a record but that record was never observed, we called that record <em>lost</em> or <em>unseen</em>. Lost records were those where some poller <a href="https://github.com/jepsen-io/jepsen/blob/ce07779ca3b81feb8553276581faa73d963a95b7/jepsen/src/jepsen/tests/kafka.clj#L1077-L1142">observed a higher offset</a>. Since consumers start at a known-consumed offset and proceed linearly, it should be impossible to poll index <span><em>n</em></span> unless some poller has already seen index <span><em>n</em> − 1</span>, and by induction, all lower indices.<a href="#fn10" id="fnref10" role="doc-noteref"><sup>10</sup></a></p>
<p>Typically, a small tail of the log has just been written but has not yet been polled. After the main body of the test, we ceased transactions, resolved all faults, waited for recovery, then began a <em>final reads</em> phase. Each process assigned its consumer to every topic-partition, rewound to offset 0, and polled until it reached the highest offset known to have been written to that partition. If our final read phase timed out, leaving some acknowledged records unobserved, we called those records <em>unseen</em>.</p>
<h2 data-number="2.2" id="abort"> Abort</h2>
<p>Following unexpected results from the queue workload, we designed an <a href="https://github.com/jepsen-io/redpanda/blob/cf66299d62bb3595c69d558c930419475e7c672d/src/jepsen/redpanda/workload/abort.clj">abort workload</a> which aborted transactions and kept track of which offsets are polled. We limited each topic to a single partition, process, producer, and consumer. Each process would create a new topic, subscribe to it, then perform a series of transactions against that topic, each involving a single poll and a variable number of sends. Once one of those transactions had polled some records, the process shifted to intentionally aborting transactions. After some time, the process returned to committing transactions.</p>
<p>We examined the offsets returned by polls after an aborted transaction, and classified them into four categories. An <em>advance</em> meant that the next transaction began its polls at an offset higher than the last one polled in the previous, aborted transaction. A <em>rewind</em> meant that the next transaction started polling at the same offset the aborted transaction started at. A <em>rewind-further</em> meant that it started at some earlier offset. Any other behavior we called <em>other</em>.</p>

<p>We begin with five issues in Bufstream proper, from liveness failures to duplicate offsets and data loss.</p>
<h2 data-number="3.1" id="stuck-consumers-1"> Stuck Consumers (#1)</h2>
<p>In 0.1.0 through 0.1.3-rc.8 our final read phase often stalled. Calls to <code>consumer.poll()</code> would return immediately with no records, even though thousands of acknowledged records remained in the log. This would continue for tens of seconds to over an hour, until either the test gave up waiting or Bufstream decided to deliver records to consumers again. This weakened our tests: those unseen records may have had safety issues, but without observing them we had no way to tell.</p>
<p>For instance, this <a href="https://s3.amazonaws.com/jepsen.io/analyses/bufstream-0.1.0/0.1.3-rc2-unseen.zip">test run</a> sent 691 acknowledged records in the first 120 seconds, then shifted to final reads. At that time, 40 of those acknowledged writes had never been observed by any poller. This situation persisted for over an hour as calls to <code>consumer.poll()</code> repeatedly returned no results. Finally, the test timed out.</p>
<p><img src="https://jepsen.io/analyses/bufstream-0.1.0/unseen.png" alt="A timeseries plot of unseen records over time, broken down by key. Three keys have a steady count of 40 unseen records that last until approximately 3788 seconds."/><br/>
</p>
<p>In other cases consumers would get stuck waiting for thousands of unseen records. Then after hundreds of seconds—for no apparent reason—Bufstream would rapidly produce the remaining values. We saw this behavior in all kinds of situations, including healthy clusters, but it was most pronounced with faults.</p>
<p>Bufstream made two patches in 0.1.3-rc.6 to help with this issue. First, when restarted, a Bufstream node could sometimes return stale, cached values for the last stable offset and high watermark.<a href="#fn11" id="fnref11" role="doc-noteref"><sup>11</sup></a> This caused some client libraries to stall, assuming no later records were available (#1). Refreshing the cache on startup resolved this issue. We discuss the second patch (#3) later in this report, as it also had safety consequences.</p>
<h2 data-number="3.2" id="stuck-producers-consumers-2"> Stuck Producers &amp; Consumers (#2)</h2>
<p>Unfortunately we continued to see regular issues with unseen writes on 0.1.3-rc.6, in response to pauses, crashes, or partitions affecting the coordinator, storage, or Bufstream nodes. In some cases, a coordinator pause could cause every Bufstream node to enter a state where the process was running, but clients <a href="https://s3.amazonaws.com/jepsen.io/analyses/bufstream-0.1.0/0.1.3-rc6-hung.zip">would time out</a> waiting for calls to <code>InitProducerId</code>. In other cases calls to <code>listOffsets</code> <a href="https://s3.amazonaws.com/jepsen.io/analyses/bufstream-0.1.0/0.1.3-rc6-hung-2.zip">would fail</a> with messages like <code>node 2008741112 being disconnected</code> or, <a href="https://s3.amazonaws.com/jepsen.io/analyses/bufstream-0.1.0/0.1.3-rc6-hung-3.zip">alternatively</a> because they <code>timed out waiting for a node assignment</code>. Calls to <code>poll</code> would complete but return no results. Killing and restarting Bufstream nodes resolved these issues. We call these metastable failures: a brief interruption in (e.g.) connectivity to the coordinator could cause long-lasting partial (or total!) unavailability in Bufstream agents.</p>
<p>Bufstream uses <a href="https://etcd.io/docs/v3.4/learning/api/#lease-api">etcd leases</a> to keep track of active Bufstream agents. Each agent subscribed, via its etcd client, to updates affecting a set of leased keys. Despite setting long timeouts, brief pauses or partitions caused etcd to delete keys tied to an agent’s lease, but updates reflecting those deletions were not necessarily relayed by the client to the agent itself. In essence, an agent would be unaware that it had lost its lease. The Bufstream team added additional polling logic to work around this problem, and unseen writes were largely resolved by 0.1.3-rc.8.</p>
<h2 data-number="3.3" id="spurious-zero-offsets-3"> Spurious Zero Offsets (#3)</h2>
<p>In versions 0.1.0 through 0.1.3-rc.2, a sent value could be assigned offset <code>0</code> (even if offset <code>0</code> had already been assigned far earlier in the test), then appear at a higher, more reasonable offset. Only the sender observed offset zero; pollers always observed the higher offset. This occurred when either etcd or Bufstream paused, or crashed, or a network partition occurred between the two.</p>
<p>Consider <a href="https://s3.amazonaws.com/jepsen.io/analyses/bufstream-0.1.0/0.1.0-inconsistent.zip">this two-minute test run</a> with a single Bufstream node, wherein we induced brief pauses in the etcd process. Six writes were assigned offset <code>0</code>, then appeared at a second, higher offset. On key <code>6</code>, value <code>26</code> was assigned offset <code>0</code> on send, then appeared at offset <code>25</code> in polls. On key <code>9</code>, <code>224</code> was assigned offset <code>0</code>, then appeared at <code>223</code>, and so on. Our checker reported these as duplicates:</p>
<div id="cb2"><pre><code><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a><span>:duplicate</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true"></a>{<span>:count</span> <span>6</span>,</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true"></a> <span>:errs</span> [{<span>:key</span> <span>6</span>,</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true"></a>         <span>:value</span> <span>26</span>,</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true"></a>         <span>:count</span> <span>2</span>,</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true"></a>         <span>:offsets</span> [<span>0</span> <span>25</span>]},</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true"></a>        {<span>:key</span> <span>9</span>,</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true"></a>         <span>:value</span> <span>224</span>,</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true"></a>         <span>:count</span> <span>2</span>,</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true"></a>         <span>:offsets</span> [<span>0</span> <span>223</span>]},</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true"></a>        {<span>:key</span> <span>9</span>,</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true"></a>         <span>:value</span> <span>69</span>,</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true"></a>         <span>:count</span> <span>2</span>,</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true"></a>         <span>:offsets</span> [<span>0</span> <span>66</span>]},</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true"></a>       ...]}</span></code></pre></div>
<p>Key <code>9</code> actually had five values at offset <code>0</code>: value <code>1</code> (which was stable), <code>67</code>, <code>68</code>, <code>69</code>, and <code>224</code>. This caused our checker to report inconsistent offsets:</p>
<div id="cb3"><pre><code><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a><span>:inconsistent-offsets</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true"></a>{<span>:count</span> <span>3</span>,</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true"></a> <span>:errs</span> [{<span>:key</span> <span>6</span>,</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true"></a>        <span>:offset</span> <span>0</span>,</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true"></a>        <span>:values</span> #{<span>1</span> <span>26</span>}},</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true"></a>       {<span>:key</span> <span>8</span>,</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true"></a>        <span>:offset</span> <span>0</span>,</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true"></a>        <span>:values</span> #{<span>1</span> <span>110</span>}},</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true"></a>       {<span>:key</span> <span>9</span>,</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true"></a>        <span>:offset</span> <span>0</span>,</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true"></a>        <span>:values</span> #{<span>1</span> <span>67</span> <span>68</span> <span>69</span> <span>224</span>}}]}</span></code></pre></div>
<p>We could reproduce this behavior readily—it occurred roughly every five minutes. However, Bufstream initially struggled to reproduce it.</p>
<p>This issue was caused by Bufstream omitting a field from the error responses sent to clients. Imagine that Bufstream sent a message to etcd to commit a new record in the log, and etcd processed that request. However, due to a process pause or network partition, Bufstream might time out waiting for a response from etcd, and send an error back to the client. While Bufstream’s response included an error code, it did <em>not</em> set the offset for the sent record to the special value <code>-1</code>, which some clients relied on as a signal of an error. Consequently, the official Java client we used in our tests interpreted this error as a successful response with offset <code>0</code>. Bufstream’s test suite used <a href="https://github.com/twmb/franz-go">Franz-go</a>, which interpreted these messages as errors. This meant that Bufstream’s test suite did not encounter this issue.</p>
<p>Bufstream fixed this issue in version 0.1.3-rc.6, and we have not observed it since.</p>
<h2 data-number="3.4" id="lost-transaction-writes-4"> Lost Transaction Writes (#4)</h2>
<p>Version 0.1.2 exhibited frequent write loss. Records written as a part of a committed transaction could vanish, never to be seen again. Readers would simply skip over those records as if they had never existed. For example, consider <a href="https://s3.amazonaws.com/jepsen.io/analyses/bufstream-0.1.0/0.1.2-lost-write.zip">this test run</a>. In just 100 seconds and 6,761 write transactions, 240 records written by committed transactions were lost. Key <code>5</code>, for instance, had a successful write of value <code>141</code>:</p>
<div id="cb4"><pre><code><span id="cb4-1"><a href="#cb4-1" aria-hidden="true"></a>{<span>:type</span> <span>:ok</span>,</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true"></a> <span>:process</span> <span>7</span>,</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true"></a> <span>:f</span> <span>:send</span>,</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true"></a> <span>:value</span> [[<span>:send</span> <span>11</span> [<span>83</span> <span>48</span>]]</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true"></a>         [<span>:send</span> <span>5</span> [<span>274</span> <span>141</span>]]]}</span></code></pre></div>
<p>Since this transaction committed successfully, we know that key <code>5</code> should have stored value <code>141</code> at offset <code>274</code>. However, every call to <code>consumer.poll()</code> would skip over that offset, returning values like:</p>
<div id="cb5"><pre><code><span id="cb5-1"><a href="#cb5-1" aria-hidden="true"></a>[[<span>272</span> <span>140</span>]</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true"></a> [<span>273</span> <span>142</span>]</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true"></a> <span>; Missing [274, 141]</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true"></a> [<span>277</span> <span>144</span>]</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true"></a> [<span>278</span> <span>145</span>]</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true"></a> [<span>282</span> <span>146</span>]</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true"></a> ...]</span></code></pre></div>
<p>This behavior happened regularly in healthy clusters, even with just a single Bufstream node. It was caused by a new concurrency safety mechanism added in 0.1.2 to mitigate the lack of idempotence in Kafka’s transaction protocol. Bufstream nodes assigned a unique number to each transaction performed by a producer within a given epoch, allowing Bufstream to safely retry some internal operations. However, a bug in the logic for tracking transaction numbers caused some transaction commits to be erroneously ignored when multiple transactions were committed across multiple epochs. This caused transactions which appeared to commit to actually abort, or vice versa.</p>
<p>Bufstream’s internal integration and unit tests initially missed this bug—we only caught it with the Jepsen test suite because of our choice of an unusually low (one second) transaction timeout. Luckily we identified the problem within a few hours of 0.1.2’s release. Bufstream took action to prevent customers from upgrading to 0.1.2, and none did. The issue was fixed in 0.1.3-rc2.</p>
<h2 data-number="3.5" id="lost-writes-due-to-server-side-filtering-5"> Lost Writes Due to Server-Side Filtering (#5)</h2>
<p>In version 0.1.3-rc.8, we regularly found short windows of write loss in response to minor faults, like pausing a Bufstream process or the coordinator, or a partition between the two. Data loss occurred both with and without transactions. Take <a href="https://s3.amazonaws.com/jepsen.io/analyses/bufstream-0.1.0/0.1.3-rc.8-write-loss.zip">this five-minute test run</a> in which 22 out of 16,770 records were acknowledged, but never polled by any consumer. On key 12, value 663 was written at offset 662:</p>
<div id="cb6"><pre><code><span id="cb6-1"><a href="#cb6-1" aria-hidden="true"></a>{<span>:type</span> <span>:ok</span>,</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true"></a> <span>:process</span> <span>38</span>,</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true"></a> <span>:f</span> <span>:send</span>,</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true"></a> <span>:value</span> [[<span>:send</span> <span>12</span> [<span>662</span> <span>663</span>]]]}</span></code></pre></div>
<p>However, every poller skipped over value 663 (and 664, which was also acknowledged). They read 665, then missed 666, 667, and so on:</p>
<div id="cb7"><pre><code><span id="cb7-1"><a href="#cb7-1" aria-hidden="true"></a>{<span>:type</span> <span>:ok</span>,</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true"></a> <span>:process</span> <span>143</span>,</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true"></a> <span>:f</span> <span>:poll</span>,</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true"></a> <span>:value</span> [[<span>:poll</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true"></a>          {...</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true"></a>           <span>12</span> [... [<span>660</span> <span>661</span>]</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true"></a>                   [<span>661</span> <span>662</span>]</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true"></a>                   <span>; No 663</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true"></a>                   <span>; No 664</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true"></a>                   [<span>664</span> <span>665</span>]</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true"></a>                   <span>; 6 missing records</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true"></a>                   [<span>671</span> <span>672</span>]</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true"></a>                   <span>; 5 more missing</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true"></a>                   [<span>677</span> <span>678</span>]</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true"></a>                   ...]}]]}</span></code></pre></div>
<p>Sometimes records would be visible to pollers for a time, then missing from later polls. In <a href="https://s3.amazonaws.com/jepsen.io/analyses/bufstream-0.1.0/0.1.3-rc.8-int-poll-skip.zip">this test run</a> with process crashes, Bufstream acknowledged the writes of values <code>101</code> through <code>106</code> at roughly 1.37 seconds into the test. These records were visible to pollers until 1.88 seconds. After that, pollers simply skipped over the records as if they had never existed.</p>
<p>To work around a bug in a popular Kafka web GUI, Bufstream introduced logic in 0.1.3-rc.8 to more strictly limit the size of responses to the fetch API. However, a bug in that filtering logic caused Bufstream to hide records from some lagging consumers—which manifested as write loss. Bufstream fixed this issue in 0.1.3-rc.12.</p>

<p>In the course of our research we uncovered several issues with the Kafka Java client, documentation, and protocol design. We present four of these issues here. These affect Kafka, Bufstream, and presumably any Kafka-compatible system.</p>
<h2 data-number="4.1" id="a-misleading-error-message-kip-588"> A Misleading Error Message (KIP-588)</h2>
<p>During our testing we encountered frequent errors like <code>ProducerFencedException: There is a newer producer with the same transactionalId which fences the current one.</code> This was particularly vexing in tests where every producer received a unique transactional ID. We spent a good deal of time verifying that producers were initialized at most once, that data from past runs was not leaking into the present, and so on. Finally, the Bufstream team identified <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-588%3A+Allow+producers+to+recover+gracefully+from+transaction+timeouts">KIP-588</a>, which notes that a ProducerFencedException is <em>also</em> thrown for a transaction timeout:</p>
<blockquote>
<p>When the producer goes back online and attempts to proceed, it will receive the exact <code>ProducerFenced</code> even though a conflicting producer doesn’t exist.</p>
</blockquote>
<p>Kafka’s Java client uses a dedicated <code>TimeoutException</code> for most timeouts, but throws <code>ProducerFencedException</code> for this particular kind of timeout instead. When this happens, the error message lies to the user, asserting a second instance of the producer exists when none actually does. KIP-588 has been open for two years; we recommend the Kafka team change this error message.</p>
<h2 data-number="4.2" id="closing-a-consumer-can-block-indefinitely-kafka-17734"> Closing a Consumer Can Block Indefinitely (KAFKA-17734)</h2>
<p>Our tests against both Bufstream and Kafka got stuck every few hours thanks to a bug in the Java client. Calls to <code>Consumer.close()</code> block on network IO by default. There is a timeout parameter which is supposed to prevent calls to <code>close()</code> from blocking indefinitely, but it doesn’t work. Neither does spawning a separate thread specifically to call <code>consumer.wakeup()</code>, which is intended to safely interrupt a consumer stuck in IO.</p>
<p>Long-running programs should be robust to network errors. This means they should be able to reliably tear down clients and their associated resources—connections, threads, allocated memory, and so on—in a reasonable amount of time. We filed <a href="https://issues.apache.org/jira/browse/KAFKA-17734">KAFKA-17734</a> to track this issue.</p>
<h2 data-number="4.3" id="unpredictable-consumer-offsets-after-transaction-failure-kafka-17582"> Unpredictable Consumer Offsets After Transaction Failure (KAFKA-17582)</h2>
<p>Kafka’s <a href="https://kafka.apache.org/documentation/">official</a> <a href="https://kafka.apache.org/38/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html">documentation</a> is largely silent about the intended behavior for consumer offsets when a transaction fails to commit. Should the consumer rewind, such that the next transaction’s polls begin with the first records observed by the aborted transaction? Or should it continue advancing, polling subsequent records? The original transaction proposal, <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=66854913#KIP98ExactlyOnceDeliveryandTransactionalMessaging-TransactionalGuarantees">KIP-98</a>, says:</p>
<blockquote>
<p>Further, since consumer progress is recorded as a write to the offsets topic, the above capability is leveraged to enable applications to batch consumed and produced messages into a single atomic unit, ie. a set of messages may be considered consumed only if the entire ‘consume-transform-produce’ executed in its entirety.</p>
</blockquote>
<p>Confluent’s <a href="https://web.archive.org/web/20240908235227/docs.confluent.io/kafka/design/delivery-semantics.html">Kafka design documentation</a> goes on to say:</p>
<blockquote>
<p>If the transaction is aborted, the consumer’s position reverts to its old value and you can specify whether output topics are visible to other consumers using the <code>isolation_level</code> property.</p>
</blockquote>
<p>It makes sense that consumers should rewind on abort. After all, aborting a transaction in any system generally undoes its effects. More critically, Kafka users typically want at-least-once delivery—advancing to later offsets could mark records from the aborted transaction as committed even though they had never been processed. In fact the official Java client does rewind, but only <em>sometimes</em>.</p>
<p>We first encountered this behavior in the queue workload, where it manifested <a href="https://s3.amazonaws.com/jepsen.io/analyses/bufstream-0.1.0/kafka-subscribe-rollback.zip">as write loss</a> during a variety of faults, both with Bufstream and Kafka. We designed the abort workload to follow up and found that even in healthy clusters, aborts led to unpredictable outcomes. For instance, here are results from <a href="https://s3.amazonaws.com/jepsen.io/analyses/bufstream-0.1.0/kafka-abort.zip">a five-minute abort test</a> with no fault injection. Most pairs of transactions advanced to later offsets, but some rewound to earlier ones. Some rewound to the start of the transaction, and others rewound even further. All rewinds were associated with a rebalance event, and all advances had no rebalances.</p>
<div id="cb8"><pre><code><span id="cb8-1"><a href="#cb8-1" aria-hidden="true"></a>{[<span>:advance</span> <span>:none</span>]             <span>17048</span>,</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true"></a> [<span>:rewind-further</span> <span>:rebalance</span>] <span>1745</span>,</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true"></a> [<span>:rewind</span> <span>:rebalance</span>]         <span>476</span>}</span></code></pre></div>
<p>For example, process 0, interacting with topic-partition <code>t374</code>, aborted a transaction which polled records <code>12</code> through <code>14</code>. Then it went on to poll and commit later records. Offsets 12 through 17 were effectively lost.<a href="#fn12" id="fnref12" role="doc-noteref"><sup>12</sup></a></p>
<div id="cb9"><pre><code><span id="cb9-1"><a href="#cb9-1" aria-hidden="true"></a>{<span>:process</span> <span>0</span>,</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true"></a> <span>:type</span>    <span>:fail</span>,</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true"></a> <span>:f</span>       <span>:poll</span>,</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true"></a> <span>:value</span>   {<span>:topic</span>   <span>&#34;t374&#34;</span>,</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true"></a>           <span>:offsets</span> [<span>12</span> <span>14</span> <span>15</span> <span>17</span>],</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true"></a>           <span>:abort</span>?  <span>true</span>}}</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true"></a>{<span>:process</span> <span>0</span>,</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true"></a> <span>:type</span>    <span>:ok</span>,</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true"></a> <span>:f</span>       <span>:poll</span>,</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true"></a> <span>:value</span>   {<span>:topic</span>   <span>&#34;t374&#34;</span>,</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true"></a>           <span>:offsets</span> [<span>18</span> <span>19</span> <span>21</span> <span>23</span>]}}</span></code></pre></div>
<p>On the other hand, if a rebalance occurred consumers could rewind to earlier offsets, preventing data loss. For example, process 15, consuming from topic-partition <code>t1208</code>, was reassigned to that same topic. That rebalance event reset the consumer’s position from offset 5 to offset 0, causing it to rewind further than the most recent aborted transaction:</p>
<div id="cb10"><pre><code><span id="cb10-1"><a href="#cb10-1" aria-hidden="true"></a>{<span>:process</span> <span>15</span>,</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true"></a> <span>:type</span>    <span>:fail</span>,</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true"></a> <span>:f</span>       <span>:poll</span>,</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true"></a> <span>:value</span>   {<span>:topic</span> <span>&#34;t1208&#34;</span>,</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true"></a>           <span>:abort</span>? <span>true</span>,</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true"></a>           <span>:offsets</span> [<span>4</span>]}}</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true"></a>{<span>:process</span> <span>15</span>,</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true"></a> <span>:type</span>    <span>:ok</span>,</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true"></a> <span>:f</span>       <span>:poll</span>,</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true"></a> <span>:value</span>   {<span>:topic</span> <span>&#34;t1208&#34;</span>,</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true"></a>           <span>:abort</span>? <span>true</span>,</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true"></a>           <span>:offsets</span> [<span>0</span> <span>1</span> <span>2</span> <span>4</span>]},</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true"></a> <span>:rebalance-log</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true"></a> {<span>:during</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true"></a>   [{<span>:type</span> <span>:assigned</span>,</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true"></a>     <span>:partitions</span> [{<span>:topic</span> <span>&#34;t1208&#34;</span>,</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true"></a>                   <span>:partition</span> <span>0</span>}]}],</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true"></a>   <span>:before</span> []}}</span></code></pre></div>
<p>We opened <a href="https://issues.apache.org/jira/browse/KAFKA-17582">KAFKA-17582</a> to ask for clarification, and learned that this behavior is intentional. Consumers continue advancing, unless they happen to be rebalanced, in which case they might rewind to an arbitrary point—whatever happens to be committed. Users are supposed to manually rewind the consumer’s position on transaction abort. Indeed, one of Kafka’s demonstration programs <a href="https://github.com/apache/kafka/blob/2.5/examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java#L132">includes a rewind method</a> for exactly this reason.</p>
<p>This directly contradicts Confluent’s documentation. It also runs contrary to KIP-98’s statement that “a set of messages may be considered consumed only if the entire ‘consume-transform-produce’ executed in its entirety.” KIP-98’s example code contains no rewind, and we could not locate any documentation guiding users to rewind by hand. We suggested that Kafka document this behavior, and consider changing consumers to rewind by default on transaction abort. We also updated our queue workload to explicitly rewind consumers.</p>
<h2 data-number="4.4" id="write-loss-aborted-reads-torn-transactions-kafka-17754"> Write Loss, Aborted Reads, Torn Transactions (KAFKA-17754)</h2>
<p>In Bufstream 0.1.0 through 0.1.3 we observed aborted read, lost writes, and atomicity violations with as little as pausing or crashing the Bufstream process, or the coordinator, or a network partition. These behaviors led us to a fundamental flaw in the Kafka transaction protocol. For example, take <a href="https://s3.amazonaws.com/jepsen.io/analyses/bufstream-0.1.0/0.1.3-rc.9-g1a-2.zip">this queue test of version 0.1.3-rc.9</a>. The test harness executed the following transaction, but aborted it intentionally:</p>
<div id="cb11"><pre><code><span id="cb11-1"><a href="#cb11-1" aria-hidden="true"></a>{<span>:type</span>    <span>:fail</span>,</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true"></a> <span>:process</span> <span>76</span>,</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true"></a> <span>:f</span>       <span>:txn</span>,</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true"></a> <span>:value</span>   [[<span>:send</span> <span>5</span> [<span>653</span> <span>424</span>]]</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true"></a>           [<span>:send</span> <span>17</span> [<span>1360</span> <span>926</span>]]</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true"></a>           [<span>:poll</span> {}]</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true"></a>           [<span>:send</span> <span>17</span> [<span>1382</span> <span>927</span>]]],</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true"></a> <span>:error</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true"></a> {<span>:type</span>          <span>:abort</span>,</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true"></a>  <span>:abort-ok</span>?     <span>true</span>,</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true"></a>  <span>:tried-commit</span>? <span>false</span>,</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true"></a>  <span>:definite</span>?     <span>true</span>,</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true"></a>  <span>:body-error</span>    {<span>:type</span> <span>:intentional-abort</span>}}}</span></code></pre></div>
<p>Process 76 selected the unique transactional ID <code>jt1234</code> on initialization. From packet captures and Bufstream debug logs, we see <code>jt1234</code> used producer ID <code>233</code>, submitted all four operations, then sent an <a href="https://kafka.apache.org/protocol#The_Messages_EndTxn"><code>EndTxn</code></a> request with <code>committed = false</code>, which denotes a transaction abort. However, fifteen separate calls to <code>poll()</code> observed this transaction’s write of <code>424</code> to key <code>5</code>—a clear case of aborted read. Even stranger, <em>no</em> poller observed the other writes from this transaction: key <code>17</code> apparently never received values <code>926</code> or <code>927</code>. Why?</p>
<p>Close inspection of the packet capture, combined with Bufstream’s logs, allowed us to reconstruct what happened. A few transactions prior, process 76 began a transaction which sent <code>1018</code> to key <code>15</code>. It sent an <code>EndTxn</code> message to commit that transaction to node <code>n3</code>. However, it did not receive a prompt response. The client then quietly sent a <em>second</em> commit message to <code>n4</code>, which returned OK, and the test harness’s call to <code>commitTransaction</code> completed successfully. The process then began and intentionally aborted a second transaction, which completed OK. So far, so good.</p>

<p>Then process 76 began a third, problematic transaction. It sent <code>424</code> to key <code>5</code> and added new partitions to the transaction. Just after accepting record <code>424</code>, node <code>n3</code> received the delayed commit message from two transactions ago. This committed the current transaction, effectively tearing it in half. The first half (sending <code>424</code> to key <code>5</code>) was committed and visible to pollers. The second half (sending <code>926</code> and <code>927</code> to key <code>17</code>) implicitly began a second transaction, which was then aborted by the client.</p>
<p>This suggests a fundamental problem in the Kafka transaction protocol. <a href="https://kafka.apache.org/protocol">The protocol is designed</a> to allow clients to submit requests over multiple TCP connections and to distribute them across multiple nodes. There is no sequence number to order requests from the same client. There is no concept of a transaction number.<a href="#fn13" id="fnref13" role="doc-noteref"><sup>13</sup></a> When a server receives a commit (or abort) message, it has no way to know what transaction the client intended to commit. It simply commits (or aborts) whatever transaction happens to be in progress.</p>
<p>This means transactions which appeared to commit could actually abort, and vice versa: we observed both aborted reads and lost writes. It also means transactions could be cut in half: a single transaction could have some of its writes lost, and others preserved. We don’t know a name for this anomaly. It’s clearly a violation of atomicity, but “atomic” is a somewhat vague term. If a reader observed some but not all of a different transaction’s writes we would call it a <em>fractured read</em>, but this anomaly occurs on the write path: no (<code>read_committed</code>) poller will ever observe the lost writes. We call this behavior a <em>torn transaction</em>.<a href="#fn14" id="fnref14" role="doc-noteref"><sup>14</sup></a></p>
<p>What does it take to get this behavior? First, an <code>EndTxn</code> message must be delayed, for instance due to network latency, packet loss, a slow computer, garbage collection, etc. Second, while that <code>EndTxn</code> arrow is hovering in the air, the client needs to move on to perform a second transaction using the same producer ID and epoch. There are several ways this could happen.</p>
<p>First, users could explicitly retry committing or aborting a transaction. The docs say they can, and the client won’t stop them. Second, the official Kafka Java client docs <a href="https://kafka.apache.org/38/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html">repeatedly instruct users</a> to call <code>abortTransaction</code> if an error occurs during <code>commitTransaction</code>.<a href="#fn15" id="fnref15" role="doc-noteref"><sup>15</sup></a> Following the documentation’s example leads directly to this behavior: if <code>commitTransaction</code> times out, one calls <code>abortTransaction</code>, and there are now multiple <code>EndTxn</code> messages in flight. Third, even if users try to avoid this by only calling <code>commitTransaction</code> <em>or</em> <code>abortTransaction</code>, the client’s internal retry mechanism <a href="https://github.com/apache/kafka/blob/8125c3da5bb6ebb35a0cb3494624d33fad4e3187/clients/src/main/java/org/apache/kafka/common/errors/TimeoutException.java#L22">treats timeouts as retryable</a> and sends multiple <code>EndTxn</code> messages automatically. In the above example, process 76 called commit or abort exactly once for every transaction it ever performed, and it still hit data loss.</p>
<p>We observed aborted reads and torn transactions due to process pauses <a href="https://s3.amazonaws.com/jepsen.io/analyses/bufstream-0.1.0/kafka-g1a.zip">in Kafka as well</a>, and opened <a href="https://issues.apache.org/jira/browse/KAFKA-17754">KAFKA-17754</a> to track the issue. Kafka’s engineers believe <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-890%3A+Transactions+Server-Side+Defense">KIP-890</a>, which was motivated by hanging transactions in Kafka, will likely fix the problem. In brief, KIP-890 revises the transaction protocol to bump the producer’s epoch on every transaction. Since servers reject messages from older epochs, this should prevent commit messages from prior transactions leaking into later ones. KIP-890 <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=235834634">was opened in November 2022</a>, and <a href="https://issues.apache.org/jira/browse/KAFKA-14402">work is ongoing</a>.</p>
<p>Bufstream has added a mechanism in 0.1.3 to reduce the frequency of these issues. Bufstream nodes now use the most recently observed etcd revision—a global, monotonically increasing integer coupled to the system state as a whole—as a logical clock on <code>AddPartitionsToTxn</code> and <code>EndTxn</code> messages. If an <code>EndTxn</code> message attempts to commit or abort a transaction with a newer <code>AddPartitionsToTxn</code>, Bufstream returns a non-retryable error. Of course this order is only known once RPC messages arrive on Bufstream nodes; it does not prevent reorderings that occur between the client and Bufstream itself. Indeed, we continue to observe aborted read, lost writes, and torn transactions in 0.1.3. We must wait for clients to resolve this issue.</p>
<table>
<thead>
<tr>
<th>№</th>
<th>Summary</th>
<th>Event Required</th>
<th>Fixed in</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Stuck consumers due to lagging highest stable offset</td>
<td>None</td>
<td>0.1.3-rc.6</td>
</tr>
<tr>
<td>2</td>
<td>Stuck producers/consumers due to etcd lease expiry</td>
<td>Pause</td>
<td>0.1.3-rc.8</td>
</tr>
<tr>
<td>3</td>
<td>Spurious zero offsets</td>
<td>Pause</td>
<td>0.1.3-rc.6</td>
</tr>
<tr>
<td>4</td>
<td>Lost transaction writes</td>
<td>None</td>
<td>0.1.3-rc.2</td>
</tr>
<tr>
<td>5</td>
<td>Lost writes due to server-side filtering</td>
<td>Pause</td>
<td>0.1.3-rc.12</td>
</tr>
<tr>
<td><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-588%3A+Allow+producers+to+recover+gracefully+from+transaction+timeouts">KIP-588</a></td>
<td>Wrong error message on transaction timeout</td>
<td>None</td>
<td>Unresolved</td>
</tr>
<tr>
<td><a href="https://issues.apache.org/jira/browse/KAFKA-17734">KAFKA-17734</a></td>
<td><code>ConsumerClient.close()</code> can block indefinitely</td>
<td>Pause</td>
<td>Unresolved</td>
</tr>
<tr>
<td><a href="https://issues.apache.org/jira/browse/KAFKA-17582">KAFKA-17582</a></td>
<td>Unpredictable consumer offsets after transaction failure</td>
<td>None</td>
<td>Unresolved</td>
</tr>
<tr>
<td><a href="https://issues.apache.org/jira/browse/KAFKA-17754">KAFKA-17754</a></td>
<td>Write loss, aborted read, torn transactions</td>
<td>Pause</td>
<td>Unresolved</td>
</tr>
</tbody>
</table>

<p>We found two liveness and three safety issues in Bufstream proper. Two issues (#1 and #2) involved consumers or producers getting stuck, sometimes indefinitely. One (#3) involved Bufstream returning <code>0</code> rather than <code>-1</code> for a sent record which failed indefinitely. The official Java client interpreted this response as a success, rather than an error. Two issues allowed the loss of committed writes due to a bug in a concurrency control mechanism (#4) and a bug in a filter to limit response sizes (#5). We identified both #4 and #5 before production users were affected. As of version 0.1.3, all five issues are resolved.</p>
<p>However, Bufstream continues to exhibit aborted reads, lost writes, and torn transactions due to the design of the Kafka transaction protocol (KAFKA-17754). These issues cannot be resolved without the help of the Kafka team and client implementers. We also note three other Kafka issues, including an incorrect error message (KIP-588), a deadlock in <code>ConsumerClient.close()</code> (KAFKA-17734), and the lack of any authoritative documentation for transaction semantics (KAFKA-17671).</p>
<p>As always, we caution that Jepsen takes an experimental approach to safety verification: we can prove the presence of bugs, but not their absence. While we make extensive efforts to find problems, we cannot prove correctness. In particular, KAFKA-17754 makes it difficult to determine if there are <em>other</em> cases of (e.g.) write loss in Bufstream.</p>
<h2 data-number="5.1" id="bufstream-recommendations"> Bufstream Recommendations</h2>
<p>Bufstream users who use the official Java Kafka client should be aware that transactions are, at present, unsafe. Aborted transactions could actually commit, committed transactions could actually abort, and transactions could be torn in half, preserving some but not all of their effects. Bufstream believes the Franz-go client is less susceptible to this problem, but we have not tested it using the same techniques as the present work. Other clients may or may not be susceptible. While Bufstream is trying to reduce the frequency of these issues, they cannot prevent them entirely. That power lies with the Kafka team and client implementers.</p>
<p>Bufstream users prior to 0.1.3 should be aware that calls to <code>producer.send()</code> might incorrectly return a zero offset for a record, rather than the actual offset. They may also encounter metastable availability issues where clients get “stuck.” We recommend upgrading to 0.1.3.</p>
<p>Bufstream’s overall architecture appears sound: relying on a coordination service like etcd to establish the order of immutable chunks of data is a relatively straightforward approach with years of <a href="https://www.infoq.com/articles/Architecture-Datomic/">prior art</a> in both OLTP and streaming systems. Kafka’s attention to KIP-588, additional deployment experience, and further testing should help identify and resolve any remaining safety bugs. In the meantime, we made two small operational recommendations for Bufstream.</p>
<p>First, we suggested Bufstream retry a network operation that often caused clusters to crash. Bufstream requests a shared file in storage on startup, as a safety check designed to prevent users from accidentally running two different Bufstream clusters on top of the same storage bucket. Bufstream exits if it can’t complete this request. This caused Bufstream to crash roughly one in thirty tests, due to <code>404 not found</code> responses from storage. Bufstream has added a layer of retries, and intends to further ameliorate this issue by retrying indefinitely while refusing client connections. Version 0.1.3 appears significantly more robust on startup.</p>
<p>Second, Jepsen suggested that Bufstream processes try to keep running when their dependencies are unavailable. Presently, agents kill themselves when they lose access to storage or the coordinator, and rely on a supervisor system like Kubernetes to restart them. This works, but it requires operators to run their own supervisor. It may also impact performance and availability. Clients must tear down and re-open TCP connections, which could create thundering-herd issues. Bufstream processes may also need to do special work on restarting: fetching resources from storage, warming caches, re-connecting to etcd, and so on. Generally speaking, services should try to keep running when their dependencies become unavailable—the service can offer backpressure, advise clients of system status, and generally recover more gracefully. Bufstream has added additional retry logic for etcd, but as of 0.1.3, still requires constant supervision to stay online. We also recommend users verify that they have a process supervisor in place, and test that it works correctly (rather than giving up) during prolonged outages.</p>
<h2 data-number="5.2" id="kafka-needs-transaction-docs"> Kafka Needs Transaction Docs</h2>
<p>Kafka’s official documentation says almost nothing about transactions, leaving users to piece together behavior from a maze of vague, confusing, and contradictory sources. We encouraged the Kafka team to write an official, centralized, easy-to-find document which lays out the intended semantics of transactions: <a href="https://issues.apache.org/jira/browse/KAFKA-17671">KAFKA-17671</a>. This document should specify exactly what users must do to use transactions safely, and what invariants they can expect in return. For instance, it should describe the rules for offsets visible to a single producer or consumer:</p>
<ul>
<li>When will a consumer observe monotonically increasing offsets?</li>
<li>When will a consumer skip over acknowledged records?</li>
<li>Is this behavior different within a single call to <code>poll</code>, versus between two subsequent calls?</li>
<li>Can a rebalance take effect in the middle of a transaction? How do rebalances affect transaction semantics?</li>
<li>When will records sent by a producer to a single topic-partition have monotonically increasing offsets?</li>
<li>When will those offsets be interleaved with writes from other producers?</li>
<li>For both consumers and producers, how do these behaviors differ within a transaction vs. between two subsequent transactions on the same client?</li>
</ul>
<p>This document should also explain the isolation properties of transactions:</p>
<ul>
<li>When is G0 (write cycle) legal?</li>
<li>When is G1a (aborted read) legal?</li>
<li>When is G1b (intermediate read) legal?</li>
<li>When is G1c (circular information flow) legal?</li>
<li>What particular cycles are prohibited, if any? For example, are cycles composed entirely of write-read edges proscribed?</li>
<li>When is fractured read legal? That is, when can a transaction observe some, but not all, of another transaction’s effects?</li>
<li>Is it legal to read values written inside the current transaction?</li>
</ul>
<p>It should describe the semantics of aborted transactions:</p>
<ul>
<li>How are explicitly aborted transactions different from those which (e.g.) crash before committing?</li>
<li>Are the values and offsets returned by <code>poll()</code> correct even if the transaction aborts?</li>
<li>What offsets should a consumer poll after a transaction crashes?</li>
<li>How should users navigate <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-1050%3A+Consistent+error+handling+for+Transactions">Kafka’s maze of transaction errors</a>?</li>
<li>How should users handle errors that occur during the transaction abort process, or during rewind?</li>
</ul>
<p>… and clarify other ambiguities in the existing documentation:</p>
<ul>
<li>Is the “committed position” the offset of the highest committed record, or the uncommitted offset one higher?</li>
<li>Is it safe to let multiple transactional IDs process records from the same topic-partitions?</li>
</ul>
<p>We also recommend that Confluent align their safety claims with Kafka’s behavior. Confluent <a href="https://docs.confluent.io/kafka/design/delivery-semantics.html">repeatedly</a> <a href="https://docs.confluent.io/platform/current/clients/consumer.html">claims</a> that Kafka offers at-least-once delivery by default. This is untrue: <code>enable.auto.commit = true</code>, and <code>auto.offset.reset = latest</code> allow records to be marked “committed” despite never being processed. Those same docs incorrectly claim that consumers rewind offsets on transaction abort. They do not, and this too could lead to data loss. Either the default behaviors should be changed, or the documentation updated.</p>
<h2 data-number="5.3" id="kafka-transactions-are-broken"> Kafka Transactions are Broken</h2>
<p>The Kafka transaction protocol is fundamentally broken and must be revised. As we showed in KAFKA-17754, anyone who uses the official Java Kafka client with Bufstream, Kafka, or presumably any Kafka-compatible system may observe aborted reads, lost writes, and torn transactions. These break the <a href="https://developer.confluent.io/courses/architecture/transactions/">most basic safety guarantees</a> Kafka transactions are supposed to provide.</p>
<p>The crux of the problem is that Kafka’s transaction system implicitly assumes ordered, reliable delivery where none exists. Processes pause, networks <a href="http://www.bailis.org/papers/partitions-queue2014.pdf">are not reliable</a>, latency <a href="https://www.researchgate.net/publication/322500050_Fallacies_of_Distributed_Computing_Explained">is non-zero</a>, and delivery across different TCP sockets is fundamentally unordered. Kafka’s protocol distributes messages across different nodes and TCP sockets <a href="https://kafka.apache.org/protocol#protocol_network">by design</a>. Clients automatically retry messages, leading to duplicates. The protocol includes neither a sequence number to reconstruct the order of messages sent by a single client,<a href="#fn16" id="fnref16" role="doc-noteref"><sup>16</sup></a> nor a transaction number to ensure messages affect the right transaction.</p>
<p>On top of this unreliable foundation, the <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=235834631#KIP890:TransactionsServerSideDefense-Motivation">Kafka transaction protocol</a> is an ordered state machine. <code>Produce</code> and <code>EndTxn</code> messages add records to, commit, or abort, whatever transaction happens to be ongoing at the time. Producer epochs provide a logical clock, but nothing ensures order <em>within</em> an epoch, and epochs are incremented infrequently. This demonstrates the importance of the <a href="https://en.wikipedia.org/wiki/End-to-end_principle">end-to-end principle</a> in protocol design: the client and transaction state machine must explicitly encode and enforce ordering “at the edges,” rather than relying on the unreliable network between them.</p>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-890%3A+Transactions+Server-Side+Defense">KIP-890</a> intends to ensure a stricter order by incrementing the epoch on every transaction commit. Client libraries could also help by re-initializing producers (which bumps the epoch) when a message is not acknowledged.</p>
<p>We know the <a href="https://docs.confluent.io/kafka-clients/java/current/overview.html">official Java Kafka client</a> is vulnerable to this problem as of version 3.8.0. We believe <a href="https://github.com/twmb/franz-go">Franz-go</a> does re-initialize on timeouts, which could mitigate or prevent these issues. We haven’t investigated other client libraries.</p>
<h2 data-number="5.4" id="future-work"> Future Work</h2>
<p>Many users rely on the Kafka Streams API for “exactly-once semantics,” rather than performing transactions themselves. Future work could explore the correctness of Streams applications.</p>
<p>While investigating issues like KAFKA-17754, we also encountered unseen writes in Kafka. Owing to time constraints we have not investigated this behavior, but unseen writes could be a sign of hanging transactions, stuck consumers, or even data loss. We are curious whether a delayed <code>Produce</code> message could slide into a future transaction, violating transactional guarantees. We also suspect that the Kafka Java Client may reuse a sequence number when a request times out, causing writes to be acknowledged but silently discarded. More Kafka testing is warranted.</p>
<p>When a rebalance event occurs, the positions of consumers may shift forward or back. It is unclear what the rules are for these rebalances. Our test suite detected various cases of consumers or producers encountering internal (or external) non-monotonic (or skipped) sends (or polls). However, we are unsure when these behaviors are legal, and our checker does not report them as outright failures. Once Kafka documents intended behavior, we would like to verify it.</p>
<p>Jepsen is a random process. The generators of operations, thread scheduler, network and disk IO, services under test, and operating systems involved are all non-deterministic. We rely on anomalies being probable enough that they occur regularly across different, random test runs. This is an effective way to identify and resolve bugs that are likely to occur in real-world systems. However, it is a terrible way to explore rare behaviors: if an anomaly occurs once in a hundred hours of testing, debugging and reproducing it becomes arduous.</p>
<p>For example, we encountered a single case of <em>non-zero</em> duplicate offsets on 0.1.3-rc.2. In <a href="https://s3.amazonaws.com/jepsen.io/analyses/bufstream-0.1.0/0.1.3-rc2-dup-nonzero.zip">this two-minute test run</a> with producer pauses, a process sent record <code>743</code> to key <code>9</code>, was assigned offset <code>1225</code>, and was able to poll the record at that offset. Other pollers begged to differ—every other process observed value <code>743</code> at offset <code>1219</code>. We were unable to reproduce this behavior after weeks of testing, and it could have been an error in our test harness. Lacking confidence, we opted not to include this in our findings. A reproducible test would have made discharging this issue significantly easier.</p>
<p>Bufstream also uses <a href="https://antithesis.com/">Antithesis</a>, a testing platform which runs an entire distributed system in a deterministic hypervisor and simulated network. This allows perfectly reproducible tests, and also lets testers rewind time to inspect the state of a system just before and after a bug occurred. We would like to combine Jepsen’s workload generation and history checking with Antithesis’ deterministic and replayable environment to make our tests more reproducible.</p>
<p><em>This work would not have been possible without the help of the Buf team, including Jacob Butcher, Mary Cutrali, Peter Edge, Rubens Farias, Alfred Fuller, Artūras Lapienė, Connor Mahony, David Marby, Chris Pine, Derek Perez, Luke Rewega, Chris Roche, Akshay Shah, Nick Snyder, and Philip Warren. Our thanks also to Artem Livshits and Justine Olshan for their support in investigating Kafka behavior. Our sincere appreciation to Irene Kannyo for her editorial support. This report was funded by Buf Technologies, Inc. and conducted in accordance with the <a href="https://jepsen.io/analyses/ethics">Jepsen ethics policy</a>.</em></p>

  </div>
</article></div>
  </body>
</html>
