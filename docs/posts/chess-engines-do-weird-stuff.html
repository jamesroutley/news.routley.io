<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://girl.surgery/chess">Original</a>
    <h1>Chess engines do weird stuff</h1>
    
    <div id="readability-page-1" class="page"><article>
  
  <p>Things LLM people can learn from</p>

  <h2>Training method</h2>
  <p>
    Since AlphaZero, lc0-style chess engines have been trained with RL. Specifically, you have the engine (search + model) play itself a bunch of times, and train the model to predict the outcome of the game.
  </p>
  <p>
    It turns out this isn&#39;t necessary. Good model vs bad model is ~200 elo, but <a href="https://www.melonimarco.it/mm/wp-content/uploads/2021/03/stockfishnodes.png">search is ~1200 elo</a>, so even a bad model + search is essentially an oracle to a good model without, and you can distill from bad model + search → good model.
  </p>
  <p>
    So RL was necessary in some sense only one time. Once a good model with search was trained, every future engine (including their competitors!)<a href="#fn1" id="fn1-ref">1</a> can distill from that, and doesn&#39;t have to generate games (expensive). lc0 trained their premier model, BT4, with distillation and it got <em>worse</em> when you put it in the RL loop.
  </p>
  <p>
    What makes distillation from search so powerful? People often compare this to distilling from best-of-n in RL, which I think is limited — a chess engine that runs the model on 50 positions is roughly equivalent to a model 30x larger, whereas LLM best-of-50 is generously worth a model 2x larger. Perhaps this was why people wanted test-time search to work so badly when RLVR was right under their noses.
  </p>

  <h2>Training at runtime</h2>
  <p>
    <a href="https://github.com/official-stockfish/Stockfish/pull/4950">A recent technique</a> is applying the distillation trick <em>at runtime</em>. At runtime, you evaluate early positions with your NN, then search them and get a more accurate picture. If your network says the position is +0.15 pawns better than search says, subtract 0.15 pawns from future evaluations. Your network live adapts to the position it&#39;s in!
  </p>

  <h2>Training on winning</h2>
  <p>
    The fundamental training objective of distilling from search is almost but not quite what we actually care about: winning. It&#39;s very correlated, but we don&#39;t actually care about how well the model estimates one position, we care about how well it performs <em>after search</em>, after looking at 100 positions.
  </p>
  <p>
    To fix this, lc0 uses a weird technique called SPSA: you randomly perturb the weights in two directions, play a bunch of games, and go the direction that wins more.<a href="#fn2" id="fn2-ref">2</a> This works very well and can get +50 elo on small models.<a href="#fn3" id="fn3-ref">3</a>
  </p>
  <p>
    Consider for a moment how insane it is that this works at all. You&#39;re modifying the weights in purely random directions. You have no gradient whatsoever. And yet it works quite well! +50 elo is ~1.5x model size or ~a year&#39;s worth of development effort!
  </p>
  <p>
    The main issue with this is that it&#39;s wildly expensive. To do a single step you must play thousands of games with dozens of moves and hundreds of position inferences per move.
  </p>
  <p>
    Like LLMs, you train for a long time on a pseudo-objective that&#39;s close to what you want, then a short time on a very expensive and limited objective that&#39;s closer to what you want.
  </p>

  <h2>Tuning through C++</h2>
  <p>
    The underlying technique of SPSA can be applied to <em>literally any number in your chess program</em>. Modify the number, see if it wins more or loses more, move in the direction that wins more. You have a hand-tuned heuristic that if there&#39;s a checkmate in the search from a position you should back off by <a href="https://github.com/official-stockfish/Stockfish/blob/54cf226604cfc9d17f432fa0b5bca56277e5561c/src/search.cpp#L1173">depth 1</a>? <a href="https://github.com/official-stockfish/Stockfish/commit/cc5c67c564f52a0611ba38d04af02636291280b6">Replace that with thousandths-of-a-depth</a> and then tune it with SPSA — turns out the optimal value is actually to back off by depth <a href="https://github.com/official-stockfish/Stockfish/commit/d9fd516547849bd5ca2a05c491aadc66fc750a39#diff-da923b7afa45cab7add143c4705b54142e46b2afe9a2627d5fa3b3474bdc8aecR108-R1192">1.09</a>, which nets you 5 elo. You can do this <em>for every number in your search algorithm</em>. You can do something that looks a lot like gradient descent <em>through arbitrary C++</em> because you have a grading function (winning).
  </p>

  <h2>Weird architecture</h2>
  <p>
    lc0 uses a standard-ish transformer architecture, which they found to be hundreds of elo better than their old convolution-based models. It&#39;s still confusing to me that the transformer biases apply to literally every application imaginable. The only substantial architectural change they use is &#34;smolgen&#34;, a system for generating attention biases. They claim smolgen is a ~1.2x throughput hit but an accuracy win equivalent to <em>2.5x</em> model size. Why is it so good? I find all the explanations poor.
  </p>

  
</article></div>
  </body>
</html>
