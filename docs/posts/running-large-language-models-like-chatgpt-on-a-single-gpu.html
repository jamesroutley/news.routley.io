<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/Ying1123/FlexGen">Original</a>
    <h1>Running large language models like ChatGPT on a single GPU</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">FlexGen is a high-throughput generation engine for running large language models with limited GPU memory (e.g., a 16GB T4 GPU or a 24GB RTX3090 gaming card!).
This is a research project developed by
<a href="https://hazyresearch.stanford.edu/" rel="nofollow">HazyResearch@Stanford</a>,
<a href="https://sky.cs.berkeley.edu/" rel="nofollow">SkyComputing@UC Berkeley</a>,
<a href="https://ds3lab.inf.ethz.ch/" rel="nofollow">DS3Lab@ETH Zurich</a>,
<a href="https://crfm.stanford.edu/" rel="nofollow">CRFM@Stanford</a>,
and <a href="https://www.together.xyz/" rel="nofollow">TogetherCompute</a>.</p>
<p dir="auto">Large language models (LLMs) are at the heart of applications like ChatGPT and Copilot, but the high computational and memory requirements of LLM inference traditionally make it feasible only with multiple high-end accelerators.
FlexGen aims to lower the resource requirements of LLM inference down to a single commodity GPU (e.g., T4, 3090) and allow flexible deployment for various hardware setups.</p>
<p dir="auto">The key features of FlexGen include:</p>
<p dir="auto"><g-emoji alias="zap" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/26a1.png">âš¡</g-emoji> <strong>Lightining Fast Offloading</strong>.</p>
<p dir="auto"><g-emoji alias="package" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4e6.png">ðŸ“¦</g-emoji> <strong>Extreme Compression</strong>.</p>
<p dir="auto"><g-emoji alias="rocket" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f680.png">ðŸš€</g-emoji> <strong>Scalability</strong>.</p>
<p dir="auto">| <a href="https://github.com/FMInference/FlexGen/blob/main/docs/paper.pdf"><strong>Read Paper</strong></a> | <a href="https://discord.gg/JfphDTkBAh" rel="nofollow"><strong>Join Discord</strong></a> |</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-content" aria-hidden="true" href="#content"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Content</h2>
<ul dir="auto">
<li><a href="#benchmark-results">Benchmark Results</a></li>
<li><a href="#install">Install</a></li>
<li><a href="#get-started-with-a-single-gpu">Get Started with a Single GPU</a></li>
<li><a href="#run-chatbot-with-opt-models-on-a-single-gpu">Run Chatbot with OPT models on a Single GPU</a></li>
<li><a href="#scaling-to-distributed-gpus">Scaling to Distributed GPUs</a></li>
<li><a href="#roadmap">Roadmap</a></li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-benchmark-results" aria-hidden="true" href="#benchmark-results"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Benchmark Results</h2>
<h3 tabindex="-1" dir="auto"><a id="user-content-generation-throughput-tokens" aria-hidden="true" href="#generation-throughput-tokens"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Generation Throughput (token/s)</h3>
<table>
<thead>
<tr>
<th>System</th>
<th>OPT-6.7B</th>
<th>OPT-30B</th>
<th>OPT-175B</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hugging Face Accelerate</td>
<td>25.12</td>
<td>0.62</td>
<td>0.01</td>
</tr>
<tr>
<td>DeepSpeed ZeRO-Inference</td>
<td>9.28</td>
<td>0.60</td>
<td>0.01</td>
</tr>
<tr>
<td>Petals*</td>
<td>-</td>
<td>-</td>
<td>0.05</td>
</tr>
<tr>
<td>FlexGen</td>
<td>25.26</td>
<td>7.32</td>
<td>0.69</td>
</tr>
<tr>
<td>FlexGen with Compression</td>
<td><strong>29.12</strong></td>
<td><strong>8.38</strong></td>
<td><strong>1.12</strong></td>
</tr>
</tbody>
</table>
<ul dir="auto">
<li>Hardware: an NVIDIA T4 (16GB) instance on GCP with 208GB of DRAM and 1.5TB of SSD.</li>
<li>Workload: input sequence length = 512, output sequence length = 32. The batch size is tuned to a value that maximizes the generation throughput for each system.</li>
<li>Metric: generation throughput (token/s) = number of the generated tokens / (time for processing prompts + time for generation).</li>
</ul>
<p dir="auto">How to <a href="https://github.com/FMInference/FlexGen/blob/main/benchmark/flexgen">reproduce</a>.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-latency-throughput-trade-off" aria-hidden="true" href="#latency-throughput-trade-off"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Latency-throughput Trade-off</h3>
<p dir="auto">The figure below shows the latency and throughput trade-off of three offloading-based systems on OPT-175B (left) and OPT-30B (right).
FlexGen achieves a new Pareto-optimal frontier with a 100x higher maximum throughput for OPT-175B.
Other systems cannot further increase throughput due to out-of-memory. &#34;FlexGen(c)&#34; is FlexGen with compression.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/FMInference/FlexGen/blob/main/docs/throughput_vs_latency.jpg"><img src="https://github.com/FMInference/FlexGen/raw/main/docs/throughput_vs_latency.jpg" alt="logo" width="500"/></a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-how-it-works" aria-hidden="true" href="#how-it-works"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>How It Works</h2>
<p dir="auto">FlexGen can be flexibly configured under various hardware resource constraints by aggregating memory and computation from the GPU, CPU, and disk. Through a linear programming optimizer, it searches for the best pattern to store and access the tensors, including weights, activations, and attention key/value (KV) cache. FlexGen further compresses both weights and KV cache to 4 bits with negligible accuracy loss.</p>
<p dir="auto">One key idea of FlexGen is to play the latency-throughput trade-off. Achieving low latency is inherently challenging for offloading methods,
but the efficiency of offloading can be greatly boosted for throughput-oriented scenarios (see the figure above).
FlexGen utilizes a block schedule to reuse weight and overlap I/O with computation, as shown in figure (b) below, while other baseline systems use an ineffiicent row-by-row schedule, as shown in figure (a) below.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/FMInference/FlexGen/raw/main/docs/block_schedule.jpg"><img src="https://github.com/FMInference/FlexGen/raw/main/docs/block_schedule.jpg" alt="logo" width="500"/></a></p>
<p dir="auto">More details can be found in <a href="https://github.com/FMInference/FlexGen/blob/main/docs/paper.pdf">our paper</a>.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-install" aria-hidden="true" href="#install"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Install</h2>
<p dir="auto">Requirements:</p>
<ul dir="auto">
<li>PyTorch &gt;= 1.12 <a href="https://pytorch.org/get-started/locally/" rel="nofollow">(Help)</a></li>
</ul>
<p dir="auto">Instructions:</p>
<div data-snippet-clipboard-copy-content="git clone https://github.com/FMInference/FlexGen.git
cd FlexGen
pip3 install -e .

# (Optional) Install openmpi for multi-gpu execution
# sudo apt install openmpi-bin"><pre><code>git clone https://github.com/FMInference/FlexGen.git
cd FlexGen
pip3 install -e .

# (Optional) Install openmpi for multi-gpu execution
# sudo apt install openmpi-bin
</code></pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-get-started-with-a-single-gpu" aria-hidden="true" href="#get-started-with-a-single-gpu"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Get Started with a Single GPU</h2>
<h3 tabindex="-1" dir="auto"><a id="user-content-opt-13b" aria-hidden="true" href="#opt-13b"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>OPT-1.3B</h3>
<p dir="auto">To get started, you can try a small model like OPT-1.3B first. It fits into a single GPU so no offloading is required.
FlexGen will automatically download weights from Hugging Face.</p>
<div data-snippet-clipboard-copy-content="python3 -m flexgen.flex_opt --model facebook/opt-1.3b"><pre><code>python3 -m flexgen.flex_opt --model facebook/opt-1.3b
</code></pre></div>
<p dir="auto">You should see some text generated by OPT-1.3B and the benchmark results.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-opt-30b" aria-hidden="true" href="#opt-30b"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>OPT-30B</h3>
<p dir="auto">To run large models like OPT-30B, you will need to use CPU offloading. You can try commands below.
The <code>--percent</code> argument specifies the offloading strategy for parameters, attention cache and hidden states separately.
The exact meaning of this argument can be found <a href="https://github.com/FMInference/FlexGen/blob/9d092d848f106cd9eaf305c12ef3590f7bcb0277/flexgen/flex_opt.py#L1271-L1279">here</a>.</p>
<div data-snippet-clipboard-copy-content="python3 -m flexgen.flex_opt --model facebook/opt-30b --percent 0 100 100 0 100 0"><pre><code>python3 -m flexgen.flex_opt --model facebook/opt-30b --percent 0 100 100 0 100 0
</code></pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-opt-175b" aria-hidden="true" href="#opt-175b"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>OPT-175B</h3>
<p dir="auto">To run OPT-175B, you need to download the weights from <a href="https://github.com/facebookresearch/metaseq/tree/main/projects/OPT">metaseq</a> and convert the weights into Alpa <a href="https://alpa.ai/tutorials/opt_serving.html#convert-opt-175b-weights-into-alpa-formats" rel="nofollow">format</a>.
You can then try to offloaind all wieghts to disk by</p>
<div data-snippet-clipboard-copy-content="python3 -m flexgen.flex_opt --model facebook/opt-175b --percent 0 0 100 0 100 0 --offload-dir YOUR_SSD_FOLDER"><pre><code>python3 -m flexgen.flex_opt --model facebook/opt-175b --percent 0 0 100 0 100 0 --offload-dir YOUR_SSD_FOLDER
</code></pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-how-to-set-the-offloading-strategy-and---percent" aria-hidden="true" href="#how-to-set-the-offloading-strategy-and---percent"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>How to set the offloading strategy and <code>--percent</code>?</h3>
<p dir="auto">We will release an automatic policy optimizer later, but now you have to manually try a few strategies.
The idea of high-throughput generation is to offload parameters and attention cache as much as possible to the CPU and disk if necessary.
You can see the reference startegies in our benchmark <a href="https://github.com/FMInference/FlexGen/blob/9d092d848f106cd9eaf305c12ef3590f7bcb0277/benchmark/flexgen/bench_suite.py#L39-L79">here</a>.
To avoid out-of-memory, you can tune the <code>--percent</code> of offload more tensors to the CPU and disk.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-scaling-to-distributed-gpus" aria-hidden="true" href="#scaling-to-distributed-gpus"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Scaling to Distributed GPUs</h2>
<p dir="auto">If you have more GPUs, FlexGen can combine offloading with pipeline parallelism to allow scaling.
For example, if you have 2 GPUs but the aggregated GPU memory is less than the model size, you still need offloading. FlexGen allow you to do pipeline parallelism with these 2 GPUs to accelerate the generation.
See examples <a href="https://github.com/FMInference/FlexGen/tree/main/benchmark/flexgen#distributed-gpus">here</a>.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-run-chatbot-with-opt-models-on-a-single-gpu" aria-hidden="true" href="#run-chatbot-with-opt-models-on-a-single-gpu"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Run Chatbot with OPT Models on a Single GPU</h2>
<p dir="auto"><a href="https://github.com/FMInference/FlexGen/blob/main/apps/chatbot.py">apps/chatbot.py</a> shows how to build a chatbot with FlexGen and OPT models.
While FlexGen is mainly optimized for large-batch throughput-oriented scenarios like dataset evaluations and information extraction,
FlexGen can also be used for interactive applications like chatbot with better performance than other offloading-based systems.
Note that FlexGen cannot achieve its best throughput in this single-batch case.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-default-commands" aria-hidden="true" href="#default-commands"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Default Commands</h3>
<p dir="auto">You can use the default commands below.
If you do not have enough GPU/CPU memory, see the <a href="#handle-out-of-memory">Handle Out-of-memory</a> section.</p>
<div data-snippet-clipboard-copy-content="# Chat with OPT-6.7B. You need at least 15GB of GPU memory.
python3 chatbot.py --model facebook/opt-6.7b"><pre><code># Chat with OPT-6.7B. You need at least 15GB of GPU memory.
python3 chatbot.py --model facebook/opt-6.7b
</code></pre></div>
<div data-snippet-clipboard-copy-content="# Chat with OPT-30B. You need at least 64GB of CPU memory.
python3 chatbot.py --model facebook/opt-30b --percent 0 100 100 0 100 0"><pre><code># Chat with OPT-30B. You need at least 64GB of CPU memory.
python3 chatbot.py --model facebook/opt-30b --percent 0 100 100 0 100 0
</code></pre></div>
<div data-snippet-clipboard-copy-content="# Chat with instruction-tuned OPT-IML-MAX-30B. You need at least 64GB of CPU memory.
python3 chatbot.py --model facebook/opt-iml-max-30b --percent 0 100 100 0 100 0"><pre><code># Chat with instruction-tuned OPT-IML-MAX-30B. You need at least 64GB of CPU memory.
python3 chatbot.py --model facebook/opt-iml-max-30b --percent 0 100 100 0 100 0
</code></pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-example-output" aria-hidden="true" href="#example-output"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Example Output</h3>
<div data-snippet-clipboard-copy-content="A chat between a curious human and a knowledgeable artificial intelligence assistant.
Human: Hello! What can you do?
Assistant: As an AI assistant, I can answer questions and chat with you.
Human: What is the name of the tallest mountain in the world?
Assistant: Everest.
Human: I am planning a trip for our anniversary. What things can we do?
Assistant: Well, there are a number of things you can do for your anniversary. First, you can play cards. Second, you can go for a hike. Third, you can go to a museum."><pre><code>A chat between a curious human and a knowledgeable artificial intelligence assistant.
Human: Hello! What can you do?
Assistant: As an AI assistant, I can answer questions and chat with you.
Human: What is the name of the tallest mountain in the world?
Assistant: Everest.
Human: I am planning a trip for our anniversary. What things can we do?
Assistant: Well, there are a number of things you can do for your anniversary. First, you can play cards. Second, you can go for a hike. Third, you can go to a museum.
</code></pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-handle-out-of-memory" aria-hidden="true" href="#handle-out-of-memory"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Handle Out-of-memory</h3>
<p dir="auto">If you do not have enough GPU/CPU memory, here are a few things you can try.
They save more memory but run slower.</p>
<ul dir="auto">
<li>Enable weight compression by adding <code>--compress-weight</code>.</li>
<li>Offload weights to disk by using <code>--percent 0 0 100 0 100 0</code>.</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-roadmap" aria-hidden="true" href="#roadmap"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Roadmap</h2>
<p dir="auto">We plan to work on the following features. Community contributions are welcome.</p>
<ul>
<li> Support Apple silicon M1/M2 deployment</li>
<li> Support Colab deployement</li>
<li> Optimize the latency of the chatbot application</li>
<li> Add a text summarization application</li>
<li> Support more models (BLOOM, CodeGen, GLM)</li>
<li> Release the cost model and policy optimizer</li>
<li> Release a pip installable package</li>
</ul>
</article>
          </div></div>
  </body>
</html>
